{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env =  UE(file_name=\"280623_1\\\\build\",seed=1,side_channels=[],worker_id=0,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "# for i in range(len(behavior_names)):\n",
    "#     print(behavior_names[i])\n",
    "#     print(\"obs:\",behavior_value[i].observation_specs, \"   act:\", behavior_value[0].action_spec)\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agentsNum = len(DecisionSteps.agent_id)\n",
    "# print(\"exist:\",DecisionSteps.agent_id,\"   Dead:\",TerminalSteps.agent_id)\n",
    "# print(\"reward:\",DecisionSteps.reward,\"reward_dead:\",TerminalSteps.reward)\n",
    "# print(\"obs:\",DecisionSteps.obs,\"DeadObs:\",TerminalSteps.obs)\n",
    "# print(\"interrupted:\", TerminalSteps.interrupted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#(3,128,128) --> (64,7,7)\n",
    "image = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of image embedding is torch.Size([3136])\n",
      "The shape of language embedding is torch.Size([128])\n",
      "The shape of mix embedding is torch.Size([256])\n",
      "The shape of lstm hidden state is torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "visual_model = VisualModule()\n",
    "vt = image\n",
    "image_emb = visual_model(vt)\n",
    "print(f'The shape of image embedding is {image_emb.size()}')\n",
    "\n",
    "index = 5\n",
    "language_model = LanguageModule(num_words,embedding_dim)\n",
    "lt = torch.eye(num_words)[:, index]\n",
    "language_emb = language_model(lt)\n",
    "print(f'The shape of language embedding is {language_emb.size()}')\n",
    "\n",
    "mixing_model = MixingModule(vision_output_dim,language_output_dim,mixing_dim)\n",
    "mix_emb = mixing_model(image_emb,language_emb)\n",
    "print(f'The shape of mix embedding is {mix_emb.size()}')\n",
    "\n",
    "lstm = LSTMModule(mixing_dim,lstm_hidden_dim)\n",
    "lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim), torch.zeros(1, lstm_hidden_dim))\n",
    "hidden_state = lstm(mix_emb.unsqueeze(0),lstm_hidden_state)\n",
    "print(f'The shape of lstm hidden state is {hidden_state[0].size()}')\n",
    "\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "action, value, lstm_hidden_state= agent(vt,lt,lstm_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1/10000  | Episode Reward: 8  | Actor loss: 9.44 | Critic loss: 25.67 | Entropy loss: -0.0002  | Total Loss: 35.11 | Total Steps: 112\n",
      "Model has been saved\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 2/10000  | Episode Reward: -68  | Actor loss: -4.47 | Critic loss: 8.27 | Entropy loss: -0.0014  | Total Loss: 3.79 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 3/10000  | Episode Reward: 3  | Actor loss: 2.85 | Critic loss: 5.50 | Entropy loss: -0.0011  | Total Loss: 8.35 | Total Steps: 278\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 4/10000  | Episode Reward: -4  | Actor loss: 7.96 | Critic loss: 17.95 | Entropy loss: -0.0003  | Total Loss: 25.91 | Total Steps: 221\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 5/10000  | Episode Reward: 5  | Actor loss: 0.29 | Critic loss: 3.15 | Entropy loss: -0.0012  | Total Loss: 3.43 | Total Steps: 84\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 6/10000  | Episode Reward: -7  | Actor loss: 6.75 | Critic loss: 13.66 | Entropy loss: -0.0004  | Total Loss: 20.41 | Total Steps: 425\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 7/10000  | Episode Reward: -28  | Actor loss: 0.31 | Critic loss: 5.79 | Entropy loss: -0.0008  | Total Loss: 6.10 | Total Steps: 359\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 8/10000  | Episode Reward: -35  | Actor loss: -4.82 | Critic loss: 8.68 | Entropy loss: -0.0014  | Total Loss: 3.86 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 9/10000  | Episode Reward: 5  | Actor loss: 2.48 | Critic loss: 8.54 | Entropy loss: -0.0007  | Total Loss: 11.02 | Total Steps: 47\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 10/10000  | Episode Reward: -39  | Actor loss: -7.15 | Critic loss: 16.78 | Entropy loss: -0.0014  | Total Loss: 9.63 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 11/10000  | Episode Reward: 5  | Actor loss: 10.46 | Critic loss: 28.65 | Entropy loss: -0.0001  | Total Loss: 39.11 | Total Steps: 109\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 12/10000  | Episode Reward: 8  | Actor loss: 2.58 | Critic loss: 6.08 | Entropy loss: -0.0008  | Total Loss: 8.67 | Total Steps: 57\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 13/10000  | Episode Reward: 1  | Actor loss: 2.04 | Critic loss: 7.24 | Entropy loss: -0.0007  | Total Loss: 9.28 | Total Steps: 149\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 14/10000  | Episode Reward: 7  | Actor loss: 0.82 | Critic loss: 4.65 | Entropy loss: -0.0010  | Total Loss: 5.47 | Total Steps: 72\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 15/10000  | Episode Reward: -48  | Actor loss: -7.46 | Critic loss: 15.81 | Entropy loss: -0.0014  | Total Loss: 8.34 | Total Steps: 500\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 16/10000  | Episode Reward: 6  | Actor loss: 3.83 | Critic loss: 7.55 | Entropy loss: -0.0006  | Total Loss: 11.38 | Total Steps: 445\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 17/10000  | Episode Reward: 10  | Actor loss: 3.47 | Critic loss: 6.66 | Entropy loss: -0.0006  | Total Loss: 10.13 | Total Steps: 48\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 18/10000  | Episode Reward: -35  | Actor loss: -6.01 | Critic loss: 11.98 | Entropy loss: -0.0014  | Total Loss: 5.96 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 19/10000  | Episode Reward: -5  | Actor loss: 1.66 | Critic loss: 4.55 | Entropy loss: -0.0008  | Total Loss: 6.21 | Total Steps: 360\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 20/10000  | Episode Reward: -40  | Actor loss: -5.07 | Critic loss: 9.90 | Entropy loss: -0.0014  | Total Loss: 4.83 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 21/10000  | Episode Reward: 7  | Actor loss: 1.52 | Critic loss: 3.99 | Entropy loss: -0.0012  | Total Loss: 5.51 | Total Steps: 193\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 22/10000  | Episode Reward: -4  | Actor loss: 1.99 | Critic loss: 5.72 | Entropy loss: -0.0008  | Total Loss: 7.70 | Total Steps: 457\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 23/10000  | Episode Reward: 5  | Actor loss: 7.83 | Critic loss: 18.35 | Entropy loss: -0.0002  | Total Loss: 26.18 | Total Steps: 416\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 24/10000  | Episode Reward: 10  | Actor loss: 4.44 | Critic loss: 7.92 | Entropy loss: -0.0005  | Total Loss: 12.36 | Total Steps: 35\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 25/10000  | Episode Reward: -23  | Actor loss: -5.36 | Critic loss: 11.94 | Entropy loss: -0.0013  | Total Loss: 6.58 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 26/10000  | Episode Reward: -40  | Actor loss: -5.39 | Critic loss: 10.07 | Entropy loss: -0.0014  | Total Loss: 4.68 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 27/10000  | Episode Reward: -30  | Actor loss: -3.80 | Critic loss: 7.50 | Entropy loss: -0.0013  | Total Loss: 3.69 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 28/10000  | Episode Reward: -27  | Actor loss: -4.30 | Critic loss: 7.84 | Entropy loss: -0.0014  | Total Loss: 3.54 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 29/10000  | Episode Reward: 7  | Actor loss: 6.53 | Critic loss: 13.87 | Entropy loss: -0.0003  | Total Loss: 20.39 | Total Steps: 219\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 30/10000  | Episode Reward: -28  | Actor loss: -5.17 | Critic loss: 10.52 | Entropy loss: -0.0014  | Total Loss: 5.35 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 31/10000  | Episode Reward: -42  | Actor loss: -5.23 | Critic loss: 9.57 | Entropy loss: -0.0014  | Total Loss: 4.34 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 32/10000  | Episode Reward: -49  | Actor loss: -4.94 | Critic loss: 9.38 | Entropy loss: -0.0014  | Total Loss: 4.44 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 33/10000  | Episode Reward: -47  | Actor loss: -6.43 | Critic loss: 15.08 | Entropy loss: -0.0014  | Total Loss: 8.65 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 34/10000  | Episode Reward: -12  | Actor loss: 5.14 | Critic loss: 12.40 | Entropy loss: -0.0003  | Total Loss: 17.53 | Total Steps: 219\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 35/10000  | Episode Reward: 0  | Actor loss: 1.13 | Critic loss: 5.02 | Entropy loss: -0.0014  | Total Loss: 6.15 | Total Steps: 198\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 36/10000  | Episode Reward: -45  | Actor loss: -3.77 | Critic loss: 6.96 | Entropy loss: -0.0014  | Total Loss: 3.19 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 37/10000  | Episode Reward: 4  | Actor loss: 5.03 | Critic loss: 11.48 | Entropy loss: -0.0004  | Total Loss: 16.52 | Total Steps: 130\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 38/10000  | Episode Reward: 5  | Actor loss: 1.91 | Critic loss: 4.10 | Entropy loss: -0.0014  | Total Loss: 6.01 | Total Steps: 198\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 39/10000  | Episode Reward: 6  | Actor loss: 3.55 | Critic loss: 8.61 | Entropy loss: -0.0006  | Total Loss: 12.16 | Total Steps: 41\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 40/10000  | Episode Reward: -4  | Actor loss: 3.68 | Critic loss: 8.76 | Entropy loss: -0.0006  | Total Loss: 12.44 | Total Steps: 241\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 41/10000  | Episode Reward: -22  | Actor loss: -4.17 | Critic loss: 7.92 | Entropy loss: -0.0014  | Total Loss: 3.75 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 42/10000  | Episode Reward: -28  | Actor loss: -5.66 | Critic loss: 12.21 | Entropy loss: -0.0014  | Total Loss: 6.55 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 43/10000  | Episode Reward: 8  | Actor loss: 5.38 | Critic loss: 10.34 | Entropy loss: -0.0005  | Total Loss: 15.72 | Total Steps: 136\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 44/10000  | Episode Reward: -6  | Actor loss: 4.82 | Critic loss: 9.55 | Entropy loss: -0.0004  | Total Loss: 14.37 | Total Steps: 328\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 45/10000  | Episode Reward: -1  | Actor loss: 12.13 | Critic loss: 36.60 | Entropy loss: -0.0000  | Total Loss: 48.73 | Total Steps: 202\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 46/10000  | Episode Reward: -56  | Actor loss: -4.76 | Critic loss: 9.73 | Entropy loss: -0.0014  | Total Loss: 4.97 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 47/10000  | Episode Reward: -25  | Actor loss: 1.25 | Critic loss: 3.51 | Entropy loss: -0.0012  | Total Loss: 4.76 | Total Steps: 385\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 48/10000  | Episode Reward: -32  | Actor loss: -4.26 | Critic loss: 8.40 | Entropy loss: -0.0014  | Total Loss: 4.13 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 49/10000  | Episode Reward: -7  | Actor loss: 3.78 | Critic loss: 7.67 | Entropy loss: -0.0006  | Total Loss: 11.45 | Total Steps: 146\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 50/10000  | Episode Reward: -11  | Actor loss: 1.87 | Critic loss: 4.55 | Entropy loss: -0.0010  | Total Loss: 6.42 | Total Steps: 376\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 51/10000  | Episode Reward: -17  | Actor loss: 5.71 | Critic loss: 11.65 | Entropy loss: -0.0004  | Total Loss: 17.37 | Total Steps: 331\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 52/10000  | Episode Reward: -32  | Actor loss: -0.02 | Critic loss: 7.01 | Entropy loss: -0.0008  | Total Loss: 6.99 | Total Steps: 458\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 53/10000  | Episode Reward: 7  | Actor loss: 2.35 | Critic loss: 6.32 | Entropy loss: -0.0008  | Total Loss: 8.66 | Total Steps: 60\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 54/10000  | Episode Reward: -40  | Actor loss: -6.26 | Critic loss: 11.83 | Entropy loss: -0.0014  | Total Loss: 5.57 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 55/10000  | Episode Reward: -4  | Actor loss: 2.85 | Critic loss: 6.51 | Entropy loss: -0.0008  | Total Loss: 9.36 | Total Steps: 157\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 56/10000  | Episode Reward: 2  | Actor loss: 1.00 | Critic loss: 4.54 | Entropy loss: -0.0012  | Total Loss: 5.54 | Total Steps: 284\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 57/10000  | Episode Reward: -5  | Actor loss: 1.85 | Critic loss: 4.45 | Entropy loss: -0.0008  | Total Loss: 6.29 | Total Steps: 259\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 58/10000  | Episode Reward: -34  | Actor loss: -3.87 | Critic loss: 7.75 | Entropy loss: -0.0013  | Total Loss: 3.88 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 59/10000  | Episode Reward: 3  | Actor loss: 0.86 | Critic loss: 6.86 | Entropy loss: -0.0010  | Total Loss: 7.73 | Total Steps: 70\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Step: 100\n",
      "Training  | Episode: 60/10000  | Episode Reward: 8  | Actor loss: 1.19 | Critic loss: 3.72 | Entropy loss: -0.0014  | Total Loss: 4.91 | Total Steps: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 61/10000  | Episode Reward: -29  | Actor loss: -5.78 | Critic loss: 12.03 | Entropy loss: -0.0014  | Total Loss: 6.25 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 62/10000  | Episode Reward: -46  | Actor loss: -4.97 | Critic loss: 8.93 | Entropy loss: -0.0014  | Total Loss: 3.96 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 63/10000  | Episode Reward: -46  | Actor loss: -5.25 | Critic loss: 9.51 | Entropy loss: -0.0014  | Total Loss: 4.25 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 64/10000  | Episode Reward: -39  | Actor loss: -8.35 | Critic loss: 19.45 | Entropy loss: -0.0014  | Total Loss: 11.10 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 65/10000  | Episode Reward: 2  | Actor loss: 1.27 | Critic loss: 5.08 | Entropy loss: -0.0013  | Total Loss: 6.36 | Total Steps: 297\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 66/10000  | Episode Reward: 7  | Actor loss: 2.46 | Critic loss: 5.91 | Entropy loss: -0.0010  | Total Loss: 8.37 | Total Steps: 170\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 67/10000  | Episode Reward: 4  | Actor loss: 8.68 | Critic loss: 22.36 | Entropy loss: -0.0002  | Total Loss: 31.04 | Total Steps: 114\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 68/10000  | Episode Reward: -14  | Actor loss: 1.37 | Critic loss: 4.18 | Entropy loss: -0.0010  | Total Loss: 5.55 | Total Steps: 474\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 69/10000  | Episode Reward: 9  | Actor loss: 1.47 | Critic loss: 3.83 | Entropy loss: -0.0013  | Total Loss: 5.30 | Total Steps: 93\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 70/10000  | Episode Reward: 5  | Actor loss: 7.92 | Critic loss: 17.23 | Entropy loss: -0.0003  | Total Loss: 25.16 | Total Steps: 218\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 71/10000  | Episode Reward: 3  | Actor loss: 0.51 | Critic loss: 6.68 | Entropy loss: -0.0009  | Total Loss: 7.19 | Total Steps: 65\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 72/10000  | Episode Reward: 2  | Actor loss: 5.43 | Critic loss: 11.57 | Entropy loss: -0.0003  | Total Loss: 17.00 | Total Steps: 125\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 73/10000  | Episode Reward: -34  | Actor loss: -7.25 | Critic loss: 15.59 | Entropy loss: -0.0014  | Total Loss: 8.34 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 74/10000  | Episode Reward: 7  | Actor loss: 1.20 | Critic loss: 3.76 | Entropy loss: -0.0011  | Total Loss: 4.96 | Total Steps: 187\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 75/10000  | Episode Reward: -46  | Actor loss: -6.08 | Critic loss: 13.28 | Entropy loss: -0.0014  | Total Loss: 7.20 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 76/10000  | Episode Reward: -1  | Actor loss: 3.90 | Critic loss: 8.53 | Entropy loss: -0.0005  | Total Loss: 12.43 | Total Steps: 340\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 77/10000  | Episode Reward: -40  | Actor loss: -5.61 | Critic loss: 10.54 | Entropy loss: -0.0014  | Total Loss: 4.93 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 78/10000  | Episode Reward: -6  | Actor loss: 6.21 | Critic loss: 12.75 | Entropy loss: -0.0004  | Total Loss: 18.96 | Total Steps: 327\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 79/10000  | Episode Reward: -29  | Actor loss: -4.34 | Critic loss: 8.13 | Entropy loss: -0.0013  | Total Loss: 3.79 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 80/10000  | Episode Reward: -8  | Actor loss: 8.89 | Critic loss: 21.00 | Entropy loss: -0.0002  | Total Loss: 29.89 | Total Steps: 314\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 81/10000  | Episode Reward: -11  | Actor loss: 0.92 | Critic loss: 7.02 | Entropy loss: -0.0009  | Total Loss: 7.94 | Total Steps: 364\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 82/10000  | Episode Reward: -26  | Actor loss: -4.15 | Critic loss: 8.15 | Entropy loss: -0.0013  | Total Loss: 4.00 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 83/10000  | Episode Reward: -33  | Actor loss: -5.45 | Critic loss: 10.23 | Entropy loss: -0.0014  | Total Loss: 4.78 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 84/10000  | Episode Reward: -30  | Actor loss: -4.48 | Critic loss: 7.94 | Entropy loss: -0.0014  | Total Loss: 3.47 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 85/10000  | Episode Reward: 6  | Actor loss: 10.96 | Critic loss: 28.74 | Entropy loss: -0.0001  | Total Loss: 39.71 | Total Steps: 108\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 86/10000  | Episode Reward: -42  | Actor loss: -6.86 | Critic loss: 13.80 | Entropy loss: -0.0014  | Total Loss: 6.94 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 87/10000  | Episode Reward: 7  | Actor loss: 7.68 | Critic loss: 16.11 | Entropy loss: -0.0003  | Total Loss: 23.79 | Total Steps: 221\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 88/10000  | Episode Reward: -22  | Actor loss: -4.87 | Critic loss: 9.49 | Entropy loss: -0.0014  | Total Loss: 4.62 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 89/10000  | Episode Reward: 7  | Actor loss: 2.16 | Critic loss: 6.75 | Entropy loss: -0.0008  | Total Loss: 8.92 | Total Steps: 54\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 90/10000  | Episode Reward: 2  | Actor loss: 8.02 | Critic loss: 18.30 | Entropy loss: -0.0002  | Total Loss: 26.32 | Total Steps: 416\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 91/10000  | Episode Reward: -14  | Actor loss: 2.54 | Critic loss: 5.88 | Entropy loss: -0.0008  | Total Loss: 8.42 | Total Steps: 456\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 92/10000  | Episode Reward: -32  | Actor loss: -5.32 | Critic loss: 10.64 | Entropy loss: -0.0014  | Total Loss: 5.32 | Total Steps: 500\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 93/10000  | Episode Reward: -20  | Actor loss: -4.41 | Critic loss: 8.47 | Entropy loss: -0.0014  | Total Loss: 4.07 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 94/10000  | Episode Reward: -21  | Actor loss: -4.55 | Critic loss: 8.12 | Entropy loss: -0.0014  | Total Loss: 3.57 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 95/10000  | Episode Reward: -41  | Actor loss: -6.00 | Critic loss: 12.82 | Entropy loss: -0.0014  | Total Loss: 6.81 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 96/10000  | Episode Reward: 3  | Actor loss: 11.94 | Critic loss: 38.03 | Entropy loss: -0.0001  | Total Loss: 49.97 | Total Steps: 103\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 97/10000  | Episode Reward: 4  | Actor loss: 5.18 | Critic loss: 9.57 | Entropy loss: -0.0005  | Total Loss: 14.75 | Total Steps: 135\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 98/10000  | Episode Reward: 4  | Actor loss: 4.79 | Critic loss: 9.50 | Entropy loss: -0.0005  | Total Loss: 14.29 | Total Steps: 234\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 99/10000  | Episode Reward: -38  | Actor loss: -6.55 | Critic loss: 14.55 | Entropy loss: -0.0013  | Total Loss: 7.99 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 100/10000  | Episode Reward: 7  | Actor loss: 2.03 | Critic loss: 6.10 | Entropy loss: -0.0008  | Total Loss: 8.12 | Total Steps: 60\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 101/10000  | Episode Reward: -34  | Average Reward -14.81  | Actor loss: -5.69 | Critic loss: 11.46 | Entropy loss: -0.0014  | Total Loss: 5.77 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 102/10000  | Episode Reward: -40  | Average Reward -14.53  | Actor loss: -4.03 | Critic loss: 7.59 | Entropy loss: -0.0014  | Total Loss: 3.57 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 103/10000  | Episode Reward: -14  | Average Reward -14.70  | Actor loss: -4.10 | Critic loss: 7.53 | Entropy loss: -0.0014  | Total Loss: 3.43 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 104/10000  | Episode Reward: -35  | Average Reward -15.01  | Actor loss: -6.42 | Critic loss: 12.43 | Entropy loss: -0.0014  | Total Loss: 6.01 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 105/10000  | Episode Reward: -13  | Average Reward -15.19  | Actor loss: 4.63 | Critic loss: 8.83 | Entropy loss: -0.0006  | Total Loss: 13.46 | Total Steps: 343\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 106/10000  | Episode Reward: -34  | Average Reward -15.46  | Actor loss: -3.99 | Critic loss: 7.31 | Entropy loss: -0.0014  | Total Loss: 3.33 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 107/10000  | Episode Reward: -53  | Average Reward -15.71  | Actor loss: -6.22 | Critic loss: 12.26 | Entropy loss: -0.0014  | Total Loss: 6.04 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 108/10000  | Episode Reward: -9  | Average Reward -15.45  | Actor loss: 7.58 | Critic loss: 17.96 | Entropy loss: -0.0003  | Total Loss: 25.54 | Total Steps: 321\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 109/10000  | Episode Reward: 9  | Average Reward -15.41  | Actor loss: 1.56 | Critic loss: 3.90 | Entropy loss: -0.0013  | Total Loss: 5.47 | Total Steps: 198\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 110/10000  | Episode Reward: 10  | Average Reward -14.92  | Actor loss: 5.16 | Critic loss: 9.24 | Entropy loss: -0.0006  | Total Loss: 14.39 | Total Steps: 39\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 111/10000  | Episode Reward: -29  | Average Reward -15.26  | Actor loss: -5.32 | Critic loss: 10.27 | Entropy loss: -0.0013  | Total Loss: 4.94 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 112/10000  | Episode Reward: -17  | Average Reward -15.51  | Actor loss: 0.40 | Critic loss: 5.41 | Entropy loss: -0.0010  | Total Loss: 5.81 | Total Steps: 473\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 113/10000  | Episode Reward: 10  | Average Reward -15.42  | Actor loss: 5.40 | Critic loss: 10.20 | Entropy loss: -0.0005  | Total Loss: 15.60 | Total Steps: 35\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 114/10000  | Episode Reward: -18  | Average Reward -15.67  | Actor loss: -4.52 | Critic loss: 8.57 | Entropy loss: -0.0014  | Total Loss: 4.05 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 115/10000  | Episode Reward: -36  | Average Reward -15.55  | Actor loss: -4.34 | Critic loss: 8.13 | Entropy loss: -0.0014  | Total Loss: 3.80 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 116/10000  | Episode Reward: -21  | Average Reward -15.82  | Actor loss: -5.03 | Critic loss: 10.36 | Entropy loss: -0.0014  | Total Loss: 5.33 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 117/10000  | Episode Reward: -3  | Average Reward -15.95  | Actor loss: 9.32 | Critic loss: 24.13 | Entropy loss: -0.0002  | Total Loss: 33.45 | Total Steps: 112\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 118/10000  | Episode Reward: -32  | Average Reward -15.92  | Actor loss: -4.21 | Critic loss: 8.05 | Entropy loss: -0.0013  | Total Loss: 3.85 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 119/10000  | Episode Reward: -30  | Average Reward -16.17  | Actor loss: -5.18 | Critic loss: 11.36 | Entropy loss: -0.0014  | Total Loss: 6.17 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 120/10000  | Episode Reward: -16  | Average Reward -15.93  | Actor loss: -0.57 | Critic loss: 5.11 | Entropy loss: -0.0013  | Total Loss: 4.53 | Total Steps: 496\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 121/10000  | Episode Reward: -14  | Average Reward -16.14  | Actor loss: 0.82 | Critic loss: 3.99 | Entropy loss: -0.0013  | Total Loss: 4.80 | Total Steps: 394\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 122/10000  | Episode Reward: 2  | Average Reward -16.08  | Actor loss: 1.27 | Critic loss: 4.12 | Entropy loss: -0.0013  | Total Loss: 5.39 | Total Steps: 297\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 123/10000  | Episode Reward: -37  | Average Reward -16.50  | Actor loss: -5.07 | Critic loss: 9.74 | Entropy loss: -0.0014  | Total Loss: 4.66 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 124/10000  | Episode Reward: -33  | Average Reward -16.93  | Actor loss: -5.08 | Critic loss: 9.52 | Entropy loss: -0.0014  | Total Loss: 4.45 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 125/10000  | Episode Reward: -5  | Average Reward -16.75  | Actor loss: 6.24 | Critic loss: 12.94 | Entropy loss: -0.0004  | Total Loss: 19.18 | Total Steps: 330\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 126/10000  | Episode Reward: -40  | Average Reward -16.75  | Actor loss: -7.46 | Critic loss: 16.91 | Entropy loss: -0.0014  | Total Loss: 9.45 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 127/10000  | Episode Reward: -30  | Average Reward -16.75  | Actor loss: -4.69 | Critic loss: 8.29 | Entropy loss: -0.0014  | Total Loss: 3.60 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 128/10000  | Episode Reward: -40  | Average Reward -16.88  | Actor loss: -6.25 | Critic loss: 12.20 | Entropy loss: -0.0014  | Total Loss: 5.95 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 129/10000  | Episode Reward: 7  | Average Reward -16.88  | Actor loss: 3.84 | Critic loss: 8.29 | Entropy loss: -0.0007  | Total Loss: 12.13 | Total Steps: 49\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 130/10000  | Episode Reward: -5  | Average Reward -16.65  | Actor loss: 2.24 | Critic loss: 5.75 | Entropy loss: -0.0010  | Total Loss: 7.99 | Total Steps: 471\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 131/10000  | Episode Reward: -10  | Average Reward -16.33  | Actor loss: 1.10 | Critic loss: 4.89 | Entropy loss: -0.0011  | Total Loss: 5.99 | Total Steps: 477\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 132/10000  | Episode Reward: 8  | Average Reward -15.76  | Actor loss: 5.52 | Critic loss: 10.62 | Entropy loss: -0.0005  | Total Loss: 16.14 | Total Steps: 134\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 133/10000  | Episode Reward: -18  | Average Reward -15.47  | Actor loss: 15.84 | Critic loss: 40.81 | Entropy loss: -0.0000  | Total Loss: 56.66 | Total Steps: 401\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 134/10000  | Episode Reward: -13  | Average Reward -15.48  | Actor loss: 3.17 | Critic loss: 7.11 | Entropy loss: -0.0007  | Total Loss: 10.29 | Total Steps: 451\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 135/10000  | Episode Reward: -48  | Average Reward -15.96  | Actor loss: -6.15 | Critic loss: 14.56 | Entropy loss: -0.0014  | Total Loss: 8.41 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 136/10000  | Episode Reward: 2  | Average Reward -15.49  | Actor loss: 2.95 | Critic loss: 6.08 | Entropy loss: -0.0008  | Total Loss: 9.03 | Total Steps: 157\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 137/10000  | Episode Reward: 7  | Average Reward -15.46  | Actor loss: 1.47 | Critic loss: 3.76 | Entropy loss: -0.0013  | Total Loss: 5.23 | Total Steps: 194\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 138/10000  | Episode Reward: -47  | Average Reward -15.98  | Actor loss: -6.65 | Critic loss: 13.03 | Entropy loss: -0.0014  | Total Loss: 6.38 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 139/10000  | Episode Reward: 2  | Average Reward -16.02  | Actor loss: 0.39 | Critic loss: 5.02 | Entropy loss: -0.0013  | Total Loss: 5.41 | Total Steps: 294\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 140/10000  | Episode Reward: -7  | Average Reward -16.05  | Actor loss: 0.96 | Critic loss: 3.91 | Entropy loss: -0.0012  | Total Loss: 4.87 | Total Steps: 385\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 141/10000  | Episode Reward: 3  | Average Reward -15.80  | Actor loss: -0.03 | Critic loss: 5.21 | Entropy loss: -0.0013  | Total Loss: 5.18 | Total Steps: 94\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 142/10000  | Episode Reward: -30  | Average Reward -15.82  | Actor loss: -6.49 | Critic loss: 13.01 | Entropy loss: -0.0014  | Total Loss: 6.52 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 143/10000  | Episode Reward: -5  | Average Reward -15.95  | Actor loss: 0.02 | Critic loss: 6.02 | Entropy loss: -0.0010  | Total Loss: 6.04 | Total Steps: 172\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 144/10000  | Episode Reward: 10  | Average Reward -15.79  | Actor loss: 7.49 | Critic loss: 15.90 | Entropy loss: -0.0003  | Total Loss: 23.40 | Total Steps: 21\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 145/10000  | Episode Reward: 7  | Average Reward -15.71  | Actor loss: 3.74 | Critic loss: 7.93 | Entropy loss: -0.0006  | Total Loss: 11.67 | Total Steps: 41\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 146/10000  | Episode Reward: -52  | Average Reward -15.67  | Actor loss: -5.23 | Critic loss: 9.61 | Entropy loss: -0.0014  | Total Loss: 4.38 | Total Steps: 500\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 147/10000  | Episode Reward: -35  | Average Reward -15.77  | Actor loss: -5.32 | Critic loss: 9.69 | Entropy loss: -0.0014  | Total Loss: 4.37 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 148/10000  | Episode Reward: -6  | Average Reward -15.51  | Actor loss: 11.75 | Critic loss: 34.64 | Entropy loss: -0.0001  | Total Loss: 46.39 | Total Steps: 205\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 149/10000  | Episode Reward: 7  | Average Reward -15.37  | Actor loss: 1.18 | Critic loss: 4.46 | Entropy loss: -0.0012  | Total Loss: 5.64 | Total Steps: 84\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 150/10000  | Episode Reward: -31  | Average Reward -15.57  | Actor loss: -5.13 | Critic loss: 9.29 | Entropy loss: -0.0014  | Total Loss: 4.16 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 151/10000  | Episode Reward: -27  | Average Reward -15.67  | Actor loss: -6.70 | Critic loss: 16.23 | Entropy loss: -0.0014  | Total Loss: 9.53 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 152/10000  | Episode Reward: -33  | Average Reward -15.68  | Actor loss: -6.04 | Critic loss: 13.09 | Entropy loss: -0.0014  | Total Loss: 7.05 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 153/10000  | Episode Reward: 8  | Average Reward -15.67  | Actor loss: 2.48 | Critic loss: 5.91 | Entropy loss: -0.0009  | Total Loss: 8.38 | Total Steps: 65\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 154/10000  | Episode Reward: -14  | Average Reward -15.41  | Actor loss: 11.47 | Critic loss: 33.44 | Entropy loss: -0.0001  | Total Loss: 44.91 | Total Steps: 405\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 155/10000  | Episode Reward: -8  | Average Reward -15.45  | Actor loss: -0.88 | Critic loss: 5.77 | Entropy loss: -0.0011  | Total Loss: 4.89 | Total Steps: 279\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 156/10000  | Episode Reward: -30  | Average Reward -15.77  | Actor loss: -4.69 | Critic loss: 8.47 | Entropy loss: -0.0014  | Total Loss: 3.78 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 157/10000  | Episode Reward: 3  | Average Reward -15.69  | Actor loss: 1.67 | Critic loss: 5.03 | Entropy loss: -0.0010  | Total Loss: 6.70 | Total Steps: 273\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 158/10000  | Episode Reward: -4  | Average Reward -15.39  | Actor loss: 0.26 | Critic loss: 4.33 | Entropy loss: -0.0014  | Total Loss: 4.58 | Total Steps: 198\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 159/10000  | Episode Reward: -38  | Average Reward -15.80  | Actor loss: -4.92 | Critic loss: 8.81 | Entropy loss: -0.0014  | Total Loss: 3.89 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 160/10000  | Episode Reward: -33  | Average Reward -16.21  | Actor loss: -4.26 | Critic loss: 7.69 | Entropy loss: -0.0014  | Total Loss: 3.43 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 161/10000  | Episode Reward: 3  | Average Reward -15.89  | Actor loss: 5.00 | Critic loss: 9.50 | Entropy loss: -0.0006  | Total Loss: 14.50 | Total Steps: 239\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 162/10000  | Episode Reward: 9  | Average Reward -15.34  | Actor loss: 3.26 | Critic loss: 6.19 | Entropy loss: -0.0008  | Total Loss: 9.45 | Total Steps: 155\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 163/10000  | Episode Reward: 8  | Average Reward -14.80  | Actor loss: 1.43 | Critic loss: 4.93 | Entropy loss: -0.0011  | Total Loss: 6.35 | Total Steps: 175\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 164/10000  | Episode Reward: -10  | Average Reward -14.51  | Actor loss: 1.19 | Critic loss: 3.58 | Entropy loss: -0.0013  | Total Loss: 4.77 | Total Steps: 495\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 165/10000  | Episode Reward: 9  | Average Reward -14.44  | Actor loss: 4.42 | Critic loss: 8.23 | Entropy loss: -0.0005  | Total Loss: 12.65 | Total Steps: 37\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 166/10000  | Episode Reward: -19  | Average Reward -14.70  | Actor loss: 2.88 | Critic loss: 9.35 | Entropy loss: -0.0005  | Total Loss: 12.23 | Total Steps: 336\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 167/10000  | Episode Reward: -34  | Average Reward -15.08  | Actor loss: -4.49 | Critic loss: 8.48 | Entropy loss: -0.0014  | Total Loss: 3.98 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 168/10000  | Episode Reward: -26  | Average Reward -15.20  | Actor loss: 5.26 | Critic loss: 11.58 | Entropy loss: -0.0003  | Total Loss: 16.84 | Total Steps: 321\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 169/10000  | Episode Reward: -51  | Average Reward -15.80  | Actor loss: -5.65 | Critic loss: 10.43 | Entropy loss: -0.0014  | Total Loss: 4.78 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 170/10000  | Episode Reward: -48  | Average Reward -16.33  | Actor loss: -4.75 | Critic loss: 8.48 | Entropy loss: -0.0014  | Total Loss: 3.72 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 171/10000  | Episode Reward: 4  | Average Reward -16.32  | Actor loss: 3.50 | Critic loss: 6.67 | Entropy loss: -0.0008  | Total Loss: 10.18 | Total Steps: 158\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 172/10000  | Episode Reward: -1  | Average Reward -16.35  | Actor loss: -1.34 | Critic loss: 6.03 | Entropy loss: -0.0013  | Total Loss: 4.68 | Total Steps: 94\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 173/10000  | Episode Reward: -27  | Average Reward -16.28  | Actor loss: -4.79 | Critic loss: 9.05 | Entropy loss: -0.0014  | Total Loss: 4.26 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 174/10000  | Episode Reward: -7  | Average Reward -16.42  | Actor loss: 1.56 | Critic loss: 9.53 | Entropy loss: -0.0005  | Total Loss: 11.09 | Total Steps: 132\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 175/10000  | Episode Reward: -30  | Average Reward -16.26  | Actor loss: -4.53 | Critic loss: 8.07 | Entropy loss: -0.0014  | Total Loss: 3.54 | Total Steps: 500\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 176/10000  | Episode Reward: 10  | Average Reward -16.15  | Actor loss: 4.20 | Critic loss: 7.87 | Entropy loss: -0.0007  | Total Loss: 12.07 | Total Steps: 47\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 177/10000  | Episode Reward: -32  | Average Reward -16.07  | Actor loss: -5.68 | Critic loss: 10.54 | Entropy loss: -0.0014  | Total Loss: 4.85 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 178/10000  | Episode Reward: -35  | Average Reward -16.36  | Actor loss: -4.42 | Critic loss: 7.65 | Entropy loss: -0.0014  | Total Loss: 3.23 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 179/10000  | Episode Reward: 3  | Average Reward -16.04  | Actor loss: 1.92 | Critic loss: 5.47 | Entropy loss: -0.0009  | Total Loss: 7.39 | Total Steps: 166\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 180/10000  | Episode Reward: -15  | Average Reward -16.11  | Actor loss: -4.11 | Critic loss: 7.42 | Entropy loss: -0.0014  | Total Loss: 3.31 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 181/10000  | Episode Reward: -21  | Average Reward -16.21  | Actor loss: -4.53 | Critic loss: 8.00 | Entropy loss: -0.0014  | Total Loss: 3.47 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 182/10000  | Episode Reward: -37  | Average Reward -16.32  | Actor loss: -3.93 | Critic loss: 7.12 | Entropy loss: -0.0014  | Total Loss: 3.19 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 183/10000  | Episode Reward: -20  | Average Reward -16.19  | Actor loss: -4.01 | Critic loss: 7.05 | Entropy loss: -0.0014  | Total Loss: 3.04 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 184/10000  | Episode Reward: -2  | Average Reward -15.91  | Actor loss: 2.75 | Critic loss: 7.69 | Entropy loss: -0.0007  | Total Loss: 10.44 | Total Steps: 151\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 185/10000  | Episode Reward: -45  | Average Reward -16.42  | Actor loss: -4.19 | Critic loss: 7.60 | Entropy loss: -0.0014  | Total Loss: 3.40 | Total Steps: 500\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 186/10000  | Episode Reward: -4  | Average Reward -16.04  | Actor loss: 8.66 | Critic loss: 20.79 | Entropy loss: -0.0002  | Total Loss: 29.44 | Total Steps: 413\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 187/10000  | Episode Reward: 2  | Average Reward -16.09  | Actor loss: 0.09 | Critic loss: 5.85 | Entropy loss: -0.0010  | Total Loss: 5.94 | Total Steps: 74\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 188/10000  | Episode Reward: -38  | Average Reward -16.25  | Actor loss: -3.95 | Critic loss: 7.24 | Entropy loss: -0.0014  | Total Loss: 3.29 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 189/10000  | Episode Reward: 1  | Average Reward -16.31  | Actor loss: 1.94 | Critic loss: 6.61 | Entropy loss: -0.0008  | Total Loss: 8.55 | Total Steps: 160\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 190/10000  | Episode Reward: -23  | Average Reward -16.56  | Actor loss: -4.69 | Critic loss: 8.17 | Entropy loss: -0.0014  | Total Loss: 3.48 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 191/10000  | Episode Reward: 7  | Average Reward -16.35  | Actor loss: 3.43 | Critic loss: 6.54 | Entropy loss: -0.0008  | Total Loss: 9.97 | Total Steps: 156\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 192/10000  | Episode Reward: 4  | Average Reward -15.99  | Actor loss: 4.07 | Critic loss: 7.83 | Entropy loss: -0.0007  | Total Loss: 11.89 | Total Steps: 146\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 193/10000  | Episode Reward: -39  | Average Reward -16.18  | Actor loss: -4.78 | Critic loss: 8.46 | Entropy loss: -0.0014  | Total Loss: 3.68 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 194/10000  | Episode Reward: 9  | Average Reward -15.88  | Actor loss: 3.16 | Critic loss: 6.24 | Entropy loss: -0.0008  | Total Loss: 9.40 | Total Steps: 56\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 195/10000  | Episode Reward: 5  | Average Reward -15.42  | Actor loss: 4.64 | Critic loss: 8.66 | Entropy loss: -0.0005  | Total Loss: 13.30 | Total Steps: 138\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 196/10000  | Episode Reward: -26  | Average Reward -15.71  | Actor loss: -4.22 | Critic loss: 7.85 | Entropy loss: -0.0014  | Total Loss: 3.63 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 197/10000  | Episode Reward: 9  | Average Reward -15.66  | Actor loss: 4.51 | Critic loss: 8.56 | Entropy loss: -0.0005  | Total Loss: 13.07 | Total Steps: 34\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 198/10000  | Episode Reward: -42  | Average Reward -16.12  | Actor loss: -7.42 | Critic loss: 16.41 | Entropy loss: -0.0014  | Total Loss: 8.98 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 199/10000  | Episode Reward: 1  | Average Reward -15.73  | Actor loss: -0.77 | Critic loss: 5.84 | Entropy loss: -0.0013  | Total Loss: 5.07 | Total Steps: 93\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 200/10000  | Episode Reward: -9  | Average Reward -15.89  | Actor loss: -1.05 | Critic loss: 6.92 | Entropy loss: -0.0012  | Total Loss: 5.87 | Total Steps: 285\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 201/10000  | Episode Reward: 3  | Average Reward -15.52  | Actor loss: 10.03 | Critic loss: 28.12 | Entropy loss: -0.0001  | Total Loss: 38.15 | Total Steps: 207\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 202/10000  | Episode Reward: -8  | Average Reward -15.20  | Actor loss: 5.66 | Critic loss: 10.63 | Entropy loss: -0.0004  | Total Loss: 16.29 | Total Steps: 428\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 203/10000  | Episode Reward: -68  | Average Reward -15.74  | Actor loss: -5.93 | Critic loss: 12.65 | Entropy loss: -0.0014  | Total Loss: 6.72 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 204/10000  | Episode Reward: 2  | Average Reward -15.37  | Actor loss: 2.03 | Critic loss: 4.80 | Entropy loss: -0.0009  | Total Loss: 6.83 | Total Steps: 367\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 205/10000  | Episode Reward: 4  | Average Reward -15.20  | Actor loss: -0.19 | Critic loss: 4.33 | Entropy loss: -0.0014  | Total Loss: 4.13 | Total Steps: 99\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 206/10000  | Episode Reward: -45  | Average Reward -15.31  | Actor loss: -6.11 | Critic loss: 13.59 | Entropy loss: -0.0014  | Total Loss: 7.47 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 207/10000  | Episode Reward: 9  | Average Reward -14.69  | Actor loss: 1.13 | Critic loss: 3.62 | Entropy loss: -0.0014  | Total Loss: 4.75 | Total Steps: 98\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 208/10000  | Episode Reward: -51  | Average Reward -15.11  | Actor loss: -6.68 | Critic loss: 14.98 | Entropy loss: -0.0014  | Total Loss: 8.30 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 209/10000  | Episode Reward: 4  | Average Reward -15.16  | Actor loss: 0.67 | Critic loss: 4.33 | Entropy loss: -0.0011  | Total Loss: 4.99 | Total Steps: 77\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 210/10000  | Episode Reward: -6  | Average Reward -15.32  | Actor loss: 2.15 | Critic loss: 5.02 | Entropy loss: -0.0011  | Total Loss: 7.17 | Total Steps: 476\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 211/10000  | Episode Reward: -18  | Average Reward -15.21  | Actor loss: 9.41 | Critic loss: 24.57 | Entropy loss: -0.0002  | Total Loss: 33.98 | Total Steps: 412\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 212/10000  | Episode Reward: 8  | Average Reward -14.96  | Actor loss: 3.59 | Critic loss: 7.46 | Entropy loss: -0.0005  | Total Loss: 11.05 | Total Steps: 37\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 213/10000  | Episode Reward: 9  | Average Reward -14.97  | Actor loss: 4.02 | Critic loss: 7.65 | Entropy loss: -0.0004  | Total Loss: 11.67 | Total Steps: 31\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 214/10000  | Episode Reward: -50  | Average Reward -15.29  | Actor loss: -4.77 | Critic loss: 9.08 | Entropy loss: -0.0014  | Total Loss: 4.31 | Total Steps: 500\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 215/10000  | Episode Reward: 10  | Average Reward -14.83  | Actor loss: 6.02 | Critic loss: 11.55 | Entropy loss: -0.0004  | Total Loss: 17.56 | Total Steps: 26\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 216/10000  | Episode Reward: -32  | Average Reward -14.94  | Actor loss: -4.70 | Critic loss: 9.28 | Entropy loss: -0.0014  | Total Loss: 4.58 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 217/10000  | Episode Reward: -22  | Average Reward -15.13  | Actor loss: -3.91 | Critic loss: 7.28 | Entropy loss: -0.0014  | Total Loss: 3.37 | Total Steps: 500\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 218/10000  | Episode Reward: -30  | Average Reward -15.11  | Actor loss: -4.26 | Critic loss: 7.58 | Entropy loss: -0.0014  | Total Loss: 3.32 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 219/10000  | Episode Reward: -4  | Average Reward -14.85  | Actor loss: 1.18 | Critic loss: 5.35 | Entropy loss: -0.0009  | Total Loss: 6.53 | Total Steps: 261\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 220/10000  | Episode Reward: -21  | Average Reward -14.90  | Actor loss: -4.60 | Critic loss: 8.24 | Entropy loss: -0.0014  | Total Loss: 3.64 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 221/10000  | Episode Reward: 3  | Average Reward -14.73  | Actor loss: 1.43 | Critic loss: 6.46 | Entropy loss: -0.0005  | Total Loss: 7.88 | Total Steps: 37\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 222/10000  | Episode Reward: 2  | Average Reward -14.73  | Actor loss: 0.89 | Critic loss: 4.68 | Entropy loss: -0.0011  | Total Loss: 5.57 | Total Steps: 176\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 223/10000  | Episode Reward: 6  | Average Reward -14.30  | Actor loss: 2.17 | Critic loss: 5.92 | Entropy loss: -0.0006  | Total Loss: 8.09 | Total Steps: 46\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 224/10000  | Episode Reward: -53  | Average Reward -14.50  | Actor loss: -6.36 | Critic loss: 12.24 | Entropy loss: -0.0014  | Total Loss: 5.88 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 225/10000  | Episode Reward: -38  | Average Reward -14.83  | Actor loss: -3.78 | Critic loss: 7.07 | Entropy loss: -0.0014  | Total Loss: 3.29 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 226/10000  | Episode Reward: 8  | Average Reward -14.35  | Actor loss: 3.33 | Critic loss: 6.91 | Entropy loss: -0.0006  | Total Loss: 10.24 | Total Steps: 44\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 227/10000  | Episode Reward: 10  | Average Reward -13.95  | Actor loss: 6.78 | Critic loss: 14.97 | Entropy loss: -0.0003  | Total Loss: 21.75 | Total Steps: 23\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 228/10000  | Episode Reward: -18  | Average Reward -13.73  | Actor loss: 10.22 | Critic loss: 26.99 | Entropy loss: -0.0002  | Total Loss: 37.20 | Total Steps: 210\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 229/10000  | Episode Reward: -45  | Average Reward -14.25  | Actor loss: -4.46 | Critic loss: 8.44 | Entropy loss: -0.0014  | Total Loss: 3.97 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 230/10000  | Episode Reward: 4  | Average Reward -14.16  | Actor loss: 6.26 | Critic loss: 12.78 | Entropy loss: -0.0004  | Total Loss: 19.04 | Total Steps: 228\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 231/10000  | Episode Reward: -7  | Average Reward -14.13  | Actor loss: -1.00 | Critic loss: 5.58 | Entropy loss: -0.0014  | Total Loss: 4.58 | Total Steps: 197\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 232/10000  | Episode Reward: -5  | Average Reward -14.26  | Actor loss: 0.71 | Critic loss: 3.67 | Entropy loss: -0.0013  | Total Loss: 4.38 | Total Steps: 496\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 233/10000  | Episode Reward: -32  | Average Reward -14.40  | Actor loss: -4.43 | Critic loss: 8.25 | Entropy loss: -0.0014  | Total Loss: 3.82 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 234/10000  | Episode Reward: -24  | Average Reward -14.51  | Actor loss: -4.05 | Critic loss: 7.81 | Entropy loss: -0.0014  | Total Loss: 3.75 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 235/10000  | Episode Reward: 1  | Average Reward -14.02  | Actor loss: 0.74 | Critic loss: 3.65 | Entropy loss: -0.0012  | Total Loss: 4.39 | Total Steps: 190\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 236/10000  | Episode Reward: -8  | Average Reward -14.12  | Actor loss: 8.58 | Critic loss: 20.05 | Entropy loss: -0.0002  | Total Loss: 28.64 | Total Steps: 211\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 237/10000  | Episode Reward: 9  | Average Reward -14.10  | Actor loss: 1.36 | Critic loss: 4.29 | Entropy loss: -0.0009  | Total Loss: 5.66 | Total Steps: 69\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 238/10000  | Episode Reward: 10  | Average Reward -13.53  | Actor loss: 3.77 | Critic loss: 6.46 | Entropy loss: -0.0005  | Total Loss: 10.24 | Total Steps: 36\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 239/10000  | Episode Reward: -11  | Average Reward -13.66  | Actor loss: 3.97 | Critic loss: 6.99 | Entropy loss: -0.0006  | Total Loss: 10.96 | Total Steps: 439\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 240/10000  | Episode Reward: -47  | Average Reward -14.06  | Actor loss: -7.97 | Critic loss: 18.05 | Entropy loss: -0.0014  | Total Loss: 10.08 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 241/10000  | Episode Reward: -31  | Average Reward -14.40  | Actor loss: -3.97 | Critic loss: 7.28 | Entropy loss: -0.0014  | Total Loss: 3.32 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 242/10000  | Episode Reward: -10  | Average Reward -14.20  | Actor loss: 3.83 | Critic loss: 10.65 | Entropy loss: -0.0004  | Total Loss: 14.48 | Total Steps: 130\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 243/10000  | Episode Reward: -6  | Average Reward -14.21  | Actor loss: -0.22 | Critic loss: 5.92 | Entropy loss: -0.0011  | Total Loss: 5.70 | Total Steps: 276\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 244/10000  | Episode Reward: 9  | Average Reward -14.22  | Actor loss: 4.88 | Critic loss: 9.08 | Entropy loss: -0.0005  | Total Loss: 13.95 | Total Steps: 32\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 245/10000  | Episode Reward: -36  | Average Reward -14.65  | Actor loss: -4.73 | Critic loss: 8.40 | Entropy loss: -0.0014  | Total Loss: 3.68 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 246/10000  | Episode Reward: -29  | Average Reward -14.42  | Actor loss: -6.44 | Critic loss: 14.04 | Entropy loss: -0.0014  | Total Loss: 7.60 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 247/10000  | Episode Reward: 7  | Average Reward -14.00  | Actor loss: 6.22 | Critic loss: 12.72 | Entropy loss: -0.0004  | Total Loss: 18.94 | Total Steps: 131\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 248/10000  | Episode Reward: -26  | Average Reward -14.20  | Actor loss: -4.08 | Critic loss: 7.80 | Entropy loss: -0.0014  | Total Loss: 3.72 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 249/10000  | Episode Reward: -9  | Average Reward -14.36  | Actor loss: 0.38 | Critic loss: 4.19 | Entropy loss: -0.0012  | Total Loss: 4.57 | Total Steps: 282\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 250/10000  | Episode Reward: -2  | Average Reward -14.07  | Actor loss: 1.18 | Critic loss: 3.09 | Entropy loss: -0.0013  | Total Loss: 4.27 | Total Steps: 492\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 251/10000  | Episode Reward: -23  | Average Reward -14.03  | Actor loss: -4.86 | Critic loss: 9.65 | Entropy loss: -0.0014  | Total Loss: 4.80 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 252/10000  | Episode Reward: -2  | Average Reward -13.72  | Actor loss: 9.28 | Critic loss: 31.39 | Entropy loss: -0.0000  | Total Loss: 40.67 | Total Steps: 202\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 253/10000  | Episode Reward: 8  | Average Reward -13.72  | Actor loss: 1.07 | Critic loss: 3.69 | Entropy loss: -0.0009  | Total Loss: 4.77 | Total Steps: 62\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 254/10000  | Episode Reward: 8  | Average Reward -13.50  | Actor loss: 3.86 | Critic loss: 7.94 | Entropy loss: -0.0003  | Total Loss: 11.80 | Total Steps: 24\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 255/10000  | Episode Reward: -55  | Average Reward -13.97  | Actor loss: -6.13 | Critic loss: 11.73 | Entropy loss: -0.0014  | Total Loss: 5.60 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 256/10000  | Episode Reward: 1  | Average Reward -13.66  | Actor loss: 2.28 | Critic loss: 6.12 | Entropy loss: -0.0007  | Total Loss: 8.40 | Total Steps: 251\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 257/10000  | Episode Reward: -34  | Average Reward -14.03  | Actor loss: -4.33 | Critic loss: 7.87 | Entropy loss: -0.0014  | Total Loss: 3.54 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 258/10000  | Episode Reward: -29  | Average Reward -14.28  | Actor loss: -4.43 | Critic loss: 7.55 | Entropy loss: -0.0014  | Total Loss: 3.12 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 259/10000  | Episode Reward: -24  | Average Reward -14.14  | Actor loss: -4.24 | Critic loss: 7.61 | Entropy loss: -0.0014  | Total Loss: 3.37 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 260/10000  | Episode Reward: -25  | Average Reward -14.06  | Actor loss: -4.10 | Critic loss: 7.22 | Entropy loss: -0.0014  | Total Loss: 3.12 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 261/10000  | Episode Reward: -15  | Average Reward -14.24  | Actor loss: 1.96 | Critic loss: 5.60 | Entropy loss: -0.0009  | Total Loss: 7.55 | Total Steps: 366\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 262/10000  | Episode Reward: 3  | Average Reward -14.30  | Actor loss: 1.74 | Critic loss: 4.86 | Entropy loss: -0.0011  | Total Loss: 6.60 | Total Steps: 378\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 263/10000  | Episode Reward: 6  | Average Reward -14.32  | Actor loss: 3.33 | Critic loss: 7.42 | Entropy loss: -0.0005  | Total Loss: 10.75 | Total Steps: 34\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 264/10000  | Episode Reward: 9  | Average Reward -14.13  | Actor loss: 3.71 | Critic loss: 6.27 | Entropy loss: -0.0004  | Total Loss: 9.98 | Total Steps: 32\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 265/10000  | Episode Reward: -43  | Average Reward -14.65  | Actor loss: -4.97 | Critic loss: 8.86 | Entropy loss: -0.0014  | Total Loss: 3.89 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 266/10000  | Episode Reward: -1  | Average Reward -14.47  | Actor loss: 2.32 | Critic loss: 4.36 | Entropy loss: -0.0009  | Total Loss: 6.67 | Total Steps: 268\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 267/10000  | Episode Reward: -33  | Average Reward -14.46  | Actor loss: -4.34 | Critic loss: 7.91 | Entropy loss: -0.0014  | Total Loss: 3.56 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 268/10000  | Episode Reward: -4  | Average Reward -14.24  | Actor loss: 0.38 | Critic loss: 4.84 | Entropy loss: -0.0006  | Total Loss: 5.22 | Total Steps: 245\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 269/10000  | Episode Reward: -2  | Average Reward -13.75  | Actor loss: 3.62 | Critic loss: 7.98 | Entropy loss: -0.0007  | Total Loss: 11.60 | Total Steps: 151\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 270/10000  | Episode Reward: -29  | Average Reward -13.56  | Actor loss: -4.77 | Critic loss: 9.13 | Entropy loss: -0.0014  | Total Loss: 4.36 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 271/10000  | Episode Reward: 3  | Average Reward -13.57  | Actor loss: 4.62 | Critic loss: 8.61 | Entropy loss: -0.0006  | Total Loss: 13.22 | Total Steps: 141\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 272/10000  | Episode Reward: -16  | Average Reward -13.72  | Actor loss: 3.35 | Critic loss: 8.67 | Entropy loss: -0.0006  | Total Loss: 12.02 | Total Steps: 344\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 273/10000  | Episode Reward: 9  | Average Reward -13.36  | Actor loss: 2.86 | Critic loss: 4.64 | Entropy loss: -0.0005  | Total Loss: 7.50 | Total Steps: 38\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 274/10000  | Episode Reward: -32  | Average Reward -13.61  | Actor loss: -7.71 | Critic loss: 17.46 | Entropy loss: -0.0014  | Total Loss: 9.75 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 275/10000  | Episode Reward: -14  | Average Reward -13.45  | Actor loss: 10.02 | Critic loss: 28.76 | Entropy loss: -0.0001  | Total Loss: 38.78 | Total Steps: 309\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 276/10000  | Episode Reward: -38  | Average Reward -13.93  | Actor loss: -4.51 | Critic loss: 8.95 | Entropy loss: -0.0014  | Total Loss: 4.43 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 277/10000  | Episode Reward: -10  | Average Reward -13.71  | Actor loss: 5.70 | Critic loss: 12.88 | Entropy loss: -0.0004  | Total Loss: 18.58 | Total Steps: 426\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 278/10000  | Episode Reward: -30  | Average Reward -13.66  | Actor loss: -4.70 | Critic loss: 8.24 | Entropy loss: -0.0014  | Total Loss: 3.54 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 279/10000  | Episode Reward: 7  | Average Reward -13.62  | Actor loss: 4.12 | Critic loss: 8.39 | Entropy loss: -0.0004  | Total Loss: 12.51 | Total Steps: 27\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 280/10000  | Episode Reward: 8  | Average Reward -13.39  | Actor loss: 0.02 | Critic loss: 4.03 | Entropy loss: -0.0013  | Total Loss: 4.05 | Total Steps: 93\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 281/10000  | Episode Reward: 9  | Average Reward -13.09  | Actor loss: 1.49 | Critic loss: 3.21 | Entropy loss: -0.0007  | Total Loss: 4.70 | Total Steps: 54\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 282/10000  | Episode Reward: -28  | Average Reward -13.00  | Actor loss: -6.02 | Critic loss: 15.20 | Entropy loss: -0.0013  | Total Loss: 9.18 | Total Steps: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "ALG_NAME = 'S0'\n",
    "ENV_ID = '1'\n",
    "TRAIN_EPISODES = 10000  # number of overall episodes for training\n",
    "TEST_EPISODES = 10  # number of overall episodes for testing\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 0.0001  #0.00005 \n",
    "speed = 1\n",
    "num_steps = 100 # the step for updating the network\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = 0\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                    # value = value.detach()\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % 100 == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.001 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(episode_reward)\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(actor_loss)\n",
    "            all_critic_loss.append(critic_loss)\n",
    "            all_entropy_loss.append(entropy_loss)\n",
    "            all_total_loss.append(ac_loss)\n",
    "            if episode >= 100:\n",
    "                avg_score = np.mean(all_episode_reward[-100:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 100 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%500 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'{ALG_NAME}_{ENV_ID}.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_average_reward)\n",
    "list_string = str(all_average_reward)\n",
    "file_path = f\"{ALG_NAME}_{ENV_ID}.txt\"  # Specify the file path and name\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write the list string to the file\n",
    "    file.write(list_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cpu\")\n",
    "episode = 9190\n",
    "speed = 1\n",
    "MAX_STEPS = 500\n",
    "TEST_EPISODES = 100\n",
    "ALG_NAME = 'S0_with'\n",
    "ENV_ID = '3'\n",
    "tracked_agent = -1\n",
    "env.reset()\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode,ALG_NAME,ENV_ID)\n",
    "average = 0\n",
    "for episode in range(TEST_EPISODES):\n",
    "            STEPS = 0\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while not done:\n",
    "                STEPS += 1                \n",
    "                lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                # value = value.detach()\n",
    "                dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                \n",
    "\n",
    "                action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                action = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    # if reward<0:\n",
    "                    #     print(f'Decision Step reward: {reward}')\n",
    "\n",
    "                if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "\n",
    "                episode_reward = episode_reward + reward\n",
    "                vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                vt = vt_new\n",
    "            average += episode_reward / TEST_EPISODES\n",
    "            print(f'Episode: {episode}, Episode reward: {episode_reward}')\n",
    "print(f'Average Episode Reward: {average}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate x-axis values as the indices of the list\n",
    "x_axis = range(len(all_average_reward))\n",
    "\n",
    "# Plotting the figure\n",
    "plt.plot(x_axis, all_average_reward)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Rewards (Average 100 episodes) vs Episode')+\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 738\n",
    "env =  UE(file_name=\"280623_1\\\\build\",seed=1,side_channels=[],worker_id=0,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# dist = np.array((2,1,2,2))\n",
    "# entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "# entropy\n",
    "\n",
    "action = torch.tensor([[0.1,0.2,0.1,1]])\n",
    "index = torch.tensor([0])\n",
    "print(action,index)\n",
    "entropy_loss = F.cross_entropy(action, index)\n",
    "entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need env.reset() at the beginning of each episode or unity file will reset it\n",
    "# The utility of workstation GPU is only 1 percent (Memory Leak?) in laptop GPU 8 seconds / episode on average\n",
    "# Can we make the unity environment accept action from GPU?\n",
    "#  Model is still hard to learn meaningful actions -- Make sure the algorithm is correct (Weight Initialization? Experience Replay?) value layer how to update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "plt.plot(all_rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_lengths)\n",
    "plt.plot(average_lengths)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "# ENV_ID = 'CartPole-v1'  # environment id\n",
    "# RANDOM_SEED = 2  # random seed, can be either an int number or None\n",
    "# RENDER = False  # render while training\n",
    "\n",
    "# ALG_NAME = 'AC'\n",
    "ALG_NAME = 'A2C'\n",
    "ENV_ID = 'S0'\n",
    "TRAIN_EPISODES = 200000  # number of overall episodes for training\n",
    "TEST_EPISODES = 10  # number of overall episodes for testing\n",
    "MAX_STEPS = 1200  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 0.001\n",
    "speed = 2\n",
    "# LR_A = 0.001  # learning rate for actor\n",
    "# LR_C = 0.01  # learning rate for critic\n",
    "\n",
    "\n",
    "###############################  Actor-Critic  ####################################\n",
    "\n",
    "\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_num, lr=0.001):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, action_num)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, action, td_error):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         loss = td_error * torch.nn.functional.cross_entropy(logits, torch.LongTensor([action]))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def get_action(self, state, greedy=False):\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         probs = torch.nn.functional.softmax(logits, dim=-1).detach().numpy()\n",
    "#         if greedy:\n",
    "#             return np.argmax(probs)\n",
    "#         return np.random.choice(len(probs[0]), p=probs[0])\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_actor.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_actor.pt')))\n",
    "\n",
    "\n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_dim, lr=0.01):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, 1)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, reward, state_, done):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         d = 0 if done else 1\n",
    "#         v_ = self.model(torch.FloatTensor(state_))\n",
    "#         v = self.model(torch.FloatTensor(state))\n",
    "#         td_error = reward + d * LAM * v_ - v\n",
    "#         loss = td_error ** 2\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return td_error.item()\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_critic.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_critic.pt')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ''' \n",
    "    choose environment\n",
    "    1. Openai gym:\n",
    "    env = gym.make()\n",
    "    2. DeepMind Control Suite:\n",
    "    env = dm_control2gym.make()\n",
    "    '''\n",
    "    # env = gym.make(ENV_ID).unwrapped\n",
    "    # # dm_control2gym.create_render_mode('example mode', show=True, return_pixel=False, height=240, width=320, camera_id=-1, overlays=(),\n",
    "    # #              depth=False, scene_option=None)\n",
    "    # # env = dm_control2gym.make(domain_name=\"cartpole\", task_name=\"balance\")\n",
    "\n",
    "    # env.seed(RANDOM_SEED)  # reproducible\n",
    "    # np.random.seed(RANDOM_SEED)\n",
    "    # torch.manual_seed(RANDOM_SEED)  # reproducible\n",
    "\n",
    "    # N_F = env.observation_space.shape[0]\n",
    "    # N_A = env.action_space.n\n",
    "\n",
    "    # print(\"observation dimension: %d\" % N_F)  # 4\n",
    "    # print(\"observation high: %s\" % env.observation_space.high)  # [ 2.4 , inf , 0.41887902 , inf]\n",
    "    # print(\"observation low : %s\" % env.observation_space.low)  # [-2.4 , -inf , -0.41887902 , -inf]\n",
    "    # print(\"num of actions: %d\" % N_A)  # 2 : left or right\n",
    "\n",
    "    # actor = Actor(state_dim=N_F, action_num=N_A, lr=LR_A)\n",
    "    # # we need a good teacher, so the teacher should learn faster than the actor\n",
    "    # critic = Critic(state_dim=N_F, lr=LR_C)\n",
    "\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    best_score = 0\n",
    "    if train:\n",
    "        all_episode_reward = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            print(f'Episode: {episode}')\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            step = 0  # number of step in this episode\n",
    "            episode_reward = 0  # rewards of all steps\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "                # if RENDER:\n",
    "                #     env.render()\n",
    "\n",
    "                # action = actor.get_action(state) state --> vt,lt\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state) #action is the direct linear output from the model\n",
    "                action_dist = Categorical(F.softmax(action,dim=1))\n",
    "                index = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "                \n",
    "                \n",
    "                # action_onehot = F.one_hot(torch.argmax(action),num_actions)\n",
    "                # print(action_onehot)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if episode<50:\n",
    "                #     if episode%2:\n",
    "                #         action_onehot = torch.tensor([1,0,1,0])\n",
    "                #     else: action_onehot = torch.tensor([1,0,0,1])                 \n",
    "                # continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "                \n",
    "                # action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                step += 1\n",
    "\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # print((vt == vt_new).all())\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    if reward<0:\n",
    "                        print(f'Decision Step reward: {reward}')\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "                \n",
    "\n",
    "                if step >= MAX_STEPS:\n",
    "                    reward = -1\n",
    "                    print(f'Max Step Reward: {reward}')\n",
    "                    done = True\n",
    "                if step % 100 == 0:\n",
    "                    print (f'Step: {step}')\n",
    "\n",
    "                \n",
    "                    \n",
    "                episode_reward = episode_reward + reward\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # try:\n",
    "                #     td_error = critic.learn(\n",
    "                #         state, reward, state_new, done\n",
    "                #     )  # learn Value-function : gradient = grad[r + lambda * V(s_new) - V(s)]\n",
    "                #     actor.learn(state, action, td_error)  # learn Policy : true_gradient = grad[logPi(s, a) * td_error]                   \n",
    "                try:\n",
    "                    # d = 0 if done else 1\n",
    "                    # v_ = self.model(torch.FloatTensor(state_))\n",
    "                    # v = self.model(torch.FloatTensor(state))\n",
    "                    if not done:\n",
    "                        action_new, value_new, lstm_hidden_state_new = agent(vt_new,lt,lstm_hidden_state)\n",
    "\n",
    "                        # initialize the value first\n",
    "                        td_error = reward + LAM * value_new - value\n",
    "                    else: td_error = reward - value\n",
    "\n",
    "\n",
    "                    critic_loss = td_error ** 2\n",
    "\n",
    "                    log_prob = action_dist.log_prob(index)\n",
    "                    actor_loss = -(log_prob * td_error)\n",
    "\n",
    "                    # logits = action # the actual model output (without softmax)\n",
    "                    # labels = index  # one hot vector of the sampled action\n",
    "                    print(action,index)\n",
    "                    entropy_loss = F.cross_entropy(action, index)\n",
    "                    print(entropy_loss)\n",
    "                    total_loss = actor_loss + 0.5* critic_loss - 0.001*entropy_loss\n",
    "                    # entropy loss entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=tf.nn.softmax(logit))\n",
    "                    # comb_loss = tf.reduce_mean((0.5 * value_loss + policy_loss - 0.01 * entropy))\n",
    "\n",
    "                    total_loss = total_loss.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward(retain_graph=True)\n",
    "                    print('hi')\n",
    "                    optimizer.step()   \n",
    "                    \n",
    "              \n",
    "                except KeyboardInterrupt:  # if Ctrl+C at running actor.learn(), then save model, or exit if not at actor.learn()\n",
    "                    agent.save(episode)\n",
    "                    print('model has been saved')\n",
    "\n",
    "                # state = state_new\n",
    "                # print(f'episode: {episode}, step:{step}')\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            if episode%500 == 0:\n",
    "                agent.save(episode)\n",
    "                print(\"Model has been saved\")\n",
    "                \n",
    "            # all_episode_reward.append(episode_reward)\n",
    "            # if episode >= 100:\n",
    "            #     avg_score = np.mean(all_episode_reward[-100:])\n",
    "            #     if avg_score > best_score:\n",
    "            #         best_score = avg_score\n",
    "            #         agent.save()\n",
    "            #         print(f'The best score for averaging previous 100 episode reward is {best_score}. Model has been saved')\n",
    "\n",
    "\n",
    "\n",
    "            # if episode == 0:\n",
    "            #     all_episode_reward.append(episode_reward)\n",
    "            # else:\n",
    "            #     reference_value = all_episode_reward[-1] * 0.9 + episode_reward * 0.1\n",
    "            #     if reference_value > all_episode_reward_best:\n",
    "            #         agent.save()\n",
    "            #         print(f'The best all_episode_reward is {reference_value}. Model has been saved')\n",
    "            #         all_episode_reward_best = reference_value\n",
    "            #     all_episode_reward.append(reference_value)\n",
    "\n",
    "            print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                  .format(episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([ALG_NAME, ENV_ID])))\n",
    "        \n",
    "\n",
    "    if not train:\n",
    "        agent.load(episode)\n",
    "        print(\"model has been loaded\")\n",
    "        tracked_agent = -1\n",
    "\n",
    "        for episode in range(TEST_EPISODES):\n",
    "            episode_time = time.time()\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index]\n",
    "            t = 0  # number of step in this episode\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # action = actor.get_action(state, greedy=True)\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if done:\n",
    "                #     reward = -20\n",
    "\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                action = F.one_hot(torch.argmax(action),num_actions)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action).reshape(1,4) #[forward, backward, right, left]\n",
    "                action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                vt_new = torch.tensor(decision_steps.obs).reshape(1,3,128,128)\n",
    "                \n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                if tracked_agent in decision_steps:\n",
    "                    # tracked_agent = decision_steps.agent_id[0]\n",
    "                    reward = decision_steps[tracked_agent].reward \n",
    "                # if done:\n",
    "                #     reward = -20  # reward shaping trick\n",
    "                if tracked_agent in terminal_steps:\n",
    "                    done = True\n",
    "                    reward = -20\n",
    "\n",
    "                episode_reward += reward\n",
    "                vt = vt_new\n",
    "                t += 1\n",
    "\n",
    "                if done or t >= MAX_STEPS:\n",
    "                    print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                          .format(episode + 1, TEST_EPISODES, episode_reward, time.time() - t0))\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to update the network every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the optimized MAX_STEP\n",
    "# speed 1: 10000+ speed 2:1822 speed 3: 1918 speed 4: 900 speed 5: 951 speed 6:964 speed 7: 1181 \n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# speed = 1\n",
    "TRAIN_EPISODES = 20\n",
    "tracked_agent = -1\n",
    "num_actions = 4\n",
    "average = 0\n",
    "# for speed in range(30,70,5):\n",
    "#     speed = speed/10\n",
    "speed = 3\n",
    "for episode in range(TRAIN_EPISODES):\n",
    "    env.reset()\n",
    "    behavior_name=list(env.behavior_specs)[0]\n",
    "    step = 0\n",
    "    while True:\n",
    "        index = random.randint(0, 3) # sample an action from action_dist\n",
    "        action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "        discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(discrete_actions)\n",
    "        env.set_actions(behavior_name,action_tuple)\n",
    "        env.step()\n",
    "        step += 1\n",
    "\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "            \n",
    "        if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "            reward = terminal_steps[tracked_agent].reward\n",
    "            if reward > 0: # hit the target\n",
    "                print(f'{episode}: {step} in total')\n",
    "                average += step\n",
    "                break\n",
    "            else:           # roll over or other conditions\n",
    "                env.reset()\n",
    "                step = 0\n",
    "                continue # roll over or other unseen conditions\n",
    "        if tracked_agent in decision_steps: # the agent which requires action\n",
    "            continue\n",
    "average /= TRAIN_EPISODES\n",
    "print(f'For speed {speed}, average random step for hitting the target is {average}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unity could not open in work station\n",
    "# The agent in terminal step will also in decison step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "object_sizes = [1, 2, 3, 4, 5]  # Object sizes on the x-axis\n",
    "max_steps = [[4721, 2468, 1587, 1249, 1084],   # Max steps for each object size\n",
    "             [4027, 1550, 738, 640, 505],\n",
    "             [3320, 1391, 464, 331, 307],\n",
    "             [3289, 882, 679, 424, 211],\n",
    "             [3057, 1203, 470, 378, 222]]\n",
    "speeds = [1, 2, 3, 4, 5]  # Discrete speeds for color-coding\n",
    "\n",
    "# Color mapping for each speed\n",
    "speed_color_mapping = {\n",
    "    1: 'red',\n",
    "    2: 'blue',\n",
    "    3: 'green',\n",
    "    4: 'orange',\n",
    "    5: 'purple'\n",
    "}\n",
    "\n",
    "# Generate scatter plot\n",
    "for i, size in enumerate(object_sizes):\n",
    "    for j, steps in enumerate(max_steps[i]):\n",
    "        speed = speeds[j]\n",
    "        color = speed_color_mapping[speed]\n",
    "        plt.scatter(size, steps, c=color)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Object Size')\n",
    "plt.ylabel('Max Step')\n",
    "plt.title('Scatter Plot')\n",
    "\n",
    "plt.xlim(0, 6)\n",
    "plt.xticks(np.arange(0, 7, 1))\n",
    "plt.ylim(0, 5000)\n",
    "plt.yticks(np.arange(0, 5001, 200))\n",
    "\n",
    "# Create legend\n",
    "legend_labels = [f'Speed {speed}' for speed, _ in speed_color_mapping.items()]\n",
    "plt.legend(legend_labels, loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a = torch.tensor([[0.2460, 0.2503, 0.2647, 0.2389]])\n",
    "b = F.softmax(a,dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1,2,3]\n",
    "avg = np.mean(a[-10:])\n",
    "avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
