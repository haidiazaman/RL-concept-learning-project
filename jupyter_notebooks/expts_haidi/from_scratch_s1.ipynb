{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:29:58.848119Z",
     "start_time": "2023-07-30T16:29:58.567837Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:30:25.644213Z",
     "start_time": "2023-07-30T16:29:58.849120Z"
    }
   },
   "outputs": [],
   "source": [
    "filename1=\"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S1a_train_260723\\\\build\"\n",
    "filename2=\"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S1a_test_260723\\\\build\"\n",
    "filename3=\"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S1b_train_260723\\\\build\"\n",
    "filename4=\"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S1b_test_260723\\\\build\"\n",
    "\n",
    "\n",
    "\n",
    "env1_train =  UE(file_name=filename1,seed=1,side_channels=[],worker_id=0,no_graphics = False)\n",
    "env1_train.reset()\n",
    "env1_test =  UE(file_name=filename2,seed=1,side_channels=[],worker_id=1,no_graphics = False)\n",
    "env1_test.reset()\n",
    "env2_train =  UE(file_name=filename3,seed=1,side_channels=[],worker_id=4,no_graphics = False)\n",
    "env2_train.reset()\n",
    "env2_test =  UE(file_name=filename4,seed=1,side_channels=[],worker_id=7,no_graphics = False)\n",
    "env2_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:30:27.236310Z",
     "start_time": "2023-07-30T16:30:25.645193Z"
    },
    "code_folding": [
     0,
     15
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\transformers_mlagents\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:30:27.251823Z",
     "start_time": "2023-07-30T16:30:27.237310Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def test(agent,test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss):\n",
    "    TEST_EPISODES = 100\n",
    "    tracked_agent = -1\n",
    "    entropy_term = 0\n",
    "    for episode in range(TEST_EPISODES):\n",
    "        test_episode += 1\n",
    "        t0 = time.time()\n",
    "        episode_reward = 0\n",
    "        # env.reset()\n",
    "        env_index = (test_episode // env_per_iteration) % 2\n",
    "        if env_index == 0: env = env1_test\n",
    "        else: env = env2_test\n",
    "        behavior_name=list(env.behavior_specs)[0]\n",
    "        spec=env.behavior_specs[behavior_name]\n",
    "        # state = env.reset().astype(np.float32)\n",
    "        STEPS = 0\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        # state -- vt, lt, lstm\n",
    "        vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "        index = int(decision_steps.obs[1])\n",
    "        if env_index: index = index + 5\n",
    "        print(f'TEST: ---{hashmap[index]}---')\n",
    "\n",
    "        lt = torch.eye(num_words)[:, index].to(device)\n",
    "        lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "        done = False\n",
    "        while True:\n",
    "\n",
    "            # Need to use when calculating the loss\n",
    "            log_probs = []\n",
    "            # values = []\n",
    "            values = torch.empty(0).to(device)\n",
    "            rewards = []\n",
    "\n",
    "            \n",
    "            lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "            STEPS += 1\n",
    "            policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "            # value = value.detach()\n",
    "            dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "            \n",
    "\n",
    "            action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "            # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "            action = action_dist.sample() # sample an action from action_dist\n",
    "            action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "            \n",
    "            log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "            # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "            # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "            entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "            discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(discrete_actions)\n",
    "            env.set_actions(behavior_name,action_tuple)\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "                # print(tracked_agent)\n",
    "\n",
    "            if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                print('TEST: Agent in terminal steps')\n",
    "                done = True\n",
    "                reward = terminal_steps[tracked_agent].reward\n",
    "                if reward > 0:\n",
    "                    pass\n",
    "                else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                print(f'TEST: Terminal Step reward: {reward}')\n",
    "\n",
    "            elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                reward = decision_steps[tracked_agent].reward\n",
    "                # print(f'Decision Step reward: {reward}')\n",
    "                if reward<0:\n",
    "                    print(f'TEST: Decision Step reward: {reward}')\n",
    "            if STEPS >= MAX_STEPS:\n",
    "                reward = -10\n",
    "                print(f'TEST: Max Step Reward: {reward}')\n",
    "                env.reset()\n",
    "                done = True\n",
    "            if STEPS % 100 == 0:\n",
    "                print (f'TEST: Step: {STEPS}')\n",
    "\n",
    "            episode_reward = episode_reward + reward\n",
    "\n",
    "            rewards.append(reward)\n",
    "            # values.append(value)\n",
    "            values = torch.cat((values, value), dim=0)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term = entropy_term + entropy\n",
    "            vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            vt = vt_new\n",
    "\n",
    "            if done:\n",
    "                # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                # Qval = Qval.detach()\n",
    "                break\n",
    "            \n",
    "            \n",
    "        discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "        cumulative = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "            discounted_rewards[t] = cumulative\n",
    "        # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "        # Advantage Actor Critic\n",
    "\n",
    "        # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "        # for t in range(len(rewards)-1):\n",
    "        #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "        \n",
    "        # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "        # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "        #     = Q_t - V_t\n",
    "        \n",
    "        # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "        # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "        # Critic_loss = A_t.pow(2) *0.5\n",
    "        # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "        # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "        \n",
    "        #update actor critic\n",
    "        \n",
    "        # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        advantage = discounted_rewards - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "        entropy_term /= STEPS\n",
    "        entropy_loss = -0.1 * entropy_term\n",
    "        ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "        test_episode_reward.append(float(episode_reward))\n",
    "        test_steps.append(STEPS)\n",
    "        test_actor_loss.append(float(actor_loss))\n",
    "        test_critic_loss.append(float(critic_loss))\n",
    "        test_entropy_loss.append(float(entropy_loss))\n",
    "        test_total_loss.append(float(ac_loss))\n",
    "\n",
    "        if test_episode >= 200:\n",
    "            avg_score = np.mean(test_episode_reward[-200:])\n",
    "            test_average_reward.append(avg_score)\n",
    "            print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                .format(episode + 1, TEST_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "        else:  print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                .format(episode + 1, TEST_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "    return test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-30T16:48:34.640Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "---prism---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\transformers_mlagents\\lib\\site-packages\\ipykernel_launcher.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1/129000  | Episode Reward: 10  | Actor loss: 0.40 | Critic loss: 5.58 | Entropy loss: -0.0002  | Total Loss: 5.98 | Total Steps: 6\n",
      "Model has been saved\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\transformers_mlagents\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.26 | Entropy loss: -0.0006  | Total Loss: 0.26 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 4  | Actor loss: 0.06 | Critic loss: 1.19 | Entropy loss: -0.0134  | Total Loss: 1.24 | Total Steps: 49\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 3.67 | Entropy loss: -0.0008  | Total Loss: 3.68 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 11.45 | Entropy loss: -0.0008  | Total Loss: 11.46 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 5/100  | Episode Reward: -118  | Actor loss: -0.02 | Critic loss: 157.24 | Entropy loss: -0.0072  | Total Loss: 157.21 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 6/100  | Episode Reward: -115  | Actor loss: -0.11 | Critic loss: 99.61 | Entropy loss: -0.0124  | Total Loss: 99.48 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 9.74 | Entropy loss: -0.0009  | Total Loss: 9.75 | Total Steps: 53\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 7  | Actor loss: 0.85 | Critic loss: 3.33 | Entropy loss: -0.0154  | Total Loss: 4.17 | Total Steps: 38\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 0.92 | Entropy loss: -0.0010  | Total Loss: 0.92 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: -8  | Actor loss: 1.40 | Critic loss: 11.50 | Entropy loss: -0.0108  | Total Loss: 12.89 | Total Steps: 96\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 11.05 | Entropy loss: -0.0005  | Total Loss: 11.06 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 2.34 | Entropy loss: -0.0006  | Total Loss: 2.34 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 2.87 | Entropy loss: -0.0012  | Total Loss: 2.87 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: -41  | Actor loss: 0.15 | Critic loss: 4.96 | Entropy loss: -0.0078  | Total Loss: 5.11 | Total Steps: 223\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 1  | Actor loss: 0.03 | Critic loss: 1.38 | Entropy loss: -0.0081  | Total Loss: 1.41 | Total Steps: 54\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 0.95 | Entropy loss: -0.0008  | Total Loss: 0.96 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: -44  | Actor loss: 0.36 | Critic loss: 5.66 | Entropy loss: -0.0081  | Total Loss: 6.01 | Total Steps: 262\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 1  | Actor loss: 0.09 | Critic loss: 11.24 | Entropy loss: -0.0139  | Total Loss: 11.32 | Total Steps: 57\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 2.45 | Entropy loss: -0.0011  | Total Loss: 2.46 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 11.91 | Entropy loss: -0.0005  | Total Loss: 11.92 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: -50  | Actor loss: 2.45 | Critic loss: 2.61 | Entropy loss: -0.0098  | Total Loss: 5.05 | Total Steps: 255\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 22/100  | Episode Reward: -121  | Actor loss: -0.09 | Critic loss: 173.38 | Entropy loss: -0.0147  | Total Loss: 173.28 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 1.09 | Entropy loss: -0.0126  | Total Loss: 1.09 | Total Steps: 45\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 16.00 | Entropy loss: -0.0689  | Total Loss: 15.94 | Total Steps: 7\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 4  | Actor loss: 0.24 | Critic loss: 12.73 | Entropy loss: -0.0153  | Total Loss: 12.96 | Total Steps: 49\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 14.39 | Entropy loss: -0.0032  | Total Loss: 14.43 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 13.00 | Entropy loss: -0.0007  | Total Loss: 13.01 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 11.24 | Entropy loss: -0.0006  | Total Loss: 11.25 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 1  | Actor loss: 0.49 | Critic loss: 8.79 | Entropy loss: -0.0065  | Total Loss: 9.27 | Total Steps: 59\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 1.08 | Entropy loss: -0.0057  | Total Loss: 1.10 | Total Steps: 50\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: -17  | Actor loss: 0.88 | Critic loss: 7.22 | Entropy loss: -0.0158  | Total Loss: 8.08 | Total Steps: 131\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.47 | Entropy loss: -0.0034  | Total Loss: 0.48 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 33/100  | Episode Reward: -10  | Actor loss: -0.00 | Critic loss: 73.61 | Entropy loss: -0.0002  | Total Loss: 73.60 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 6.55 | Entropy loss: -0.0005  | Total Loss: 6.55 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Actor loss: 0.07 | Critic loss: 17.43 | Entropy loss: -0.0018  | Total Loss: 17.50 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 36/100  | Episode Reward: -121  | Actor loss: -0.05 | Critic loss: 130.08 | Entropy loss: -0.0050  | Total Loss: 130.03 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.34 | Entropy loss: -0.0013  | Total Loss: 0.35 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 4  | Actor loss: 0.16 | Critic loss: 1.44 | Entropy loss: -0.0114  | Total Loss: 1.59 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.28 | Entropy loss: -0.0025  | Total Loss: 0.28 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 1.77 | Entropy loss: -0.0106  | Total Loss: 1.76 | Total Steps: 48\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 2.21 | Entropy loss: -0.0009  | Total Loss: 2.21 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 10.97 | Entropy loss: -0.0062  | Total Loss: 10.98 | Total Steps: 53\n",
      "TEST: ---sphere---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 43/100  | Episode Reward: -13  | Actor loss: -0.00 | Critic loss: 80.23 | Entropy loss: -0.0022  | Total Loss: 80.23 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 4  | Actor loss: 0.18 | Critic loss: 3.33 | Entropy loss: -0.0007  | Total Loss: 3.51 | Total Steps: 47\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 45/100  | Episode Reward: -118  | Actor loss: -0.02 | Critic loss: 83.81 | Entropy loss: -0.0064  | Total Loss: 83.78 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 10.42 | Entropy loss: -0.0014  | Total Loss: 10.42 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 1.74 | Entropy loss: -0.0019  | Total Loss: 1.75 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 48/100  | Episode Reward: -118  | Actor loss: -0.10 | Critic loss: 119.81 | Entropy loss: -0.0080  | Total Loss: 119.71 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 2.13 | Entropy loss: -0.0021  | Total Loss: 2.15 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.27 | Entropy loss: -0.0009  | Total Loss: 0.27 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 8.41 | Entropy loss: -0.0005  | Total Loss: 8.41 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: -20  | Actor loss: 0.97 | Critic loss: 11.73 | Entropy loss: -0.0081  | Total Loss: 12.70 | Total Steps: 144\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: -2  | Actor loss: 0.01 | Critic loss: 8.86 | Entropy loss: -0.0079  | Total Loss: 8.87 | Total Steps: 73\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 9.01 | Entropy loss: -0.0005  | Total Loss: 9.02 | Total Steps: 49\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Actor loss: 0.02 | Critic loss: 12.84 | Entropy loss: -0.0016  | Total Loss: 12.85 | Total Steps: 34\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 13.00 | Entropy loss: -0.0006  | Total Loss: 13.01 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 18.95 | Entropy loss: -0.0011  | Total Loss: 18.97 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 10.37 | Entropy loss: -0.0018  | Total Loss: 10.38 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: -113  | Actor loss: 0.22 | Critic loss: 11.47 | Entropy loss: -0.0124  | Total Loss: 11.68 | Total Steps: 461\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 5.82 | Entropy loss: -0.0012  | Total Loss: 5.82 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 5.01 | Entropy loss: -0.0006  | Total Loss: 5.02 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0006  | Total Loss: 0.97 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 10.90 | Entropy loss: -0.0008  | Total Loss: 10.91 | Total Steps: 53\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 9.89 | Entropy loss: -0.0006  | Total Loss: 9.90 | Total Steps: 49\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 65/100  | Episode Reward: -124  | Actor loss: -0.04 | Critic loss: 95.57 | Entropy loss: -0.0144  | Total Loss: 95.51 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 1  | Actor loss: 1.76 | Critic loss: 9.93 | Entropy loss: -0.0057  | Total Loss: 11.68 | Total Steps: 60\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 1  | Actor loss: 0.11 | Critic loss: 3.68 | Entropy loss: -0.0158  | Total Loss: 3.77 | Total Steps: 54\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 68/100  | Episode Reward: -124  | Actor loss: -0.01 | Critic loss: 135.53 | Entropy loss: -0.0054  | Total Loss: 135.52 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.34 | Entropy loss: -0.0014  | Total Loss: 0.34 | Total Steps: 6\n",
      "TEST: ---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 1  | Actor loss: 2.21 | Critic loss: 8.75 | Entropy loss: -0.0105  | Total Loss: 10.94 | Total Steps: 51\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 5.96 | Entropy loss: -0.0021  | Total Loss: 5.96 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 7.17 | Entropy loss: -0.0008  | Total Loss: 7.18 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: -74  | Actor loss: 1.15 | Critic loss: 7.79 | Entropy loss: -0.0082  | Total Loss: 8.94 | Total Steps: 447\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 6.75 | Entropy loss: -0.0013  | Total Loss: 6.75 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 2.12 | Entropy loss: -0.0080  | Total Loss: 2.12 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Actor loss: 1.76 | Critic loss: 15.69 | Entropy loss: -0.0013  | Total Loss: 17.46 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 77/100  | Episode Reward: -10  | Actor loss: -0.00 | Critic loss: 76.63 | Entropy loss: -0.0001  | Total Loss: 76.63 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Actor loss: 0.06 | Critic loss: 16.22 | Entropy loss: -0.0020  | Total Loss: 16.28 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 11.72 | Entropy loss: -0.0003  | Total Loss: 11.73 | Total Steps: 34\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 4  | Actor loss: 0.57 | Critic loss: 5.61 | Entropy loss: -0.0219  | Total Loss: 6.16 | Total Steps: 46\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 13.11 | Entropy loss: -0.0014  | Total Loss: 13.12 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 5.65 | Entropy loss: -0.0008  | Total Loss: 5.65 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 4  | Actor loss: 0.02 | Critic loss: 2.58 | Entropy loss: -0.0189  | Total Loss: 2.58 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 0.50 | Entropy loss: -0.0039  | Total Loss: 0.51 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 1.57 | Entropy loss: -0.0093  | Total Loss: 1.57 | Total Steps: 73\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 86/100  | Episode Reward: -11  | Actor loss: -0.00 | Critic loss: 74.10 | Entropy loss: -0.0022  | Total Loss: 74.10 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 19.24 | Entropy loss: -0.0015  | Total Loss: 19.27 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 13.11 | Entropy loss: -0.0007  | Total Loss: 13.13 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 4  | Actor loss: 0.05 | Critic loss: 1.16 | Entropy loss: -0.0034  | Total Loss: 1.21 | Total Steps: 79\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 7  | Actor loss: 0.02 | Critic loss: 12.71 | Entropy loss: -0.0004  | Total Loss: 12.73 | Total Steps: 36\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 17.41 | Entropy loss: -0.0014  | Total Loss: 17.46 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.51 | Entropy loss: -0.0010  | Total Loss: 0.52 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 4.76 | Entropy loss: -0.0232  | Total Loss: 4.75 | Total Steps: 49\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.34 | Entropy loss: -0.0044  | Total Loss: 0.34 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 10.92 | Entropy loss: -0.0007  | Total Loss: 10.93 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Actor loss: 2.11 | Critic loss: 15.82 | Entropy loss: -0.0017  | Total Loss: 17.93 | Total Steps: 47\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 11.98 | Entropy loss: -0.0006  | Total Loss: 11.99 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 6.04 | Entropy loss: -0.0011  | Total Loss: 6.05 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 1  | Actor loss: 0.07 | Critic loss: 12.25 | Entropy loss: -0.0008  | Total Loss: 12.32 | Total Steps: 50\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Actor loss: 0.17 | Critic loss: 16.59 | Entropy loss: -0.0108  | Total Loss: 16.75 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2/129000  | Episode Reward: 4  | Actor loss: -0.13 | Critic loss: 10.94 | Entropy loss: -0.0005  | Total Loss: 10.82 | Total Steps: 47\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 3/129000  | Episode Reward: -2  | Actor loss: -0.03 | Critic loss: 16.82 | Entropy loss: -0.0013  | Total Loss: 16.78 | Total Steps: 71\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 4/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.86 | Entropy loss: -0.0000  | Total Loss: 1.87 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 5/129000  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 6.00 | Entropy loss: -0.0000  | Total Loss: 6.00 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 6/129000  | Episode Reward: 7  | Actor loss: 0.03 | Critic loss: 7.07 | Entropy loss: -0.0001  | Total Loss: 7.09 | Total Steps: 29\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 7/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 3.32 | Entropy loss: -0.0000  | Total Loss: 3.33 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 8/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.60 | Entropy loss: -0.0000  | Total Loss: 0.61 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 9/129000  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 7.67 | Entropy loss: -0.0001  | Total Loss: 7.68 | Total Steps: 34\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 10/129000  | Episode Reward: 4  | Actor loss: -0.01 | Critic loss: 6.57 | Entropy loss: -0.0024  | Total Loss: 6.56 | Total Steps: 44\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 11/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.91 | Entropy loss: -0.0000  | Total Loss: 1.92 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 12/129000  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 9.26 | Entropy loss: -0.0002  | Total Loss: 9.27 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 13/129000  | Episode Reward: 4  | Actor loss: -0.12 | Critic loss: 9.69 | Entropy loss: -0.0006  | Total Loss: 9.57 | Total Steps: 47\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 14/129000  | Episode Reward: 4  | Actor loss: -0.40 | Critic loss: 11.93 | Entropy loss: -0.0021  | Total Loss: 11.53 | Total Steps: 54\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 15/129000  | Episode Reward: 10  | Actor loss: 0.27 | Critic loss: 6.12 | Entropy loss: -0.0002  | Total Loss: 6.38 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 16/129000  | Episode Reward: 10  | Actor loss: 0.08 | Critic loss: 5.72 | Entropy loss: -0.0003  | Total Loss: 5.80 | Total Steps: 30\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 17/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.87 | Entropy loss: -0.0000  | Total Loss: 1.87 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 18/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 19/129000  | Episode Reward: 7  | Actor loss: 0.02 | Critic loss: 8.62 | Entropy loss: -0.0002  | Total Loss: 8.64 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 20/129000  | Episode Reward: 7  | Actor loss: -0.01 | Critic loss: 3.58 | Entropy loss: -0.0001  | Total Loss: 3.57 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 21/129000  | Episode Reward: 4  | Actor loss: -0.05 | Critic loss: 11.20 | Entropy loss: -0.0012  | Total Loss: 11.15 | Total Steps: 45\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 22/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.37 | Entropy loss: -0.0000  | Total Loss: 0.37 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 23/129000  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 5.21 | Entropy loss: -0.0002  | Total Loss: 5.22 | Total Steps: 30\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 24/129000  | Episode Reward: 4  | Actor loss: -0.03 | Critic loss: 9.03 | Entropy loss: -0.0003  | Total Loss: 9.00 | Total Steps: 43\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 25/129000  | Episode Reward: 7  | Actor loss: -0.12 | Critic loss: 5.43 | Entropy loss: -0.0019  | Total Loss: 5.31 | Total Steps: 53\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 26/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0000  | Total Loss: 0.74 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 27/129000  | Episode Reward: 4  | Actor loss: -0.11 | Critic loss: 6.91 | Entropy loss: -0.0015  | Total Loss: 6.80 | Total Steps: 45\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 28/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0000  | Total Loss: 1.67 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 29/129000  | Episode Reward: 7  | Actor loss: 0.08 | Critic loss: 3.82 | Entropy loss: -0.0006  | Total Loss: 3.90 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 30/129000  | Episode Reward: 4  | Actor loss: -0.00 | Critic loss: 7.34 | Entropy loss: -0.0001  | Total Loss: 7.33 | Total Steps: 46\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 31/129000  | Episode Reward: 10  | Actor loss: 0.35 | Critic loss: 4.62 | Entropy loss: -0.0014  | Total Loss: 4.96 | Total Steps: 31\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 32/129000  | Episode Reward: 7  | Actor loss: -0.03 | Critic loss: 6.24 | Entropy loss: -0.0004  | Total Loss: 6.21 | Total Steps: 30\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 33/129000  | Episode Reward: -8  | Actor loss: -0.39 | Critic loss: 29.18 | Entropy loss: -0.0024  | Total Loss: 28.79 | Total Steps: 86\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 34/129000  | Episode Reward: 7  | Actor loss: -0.00 | Critic loss: 3.01 | Entropy loss: -0.0001  | Total Loss: 3.01 | Total Steps: 42\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 35/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 4.76 | Entropy loss: -0.0000  | Total Loss: 4.77 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 36/129000  | Episode Reward: 1  | Actor loss: -0.12 | Critic loss: 10.78 | Entropy loss: -0.0012  | Total Loss: 10.66 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 37/129000  | Episode Reward: 4  | Actor loss: -0.08 | Critic loss: 5.35 | Entropy loss: -0.0032  | Total Loss: 5.26 | Total Steps: 233\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 38/129000  | Episode Reward: 7  | Actor loss: 0.03 | Critic loss: 8.52 | Entropy loss: -0.0001  | Total Loss: 8.54 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 39/129000  | Episode Reward: 7  | Actor loss: -0.09 | Critic loss: 4.76 | Entropy loss: -0.0030  | Total Loss: 4.67 | Total Steps: 92\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 40/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 6.80 | Entropy loss: -0.0000  | Total Loss: 6.81 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 41/129000  | Episode Reward: 7  | Actor loss: 0.05 | Critic loss: 4.87 | Entropy loss: -0.0005  | Total Loss: 4.92 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 42/129000  | Episode Reward: 1  | Actor loss: -0.05 | Critic loss: 12.83 | Entropy loss: -0.0004  | Total Loss: 12.78 | Total Steps: 53\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 43/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.40 | Entropy loss: -0.0000  | Total Loss: 1.41 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 44/129000  | Episode Reward: 7  | Actor loss: 0.20 | Critic loss: 5.79 | Entropy loss: -0.0013  | Total Loss: 5.98 | Total Steps: 36\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 45/129000  | Episode Reward: 7  | Actor loss: -0.02 | Critic loss: 0.97 | Entropy loss: -0.0004  | Total Loss: 0.95 | Total Steps: 38\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 46/129000  | Episode Reward: -14  | Actor loss: -0.61 | Critic loss: 21.63 | Entropy loss: -0.0043  | Total Loss: 21.02 | Total Steps: 115\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Step: 250\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 47/129000  | Episode Reward: -46  | Actor loss: -0.44 | Critic loss: 16.49 | Entropy loss: -0.0084  | Total Loss: 16.04 | Total Steps: 500\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 48/129000  | Episode Reward: 4  | Actor loss: -0.11 | Critic loss: 4.72 | Entropy loss: -0.0018  | Total Loss: 4.61 | Total Steps: 79\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 49/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 14.02 | Entropy loss: -0.0000  | Total Loss: 14.04 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 50/129000  | Episode Reward: 10  | Actor loss: 0.09 | Critic loss: 6.99 | Entropy loss: -0.0002  | Total Loss: 7.07 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 51/129000  | Episode Reward: 4  | Actor loss: -0.04 | Critic loss: 4.21 | Entropy loss: -0.0007  | Total Loss: 4.16 | Total Steps: 54\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 52/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.18 | Entropy loss: -0.0000  | Total Loss: 1.18 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 53/129000  | Episode Reward: 7  | Actor loss: 0.14 | Critic loss: 5.34 | Entropy loss: -0.0015  | Total Loss: 5.48 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 54/129000  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 7.87 | Entropy loss: -0.0005  | Total Loss: 7.88 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 55/129000  | Episode Reward: -14  | Actor loss: -0.26 | Critic loss: 19.54 | Entropy loss: -0.0037  | Total Loss: 19.28 | Total Steps: 129\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 56/129000  | Episode Reward: 7  | Actor loss: 0.14 | Critic loss: 7.57 | Entropy loss: -0.0004  | Total Loss: 7.70 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 57/129000  | Episode Reward: 7  | Actor loss: -0.00 | Critic loss: 1.94 | Entropy loss: -0.0001  | Total Loss: 1.94 | Total Steps: 38\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 58/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 13.39 | Entropy loss: -0.0000  | Total Loss: 13.41 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 59/129000  | Episode Reward: 4  | Actor loss: -0.24 | Critic loss: 9.10 | Entropy loss: -0.0013  | Total Loss: 8.86 | Total Steps: 44\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 60/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.72 | Entropy loss: -0.0000  | Total Loss: 1.73 | Total Steps: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 61/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.11 | Entropy loss: -0.0000  | Total Loss: 1.12 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 62/129000  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 4.51 | Entropy loss: -0.0001  | Total Loss: 4.53 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 63/129000  | Episode Reward: 4  | Actor loss: 0.30 | Critic loss: 10.35 | Entropy loss: -0.0069  | Total Loss: 10.64 | Total Steps: 58\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 64/129000  | Episode Reward: 7  | Actor loss: 0.06 | Critic loss: 5.12 | Entropy loss: -0.0003  | Total Loss: 5.18 | Total Steps: 29\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 65/129000  | Episode Reward: 10  | Actor loss: 0.41 | Critic loss: 3.74 | Entropy loss: -0.0036  | Total Loss: 4.15 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 66/129000  | Episode Reward: 1  | Actor loss: -0.91 | Critic loss: 8.62 | Entropy loss: -0.0040  | Total Loss: 7.70 | Total Steps: 55\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 67/129000  | Episode Reward: -11  | Actor loss: -0.88 | Critic loss: 17.91 | Entropy loss: -0.0110  | Total Loss: 17.02 | Total Steps: 133\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 68/129000  | Episode Reward: 7  | Actor loss: 0.06 | Critic loss: 6.02 | Entropy loss: -0.0003  | Total Loss: 6.07 | Total Steps: 30\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 69/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 11.75 | Entropy loss: -0.0000  | Total Loss: 11.76 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 70/129000  | Episode Reward: 4  | Actor loss: -0.12 | Critic loss: 5.85 | Entropy loss: -0.0011  | Total Loss: 5.74 | Total Steps: 53\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 71/129000  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 14.41 | Entropy loss: -0.0000  | Total Loss: 14.46 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 72/129000  | Episode Reward: 4  | Actor loss: -0.05 | Critic loss: 9.24 | Entropy loss: -0.0003  | Total Loss: 9.20 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 73/129000  | Episode Reward: 7  | Actor loss: 0.11 | Critic loss: 7.02 | Entropy loss: -0.0003  | Total Loss: 7.12 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 74/129000  | Episode Reward: 7  | Actor loss: 0.02 | Critic loss: 8.42 | Entropy loss: -0.0001  | Total Loss: 8.44 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 75/129000  | Episode Reward: -5  | Actor loss: -2.16 | Critic loss: 18.37 | Entropy loss: -0.0077  | Total Loss: 16.20 | Total Steps: 74\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 76/129000  | Episode Reward: 10  | Actor loss: 0.09 | Critic loss: 3.82 | Entropy loss: -0.0005  | Total Loss: 3.91 | Total Steps: 34\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 77/129000  | Episode Reward: 7  | Actor loss: 0.50 | Critic loss: 3.38 | Entropy loss: -0.0027  | Total Loss: 3.88 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 78/129000  | Episode Reward: 4  | Actor loss: -0.02 | Critic loss: 3.86 | Entropy loss: -0.0002  | Total Loss: 3.84 | Total Steps: 42\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 79/129000  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 13.36 | Entropy loss: -0.0000  | Total Loss: 13.38 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 80/129000  | Episode Reward: 4  | Actor loss: 0.10 | Critic loss: 7.05 | Entropy loss: -0.0018  | Total Loss: 7.15 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 81/129000  | Episode Reward: 4  | Actor loss: -0.07 | Critic loss: 6.81 | Entropy loss: -0.0003  | Total Loss: 6.75 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 82/129000  | Episode Reward: 1  | Actor loss: -0.32 | Critic loss: 11.01 | Entropy loss: -0.0020  | Total Loss: 10.68 | Total Steps: 75\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 83/129000  | Episode Reward: 10  | Actor loss: 0.82 | Critic loss: 3.55 | Entropy loss: -0.0016  | Total Loss: 4.37 | Total Steps: 13\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 84/129000  | Episode Reward: 7  | Actor loss: -0.05 | Critic loss: 0.87 | Entropy loss: -0.0008  | Total Loss: 0.82 | Total Steps: 38\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 85/129000  | Episode Reward: -17  | Actor loss: -1.23 | Critic loss: 23.72 | Entropy loss: -0.0099  | Total Loss: 22.48 | Total Steps: 137\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 86/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.58 | Entropy loss: -0.0000  | Total Loss: 1.58 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 87/129000  | Episode Reward: 4  | Actor loss: -0.04 | Critic loss: 4.32 | Entropy loss: -0.0003  | Total Loss: 4.28 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 88/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 89/129000  | Episode Reward: 7  | Actor loss: -0.03 | Critic loss: 4.66 | Entropy loss: -0.0003  | Total Loss: 4.63 | Total Steps: 42\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 90/129000  | Episode Reward: 1  | Actor loss: -0.38 | Critic loss: 6.52 | Entropy loss: -0.0044  | Total Loss: 6.13 | Total Steps: 51\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 91/129000  | Episode Reward: 1  | Actor loss: -0.17 | Critic loss: 9.61 | Entropy loss: -0.0008  | Total Loss: 9.44 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 92/129000  | Episode Reward: 7  | Actor loss: 0.09 | Critic loss: 5.31 | Entropy loss: -0.0004  | Total Loss: 5.40 | Total Steps: 30\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 93/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 2.08 | Entropy loss: -0.0000  | Total Loss: 2.08 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 94/129000  | Episode Reward: 4  | Actor loss: -0.03 | Critic loss: 8.46 | Entropy loss: -0.0023  | Total Loss: 8.43 | Total Steps: 44\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 95/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.62 | Entropy loss: -0.0000  | Total Loss: 1.63 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 96/129000  | Episode Reward: 4  | Actor loss: -0.01 | Critic loss: 5.80 | Entropy loss: -0.0004  | Total Loss: 5.78 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 97/129000  | Episode Reward: 7  | Actor loss: 0.06 | Critic loss: 4.39 | Entropy loss: -0.0038  | Total Loss: 4.45 | Total Steps: 50\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 98/129000  | Episode Reward: 4  | Actor loss: 0.25 | Critic loss: 6.38 | Entropy loss: -0.0050  | Total Loss: 6.62 | Total Steps: 37\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 99/129000  | Episode Reward: 7  | Actor loss: 0.17 | Critic loss: 4.59 | Entropy loss: -0.0047  | Total Loss: 4.76 | Total Steps: 45\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 100/129000  | Episode Reward: 1  | Actor loss: -0.16 | Critic loss: 9.22 | Entropy loss: -0.0010  | Total Loss: 9.06 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 101/129000  | Episode Reward: 7  | Actor loss: 0.05 | Critic loss: 5.25 | Entropy loss: -0.0003  | Total Loss: 5.31 | Total Steps: 30\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 3.25 | Entropy loss: -0.0028  | Total Loss: 3.25 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 8.43 | Entropy loss: -0.0007  | Total Loss: 8.43 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 1  | Actor loss: 0.00 | Critic loss: 3.50 | Entropy loss: -0.0417  | Total Loss: 3.46 | Total Steps: 59\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: -44  | Actor loss: 0.41 | Critic loss: 15.27 | Entropy loss: -0.0373  | Total Loss: 15.64 | Total Steps: 317\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 10  | Actor loss: 0.54 | Critic loss: 5.37 | Entropy loss: -0.0441  | Total Loss: 5.87 | Total Steps: 10\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 1  | Actor loss: 0.03 | Critic loss: 6.24 | Entropy loss: -0.0344  | Total Loss: 6.24 | Total Steps: 64\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 13.97 | Entropy loss: -0.0021  | Total Loss: 13.98 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 2.62 | Entropy loss: -0.0112  | Total Loss: 2.62 | Total Steps: 45\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Actor loss: 0.67 | Critic loss: 11.65 | Entropy loss: -0.0054  | Total Loss: 12.32 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Actor loss: 0.23 | Critic loss: 8.36 | Entropy loss: -0.0155  | Total Loss: 8.58 | Total Steps: 41\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 14.62 | Entropy loss: -0.0014  | Total Loss: 14.62 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Actor loss: 0.08 | Critic loss: 6.07 | Entropy loss: -0.0109  | Total Loss: 6.14 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 4  | Actor loss: 0.04 | Critic loss: 1.94 | Entropy loss: -0.0466  | Total Loss: 1.94 | Total Steps: 61\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 2.96 | Entropy loss: -0.0037  | Total Loss: 2.97 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 4  | Actor loss: 0.02 | Critic loss: 2.88 | Entropy loss: -0.0087  | Total Loss: 2.89 | Total Steps: 45\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Actor loss: 0.52 | Critic loss: 10.48 | Entropy loss: -0.0162  | Total Loss: 10.99 | Total Steps: 12\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 4  | Actor loss: 0.03 | Critic loss: 4.96 | Entropy loss: -0.0151  | Total Loss: 4.98 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 14.66 | Entropy loss: -0.0014  | Total Loss: 14.67 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 6.00 | Entropy loss: -0.0085  | Total Loss: 5.99 | Total Steps: 29\n",
      "TEST: ---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 0  | Actor loss: 0.02 | Critic loss: 1.80 | Entropy loss: -0.0372  | Total Loss: 1.79 | Total Steps: 64\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Actor loss: 0.20 | Critic loss: 11.45 | Entropy loss: -0.0084  | Total Loss: 11.64 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 3.37 | Entropy loss: -0.0096  | Total Loss: 3.37 | Total Steps: 45\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.50 | Entropy loss: -0.0021  | Total Loss: 1.51 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Actor loss: 0.06 | Critic loss: 4.67 | Entropy loss: -0.0030  | Total Loss: 4.72 | Total Steps: 48\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 7.64 | Entropy loss: -0.0093  | Total Loss: 7.64 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 5.95 | Entropy loss: -0.0014  | Total Loss: 5.96 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 4.43 | Entropy loss: -0.0047  | Total Loss: 4.43 | Total Steps: 44\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 10  | Actor loss: 0.67 | Critic loss: 11.65 | Entropy loss: -0.0043  | Total Loss: 12.32 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: -11  | Actor loss: 0.47 | Critic loss: 5.36 | Entropy loss: -0.0069  | Total Loss: 5.83 | Total Steps: 126\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 9  | Actor loss: 0.02 | Critic loss: 1.66 | Entropy loss: -0.0324  | Total Loss: 1.64 | Total Steps: 65\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.65 | Entropy loss: -0.0065  | Total Loss: 1.66 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 4  | Actor loss: 0.00 | Critic loss: 4.41 | Entropy loss: -0.0044  | Total Loss: 4.41 | Total Steps: 289\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.87 | Entropy loss: -0.0659  | Total Loss: 1.81 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.64 | Entropy loss: -0.0115  | Total Loss: 1.63 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: -8  | Actor loss: 6.32 | Critic loss: 15.88 | Entropy loss: -0.0437  | Total Loss: 22.15 | Total Steps: 302\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 10.91 | Entropy loss: -0.0082  | Total Loss: 10.93 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 1  | Actor loss: 0.00 | Critic loss: 3.29 | Entropy loss: -0.0041  | Total Loss: 3.29 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 8.57 | Entropy loss: -0.0099  | Total Loss: 8.59 | Total Steps: 36\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 5.62 | Entropy loss: -0.0077  | Total Loss: 5.62 | Total Steps: 34\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 14.47 | Entropy loss: -0.0011  | Total Loss: 14.47 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 1  | Actor loss: 0.43 | Critic loss: 3.92 | Entropy loss: -0.0333  | Total Loss: 4.32 | Total Steps: 56\n",
      "TEST: ---black---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 42/100  | Episode Reward: -10  | Actor loss: -0.00 | Critic loss: 75.02 | Entropy loss: -0.0002  | Total Loss: 75.01 | Total Steps: 500\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 1  | Actor loss: 0.06 | Critic loss: 5.72 | Entropy loss: -0.0101  | Total Loss: 5.76 | Total Steps: 51\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.30 | Entropy loss: -0.0454  | Total Loss: 1.28 | Total Steps: 10\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 2.71 | Entropy loss: -0.0081  | Total Loss: 2.71 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 1  | Actor loss: 0.06 | Critic loss: 4.11 | Entropy loss: -0.0010  | Total Loss: 4.17 | Total Steps: 50\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Actor loss: 5.89 | Critic loss: 5.64 | Entropy loss: -0.0430  | Total Loss: 11.49 | Total Steps: 102\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 1  | Actor loss: 0.00 | Critic loss: 7.11 | Entropy loss: -0.0017  | Total Loss: 7.11 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 10.72 | Entropy loss: -0.0012  | Total Loss: 10.74 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 2.57 | Entropy loss: -0.0007  | Total Loss: 2.57 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 5  | Actor loss: 0.10 | Critic loss: 5.44 | Entropy loss: -0.0299  | Total Loss: 5.51 | Total Steps: 70\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 5.22 | Entropy loss: -0.0021  | Total Loss: 5.23 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 0  | Actor loss: 0.02 | Critic loss: 2.57 | Entropy loss: -0.0179  | Total Loss: 2.57 | Total Steps: 55\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 3.45 | Entropy loss: -0.0079  | Total Loss: 3.45 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 5.95 | Entropy loss: -0.0013  | Total Loss: 5.96 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.85 | Entropy loss: -0.0010  | Total Loss: 1.85 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 5.27 | Entropy loss: -0.0340  | Total Loss: 5.27 | Total Steps: 9\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Actor loss: 0.67 | Critic loss: 11.65 | Entropy loss: -0.0092  | Total Loss: 12.31 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 3.48 | Entropy loss: -0.0494  | Total Loss: 3.43 | Total Steps: 12\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 3.03 | Entropy loss: -0.0086  | Total Loss: 3.03 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: -8  | Actor loss: 0.01 | Critic loss: 3.74 | Entropy loss: -0.0109  | Total Loss: 3.74 | Total Steps: 87\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 1  | Actor loss: 0.08 | Critic loss: 7.05 | Entropy loss: -0.0197  | Total Loss: 7.10 | Total Steps: 56\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 1  | Actor loss: 0.38 | Critic loss: 12.25 | Entropy loss: -0.0308  | Total Loss: 12.60 | Total Steps: 62\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: -11  | Actor loss: 7.08 | Critic loss: 8.46 | Entropy loss: -0.0401  | Total Loss: 15.51 | Total Steps: 280\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 10  | Actor loss: 0.20 | Critic loss: 11.45 | Entropy loss: -0.0089  | Total Loss: 11.64 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 1  | Actor loss: 0.06 | Critic loss: 4.09 | Entropy loss: -0.0010  | Total Loss: 4.15 | Total Steps: 50\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 67/100  | Episode Reward: -70  | Actor loss: -0.54 | Critic loss: 80.21 | Entropy loss: -0.0384  | Total Loss: 79.63 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 15.88 | Entropy loss: -0.0762  | Total Loss: 15.81 | Total Steps: 7\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 5  | Actor loss: 0.04 | Critic loss: 2.57 | Entropy loss: -0.0434  | Total Loss: 2.57 | Total Steps: 72\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 3.04 | Entropy loss: -0.0076  | Total Loss: 3.04 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 1  | Actor loss: 0.02 | Critic loss: 2.36 | Entropy loss: -0.0104  | Total Loss: 2.37 | Total Steps: 53\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 6  | Actor loss: 0.02 | Critic loss: 2.04 | Entropy loss: -0.0450  | Total Loss: 2.01 | Total Steps: 66\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 4  | Actor loss: 0.30 | Critic loss: 6.68 | Entropy loss: -0.0061  | Total Loss: 6.98 | Total Steps: 44\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 14.48 | Entropy loss: -0.0010  | Total Loss: 14.49 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 1  | Actor loss: 0.41 | Critic loss: 19.35 | Entropy loss: -0.0193  | Total Loss: 19.74 | Total Steps: 51\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Actor loss: 0.02 | Critic loss: 7.03 | Entropy loss: -0.0077  | Total Loss: 7.05 | Total Steps: 49\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 2.12 | Entropy loss: -0.0016  | Total Loss: 2.13 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 7  | Actor loss: 0.61 | Critic loss: 25.31 | Entropy loss: -0.0080  | Total Loss: 25.91 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 0  | Actor loss: 0.03 | Critic loss: 1.99 | Entropy loss: -0.0360  | Total Loss: 1.98 | Total Steps: 71\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 6.96 | Entropy loss: -0.0132  | Total Loss: 6.95 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: -2  | Actor loss: 0.05 | Critic loss: 2.67 | Entropy loss: -0.0183  | Total Loss: 2.70 | Total Steps: 62\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 14.11 | Entropy loss: -0.0015  | Total Loss: 14.12 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.78 | Entropy loss: -0.0012  | Total Loss: 1.80 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 7  | Actor loss: 0.00 | Critic loss: 5.35 | Entropy loss: -0.0136  | Total Loss: 5.33 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 7  | Actor loss: 0.04 | Critic loss: 5.70 | Entropy loss: -0.0203  | Total Loss: 5.72 | Total Steps: 51\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 2.01 | Entropy loss: -0.0040  | Total Loss: 2.01 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 6.04 | Entropy loss: -0.0012  | Total Loss: 6.05 | Total Steps: 30\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 4  | Actor loss: 0.06 | Critic loss: 3.40 | Entropy loss: -0.0061  | Total Loss: 3.45 | Total Steps: 79\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 5.65 | Entropy loss: -0.0817  | Total Loss: 5.61 | Total Steps: 9\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.48 | Entropy loss: -0.0141  | Total Loss: 1.47 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.79 | Entropy loss: -0.0031  | Total Loss: 1.80 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 4  | Actor loss: 0.00 | Critic loss: 4.23 | Entropy loss: -0.0010  | Total Loss: 4.23 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 10.69 | Entropy loss: -0.0011  | Total Loss: 10.72 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: -5  | Actor loss: 0.22 | Critic loss: 5.64 | Entropy loss: -0.0543  | Total Loss: 5.80 | Total Steps: 79\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 4  | Actor loss: 0.04 | Critic loss: 11.72 | Entropy loss: -0.0093  | Total Loss: 11.75 | Total Steps: 49\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 5.21 | Entropy loss: -0.0014  | Total Loss: 5.22 | Total Steps: 30\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 4  | Actor loss: 0.00 | Critic loss: 2.87 | Entropy loss: -0.0035  | Total Loss: 2.87 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 5.99 | Entropy loss: -0.0096  | Total Loss: 6.03 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 1  | Actor loss: 0.01 | Critic loss: 4.09 | Entropy loss: -0.0036  | Total Loss: 4.09 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 1  | Average Reward -2.56  | Actor loss: 0.02 | Critic loss: 2.68 | Entropy loss: -0.0103  | Total Loss: 2.69 | Total Steps: 51\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 102/129000  | Episode Reward: 10  | Actor loss: 0.99 | Critic loss: 1.25 | Entropy loss: -0.0021  | Total Loss: 2.24 | Total Steps: 7\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 103/129000  | Episode Reward: 7  | Actor loss: -0.91 | Critic loss: 4.73 | Entropy loss: -0.0060  | Total Loss: 3.81 | Total Steps: 48\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 104/129000  | Episode Reward: 7  | Actor loss: 0.05 | Critic loss: 6.36 | Entropy loss: -0.0004  | Total Loss: 6.41 | Total Steps: 34\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 105/129000  | Episode Reward: 4  | Actor loss: 0.03 | Critic loss: 4.29 | Entropy loss: -0.0015  | Total Loss: 4.31 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 106/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.04 | Total Steps: 6\n",
      "---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 107/129000  | Episode Reward: 3  | Actor loss: -0.91 | Critic loss: 6.74 | Entropy loss: -0.0102  | Total Loss: 5.83 | Total Steps: 65\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 108/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.06 | Entropy loss: -0.0001  | Total Loss: 1.06 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 109/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.04 | Entropy loss: -0.0000  | Total Loss: 1.04 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 110/129000  | Episode Reward: 7  | Actor loss: -0.04 | Critic loss: 6.92 | Entropy loss: -0.0034  | Total Loss: 6.87 | Total Steps: 45\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 111/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 2.46 | Entropy loss: -0.0000  | Total Loss: 2.48 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 112/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 113/129000  | Episode Reward: 3  | Actor loss: -0.43 | Critic loss: 7.87 | Entropy loss: -0.0073  | Total Loss: 7.43 | Total Steps: 62\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 114/129000  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 1.35 | Entropy loss: -0.0001  | Total Loss: 1.40 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 115/129000  | Episode Reward: -2  | Actor loss: -1.36 | Critic loss: 10.78 | Entropy loss: -0.0161  | Total Loss: 9.40 | Total Steps: 104\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 116/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.81 | Entropy loss: -0.0002  | Total Loss: 1.83 | Total Steps: 34\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 117/129000  | Episode Reward: 4  | Actor loss: -0.45 | Critic loss: 6.84 | Entropy loss: -0.0017  | Total Loss: 6.39 | Total Steps: 52\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 118/129000  | Episode Reward: 4  | Actor loss: -0.09 | Critic loss: 8.30 | Entropy loss: -0.0044  | Total Loss: 8.20 | Total Steps: 46\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 119/129000  | Episode Reward: 4  | Actor loss: -0.62 | Critic loss: 6.58 | Entropy loss: -0.0067  | Total Loss: 5.96 | Total Steps: 51\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 120/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.68 | Entropy loss: -0.0000  | Total Loss: 1.69 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 121/129000  | Episode Reward: 10  | Actor loss: 0.14 | Critic loss: 0.88 | Entropy loss: -0.0002  | Total Loss: 1.02 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 122/129000  | Episode Reward: 0  | Actor loss: -1.07 | Critic loss: 7.77 | Entropy loss: -0.0060  | Total Loss: 6.69 | Total Steps: 58\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 123/129000  | Episode Reward: 6  | Actor loss: -0.58 | Critic loss: 4.44 | Entropy loss: -0.0144  | Total Loss: 3.84 | Total Steps: 73\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 124/129000  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 7.21 | Entropy loss: -0.0012  | Total Loss: 7.22 | Total Steps: 43\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 125/129000  | Episode Reward: 10  | Actor loss: 0.06 | Critic loss: 3.47 | Entropy loss: -0.0004  | Total Loss: 3.53 | Total Steps: 30\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 126/129000  | Episode Reward: 10  | Actor loss: 0.36 | Critic loss: 2.35 | Entropy loss: -0.0006  | Total Loss: 2.71 | Total Steps: 11\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 127/129000  | Episode Reward: 1  | Actor loss: -0.07 | Critic loss: 6.86 | Entropy loss: -0.0003  | Total Loss: 6.79 | Total Steps: 50\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 128/129000  | Episode Reward: 4  | Actor loss: 0.24 | Critic loss: 5.06 | Entropy loss: -0.0037  | Total Loss: 5.29 | Total Steps: 46\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 129/129000  | Episode Reward: 1  | Actor loss: -0.05 | Critic loss: 10.63 | Entropy loss: -0.0003  | Total Loss: 10.58 | Total Steps: 52\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 130/129000  | Episode Reward: 4  | Actor loss: -0.07 | Critic loss: 6.22 | Entropy loss: -0.0032  | Total Loss: 6.15 | Total Steps: 47\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 131/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.70 | Entropy loss: -0.0000  | Total Loss: 1.70 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 132/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.21 | Entropy loss: -0.0000  | Total Loss: 1.23 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 133/129000  | Episode Reward: 1  | Actor loss: -0.13 | Critic loss: 8.44 | Entropy loss: -0.0068  | Total Loss: 8.30 | Total Steps: 63\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 134/129000  | Episode Reward: 7  | Actor loss: 0.06 | Critic loss: 8.35 | Entropy loss: -0.0003  | Total Loss: 8.42 | Total Steps: 32\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 135/129000  | Episode Reward: 7  | Actor loss: -0.02 | Critic loss: 4.57 | Entropy loss: -0.0036  | Total Loss: 4.54 | Total Steps: 44\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 136/129000  | Episode Reward: 7  | Actor loss: 0.07 | Critic loss: 6.98 | Entropy loss: -0.0006  | Total Loss: 7.06 | Total Steps: 32\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 137/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.71 | Entropy loss: -0.0000  | Total Loss: 1.72 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 138/129000  | Episode Reward: 4  | Actor loss: -0.20 | Critic loss: 7.26 | Entropy loss: -0.0091  | Total Loss: 7.05 | Total Steps: 60\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 139/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.35 | Entropy loss: -0.0000  | Total Loss: 1.35 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 140/129000  | Episode Reward: 10  | Actor loss: 0.91 | Critic loss: 1.72 | Entropy loss: -0.0015  | Total Loss: 2.63 | Total Steps: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 141/129000  | Episode Reward: 4  | Actor loss: 0.00 | Critic loss: 8.13 | Entropy loss: -0.0013  | Total Loss: 8.13 | Total Steps: 46\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 142/129000  | Episode Reward: 4  | Actor loss: 0.01 | Critic loss: 9.43 | Entropy loss: -0.0106  | Total Loss: 9.44 | Total Steps: 59\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 143/129000  | Episode Reward: 1  | Actor loss: -0.18 | Critic loss: 8.58 | Entropy loss: -0.0008  | Total Loss: 8.40 | Total Steps: 52\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 144/129000  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 0.80 | Entropy loss: -0.0000  | Total Loss: 0.83 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 145/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 1.24 | Entropy loss: -0.0000  | Total Loss: 1.24 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 146/129000  | Episode Reward: 7  | Actor loss: 0.11 | Critic loss: 4.59 | Entropy loss: -0.0005  | Total Loss: 4.70 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 147/129000  | Episode Reward: 7  | Actor loss: 0.10 | Critic loss: 8.59 | Entropy loss: -0.0004  | Total Loss: 8.70 | Total Steps: 29\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 148/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 6.69 | Entropy loss: -0.0000  | Total Loss: 6.70 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 149/129000  | Episode Reward: 4  | Actor loss: -0.11 | Critic loss: 8.25 | Entropy loss: -0.0005  | Total Loss: 8.14 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 150/129000  | Episode Reward: 10  | Actor loss: 0.05 | Critic loss: 0.66 | Entropy loss: -0.0001  | Total Loss: 0.70 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 151/129000  | Episode Reward: -2  | Actor loss: -1.64 | Critic loss: 12.04 | Entropy loss: -0.0080  | Total Loss: 10.40 | Total Steps: 57\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 152/129000  | Episode Reward: 10  | Actor loss: 0.03 | Critic loss: 1.72 | Entropy loss: -0.0001  | Total Loss: 1.75 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 153/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.86 | Entropy loss: -0.0000  | Total Loss: 0.87 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 154/129000  | Episode Reward: 7  | Actor loss: 0.07 | Critic loss: 7.87 | Entropy loss: -0.0006  | Total Loss: 7.94 | Total Steps: 32\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 155/129000  | Episode Reward: 7  | Actor loss: -0.01 | Critic loss: 1.47 | Entropy loss: -0.0001  | Total Loss: 1.46 | Total Steps: 38\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 156/129000  | Episode Reward: 7  | Actor loss: -0.21 | Critic loss: 3.73 | Entropy loss: -0.0039  | Total Loss: 3.52 | Total Steps: 47\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 157/129000  | Episode Reward: 7  | Actor loss: 0.05 | Critic loss: 4.06 | Entropy loss: -0.0009  | Total Loss: 4.11 | Total Steps: 47\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 158/129000  | Episode Reward: 4  | Actor loss: -0.19 | Critic loss: 8.35 | Entropy loss: -0.0016  | Total Loss: 8.15 | Total Steps: 59\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 159/129000  | Episode Reward: 1  | Actor loss: -0.05 | Critic loss: 11.67 | Entropy loss: -0.0004  | Total Loss: 11.62 | Total Steps: 53\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 160/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 161/129000  | Episode Reward: 4  | Actor loss: -0.30 | Critic loss: 6.87 | Entropy loss: -0.0041  | Total Loss: 6.56 | Total Steps: 55\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 162/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 0.79 | Entropy loss: -0.0000  | Total Loss: 0.80 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 163/129000  | Episode Reward: 4  | Actor loss: -0.02 | Critic loss: 5.23 | Entropy loss: -0.0003  | Total Loss: 5.21 | Total Steps: 52\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 164/129000  | Episode Reward: 10  | Actor loss: 0.36 | Critic loss: 0.78 | Entropy loss: -0.0016  | Total Loss: 1.14 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 165/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 0.70 | Entropy loss: -0.0000  | Total Loss: 0.72 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 166/129000  | Episode Reward: 10  | Actor loss: 0.71 | Critic loss: 1.49 | Entropy loss: -0.0022  | Total Loss: 2.19 | Total Steps: 13\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 167/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.65 | Entropy loss: -0.0000  | Total Loss: 1.65 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 168/129000  | Episode Reward: 1  | Actor loss: -0.46 | Critic loss: 8.50 | Entropy loss: -0.0033  | Total Loss: 8.03 | Total Steps: 43\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 169/129000  | Episode Reward: 10  | Actor loss: 0.04 | Critic loss: 1.51 | Entropy loss: -0.0001  | Total Loss: 1.55 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 170/129000  | Episode Reward: 10  | Actor loss: -0.09 | Critic loss: 3.39 | Entropy loss: -0.0140  | Total Loss: 3.28 | Total Steps: 69\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 171/129000  | Episode Reward: 4  | Actor loss: -0.17 | Critic loss: 9.27 | Entropy loss: -0.0030  | Total Loss: 9.10 | Total Steps: 41\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 172/129000  | Episode Reward: 10  | Actor loss: -0.02 | Critic loss: 2.32 | Entropy loss: -0.0012  | Total Loss: 2.30 | Total Steps: 43\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 173/129000  | Episode Reward: 4  | Actor loss: 0.05 | Critic loss: 6.61 | Entropy loss: -0.0042  | Total Loss: 6.65 | Total Steps: 55\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 174/129000  | Episode Reward: 10  | Actor loss: 0.63 | Critic loss: 3.08 | Entropy loss: -0.0014  | Total Loss: 3.71 | Total Steps: 12\n",
      "---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 175/129000  | Episode Reward: 10  | Actor loss: 0.15 | Critic loss: 5.48 | Entropy loss: -0.0039  | Total Loss: 5.63 | Total Steps: 36\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 176/129000  | Episode Reward: 1  | Actor loss: -0.04 | Critic loss: 12.62 | Entropy loss: -0.0004  | Total Loss: 12.58 | Total Steps: 53\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 177/129000  | Episode Reward: 4  | Actor loss: -0.02 | Critic loss: 7.27 | Entropy loss: -0.0021  | Total Loss: 7.24 | Total Steps: 55\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 178/129000  | Episode Reward: 10  | Actor loss: 0.06 | Critic loss: 4.43 | Entropy loss: -0.0046  | Total Loss: 4.48 | Total Steps: 38\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 179/129000  | Episode Reward: 7  | Actor loss: -0.40 | Critic loss: 4.99 | Entropy loss: -0.0089  | Total Loss: 4.58 | Total Steps: 52\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 180/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 0.95 | Entropy loss: -0.0000  | Total Loss: 0.96 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 181/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 2.45 | Entropy loss: -0.0000  | Total Loss: 2.46 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 182/129000  | Episode Reward: 4  | Actor loss: -0.05 | Critic loss: 6.69 | Entropy loss: -0.0003  | Total Loss: 6.64 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 183/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 0.48 | Entropy loss: -0.0000  | Total Loss: 0.50 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 184/129000  | Episode Reward: 10  | Actor loss: 0.16 | Critic loss: 4.74 | Entropy loss: -0.0028  | Total Loss: 4.90 | Total Steps: 51\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 185/129000  | Episode Reward: 10  | Actor loss: 0.00 | Critic loss: 3.50 | Entropy loss: -0.0004  | Total Loss: 3.50 | Total Steps: 31\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 186/129000  | Episode Reward: 1  | Actor loss: -1.96 | Critic loss: 7.07 | Entropy loss: -0.0134  | Total Loss: 5.10 | Total Steps: 64\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 187/129000  | Episode Reward: 2  | Actor loss: -0.96 | Critic loss: 7.55 | Entropy loss: -0.0090  | Total Loss: 6.58 | Total Steps: 67\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 188/129000  | Episode Reward: -7  | Actor loss: -1.55 | Critic loss: 22.80 | Entropy loss: -0.0077  | Total Loss: 21.24 | Total Steps: 89\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 189/129000  | Episode Reward: 10  | Actor loss: 0.36 | Critic loss: 3.34 | Entropy loss: -0.0004  | Total Loss: 3.70 | Total Steps: 8\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 190/129000  | Episode Reward: 7  | Actor loss: 0.01 | Critic loss: 5.26 | Entropy loss: -0.0001  | Total Loss: 5.27 | Total Steps: 29\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 191/129000  | Episode Reward: 1  | Actor loss: -0.91 | Critic loss: 13.10 | Entropy loss: -0.0062  | Total Loss: 12.19 | Total Steps: 48\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 192/129000  | Episode Reward: 7  | Actor loss: -0.57 | Critic loss: 4.52 | Entropy loss: -0.0039  | Total Loss: 3.94 | Total Steps: 49\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 193/129000  | Episode Reward: 7  | Actor loss: 0.16 | Critic loss: 9.55 | Entropy loss: -0.0007  | Total Loss: 9.72 | Total Steps: 29\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 194/129000  | Episode Reward: 10  | Actor loss: -0.20 | Critic loss: 4.00 | Entropy loss: -0.0068  | Total Loss: 3.79 | Total Steps: 48\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 195/129000  | Episode Reward: 3  | Actor loss: 0.02 | Critic loss: 5.18 | Entropy loss: -0.0015  | Total Loss: 5.20 | Total Steps: 45\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 196/129000  | Episode Reward: 4  | Actor loss: 0.05 | Critic loss: 4.73 | Entropy loss: -0.0023  | Total Loss: 4.78 | Total Steps: 44\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 197/129000  | Episode Reward: 10  | Actor loss: 0.01 | Critic loss: 1.33 | Entropy loss: -0.0000  | Total Loss: 1.33 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 198/129000  | Episode Reward: 10  | Actor loss: 0.02 | Critic loss: 1.10 | Entropy loss: -0.0000  | Total Loss: 1.12 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 199/129000  | Episode Reward: 1  | Actor loss: 0.03 | Critic loss: 9.07 | Entropy loss: -0.0038  | Total Loss: 9.10 | Total Steps: 54\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 200/129000  | Episode Reward: 1  | Actor loss: -0.64 | Critic loss: 8.54 | Entropy loss: -0.0033  | Total Loss: 7.90 | Total Steps: 60\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 5.82. Model has been saved-----\n",
      "Training  | Episode: 201/129000  | Episode Reward: 10  | Average Reward 5.82  | Actor loss: 2.34 | Critic loss: 12.24 | Entropy loss: -0.0040  | Total Loss: 14.58 | Total Steps: 23\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward -2.56  | Actor loss: 0.02 | Critic loss: 12.79 | Entropy loss: -0.0004  | Total Loss: 12.80 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 1  | Average Reward -2.58  | Actor loss: 0.01 | Critic loss: 8.55 | Entropy loss: -0.0007  | Total Loss: 8.56 | Total Steps: 50\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 10  | Average Reward -2.58  | Actor loss: 0.01 | Critic loss: 15.34 | Entropy loss: -0.0005  | Total Loss: 15.36 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward -2.56  | Actor loss: 1.14 | Critic loss: 20.32 | Entropy loss: -0.0058  | Total Loss: 21.45 | Total Steps: 30\n",
      "TEST: ---cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 4  | Average Reward -1.96  | Actor loss: 0.01 | Critic loss: 1.54 | Entropy loss: -0.0059  | Total Loss: 1.55 | Total Steps: 230\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 4  | Average Reward -1.36  | Actor loss: 2.58 | Critic loss: 19.81 | Entropy loss: -0.0098  | Total Loss: 22.39 | Total Steps: 48\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: -2  | Average Reward -1.38  | Actor loss: 0.10 | Critic loss: 29.52 | Entropy loss: -0.0560  | Total Loss: 29.56 | Total Steps: 96\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward -1.39  | Actor loss: 0.16 | Critic loss: 1.96 | Entropy loss: -0.0076  | Total Loss: 2.11 | Total Steps: 48\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 4  | Average Reward -1.39  | Actor loss: 0.00 | Critic loss: 1.88 | Entropy loss: -0.0012  | Total Loss: 1.88 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 10/100  | Episode Reward: -10  | Average Reward -1.40  | Actor loss: -0.00 | Critic loss: 75.10 | Entropy loss: -0.0003  | Total Loss: 75.10 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward -1.39  | Actor loss: 0.00 | Critic loss: 6.99 | Entropy loss: -0.0002  | Total Loss: 7.00 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Average Reward -1.39  | Actor loss: 1.47 | Critic loss: 22.89 | Entropy loss: -0.0112  | Total Loss: 24.35 | Total Steps: 37\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward -1.39  | Actor loss: 0.52 | Critic loss: 15.04 | Entropy loss: -0.0292  | Total Loss: 15.53 | Total Steps: 30\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 10  | Average Reward -1.13  | Actor loss: 0.76 | Critic loss: 21.46 | Entropy loss: -0.0109  | Total Loss: 22.21 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 1  | Average Reward -1.13  | Actor loss: 0.01 | Critic loss: 3.94 | Entropy loss: -0.0018  | Total Loss: 3.94 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 1  | Average Reward -1.15  | Actor loss: 0.01 | Critic loss: 0.98 | Entropy loss: -0.0126  | Total Loss: 0.98 | Total Steps: 56\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward -0.88  | Actor loss: 0.02 | Critic loss: 2.59 | Entropy loss: -0.0200  | Total Loss: 2.59 | Total Steps: 13\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 1  | Average Reward -0.88  | Actor loss: 0.01 | Critic loss: 2.87 | Entropy loss: -0.0022  | Total Loss: 2.87 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: -2  | Average Reward -0.92  | Actor loss: 0.00 | Critic loss: 1.48 | Entropy loss: -0.0275  | Total Loss: 1.46 | Total Steps: 48\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward -0.91  | Actor loss: 0.03 | Critic loss: 18.07 | Entropy loss: -0.0050  | Total Loss: 18.10 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward -0.60  | Actor loss: 0.39 | Critic loss: 6.64 | Entropy loss: -0.0439  | Total Loss: 6.99 | Total Steps: 10\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: -8  | Average Reward -0.04  | Actor loss: 0.04 | Critic loss: 9.17 | Entropy loss: -0.0231  | Total Loss: 9.19 | Total Steps: 119\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward -0.01  | Actor loss: 0.00 | Critic loss: 1.97 | Entropy loss: -0.0042  | Total Loss: 1.97 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 24/100  | Episode Reward: -124  | Average Reward -0.68  | Actor loss: -6.77 | Critic loss: 89.92 | Entropy loss: -0.0229  | Total Loss: 83.13 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 4  | Average Reward -0.68  | Actor loss: 0.01 | Critic loss: 3.87 | Entropy loss: -0.0040  | Total Loss: 3.88 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward -0.69  | Actor loss: 0.33 | Critic loss: 15.74 | Entropy loss: -0.0051  | Total Loss: 16.06 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward -0.69  | Actor loss: 0.00 | Critic loss: 2.66 | Entropy loss: -0.0035  | Total Loss: 2.66 | Total Steps: 34\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward -0.69  | Actor loss: 0.45 | Critic loss: 10.93 | Entropy loss: -0.0236  | Total Loss: 11.36 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 4  | Average Reward -0.68  | Actor loss: 0.06 | Critic loss: 28.98 | Entropy loss: -0.0337  | Total Loss: 29.00 | Total Steps: 68\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: -5  | Average Reward -0.71  | Actor loss: 0.01 | Critic loss: 10.64 | Entropy loss: -0.0246  | Total Loss: 10.62 | Total Steps: 50\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Average Reward -0.57  | Actor loss: 0.00 | Critic loss: 2.10 | Entropy loss: -0.0045  | Total Loss: 2.09 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 7  | Average Reward -0.59  | Actor loss: 0.00 | Critic loss: 9.37 | Entropy loss: -0.0069  | Total Loss: 9.36 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 7  | Average Reward -0.51  | Actor loss: 0.00 | Critic loss: 2.24 | Entropy loss: -0.0007  | Total Loss: 2.24 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Average Reward -0.51  | Actor loss: 0.81 | Critic loss: 21.09 | Entropy loss: -0.0065  | Total Loss: 21.89 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 7  | Average Reward -0.52  | Actor loss: 3.26 | Critic loss: 26.64 | Entropy loss: -0.0359  | Total Loss: 29.87 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward 0.14  | Actor loss: 0.00 | Critic loss: 2.06 | Entropy loss: -0.0065  | Total Loss: 2.05 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 10  | Average Reward 0.14  | Actor loss: 0.02 | Critic loss: 16.74 | Entropy loss: -0.0015  | Total Loss: 16.75 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 4  | Average Reward 0.14  | Actor loss: 0.01 | Critic loss: 3.61 | Entropy loss: -0.0200  | Total Loss: 3.60 | Total Steps: 35\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 1  | Average Reward 0.09  | Actor loss: 4.66 | Critic loss: 33.78 | Entropy loss: -0.0184  | Total Loss: 38.43 | Total Steps: 61\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 7  | Average Reward 0.10  | Actor loss: 1.48 | Critic loss: 21.82 | Entropy loss: -0.0137  | Total Loss: 23.28 | Total Steps: 39\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward 0.09  | Actor loss: 0.17 | Critic loss: 28.39 | Entropy loss: -0.0287  | Total Loss: 28.53 | Total Steps: 35\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 1  | Average Reward 0.09  | Actor loss: 0.10 | Critic loss: 5.65 | Entropy loss: -0.0028  | Total Loss: 5.75 | Total Steps: 53\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward 0.19  | Actor loss: 0.01 | Critic loss: 15.68 | Entropy loss: -0.0008  | Total Loss: 15.69 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 10  | Average Reward 0.22  | Actor loss: 0.03 | Critic loss: 4.06 | Entropy loss: -0.0162  | Total Loss: 4.07 | Total Steps: 13\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 7  | Average Reward 0.84  | Actor loss: 0.90 | Critic loss: 20.26 | Entropy loss: -0.0177  | Total Loss: 21.14 | Total Steps: 40\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 0  | Average Reward 0.80  | Actor loss: 0.02 | Critic loss: 1.83 | Entropy loss: -0.0432  | Total Loss: 1.80 | Total Steps: 86\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward 0.84  | Actor loss: 0.03 | Critic loss: 2.81 | Entropy loss: -0.0639  | Total Loss: 2.78 | Total Steps: 17\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 1.48  | Actor loss: 0.04 | Critic loss: 11.55 | Entropy loss: -0.0568  | Total Loss: 11.53 | Total Steps: 7\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 4  | Average Reward 1.50  | Actor loss: 3.66 | Critic loss: 25.11 | Entropy loss: -0.0284  | Total Loss: 28.75 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 10  | Average Reward 1.50  | Actor loss: 0.21 | Critic loss: 8.27 | Entropy loss: -0.0597  | Total Loss: 8.42 | Total Steps: 14\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 7  | Average Reward 1.48  | Actor loss: 2.50 | Critic loss: 15.70 | Entropy loss: -0.0074  | Total Loss: 18.19 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward 1.63  | Actor loss: 3.14 | Critic loss: 35.35 | Entropy loss: -0.0277  | Total Loss: 38.46 | Total Steps: 20\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 10  | Average Reward 1.69  | Actor loss: 2.87 | Critic loss: 35.83 | Entropy loss: -0.0475  | Total Loss: 38.65 | Total Steps: 24\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Average Reward 1.71  | Actor loss: 0.01 | Critic loss: 1.42 | Entropy loss: -0.0054  | Total Loss: 1.43 | Total Steps: 43\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 10  | Average Reward 1.72  | Actor loss: 0.00 | Critic loss: 2.04 | Entropy loss: -0.0013  | Total Loss: 2.04 | Total Steps: 6\n",
      "TEST: ---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: -8  | Average Reward 1.65  | Actor loss: 0.07 | Critic loss: 10.24 | Entropy loss: -0.0086  | Total Loss: 10.30 | Total Steps: 107\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 1.65  | Actor loss: 0.00 | Critic loss: 2.10 | Entropy loss: -0.0018  | Total Loss: 2.10 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 4  | Average Reward 1.66  | Actor loss: 0.00 | Critic loss: 2.28 | Entropy loss: -0.0286  | Total Loss: 2.25 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward 2.27  | Actor loss: 14.44 | Critic loss: 19.67 | Entropy loss: -0.0501  | Total Loss: 34.07 | Total Steps: 7\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 10  | Average Reward 2.31  | Actor loss: 0.00 | Critic loss: 3.95 | Entropy loss: -0.0025  | Total Loss: 3.95 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward 2.29  | Actor loss: 0.01 | Critic loss: 15.19 | Entropy loss: -0.0009  | Total Loss: 15.21 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 4  | Average Reward 2.29  | Actor loss: 0.00 | Critic loss: 9.21 | Entropy loss: -0.0028  | Total Loss: 9.22 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: -2  | Average Reward 2.27  | Actor loss: 1.63 | Critic loss: 15.65 | Entropy loss: -0.0487  | Total Loss: 17.24 | Total Steps: 172\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: -2  | Average Reward 2.25  | Actor loss: 0.82 | Critic loss: 17.22 | Entropy loss: -0.0176  | Total Loss: 18.02 | Total Steps: 77\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: -5  | Average Reward 2.84  | Actor loss: 3.42 | Critic loss: 34.81 | Entropy loss: -0.0455  | Total Loss: 38.18 | Total Steps: 134\n",
      "TEST: ---cylinder---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward 2.87  | Actor loss: 0.29 | Critic loss: 11.79 | Entropy loss: -0.0052  | Total Loss: 12.07 | Total Steps: 160\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 10  | Average Reward 2.92  | Actor loss: 0.01 | Critic loss: 13.56 | Entropy loss: -0.0878  | Total Loss: 13.48 | Total Steps: 7\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: -11  | Average Reward 3.48  | Actor loss: 0.06 | Critic loss: 8.93 | Entropy loss: -0.0294  | Total Loss: 8.97 | Total Steps: 102\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 7  | Average Reward 3.46  | Actor loss: 0.01 | Critic loss: 3.97 | Entropy loss: -0.0048  | Total Loss: 3.97 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 3.51  | Actor loss: 0.00 | Critic loss: 2.29 | Entropy loss: -0.0012  | Total Loss: 2.29 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 71/100  | Episode Reward: -109  | Average Reward 2.92  | Actor loss: -8.46 | Critic loss: 73.37 | Entropy loss: -0.0301  | Total Loss: 64.88 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 4  | Average Reward 2.93  | Actor loss: 0.42 | Critic loss: 10.68 | Entropy loss: -0.0280  | Total Loss: 11.08 | Total Steps: 77\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 1  | Average Reward 3.31  | Actor loss: 0.01 | Critic loss: 14.34 | Entropy loss: -0.0142  | Total Loss: 14.34 | Total Steps: 106\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 3.33  | Actor loss: 0.00 | Critic loss: 1.56 | Entropy loss: -0.0027  | Total Loss: 1.56 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 3.35  | Actor loss: 0.98 | Critic loss: 5.45 | Entropy loss: -0.0286  | Total Loss: 6.40 | Total Steps: 7\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 10  | Average Reward 3.38  | Actor loss: 0.00 | Critic loss: 2.12 | Entropy loss: -0.0052  | Total Loss: 2.12 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 1  | Average Reward 3.44  | Actor loss: 0.01 | Critic loss: 2.20 | Entropy loss: -0.0027  | Total Loss: 2.21 | Total Steps: 53\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward 3.40  | Actor loss: 1.44 | Critic loss: 20.83 | Entropy loss: -0.0196  | Total Loss: 22.25 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 10  | Average Reward 3.42  | Actor loss: 10.91 | Critic loss: 19.26 | Entropy loss: -0.0472  | Total Loss: 30.12 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 7  | Average Reward 3.44  | Actor loss: 0.01 | Critic loss: 14.96 | Entropy loss: -0.0025  | Total Loss: 14.97 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 3.45  | Actor loss: 1.45 | Critic loss: 8.28 | Entropy loss: -0.0161  | Total Loss: 9.71 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: -6  | Average Reward 3.40  | Actor loss: 0.02 | Critic loss: 3.95 | Entropy loss: -0.0284  | Total Loss: 3.95 | Total Steps: 111\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 7  | Average Reward 3.42  | Actor loss: 0.09 | Critic loss: 6.89 | Entropy loss: -0.0159  | Total Loss: 6.97 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Average Reward 3.42  | Actor loss: 0.23 | Critic loss: 9.41 | Entropy loss: -0.0376  | Total Loss: 9.60 | Total Steps: 16\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 1  | Average Reward 3.38  | Actor loss: 0.01 | Critic loss: 3.33 | Entropy loss: -0.0024  | Total Loss: 3.33 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward 3.48  | Actor loss: 0.41 | Critic loss: 13.35 | Entropy loss: -0.0051  | Total Loss: 13.76 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 4  | Average Reward 3.44  | Actor loss: 0.01 | Critic loss: 15.44 | Entropy loss: -0.0150  | Total Loss: 15.44 | Total Steps: 32\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward 3.46  | Actor loss: 0.04 | Critic loss: 11.46 | Entropy loss: -0.0465  | Total Loss: 11.45 | Total Steps: 7\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 4  | Average Reward 3.46  | Actor loss: 0.11 | Critic loss: 8.43 | Entropy loss: -0.0029  | Total Loss: 8.54 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 4  | Average Reward 3.44  | Actor loss: 5.77 | Critic loss: 30.13 | Entropy loss: -0.0286  | Total Loss: 35.87 | Total Steps: 63\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward 3.42  | Actor loss: 0.01 | Critic loss: 2.47 | Entropy loss: -0.0074  | Total Loss: 2.47 | Total Steps: 44\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward 3.42  | Actor loss: 0.02 | Critic loss: 17.40 | Entropy loss: -0.0016  | Total Loss: 17.41 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 93/100  | Episode Reward: -97  | Average Reward 2.92  | Actor loss: -2.46 | Critic loss: 80.84 | Entropy loss: -0.0261  | Total Loss: 78.36 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 4  | Average Reward 2.90  | Actor loss: 0.17 | Critic loss: 9.61 | Entropy loss: -0.0022  | Total Loss: 9.78 | Total Steps: 43\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 4  | Average Reward 2.90  | Actor loss: 0.13 | Critic loss: 5.36 | Entropy loss: -0.0085  | Total Loss: 5.48 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 96/100  | Episode Reward: -91  | Average Reward 2.42  | Actor loss: -0.01 | Critic loss: 60.45 | Entropy loss: -0.0363  | Total Loss: 60.41 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 1  | Average Reward 2.39  | Actor loss: 25.23 | Critic loss: 22.63 | Entropy loss: -0.0208  | Total Loss: 47.84 | Total Steps: 52\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 4  | Average Reward 2.39  | Actor loss: 0.15 | Critic loss: 9.15 | Entropy loss: -0.0067  | Total Loss: 9.30 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 4  | Average Reward 2.40  | Actor loss: 0.18 | Critic loss: 15.06 | Entropy loss: -0.0007  | Total Loss: 15.24 | Total Steps: 47\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Average Reward 2.40  | Actor loss: 0.01 | Critic loss: 1.76 | Entropy loss: -0.0005  | Total Loss: 1.77 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 5.85. Model has been saved-----\n",
      "Training  | Episode: 202/129000  | Episode Reward: 10  | Average Reward 5.85  | Actor loss: 0.01 | Critic loss: 3.70 | Entropy loss: -0.0000  | Total Loss: 3.70 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 5.91. Model has been saved-----\n",
      "Training  | Episode: 203/129000  | Episode Reward: 10  | Average Reward 5.91  | Actor loss: -1.16 | Critic loss: 8.23 | Entropy loss: -0.0041  | Total Loss: 7.06 | Total Steps: 31\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 204/129000  | Episode Reward: 4  | Average Reward 5.88  | Actor loss: -0.39 | Critic loss: 10.71 | Entropy loss: -0.0041  | Total Loss: 10.31 | Total Steps: 46\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 205/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: 0.16 | Critic loss: 7.12 | Entropy loss: -0.0032  | Total Loss: 7.28 | Total Steps: 40\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 206/129000  | Episode Reward: 4  | Average Reward 5.87  | Actor loss: -0.08 | Critic loss: 9.68 | Entropy loss: -0.0008  | Total Loss: 9.60 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 207/129000  | Episode Reward: 4  | Average Reward 5.83  | Actor loss: -0.69 | Critic loss: 6.22 | Entropy loss: -0.0060  | Total Loss: 5.53 | Total Steps: 45\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 208/129000  | Episode Reward: 10  | Average Reward 5.83  | Actor loss: 0.11 | Critic loss: 4.75 | Entropy loss: -0.0003  | Total Loss: 4.86 | Total Steps: 29\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 209/129000  | Episode Reward: 10  | Average Reward 5.85  | Actor loss: 0.01 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 210/129000  | Episode Reward: -2  | Average Reward 5.82  | Actor loss: -0.49 | Critic loss: 17.42 | Entropy loss: -0.0032  | Total Loss: 16.93 | Total Steps: 72\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 211/129000  | Episode Reward: 1  | Average Reward 5.78  | Actor loss: -0.04 | Critic loss: 12.32 | Entropy loss: -0.0033  | Total Loss: 12.28 | Total Steps: 53\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 212/129000  | Episode Reward: -2  | Average Reward 5.73  | Actor loss: -1.64 | Critic loss: 11.43 | Entropy loss: -0.0109  | Total Loss: 9.78 | Total Steps: 51\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 213/129000  | Episode Reward: 6  | Average Reward 5.74  | Actor loss: 0.25 | Critic loss: 5.93 | Entropy loss: -0.0044  | Total Loss: 6.18 | Total Steps: 49\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 214/129000  | Episode Reward: 7  | Average Reward 5.75  | Actor loss: -0.02 | Critic loss: 5.83 | Entropy loss: -0.0004  | Total Loss: 5.81 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 215/129000  | Episode Reward: 7  | Average Reward 5.74  | Actor loss: 0.08 | Critic loss: 7.89 | Entropy loss: -0.0005  | Total Loss: 7.97 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 216/129000  | Episode Reward: 7  | Average Reward 5.72  | Actor loss: 0.36 | Critic loss: 9.84 | Entropy loss: -0.0014  | Total Loss: 10.20 | Total Steps: 31\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 217/129000  | Episode Reward: -2  | Average Reward 5.67  | Actor loss: -0.66 | Critic loss: 15.91 | Entropy loss: -0.0071  | Total Loss: 15.24 | Total Steps: 78\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 218/129000  | Episode Reward: 7  | Average Reward 5.65  | Actor loss: 0.16 | Critic loss: 4.62 | Entropy loss: -0.0005  | Total Loss: 4.78 | Total Steps: 30\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 219/129000  | Episode Reward: 10  | Average Reward 5.67  | Actor loss: 0.14 | Critic loss: 4.93 | Entropy loss: -0.0005  | Total Loss: 5.07 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 220/129000  | Episode Reward: 7  | Average Reward 5.67  | Actor loss: 0.26 | Critic loss: 7.91 | Entropy loss: -0.0007  | Total Loss: 8.17 | Total Steps: 29\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 221/129000  | Episode Reward: 10  | Average Reward 5.70  | Actor loss: 0.93 | Critic loss: 8.51 | Entropy loss: -0.0030  | Total Loss: 9.44 | Total Steps: 19\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 222/129000  | Episode Reward: 10  | Average Reward 5.70  | Actor loss: 0.56 | Critic loss: 5.50 | Entropy loss: -0.0024  | Total Loss: 6.06 | Total Steps: 31\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 223/129000  | Episode Reward: 2  | Average Reward 5.67  | Actor loss: -1.10 | Critic loss: 9.94 | Entropy loss: -0.0074  | Total Loss: 8.83 | Total Steps: 71\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 224/129000  | Episode Reward: 4  | Average Reward 5.67  | Actor loss: -0.19 | Critic loss: 8.92 | Entropy loss: -0.0108  | Total Loss: 8.72 | Total Steps: 61\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 225/129000  | Episode Reward: -6  | Average Reward 5.61  | Actor loss: -0.87 | Critic loss: 16.07 | Entropy loss: -0.0159  | Total Loss: 15.19 | Total Steps: 157\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 226/129000  | Episode Reward: 7  | Average Reward 5.59  | Actor loss: 0.06 | Critic loss: 8.06 | Entropy loss: -0.0004  | Total Loss: 8.12 | Total Steps: 32\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 227/129000  | Episode Reward: 7  | Average Reward 5.61  | Actor loss: 0.48 | Critic loss: 7.47 | Entropy loss: -0.0017  | Total Loss: 7.95 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 228/129000  | Episode Reward: 4  | Average Reward 5.58  | Actor loss: -0.02 | Critic loss: 6.80 | Entropy loss: -0.0011  | Total Loss: 6.78 | Total Steps: 44\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 229/129000  | Episode Reward: 7  | Average Reward 5.58  | Actor loss: -0.99 | Critic loss: 7.86 | Entropy loss: -0.0064  | Total Loss: 6.86 | Total Steps: 44\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 230/129000  | Episode Reward: 4  | Average Reward 5.58  | Actor loss: -0.00 | Critic loss: 7.63 | Entropy loss: -0.0006  | Total Loss: 7.63 | Total Steps: 53\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 231/129000  | Episode Reward: 10  | Average Reward 5.58  | Actor loss: 1.24 | Critic loss: 10.09 | Entropy loss: -0.0015  | Total Loss: 11.33 | Total Steps: 10\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 232/129000  | Episode Reward: 7  | Average Reward 5.58  | Actor loss: 0.12 | Critic loss: 8.23 | Entropy loss: -0.0003  | Total Loss: 8.35 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 233/129000  | Episode Reward: 7  | Average Reward 5.65  | Actor loss: -0.09 | Critic loss: 3.23 | Entropy loss: -0.0012  | Total Loss: 3.14 | Total Steps: 51\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 234/129000  | Episode Reward: 4  | Average Reward 5.63  | Actor loss: -0.57 | Critic loss: 7.08 | Entropy loss: -0.0068  | Total Loss: 6.51 | Total Steps: 91\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 235/129000  | Episode Reward: 1  | Average Reward 5.59  | Actor loss: -0.59 | Critic loss: 8.33 | Entropy loss: -0.0039  | Total Loss: 7.74 | Total Steps: 46\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 236/129000  | Episode Reward: 10  | Average Reward 5.63  | Actor loss: 3.47 | Critic loss: 12.40 | Entropy loss: -0.0018  | Total Loss: 15.87 | Total Steps: 7\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 237/129000  | Episode Reward: 10  | Average Reward 5.67  | Actor loss: 0.31 | Critic loss: 8.12 | Entropy loss: -0.0002  | Total Loss: 8.43 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 238/129000  | Episode Reward: 10  | Average Reward 5.68  | Actor loss: 0.05 | Critic loss: 14.83 | Entropy loss: -0.0000  | Total Loss: 14.88 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 239/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: 0.03 | Critic loss: 6.91 | Entropy loss: -0.0002  | Total Loss: 6.94 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 240/129000  | Episode Reward: 4  | Average Reward 5.65  | Actor loss: -0.04 | Critic loss: 6.21 | Entropy loss: -0.0003  | Total Loss: 6.17 | Total Steps: 42\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 241/129000  | Episode Reward: 7  | Average Reward 5.65  | Actor loss: 0.22 | Critic loss: 3.27 | Entropy loss: -0.0012  | Total Loss: 3.49 | Total Steps: 31\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 242/129000  | Episode Reward: 10  | Average Reward 5.70  | Actor loss: 0.01 | Critic loss: 2.81 | Entropy loss: -0.0000  | Total Loss: 2.81 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 243/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: -0.47 | Critic loss: 8.51 | Entropy loss: -0.0015  | Total Loss: 8.03 | Total Steps: 28\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 244/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: 0.03 | Critic loss: 2.88 | Entropy loss: -0.0023  | Total Loss: 2.91 | Total Steps: 47\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 245/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: 0.08 | Critic loss: 8.31 | Entropy loss: -0.0003  | Total Loss: 8.39 | Total Steps: 29\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 246/129000  | Episode Reward: 10  | Average Reward 5.80  | Actor loss: 0.02 | Critic loss: 3.32 | Entropy loss: -0.0000  | Total Loss: 3.34 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.08. Model has been saved-----\n",
      "Training  | Episode: 247/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 0.21 | Critic loss: 16.84 | Entropy loss: -0.0001  | Total Loss: 17.05 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 248/129000  | Episode Reward: 4  | Average Reward 6.08  | Actor loss: -0.04 | Critic loss: 8.71 | Entropy loss: -0.0005  | Total Loss: 8.67 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 249/129000  | Episode Reward: 4  | Average Reward 6.05  | Actor loss: -0.01 | Critic loss: 4.80 | Entropy loss: -0.0004  | Total Loss: 4.79 | Total Steps: 42\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 250/129000  | Episode Reward: 10  | Average Reward 6.05  | Actor loss: 0.06 | Critic loss: 3.70 | Entropy loss: -0.0003  | Total Loss: 3.76 | Total Steps: 34\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 251/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 1.06 | Critic loss: 9.20 | Entropy loss: -0.0006  | Total Loss: 10.26 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 252/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 0.04 | Critic loss: 4.47 | Entropy loss: -0.0002  | Total Loss: 4.51 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 253/129000  | Episode Reward: 4  | Average Reward 6.07  | Actor loss: -0.06 | Critic loss: 6.94 | Entropy loss: -0.0003  | Total Loss: 6.88 | Total Steps: 47\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.095. Model has been saved-----\n",
      "Training  | Episode: 254/129000  | Episode Reward: 7  | Average Reward 6.09  | Actor loss: -0.11 | Critic loss: 2.78 | Entropy loss: -0.0006  | Total Loss: 2.67 | Total Steps: 34\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.15. Model has been saved-----\n",
      "Training  | Episode: 255/129000  | Episode Reward: -3  | Average Reward 6.15  | Actor loss: -1.89 | Critic loss: 12.71 | Entropy loss: -0.0102  | Total Loss: 10.81 | Total Steps: 71\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.165. Model has been saved-----\n",
      "Training  | Episode: 256/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.12 | Critic loss: 5.97 | Entropy loss: -0.0001  | Total Loss: 6.08 | Total Steps: 6\n",
      "---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.18. Model has been saved-----\n",
      "Training  | Episode: 257/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.04 | Critic loss: 2.60 | Entropy loss: -0.0003  | Total Loss: 2.64 | Total Steps: 34\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 258/129000  | Episode Reward: 1  | Average Reward 6.13  | Actor loss: -0.14 | Critic loss: 14.02 | Entropy loss: -0.0009  | Total Loss: 13.87 | Total Steps: 52\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 259/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.56 | Critic loss: 3.83 | Entropy loss: -0.0012  | Total Loss: 4.38 | Total Steps: 13\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 260/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.83 | Critic loss: 7.26 | Entropy loss: -0.0069  | Total Loss: 8.09 | Total Steps: 35\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 261/129000  | Episode Reward: 7  | Average Reward 6.15  | Actor loss: 0.01 | Critic loss: 6.31 | Entropy loss: -0.0001  | Total Loss: 6.32 | Total Steps: 34\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 262/129000  | Episode Reward: 10  | Average Reward 6.15  | Actor loss: 1.13 | Critic loss: 2.55 | Entropy loss: -0.0019  | Total Loss: 3.69 | Total Steps: 11\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 263/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.40 | Critic loss: 7.65 | Entropy loss: -0.0018  | Total Loss: 8.05 | Total Steps: 33\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 264/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.04 | Critic loss: 5.43 | Entropy loss: -0.0003  | Total Loss: 5.47 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 265/129000  | Episode Reward: -17  | Average Reward 6.03  | Actor loss: -0.51 | Critic loss: 32.88 | Entropy loss: -0.0028  | Total Loss: 32.37 | Total Steps: 117\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 266/129000  | Episode Reward: 7  | Average Reward 6.06  | Actor loss: 0.25 | Critic loss: 4.76 | Entropy loss: -0.0015  | Total Loss: 5.01 | Total Steps: 30\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 267/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.20 | Critic loss: 3.96 | Entropy loss: -0.0016  | Total Loss: 4.16 | Total Steps: 48\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 268/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.06 | Critic loss: 6.44 | Entropy loss: -0.0003  | Total Loss: 6.50 | Total Steps: 29\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 269/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.01 | Critic loss: 1.99 | Entropy loss: -0.0000  | Total Loss: 1.99 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.195. Model has been saved-----\n",
      "Training  | Episode: 270/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.01 | Critic loss: 3.80 | Entropy loss: -0.0000  | Total Loss: 3.80 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 271/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.02 | Critic loss: 2.07 | Entropy loss: -0.0000  | Total Loss: 2.09 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.225. Model has been saved-----\n",
      "Training  | Episode: 272/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.03 | Critic loss: 11.44 | Entropy loss: -0.0000  | Total Loss: 11.48 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.24. Model has been saved-----\n",
      "Training  | Episode: 273/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: -0.03 | Critic loss: 2.95 | Entropy loss: -0.0003  | Total Loss: 2.92 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 274/129000  | Episode Reward: 7  | Average Reward 6.24  | Actor loss: -0.99 | Critic loss: 6.05 | Entropy loss: -0.0028  | Total Loss: 5.06 | Total Steps: 29\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.315. Model has been saved-----\n",
      "Training  | Episode: 275/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.03 | Critic loss: 3.21 | Entropy loss: -0.0000  | Total Loss: 3.24 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 276/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.01 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 277/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 11.43 | Entropy loss: -0.0012  | Total Loss: 11.44 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 278/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.42 | Critic loss: 9.58 | Entropy loss: -0.0028  | Total Loss: 9.16 | Total Steps: 53\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 279/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.04 | Critic loss: 10.90 | Entropy loss: -0.0000  | Total Loss: 10.94 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 280/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.06 | Critic loss: 8.25 | Entropy loss: -0.0004  | Total Loss: 8.19 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 281/129000  | Episode Reward: -2  | Average Reward 6.27  | Actor loss: -0.41 | Critic loss: 15.45 | Entropy loss: -0.0030  | Total Loss: 15.03 | Total Steps: 81\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 282/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.04 | Critic loss: 3.33 | Entropy loss: -0.0003  | Total Loss: 3.37 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 283/129000  | Episode Reward: 7  | Average Reward 6.30  | Actor loss: 0.05 | Critic loss: 3.68 | Entropy loss: -0.0003  | Total Loss: 3.72 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 284/129000  | Episode Reward: 4  | Average Reward 6.29  | Actor loss: -0.14 | Critic loss: 7.05 | Entropy loss: -0.0007  | Total Loss: 6.91 | Total Steps: 43\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.42. Model has been saved-----\n",
      "Training  | Episode: 285/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.04 | Critic loss: 11.17 | Entropy loss: -0.0000  | Total Loss: 11.21 | Total Steps: 6\n",
      "---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 286/129000  | Episode Reward: 1  | Average Reward 6.38  | Actor loss: -0.16 | Critic loss: 13.46 | Entropy loss: -0.0011  | Total Loss: 13.29 | Total Steps: 53\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 287/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.02 | Critic loss: 1.49 | Entropy loss: -0.0000  | Total Loss: 1.51 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 288/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.03 | Critic loss: 3.41 | Entropy loss: -0.0000  | Total Loss: 3.44 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 289/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: 0.01 | Critic loss: 6.31 | Entropy loss: -0.0003  | Total Loss: 6.32 | Total Steps: 42\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.45. Model has been saved-----\n",
      "Training  | Episode: 290/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.01 | Critic loss: 5.06 | Entropy loss: -0.0000  | Total Loss: 5.06 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.495. Model has been saved-----\n",
      "Training  | Episode: 291/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 3.02 | Entropy loss: -0.0000  | Total Loss: 3.03 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 292/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.25 | Critic loss: 6.75 | Entropy loss: -0.0025  | Total Loss: 7.00 | Total Steps: 45\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 293/129000  | Episode Reward: 7  | Average Reward 6.48  | Actor loss: -0.13 | Critic loss: 6.58 | Entropy loss: -0.0008  | Total Loss: 6.44 | Total Steps: 46\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.51. Model has been saved-----\n",
      "Training  | Episode: 294/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.00 | Critic loss: 1.67 | Entropy loss: -0.0000  | Total Loss: 1.67 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 295/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.01 | Critic loss: 1.84 | Entropy loss: -0.0000  | Total Loss: 1.85 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 296/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.32 | Critic loss: 9.10 | Entropy loss: -0.0021  | Total Loss: 8.79 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 297/129000  | Episode Reward: 1  | Average Reward 6.48  | Actor loss: -0.07 | Critic loss: 13.63 | Entropy loss: -0.0003  | Total Loss: 13.56 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 298/129000  | Episode Reward: 1  | Average Reward 6.46  | Actor loss: -0.30 | Critic loss: 14.62 | Entropy loss: -0.0010  | Total Loss: 14.32 | Total Steps: 52\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 299/129000  | Episode Reward: -2  | Average Reward 6.42  | Actor loss: -0.23 | Critic loss: 18.51 | Entropy loss: -0.0017  | Total Loss: 18.27 | Total Steps: 71\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 300/129000  | Episode Reward: 4  | Average Reward 6.43  | Actor loss: -0.22 | Critic loss: 8.51 | Entropy loss: -0.0011  | Total Loss: 8.30 | Total Steps: 47\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 301/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.01 | Critic loss: 0.84 | Entropy loss: -0.0000  | Total Loss: 0.85 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 1  | Average Reward 2.40  | Actor loss: 0.06 | Critic loss: 1.78 | Entropy loss: -0.0139  | Total Loss: 1.83 | Total Steps: 53\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Average Reward 2.40  | Actor loss: 0.01 | Critic loss: 2.28 | Entropy loss: -0.0044  | Total Loss: 2.28 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 10  | Average Reward 2.45  | Actor loss: 0.00 | Critic loss: 3.86 | Entropy loss: -0.0006  | Total Loss: 3.86 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward 2.71  | Actor loss: 0.07 | Critic loss: 2.16 | Entropy loss: -0.0089  | Total Loss: 2.21 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 7  | Average Reward 2.69  | Actor loss: 0.02 | Critic loss: 3.68 | Entropy loss: -0.0012  | Total Loss: 3.70 | Total Steps: 36\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 10  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 5.40 | Entropy loss: -0.0009  | Total Loss: 5.41 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 4  | Average Reward 2.72  | Actor loss: 0.01 | Critic loss: 2.72 | Entropy loss: -0.0048  | Total Loss: 2.73 | Total Steps: 49\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward 2.72  | Actor loss: 0.00 | Critic loss: 2.55 | Entropy loss: -0.0027  | Total Loss: 2.56 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 7  | Average Reward 2.71  | Actor loss: 0.00 | Critic loss: 2.70 | Entropy loss: -0.0086  | Total Loss: 2.70 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 0.00 | Critic loss: 4.63 | Entropy loss: -0.0005  | Total Loss: 4.63 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward 2.72  | Actor loss: 0.03 | Critic loss: 2.31 | Entropy loss: -0.0017  | Total Loss: 2.35 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 4  | Average Reward 2.71  | Actor loss: 0.04 | Critic loss: 2.26 | Entropy loss: -0.0387  | Total Loss: 2.26 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward 2.72  | Actor loss: 0.03 | Critic loss: 1.38 | Entropy loss: -0.0251  | Total Loss: 1.38 | Total Steps: 65\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 1  | Average Reward 2.72  | Actor loss: 0.86 | Critic loss: 3.13 | Entropy loss: -0.0041  | Total Loss: 3.99 | Total Steps: 52\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 1  | Average Reward 2.71  | Actor loss: 0.02 | Critic loss: 2.25 | Entropy loss: -0.0089  | Total Loss: 2.26 | Total Steps: 51\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 0.05 | Critic loss: 3.57 | Entropy loss: -0.0429  | Total Loss: 3.57 | Total Steps: 10\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 1  | Average Reward 2.69  | Actor loss: 0.01 | Critic loss: 2.98 | Entropy loss: -0.0035  | Total Loss: 2.99 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: -2  | Average Reward 2.65  | Actor loss: 0.03 | Critic loss: 5.79 | Entropy loss: -0.0315  | Total Loss: 5.79 | Total Steps: 65\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 2.66  | Actor loss: 0.00 | Critic loss: 1.52 | Entropy loss: -0.0057  | Total Loss: 1.51 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 0.09 | Critic loss: 4.52 | Entropy loss: -0.0031  | Total Loss: 4.61 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 0.84 | Critic loss: 9.62 | Entropy loss: -0.0058  | Total Loss: 10.45 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 10  | Average Reward 2.74  | Actor loss: 0.00 | Critic loss: 3.20 | Entropy loss: -0.0016  | Total Loss: 3.20 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: -32  | Average Reward 2.53  | Actor loss: 6.27 | Critic loss: 7.61 | Entropy loss: -0.0369  | Total Loss: 13.84 | Total Steps: 324\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: -2  | Average Reward 2.50  | Actor loss: 0.02 | Critic loss: 1.13 | Entropy loss: -0.0421  | Total Loss: 1.11 | Total Steps: 61\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward 2.50  | Actor loss: 4.02 | Critic loss: 6.86 | Entropy loss: -0.0192  | Total Loss: 10.86 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward 2.50  | Actor loss: 0.00 | Critic loss: 3.99 | Entropy loss: -0.0048  | Total Loss: 3.99 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 4  | Average Reward 2.50  | Actor loss: 0.02 | Critic loss: 4.27 | Entropy loss: -0.0008  | Total Loss: 4.29 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward 2.47  | Actor loss: 0.07 | Critic loss: 2.73 | Entropy loss: -0.0039  | Total Loss: 2.80 | Total Steps: 76\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 1  | Average Reward 2.53  | Actor loss: 0.90 | Critic loss: 2.98 | Entropy loss: -0.0029  | Total Loss: 3.88 | Total Steps: 52\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 10  | Average Reward 2.54  | Actor loss: 0.00 | Critic loss: 3.14 | Entropy loss: -0.0011  | Total Loss: 3.15 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward 2.52  | Actor loss: 0.04 | Critic loss: 7.47 | Entropy loss: -0.0251  | Total Loss: 7.48 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 7  | Average Reward 2.54  | Actor loss: 0.00 | Critic loss: 2.84 | Entropy loss: -0.0013  | Total Loss: 2.85 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward 2.54  | Actor loss: 0.36 | Critic loss: 9.57 | Entropy loss: -0.0031  | Total Loss: 9.93 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Average Reward 2.54  | Actor loss: 0.01 | Critic loss: 5.81 | Entropy loss: -0.0012  | Total Loss: 5.81 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Average Reward 2.62  | Actor loss: 0.00 | Critic loss: 3.55 | Entropy loss: -0.0024  | Total Loss: 3.55 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward 2.62  | Actor loss: 0.00 | Critic loss: 2.22 | Entropy loss: -0.0017  | Total Loss: 2.23 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward 2.64  | Actor loss: 0.01 | Critic loss: 2.82 | Entropy loss: -0.0261  | Total Loss: 2.80 | Total Steps: 52\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: -2  | Average Reward 2.58  | Actor loss: 0.00 | Critic loss: 8.56 | Entropy loss: -0.0057  | Total Loss: 8.56 | Total Steps: 85\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 7  | Average Reward 2.58  | Actor loss: 0.02 | Critic loss: 5.32 | Entropy loss: -0.0028  | Total Loss: 5.34 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 2.60  | Actor loss: 0.02 | Critic loss: 4.75 | Entropy loss: -0.0012  | Total Loss: 4.77 | Total Steps: 6\n",
      "TEST: ---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 1  | Average Reward 2.60  | Actor loss: 0.05 | Critic loss: 2.04 | Entropy loss: -0.0017  | Total Loss: 2.08 | Total Steps: 53\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 7  | Average Reward 2.68  | Actor loss: 0.05 | Critic loss: 2.46 | Entropy loss: -0.0031  | Total Loss: 2.51 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 10  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 0.61 | Entropy loss: -0.0480  | Total Loss: 0.57 | Total Steps: 10\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 2.71  | Actor loss: 0.02 | Critic loss: 5.58 | Entropy loss: -0.0042  | Total Loss: 5.60 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: -3  | Average Reward 2.65  | Actor loss: 0.02 | Critic loss: 2.58 | Entropy loss: -0.0323  | Total Loss: 2.57 | Total Steps: 77\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 2.69  | Actor loss: 0.01 | Critic loss: 0.66 | Entropy loss: -0.0151  | Total Loss: 0.66 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 0  | Average Reward 2.67  | Actor loss: 0.03 | Critic loss: 1.95 | Entropy loss: -0.0237  | Total Loss: 1.95 | Total Steps: 68\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward 2.70  | Actor loss: 0.00 | Critic loss: 6.38 | Entropy loss: -0.0099  | Total Loss: 6.38 | Total Steps: 60\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 2.70  | Actor loss: 0.01 | Critic loss: 0.66 | Entropy loss: -0.0116  | Total Loss: 0.66 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward 2.69  | Actor loss: 0.18 | Critic loss: 5.25 | Entropy loss: -0.0079  | Total Loss: 5.42 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 4  | Average Reward 2.68  | Actor loss: 1.00 | Critic loss: 3.85 | Entropy loss: -0.0052  | Total Loss: 4.85 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 4  | Average Reward 2.67  | Actor loss: 0.04 | Critic loss: 2.55 | Entropy loss: -0.0031  | Total Loss: 2.58 | Total Steps: 48\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 4  | Average Reward 2.69  | Actor loss: 0.01 | Critic loss: 2.72 | Entropy loss: -0.0048  | Total Loss: 2.73 | Total Steps: 49\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward 2.70  | Actor loss: 0.00 | Critic loss: 3.16 | Entropy loss: -0.0016  | Total Loss: 3.16 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 3.23 | Critic loss: 12.39 | Entropy loss: -0.0132  | Total Loss: 15.61 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 0.00 | Critic loss: 3.14 | Entropy loss: -0.0029  | Total Loss: 3.15 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -2  | Average Reward 2.65  | Actor loss: 0.13 | Critic loss: 4.68 | Entropy loss: -0.0304  | Total Loss: 4.78 | Total Steps: 59\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 2.65  | Actor loss: 0.05 | Critic loss: 2.28 | Entropy loss: -0.0063  | Total Loss: 2.33 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward 2.65  | Actor loss: 0.00 | Critic loss: 3.79 | Entropy loss: -0.0084  | Total Loss: 3.78 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 4  | Average Reward 2.62  | Actor loss: 0.01 | Critic loss: 2.03 | Entropy loss: -0.0403  | Total Loss: 2.00 | Total Steps: 58\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 1  | Average Reward 2.67  | Actor loss: 0.01 | Critic loss: 4.13 | Entropy loss: -0.0251  | Total Loss: 4.11 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 4  | Average Reward 2.69  | Actor loss: 0.00 | Critic loss: 2.93 | Entropy loss: -0.0047  | Total Loss: 2.93 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 7  | Average Reward 2.71  | Actor loss: 0.00 | Critic loss: 10.54 | Entropy loss: -0.0012  | Total Loss: 10.54 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward 2.82  | Actor loss: 0.01 | Critic loss: 5.39 | Entropy loss: -0.0009  | Total Loss: 5.39 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 65/100  | Episode Reward: -10  | Average Reward 2.72  | Actor loss: -0.00 | Critic loss: 80.19 | Entropy loss: -0.0001  | Total Loss: 80.19 | Total Steps: 500\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 10  | Average Reward 2.77  | Actor loss: 0.01 | Critic loss: 1.62 | Entropy loss: -0.0006  | Total Loss: 1.62 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 2  | Average Reward 3.12  | Actor loss: 10.52 | Critic loss: 7.39 | Entropy loss: -0.0280  | Total Loss: 17.88 | Total Steps: 69\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 4  | Average Reward 3.10  | Actor loss: 2.17 | Critic loss: 2.30 | Entropy loss: -0.0159  | Total Loss: 4.45 | Total Steps: 37\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Average Reward 3.12  | Actor loss: 0.00 | Critic loss: 3.50 | Entropy loss: -0.0036  | Total Loss: 3.50 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 3.12  | Actor loss: 0.05 | Critic loss: 4.61 | Entropy loss: -0.0400  | Total Loss: 4.62 | Total Steps: 69\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 9  | Average Reward 3.16  | Actor loss: 0.02 | Critic loss: 4.27 | Entropy loss: -0.0518  | Total Loss: 4.25 | Total Steps: 82\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 4  | Average Reward 3.15  | Actor loss: 0.02 | Critic loss: 3.00 | Entropy loss: -0.0043  | Total Loss: 3.02 | Total Steps: 49\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 3.18  | Actor loss: 0.01 | Critic loss: 4.08 | Entropy loss: -0.0013  | Total Loss: 4.09 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 3.19  | Actor loss: 0.03 | Critic loss: 2.03 | Entropy loss: -0.0013  | Total Loss: 2.06 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 3.24  | Actor loss: 0.00 | Critic loss: 3.57 | Entropy loss: -0.0016  | Total Loss: 3.57 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 1  | Average Reward 3.23  | Actor loss: 0.08 | Critic loss: 1.57 | Entropy loss: -0.0111  | Total Loss: 1.63 | Total Steps: 52\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 7  | Average Reward 3.21  | Actor loss: 0.00 | Critic loss: 8.02 | Entropy loss: -0.0195  | Total Loss: 8.00 | Total Steps: 39\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 3.23  | Actor loss: 0.01 | Critic loss: 4.38 | Entropy loss: -0.0039  | Total Loss: 4.38 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 7  | Average Reward 3.26  | Actor loss: 0.02 | Critic loss: 5.80 | Entropy loss: -0.0297  | Total Loss: 5.78 | Total Steps: 38\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 10  | Average Reward 3.27  | Actor loss: 0.03 | Critic loss: 1.94 | Entropy loss: -0.0061  | Total Loss: 1.96 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 3.33  | Actor loss: 0.02 | Critic loss: 0.86 | Entropy loss: -0.0130  | Total Loss: 0.86 | Total Steps: 12\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 1  | Average Reward 3.31  | Actor loss: 0.01 | Critic loss: 3.43 | Entropy loss: -0.0129  | Total Loss: 3.42 | Total Steps: 45\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 10  | Average Reward 3.31  | Actor loss: 0.01 | Critic loss: 0.66 | Entropy loss: -0.0325  | Total Loss: 0.64 | Total Steps: 10\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 1  | Average Reward 3.27  | Actor loss: 0.06 | Critic loss: 1.42 | Entropy loss: -0.0037  | Total Loss: 1.47 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 7  | Average Reward 3.27  | Actor loss: 0.18 | Critic loss: 5.25 | Entropy loss: -0.0076  | Total Loss: 5.42 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward 3.26  | Actor loss: 0.05 | Critic loss: 2.42 | Entropy loss: -0.0034  | Total Loss: 2.47 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Average Reward 3.27  | Actor loss: 1.30 | Critic loss: 10.13 | Entropy loss: -0.0078  | Total Loss: 11.42 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 4  | Average Reward 3.27  | Actor loss: 0.02 | Critic loss: 4.00 | Entropy loss: -0.0009  | Total Loss: 4.02 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 4  | Average Reward 3.25  | Actor loss: 0.00 | Critic loss: 3.93 | Entropy loss: -0.0005  | Total Loss: 3.93 | Total Steps: 49\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 6  | Average Reward 3.23  | Actor loss: 0.01 | Critic loss: 0.81 | Entropy loss: -0.0338  | Total Loss: 0.79 | Total Steps: 62\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Average Reward 3.23  | Actor loss: 0.00 | Critic loss: 2.30 | Entropy loss: -0.0064  | Total Loss: 2.30 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: -47  | Average Reward 2.97  | Actor loss: 6.74 | Critic loss: 11.52 | Entropy loss: -0.0363  | Total Loss: 18.22 | Total Steps: 449\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 4  | Average Reward 2.94  | Actor loss: 0.01 | Critic loss: 3.54 | Entropy loss: -0.0036  | Total Loss: 3.55 | Total Steps: 43\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 4  | Average Reward 2.98  | Actor loss: 0.27 | Critic loss: 4.53 | Entropy loss: -0.0134  | Total Loss: 4.79 | Total Steps: 51\n",
      "TEST: ---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 95/100  | Episode Reward: -49  | Average Reward 2.72  | Actor loss: -16.29 | Critic loss: 131.36 | Entropy loss: -0.0375  | Total Loss: 115.03 | Total Steps: 500\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 10  | Average Reward 2.73  | Actor loss: 0.03 | Critic loss: 1.67 | Entropy loss: -0.0243  | Total Loss: 1.68 | Total Steps: 11\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 4  | Average Reward 2.73  | Actor loss: 0.02 | Critic loss: 4.08 | Entropy loss: -0.0046  | Total Loss: 4.09 | Total Steps: 43\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 4  | Average Reward 2.71  | Actor loss: 0.19 | Critic loss: 14.27 | Entropy loss: -0.0141  | Total Loss: 14.45 | Total Steps: 50\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 3.90 | Critic loss: 6.40 | Entropy loss: -0.0185  | Total Loss: 10.29 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward 2.77  | Actor loss: 0.18 | Critic loss: 10.64 | Entropy loss: -0.0140  | Total Loss: 10.80 | Total Steps: 31\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 302/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.09 | Critic loss: 2.22 | Entropy loss: -0.0007  | Total Loss: 2.30 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 303/129000  | Episode Reward: -2  | Average Reward 6.41  | Actor loss: -1.20 | Critic loss: 11.49 | Entropy loss: -0.0090  | Total Loss: 10.29 | Total Steps: 67\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 304/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.01 | Critic loss: 0.83 | Entropy loss: -0.0001  | Total Loss: 0.85 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 305/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.09 | Critic loss: 7.18 | Entropy loss: -0.0005  | Total Loss: 7.27 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 306/129000  | Episode Reward: 4  | Average Reward 6.41  | Actor loss: 0.05 | Critic loss: 6.72 | Entropy loss: -0.0011  | Total Loss: 6.77 | Total Steps: 49\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 307/129000  | Episode Reward: 4  | Average Reward 6.41  | Actor loss: -0.04 | Critic loss: 8.33 | Entropy loss: -0.0003  | Total Loss: 8.29 | Total Steps: 42\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 308/129000  | Episode Reward: 7  | Average Reward 6.39  | Actor loss: 0.09 | Critic loss: 7.88 | Entropy loss: -0.0004  | Total Loss: 7.97 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 309/129000  | Episode Reward: 1  | Average Reward 6.35  | Actor loss: -0.27 | Critic loss: 8.72 | Entropy loss: -0.0017  | Total Loss: 8.45 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 310/129000  | Episode Reward: 1  | Average Reward 6.32  | Actor loss: -0.02 | Critic loss: 12.23 | Entropy loss: -0.0002  | Total Loss: 12.21 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 311/129000  | Episode Reward: 4  | Average Reward 6.29  | Actor loss: -0.03 | Critic loss: 4.52 | Entropy loss: -0.0004  | Total Loss: 4.50 | Total Steps: 43\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 312/129000  | Episode Reward: 4  | Average Reward 6.26  | Actor loss: -0.01 | Critic loss: 6.10 | Entropy loss: -0.0001  | Total Loss: 6.10 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 313/129000  | Episode Reward: 7  | Average Reward 6.28  | Actor loss: -0.56 | Critic loss: 3.91 | Entropy loss: -0.0094  | Total Loss: 3.33 | Total Steps: 60\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 314/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.00 | Critic loss: 1.86 | Entropy loss: -0.0000  | Total Loss: 1.86 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 315/129000  | Episode Reward: 10  | Average Reward 6.34  | Actor loss: 1.66 | Critic loss: 1.87 | Entropy loss: -0.0047  | Total Loss: 3.53 | Total Steps: 13\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 316/129000  | Episode Reward: 4  | Average Reward 6.31  | Actor loss: -0.25 | Critic loss: 4.85 | Entropy loss: -0.0011  | Total Loss: 4.59 | Total Steps: 42\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 317/129000  | Episode Reward: 2  | Average Reward 6.30  | Actor loss: -0.03 | Critic loss: 8.68 | Entropy loss: -0.0036  | Total Loss: 8.64 | Total Steps: 46\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 318/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.10 | Critic loss: 4.37 | Entropy loss: -0.0004  | Total Loss: 4.47 | Total Steps: 29\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 319/129000  | Episode Reward: 10  | Average Reward 6.36  | Actor loss: 0.03 | Critic loss: 1.21 | Entropy loss: -0.0000  | Total Loss: 1.25 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 320/129000  | Episode Reward: 7  | Average Reward 6.34  | Actor loss: -0.06 | Critic loss: 2.91 | Entropy loss: -0.0005  | Total Loss: 2.86 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 321/129000  | Episode Reward: 10  | Average Reward 6.34  | Actor loss: 0.02 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.01 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 322/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: -0.09 | Critic loss: 5.54 | Entropy loss: -0.0019  | Total Loss: 5.45 | Total Steps: 30\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 323/129000  | Episode Reward: 10  | Average Reward 6.40  | Actor loss: -0.07 | Critic loss: 3.27 | Entropy loss: -0.0008  | Total Loss: 3.20 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 324/129000  | Episode Reward: 3  | Average Reward 6.39  | Actor loss: -1.02 | Critic loss: 10.19 | Entropy loss: -0.0052  | Total Loss: 9.17 | Total Steps: 54\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 325/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: -0.01 | Critic loss: 3.19 | Entropy loss: -0.0019  | Total Loss: 3.18 | Total Steps: 30\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 326/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 0.01 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 327/129000  | Episode Reward: 10  | Average Reward 6.44  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0000  | Total Loss: 1.38 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 328/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.01 | Critic loss: 1.02 | Entropy loss: -0.0000  | Total Loss: 1.03 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 329/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.10 | Critic loss: 6.05 | Entropy loss: -0.0005  | Total Loss: 5.95 | Total Steps: 44\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.515. Model has been saved-----\n",
      "Training  | Episode: 330/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.01 | Critic loss: 1.43 | Entropy loss: -0.0000  | Total Loss: 1.44 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 331/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.10 | Critic loss: 7.47 | Entropy loss: -0.0009  | Total Loss: 7.37 | Total Steps: 49\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 332/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.35 | Critic loss: 3.33 | Entropy loss: -0.0004  | Total Loss: 3.67 | Total Steps: 8\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 333/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.09 | Critic loss: 6.47 | Entropy loss: -0.0005  | Total Loss: 6.56 | Total Steps: 29\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.53. Model has been saved-----\n",
      "Training  | Episode: 334/129000  | Episode Reward: 10  | Average Reward 6.53  | Actor loss: 0.01 | Critic loss: 2.25 | Entropy loss: -0.0000  | Total Loss: 2.26 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.545. Model has been saved-----\n",
      "Training  | Episode: 335/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.00 | Critic loss: 1.26 | Entropy loss: -0.0000  | Total Loss: 1.27 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 336/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: -0.04 | Critic loss: 5.63 | Entropy loss: -0.0006  | Total Loss: 5.60 | Total Steps: 52\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 337/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.16 | Critic loss: 4.87 | Entropy loss: -0.0013  | Total Loss: 4.71 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 338/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.84 | Critic loss: 8.22 | Entropy loss: -0.0040  | Total Loss: 7.37 | Total Steps: 56\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 339/129000  | Episode Reward: 8  | Average Reward 6.50  | Actor loss: -0.63 | Critic loss: 3.51 | Entropy loss: -0.0073  | Total Loss: 2.87 | Total Steps: 61\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 340/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.20 | Critic loss: 2.19 | Entropy loss: -0.0050  | Total Loss: 1.99 | Total Steps: 61\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 341/129000  | Episode Reward: 10  | Average Reward 6.52  | Actor loss: 0.02 | Critic loss: 2.07 | Entropy loss: -0.0001  | Total Loss: 2.09 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.55. Model has been saved-----\n",
      "Training  | Episode: 342/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.00 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 0.99 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.565. Model has been saved-----\n",
      "Training  | Episode: 343/129000  | Episode Reward: 4  | Average Reward 6.57  | Actor loss: -0.04 | Critic loss: 4.71 | Entropy loss: -0.0003  | Total Loss: 4.68 | Total Steps: 42\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 344/129000  | Episode Reward: -3  | Average Reward 6.50  | Actor loss: -0.63 | Critic loss: 13.93 | Entropy loss: -0.0030  | Total Loss: 13.30 | Total Steps: 55\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 345/129000  | Episode Reward: 4  | Average Reward 6.47  | Actor loss: -0.06 | Critic loss: 7.67 | Entropy loss: -0.0002  | Total Loss: 7.61 | Total Steps: 52\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 346/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.61 | Critic loss: 3.78 | Entropy loss: -0.0008  | Total Loss: 4.39 | Total Steps: 11\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 347/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.07 | Critic loss: 2.91 | Entropy loss: -0.0001  | Total Loss: 2.98 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 348/129000  | Episode Reward: -2  | Average Reward 6.44  | Actor loss: -1.69 | Critic loss: 11.98 | Entropy loss: -0.0138  | Total Loss: 10.28 | Total Steps: 71\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 349/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.04 | Critic loss: 3.28 | Entropy loss: -0.0001  | Total Loss: 3.32 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 350/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.13 | Critic loss: 4.95 | Entropy loss: -0.0005  | Total Loss: 5.08 | Total Steps: 29\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 351/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.09 | Critic loss: 5.42 | Entropy loss: -0.0005  | Total Loss: 5.51 | Total Steps: 29\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 352/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.78 | Critic loss: 9.20 | Entropy loss: -0.0072  | Total Loss: 8.42 | Total Steps: 43\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 353/129000  | Episode Reward: 7  | Average Reward 6.47  | Actor loss: 0.47 | Critic loss: 4.16 | Entropy loss: -0.0022  | Total Loss: 4.62 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 354/129000  | Episode Reward: 7  | Average Reward 6.47  | Actor loss: 0.04 | Critic loss: 8.13 | Entropy loss: -0.0002  | Total Loss: 8.17 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 355/129000  | Episode Reward: 1  | Average Reward 6.44  | Actor loss: -0.65 | Critic loss: 8.07 | Entropy loss: -0.0027  | Total Loss: 7.42 | Total Steps: 57\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 356/129000  | Episode Reward: 7  | Average Reward 6.44  | Actor loss: 0.01 | Critic loss: 3.46 | Entropy loss: -0.0001  | Total Loss: 3.47 | Total Steps: 34\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 357/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.82 | Critic loss: 1.03 | Entropy loss: -0.0015  | Total Loss: 1.85 | Total Steps: 9\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 358/129000  | Episode Reward: 1  | Average Reward 6.44  | Actor loss: -0.76 | Critic loss: 7.74 | Entropy loss: -0.0037  | Total Loss: 6.98 | Total Steps: 57\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 359/129000  | Episode Reward: 7  | Average Reward 6.47  | Actor loss: -0.02 | Critic loss: 3.56 | Entropy loss: -0.0003  | Total Loss: 3.54 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 360/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.19 | Critic loss: 2.46 | Entropy loss: -0.0026  | Total Loss: 2.65 | Total Steps: 43\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 361/129000  | Episode Reward: 6  | Average Reward 6.46  | Actor loss: -0.97 | Critic loss: 4.87 | Entropy loss: -0.0076  | Total Loss: 3.89 | Total Steps: 61\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 362/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: -0.40 | Critic loss: 4.43 | Entropy loss: -0.0067  | Total Loss: 4.03 | Total Steps: 42\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 363/129000  | Episode Reward: 1  | Average Reward 6.45  | Actor loss: -0.37 | Critic loss: 11.43 | Entropy loss: -0.0035  | Total Loss: 11.05 | Total Steps: 55\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 364/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.08 | Critic loss: 6.68 | Entropy loss: -0.0003  | Total Loss: 6.76 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 365/129000  | Episode Reward: 4  | Average Reward 6.41  | Actor loss: -0.95 | Critic loss: 6.45 | Entropy loss: -0.0094  | Total Loss: 5.49 | Total Steps: 48\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 366/129000  | Episode Reward: 1  | Average Reward 6.36  | Actor loss: -0.39 | Critic loss: 11.91 | Entropy loss: -0.0032  | Total Loss: 11.51 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 367/129000  | Episode Reward: 10  | Average Reward 6.36  | Actor loss: 0.01 | Critic loss: 1.47 | Entropy loss: -0.0000  | Total Loss: 1.48 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 368/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.00 | Critic loss: 1.70 | Entropy loss: -0.0000  | Total Loss: 1.71 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 369/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: -0.33 | Critic loss: 2.10 | Entropy loss: -0.0025  | Total Loss: 1.77 | Total Steps: 61\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 370/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.04 | Critic loss: 0.64 | Entropy loss: -0.0001  | Total Loss: 0.67 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 371/129000  | Episode Reward: 4  | Average Reward 6.41  | Actor loss: -0.37 | Critic loss: 4.97 | Entropy loss: -0.0035  | Total Loss: 4.60 | Total Steps: 41\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 372/129000  | Episode Reward: 4  | Average Reward 6.38  | Actor loss: 0.03 | Critic loss: 7.32 | Entropy loss: -0.0016  | Total Loss: 7.35 | Total Steps: 44\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 373/129000  | Episode Reward: 7  | Average Reward 6.39  | Actor loss: -0.31 | Critic loss: 4.81 | Entropy loss: -0.0090  | Total Loss: 4.49 | Total Steps: 47\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 374/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: -1.61 | Critic loss: 7.94 | Entropy loss: -0.0031  | Total Loss: 6.33 | Total Steps: 32\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 375/129000  | Episode Reward: 4  | Average Reward 6.34  | Actor loss: -0.74 | Critic loss: 5.55 | Entropy loss: -0.0088  | Total Loss: 4.80 | Total Steps: 67\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 376/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 0.60 | Critic loss: 3.94 | Entropy loss: -0.0006  | Total Loss: 4.54 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 377/129000  | Episode Reward: 1  | Average Reward 6.38  | Actor loss: -0.36 | Critic loss: 9.25 | Entropy loss: -0.0100  | Total Loss: 8.88 | Total Steps: 70\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 378/129000  | Episode Reward: 10  | Average Reward 6.38  | Actor loss: 0.02 | Critic loss: 2.95 | Entropy loss: -0.0001  | Total Loss: 2.97 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 379/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 1.21 | Critic loss: 2.23 | Entropy loss: -0.0043  | Total Loss: 3.44 | Total Steps: 11\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 380/129000  | Episode Reward: 4  | Average Reward 6.36  | Actor loss: -0.01 | Critic loss: 8.39 | Entropy loss: -0.0005  | Total Loss: 8.38 | Total Steps: 53\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 381/129000  | Episode Reward: 10  | Average Reward 6.36  | Actor loss: 0.08 | Critic loss: 0.78 | Entropy loss: -0.0001  | Total Loss: 0.86 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 382/129000  | Episode Reward: 1  | Average Reward 6.34  | Actor loss: -0.03 | Critic loss: 12.35 | Entropy loss: -0.0005  | Total Loss: 12.32 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 383/129000  | Episode Reward: 7  | Average Reward 6.33  | Actor loss: 0.12 | Critic loss: 7.90 | Entropy loss: -0.0005  | Total Loss: 8.01 | Total Steps: 29\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 384/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.01 | Critic loss: 0.94 | Entropy loss: -0.0000  | Total Loss: 0.95 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 385/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.00 | Critic loss: 0.88 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 386/129000  | Episode Reward: 10  | Average Reward 6.38  | Actor loss: 0.94 | Critic loss: 0.91 | Entropy loss: -0.0022  | Total Loss: 1.84 | Total Steps: 7\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 387/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 1.75 | Critic loss: 2.11 | Entropy loss: -0.0043  | Total Loss: 3.85 | Total Steps: 12\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 388/129000  | Episode Reward: 4  | Average Reward 6.47  | Actor loss: -0.12 | Critic loss: 4.50 | Entropy loss: -0.0008  | Total Loss: 4.38 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 389/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.41 | Critic loss: 2.75 | Entropy loss: -0.0006  | Total Loss: 3.16 | Total Steps: 11\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 390/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.01 | Critic loss: 5.14 | Entropy loss: -0.0000  | Total Loss: 5.16 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 391/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.57 | Critic loss: 7.45 | Entropy loss: -0.0055  | Total Loss: 6.87 | Total Steps: 51\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 392/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.04 | Critic loss: 4.22 | Entropy loss: -0.0003  | Total Loss: 4.27 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 393/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: -0.01 | Critic loss: 4.53 | Entropy loss: -0.0020  | Total Loss: 4.51 | Total Steps: 40\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 394/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 1.23 | Critic loss: 4.49 | Entropy loss: -0.0036  | Total Loss: 5.72 | Total Steps: 15\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 395/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.04 | Critic loss: 12.10 | Entropy loss: -0.0008  | Total Loss: 12.06 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 396/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.11 | Critic loss: 6.32 | Entropy loss: -0.0044  | Total Loss: 6.21 | Total Steps: 79\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 397/129000  | Episode Reward: 7  | Average Reward 6.47  | Actor loss: 0.05 | Critic loss: 7.01 | Entropy loss: -0.0004  | Total Loss: 7.06 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 398/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.02 | Critic loss: 4.17 | Entropy loss: -0.0002  | Total Loss: 4.18 | Total Steps: 29\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 399/129000  | Episode Reward: 1  | Average Reward 6.46  | Actor loss: -0.99 | Critic loss: 8.56 | Entropy loss: -0.0077  | Total Loss: 7.56 | Total Steps: 45\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 400/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: 0.02 | Critic loss: 7.29 | Entropy loss: -0.0002  | Total Loss: 7.31 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 401/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: -0.10 | Critic loss: 8.71 | Entropy loss: -0.0005  | Total Loss: 8.60 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: -2  | Average Reward 2.71  | Actor loss: 0.59 | Critic loss: 12.56 | Entropy loss: -0.0464  | Total Loss: 13.10 | Total Steps: 54\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 8.34 | Entropy loss: -0.0094  | Total Loss: 8.33 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 1  | Average Reward 2.69  | Actor loss: 0.04 | Critic loss: 2.46 | Entropy loss: -0.0122  | Total Loss: 2.49 | Total Steps: 58\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 1  | Average Reward 2.66  | Actor loss: 0.06 | Critic loss: 7.25 | Entropy loss: -0.0176  | Total Loss: 7.29 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: -5  | Average Reward 2.62  | Actor loss: 1.74 | Critic loss: 16.79 | Entropy loss: -0.0197  | Total Loss: 18.50 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 6/100  | Episode Reward: -112  | Average Reward 2.04  | Actor loss: -0.02 | Critic loss: 81.56 | Entropy loss: -0.0193  | Total Loss: 81.52 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 10  | Average Reward 2.10  | Actor loss: 1.38 | Critic loss: 12.75 | Entropy loss: -0.0116  | Total Loss: 14.11 | Total Steps: 6\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward 2.10  | Actor loss: 3.02 | Critic loss: 30.14 | Entropy loss: -0.0121  | Total Loss: 33.14 | Total Steps: 51\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 4  | Average Reward 2.10  | Actor loss: 0.00 | Critic loss: 2.09 | Entropy loss: -0.0010  | Total Loss: 2.09 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 7  | Average Reward 2.18  | Actor loss: 0.01 | Critic loss: 14.44 | Entropy loss: -0.0006  | Total Loss: 14.45 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 1  | Average Reward 2.13  | Actor loss: 1.17 | Critic loss: 14.40 | Entropy loss: -0.0154  | Total Loss: 15.56 | Total Steps: 56\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 10  | Average Reward 2.15  | Actor loss: 0.00 | Critic loss: 2.61 | Entropy loss: -0.0028  | Total Loss: 2.61 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 13/100  | Episode Reward: -130  | Average Reward 1.47  | Actor loss: -0.21 | Critic loss: 100.49 | Entropy loss: -0.0208  | Total Loss: 100.26 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 7  | Average Reward 1.45  | Actor loss: 0.01 | Critic loss: 14.42 | Entropy loss: -0.0013  | Total Loss: 14.43 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: -3  | Average Reward 1.43  | Actor loss: 1.28 | Critic loss: 20.56 | Entropy loss: -0.0339  | Total Loss: 21.81 | Total Steps: 72\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 1.48  | Actor loss: 1.13 | Critic loss: 12.45 | Entropy loss: -0.0538  | Total Loss: 13.53 | Total Steps: 7\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 7  | Average Reward 1.46  | Actor loss: 0.08 | Critic loss: 12.94 | Entropy loss: -0.0190  | Total Loss: 13.00 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward 1.50  | Actor loss: 2.38 | Critic loss: 13.41 | Entropy loss: -0.0139  | Total Loss: 15.78 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 4  | Average Reward 1.53  | Actor loss: 0.00 | Critic loss: 2.34 | Entropy loss: -0.0129  | Total Loss: 2.33 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Average Reward 1.52  | Actor loss: 0.00 | Critic loss: 7.87 | Entropy loss: -0.0006  | Total Loss: 7.88 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward 1.52  | Actor loss: 0.02 | Critic loss: 18.10 | Entropy loss: -0.0005  | Total Loss: 18.11 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: -53  | Average Reward 1.29  | Actor loss: 10.61 | Critic loss: 13.20 | Entropy loss: -0.0184  | Total Loss: 23.79 | Total Steps: 288\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 7  | Average Reward 1.28  | Actor loss: 0.00 | Critic loss: 5.81 | Entropy loss: -0.0047  | Total Loss: 5.80 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Average Reward 1.95  | Actor loss: 17.08 | Critic loss: 25.87 | Entropy loss: -0.0636  | Total Loss: 42.89 | Total Steps: 9\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 4  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 0.95 | Entropy loss: -0.0288  | Total Loss: 0.94 | Total Steps: 51\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 1.97  | Actor loss: 0.00 | Critic loss: 2.34 | Entropy loss: -0.0051  | Total Loss: 2.33 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 10  | Average Reward 1.98  | Actor loss: 1.45 | Critic loss: 12.27 | Entropy loss: -0.0093  | Total Loss: 13.72 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward 1.98  | Actor loss: 0.01 | Critic loss: 3.67 | Entropy loss: -0.0008  | Total Loss: 3.68 | Total Steps: 49\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 4  | Average Reward 1.98  | Actor loss: 0.59 | Critic loss: 19.86 | Entropy loss: -0.0034  | Total Loss: 20.44 | Total Steps: 47\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 4  | Average Reward 2.02  | Actor loss: 9.08 | Critic loss: 5.53 | Entropy loss: -0.0086  | Total Loss: 14.61 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 1  | Average Reward 1.98  | Actor loss: 0.84 | Critic loss: 15.77 | Entropy loss: -0.0414  | Total Loss: 16.57 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 7  | Average Reward 1.98  | Actor loss: 0.14 | Critic loss: 10.34 | Entropy loss: -0.0017  | Total Loss: 10.48 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward 2.00  | Actor loss: 0.03 | Critic loss: 19.28 | Entropy loss: -0.0007  | Total Loss: 19.31 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 7  | Average Reward 1.98  | Actor loss: 0.00 | Critic loss: 8.50 | Entropy loss: -0.0067  | Total Loss: 8.50 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Average Reward 2.00  | Actor loss: 0.65 | Critic loss: 20.34 | Entropy loss: -0.0045  | Total Loss: 20.98 | Total Steps: 65\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward 2.00  | Actor loss: 0.02 | Critic loss: 11.20 | Entropy loss: -0.0373  | Total Loss: 11.18 | Total Steps: 10\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward 1.97  | Actor loss: 0.07 | Critic loss: 5.37 | Entropy loss: -0.0076  | Total Loss: 5.44 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 1  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 1.59 | Entropy loss: -0.0056  | Total Loss: 1.59 | Total Steps: 51\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 7  | Average Reward 1.98  | Actor loss: 0.11 | Critic loss: 9.58 | Entropy loss: -0.0124  | Total Loss: 9.67 | Total Steps: 30\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: -8  | Average Reward 1.91  | Actor loss: 0.02 | Critic loss: 10.33 | Entropy loss: -0.0097  | Total Loss: 10.34 | Total Steps: 99\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 7  | Average Reward 1.92  | Actor loss: 0.01 | Critic loss: 14.45 | Entropy loss: -0.0010  | Total Loss: 14.46 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 4  | Average Reward 1.94  | Actor loss: 0.00 | Critic loss: 11.43 | Entropy loss: -0.0099  | Total Loss: 11.43 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 10  | Average Reward 1.95  | Actor loss: 0.14 | Critic loss: 6.32 | Entropy loss: -0.0063  | Total Loss: 6.46 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 4  | Average Reward 1.92  | Actor loss: 0.60 | Critic loss: 19.88 | Entropy loss: -0.0050  | Total Loss: 20.48 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 4  | Average Reward 1.91  | Actor loss: 0.59 | Critic loss: 13.91 | Entropy loss: -0.0270  | Total Loss: 14.47 | Total Steps: 55\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 7  | Average Reward 1.94  | Actor loss: 0.42 | Critic loss: 13.76 | Entropy loss: -0.0089  | Total Loss: 14.17 | Total Steps: 49\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward 1.94  | Actor loss: 0.01 | Critic loss: 10.76 | Entropy loss: -0.0018  | Total Loss: 10.77 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 1.94  | Actor loss: 0.05 | Critic loss: 9.06 | Entropy loss: -0.0005  | Total Loss: 9.11 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 1.97  | Actor loss: 0.04 | Critic loss: 11.30 | Entropy loss: -0.0451  | Total Loss: 11.29 | Total Steps: 7\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 1  | Average Reward 1.93  | Actor loss: 0.00 | Critic loss: 2.85 | Entropy loss: -0.0115  | Total Loss: 2.84 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 51/100  | Episode Reward: -118  | Average Reward 1.30  | Actor loss: -4.47 | Critic loss: 102.57 | Entropy loss: -0.0270  | Total Loss: 98.07 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 4  | Average Reward 1.27  | Actor loss: 0.00 | Critic loss: 11.06 | Entropy loss: -0.0076  | Total Loss: 11.06 | Total Steps: 47\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 4  | Average Reward 1.24  | Actor loss: 0.00 | Critic loss: 6.82 | Entropy loss: -0.0033  | Total Loss: 6.81 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward 1.25  | Actor loss: 0.03 | Critic loss: 20.05 | Entropy loss: -0.0010  | Total Loss: 20.08 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 4  | Average Reward 1.23  | Actor loss: 0.01 | Critic loss: 1.98 | Entropy loss: -0.0140  | Total Loss: 1.97 | Total Steps: 44\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 1  | Average Reward 1.27  | Actor loss: 0.58 | Critic loss: 12.43 | Entropy loss: -0.0282  | Total Loss: 12.98 | Total Steps: 55\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -11  | Average Reward 1.17  | Actor loss: 5.35 | Critic loss: 14.10 | Entropy loss: -0.0342  | Total Loss: 19.42 | Total Steps: 116\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 1.20  | Actor loss: 0.08 | Critic loss: 10.49 | Entropy loss: -0.0017  | Total Loss: 10.57 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 59/100  | Episode Reward: -109  | Average Reward 0.60  | Actor loss: -23.24 | Critic loss: 103.88 | Entropy loss: -0.0217  | Total Loss: 80.62 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 10  | Average Reward 0.60  | Actor loss: 0.08 | Critic loss: 10.47 | Entropy loss: -0.0012  | Total Loss: 10.55 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 4  | Average Reward 0.58  | Actor loss: 0.30 | Critic loss: 7.63 | Entropy loss: -0.0070  | Total Loss: 7.93 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward 0.60  | Actor loss: 0.12 | Critic loss: 10.00 | Entropy loss: -0.0006  | Total Loss: 10.13 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 4  | Average Reward 0.63  | Actor loss: 0.03 | Critic loss: 1.16 | Entropy loss: -0.0083  | Total Loss: 1.18 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 1  | Average Reward 0.65  | Actor loss: 0.54 | Critic loss: 4.54 | Entropy loss: -0.0092  | Total Loss: 5.07 | Total Steps: 52\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward 0.69  | Actor loss: 0.08 | Critic loss: 8.58 | Entropy loss: -0.0013  | Total Loss: 8.66 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 10  | Average Reward 0.70  | Actor loss: 2.57 | Critic loss: 13.43 | Entropy loss: -0.0125  | Total Loss: 15.99 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 7  | Average Reward 0.69  | Actor loss: 0.09 | Critic loss: 11.81 | Entropy loss: -0.0047  | Total Loss: 11.89 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 4  | Average Reward 0.77  | Actor loss: 0.92 | Critic loss: 21.86 | Entropy loss: -0.0180  | Total Loss: 22.77 | Total Steps: 49\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Average Reward 0.78  | Actor loss: 0.00 | Critic loss: 6.71 | Entropy loss: -0.0009  | Total Loss: 6.71 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 4  | Average Reward 0.75  | Actor loss: 0.16 | Critic loss: 13.44 | Entropy loss: -0.0004  | Total Loss: 13.60 | Total Steps: 47\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 71/100  | Episode Reward: -115  | Average Reward 0.72  | Actor loss: -0.01 | Critic loss: 139.31 | Entropy loss: -0.0211  | Total Loss: 139.28 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 4  | Average Reward 0.72  | Actor loss: 4.58 | Critic loss: 16.27 | Entropy loss: -0.0308  | Total Loss: 20.82 | Total Steps: 64\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 0.77  | Actor loss: 0.00 | Critic loss: 2.23 | Entropy loss: -0.0055  | Total Loss: 2.22 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 0.77  | Actor loss: 0.03 | Critic loss: 13.42 | Entropy loss: -0.0417  | Total Loss: 13.40 | Total Steps: 7\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 0.77  | Actor loss: 0.11 | Critic loss: 5.83 | Entropy loss: -0.0104  | Total Loss: 5.93 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: -47  | Average Reward 0.48  | Actor loss: 0.35 | Critic loss: 5.49 | Entropy loss: -0.0124  | Total Loss: 5.82 | Total Steps: 239\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 4  | Average Reward 0.49  | Actor loss: 0.01 | Critic loss: 8.08 | Entropy loss: -0.0006  | Total Loss: 8.09 | Total Steps: 46\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward 0.49  | Actor loss: 0.70 | Critic loss: 13.71 | Entropy loss: -0.0331  | Total Loss: 14.37 | Total Steps: 92\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 79/100  | Episode Reward: -130  | Average Reward -0.20  | Actor loss: -0.31 | Critic loss: 139.94 | Entropy loss: -0.0184  | Total Loss: 139.62 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 1  | Average Reward -0.23  | Actor loss: 0.75 | Critic loss: 5.85 | Entropy loss: -0.0044  | Total Loss: 6.60 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 4  | Average Reward -0.27  | Actor loss: 0.01 | Critic loss: 16.10 | Entropy loss: -0.0058  | Total Loss: 16.10 | Total Steps: 47\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward -0.18  | Actor loss: 0.00 | Critic loss: 2.95 | Entropy loss: -0.0013  | Total Loss: 2.95 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: -2  | Average Reward -0.23  | Actor loss: 0.03 | Critic loss: 3.77 | Entropy loss: -0.0177  | Total Loss: 3.79 | Total Steps: 46\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 7  | Average Reward -0.24  | Actor loss: 0.01 | Critic loss: 14.86 | Entropy loss: -0.0012  | Total Loss: 14.87 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 85/100  | Episode Reward: -112  | Average Reward -0.81  | Actor loss: -3.60 | Critic loss: 84.57 | Entropy loss: -0.0233  | Total Loss: 80.95 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: -5  | Average Reward -0.87  | Actor loss: 0.10 | Critic loss: 3.04 | Entropy loss: -0.0181  | Total Loss: 3.12 | Total Steps: 90\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward -0.85  | Actor loss: 0.84 | Critic loss: 7.54 | Entropy loss: -0.0125  | Total Loss: 8.36 | Total Steps: 35\n",
      "TEST: ---capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 4  | Average Reward -0.89  | Actor loss: 0.01 | Critic loss: 11.34 | Entropy loss: -0.0096  | Total Loss: 11.34 | Total Steps: 76\n",
      "TEST: ---cube---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 89/100  | Episode Reward: -10  | Average Reward -0.95  | Actor loss: -0.00 | Critic loss: 76.74 | Entropy loss: -0.0032  | Total Loss: 76.73 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward -0.93  | Actor loss: 1.61 | Critic loss: 12.92 | Entropy loss: -0.0129  | Total Loss: 14.52 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward -0.93  | Actor loss: 0.01 | Critic loss: 2.57 | Entropy loss: -0.0195  | Total Loss: 2.56 | Total Steps: 51\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward -0.93  | Actor loss: 0.00 | Critic loss: 3.47 | Entropy loss: -0.0036  | Total Loss: 3.47 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 7  | Average Reward -0.41  | Actor loss: 0.01 | Critic loss: 14.42 | Entropy loss: -0.0007  | Total Loss: 14.43 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 94/100  | Episode Reward: -124  | Average Reward -1.04  | Actor loss: -0.39 | Critic loss: 113.23 | Entropy loss: -0.0111  | Total Loss: 112.83 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward -1.03  | Actor loss: 0.14 | Critic loss: 10.34 | Entropy loss: -0.0009  | Total Loss: 10.48 | Total Steps: 38\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 10  | Average Reward -0.53  | Actor loss: 0.00 | Critic loss: 2.61 | Entropy loss: -0.0004  | Total Loss: 2.61 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 97/100  | Episode Reward: -97  | Average Reward -1.01  | Actor loss: -0.47 | Critic loss: 90.96 | Entropy loss: -0.0291  | Total Loss: 90.46 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward -1.00  | Actor loss: 0.76 | Critic loss: 8.82 | Entropy loss: -0.0263  | Total Loss: 9.56 | Total Steps: 33\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 4  | Average Reward -1.00  | Actor loss: 0.00 | Critic loss: 2.22 | Entropy loss: -0.0014  | Total Loss: 2.22 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward -1.01  | Actor loss: 0.01 | Critic loss: 12.62 | Entropy loss: -0.0014  | Total Loss: 12.62 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 402/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: 0.32 | Critic loss: 7.25 | Entropy loss: -0.0019  | Total Loss: 7.57 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 403/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.01 | Critic loss: 7.82 | Entropy loss: -0.0001  | Total Loss: 7.83 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 404/129000  | Episode Reward: 4  | Average Reward 6.43  | Actor loss: -0.04 | Critic loss: 8.18 | Entropy loss: -0.0017  | Total Loss: 8.13 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 405/129000  | Episode Reward: 1  | Average Reward 6.40  | Actor loss: -0.05 | Critic loss: 10.32 | Entropy loss: -0.0003  | Total Loss: 10.27 | Total Steps: 52\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 406/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.11 | Critic loss: 4.19 | Entropy loss: -0.0004  | Total Loss: 4.30 | Total Steps: 30\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 407/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.24 | Critic loss: 5.38 | Entropy loss: -0.0013  | Total Loss: 5.62 | Total Steps: 32\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 408/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: -0.05 | Critic loss: 3.85 | Entropy loss: -0.0007  | Total Loss: 3.80 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 409/129000  | Episode Reward: -2  | Average Reward 6.38  | Actor loss: -0.63 | Critic loss: 12.83 | Entropy loss: -0.0059  | Total Loss: 12.20 | Total Steps: 105\n",
      "---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 410/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -0.25 | Critic loss: 8.15 | Entropy loss: -0.0012  | Total Loss: 7.91 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 411/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.01 | Critic loss: 7.42 | Entropy loss: -0.0000  | Total Loss: 7.43 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 412/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.00 | Critic loss: 4.05 | Entropy loss: -0.0005  | Total Loss: 4.06 | Total Steps: 47\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 413/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.22 | Critic loss: 5.43 | Entropy loss: -0.0015  | Total Loss: 5.65 | Total Steps: 45\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 414/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.16 | Critic loss: 6.03 | Entropy loss: -0.0005  | Total Loss: 6.19 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 415/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.33 | Critic loss: 5.21 | Entropy loss: -0.0038  | Total Loss: 5.54 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 416/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.00 | Critic loss: 5.18 | Entropy loss: -0.0004  | Total Loss: 5.19 | Total Steps: 44\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 417/129000  | Episode Reward: 4  | Average Reward 6.54  | Actor loss: -0.03 | Critic loss: 7.05 | Entropy loss: -0.0002  | Total Loss: 7.02 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 418/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: 0.04 | Critic loss: 3.57 | Entropy loss: -0.0003  | Total Loss: 3.62 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 419/129000  | Episode Reward: 1  | Average Reward 6.50  | Actor loss: -0.55 | Critic loss: 11.85 | Entropy loss: -0.0072  | Total Loss: 11.29 | Total Steps: 93\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 420/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 1.22 | Critic loss: 5.94 | Entropy loss: -0.0035  | Total Loss: 7.16 | Total Steps: 14\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 421/129000  | Episode Reward: 1  | Average Reward 6.46  | Actor loss: -0.06 | Critic loss: 10.70 | Entropy loss: -0.0003  | Total Loss: 10.64 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 422/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.01 | Critic loss: 3.35 | Entropy loss: -0.0000  | Total Loss: 3.36 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 423/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.05 | Critic loss: 3.13 | Entropy loss: -0.0007  | Total Loss: 3.08 | Total Steps: 46\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 424/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.11 | Critic loss: 7.30 | Entropy loss: -0.0004  | Total Loss: 7.41 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 425/129000  | Episode Reward: 1  | Average Reward 6.54  | Actor loss: -0.05 | Critic loss: 11.63 | Entropy loss: -0.0006  | Total Loss: 11.58 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 426/129000  | Episode Reward: -2  | Average Reward 6.50  | Actor loss: -0.02 | Critic loss: 15.21 | Entropy loss: -0.0039  | Total Loss: 15.19 | Total Steps: 72\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 427/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.07 | Critic loss: 7.39 | Entropy loss: -0.0001  | Total Loss: 7.46 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 428/129000  | Episode Reward: 1  | Average Reward 6.50  | Actor loss: -0.20 | Critic loss: 10.88 | Entropy loss: -0.0009  | Total Loss: 10.68 | Total Steps: 51\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 429/129000  | Episode Reward: 1  | Average Reward 6.46  | Actor loss: -1.05 | Critic loss: 6.32 | Entropy loss: -0.0097  | Total Loss: 5.26 | Total Steps: 62\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 430/129000  | Episode Reward: 7  | Average Reward 6.48  | Actor loss: 0.27 | Critic loss: 5.43 | Entropy loss: -0.0030  | Total Loss: 5.70 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 431/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: 0.02 | Critic loss: 8.12 | Entropy loss: -0.0014  | Total Loss: 8.13 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 432/129000  | Episode Reward: 4  | Average Reward 6.43  | Actor loss: -0.04 | Critic loss: 6.60 | Entropy loss: -0.0003  | Total Loss: 6.57 | Total Steps: 43\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 433/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.03 | Critic loss: 3.75 | Entropy loss: -0.0000  | Total Loss: 3.78 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 434/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: -0.32 | Critic loss: 9.98 | Entropy loss: -0.0022  | Total Loss: 9.66 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 435/129000  | Episode Reward: 1  | Average Reward 6.45  | Actor loss: -1.44 | Critic loss: 8.33 | Entropy loss: -0.0086  | Total Loss: 6.87 | Total Steps: 65\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 436/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: -0.03 | Critic loss: 4.94 | Entropy loss: -0.0004  | Total Loss: 4.92 | Total Steps: 47\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 437/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.92 | Critic loss: 7.77 | Entropy loss: -0.0008  | Total Loss: 8.69 | Total Steps: 8\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 438/129000  | Episode Reward: -2  | Average Reward 6.38  | Actor loss: -0.41 | Critic loss: 11.19 | Entropy loss: -0.0025  | Total Loss: 10.78 | Total Steps: 52\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 439/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: 0.19 | Critic loss: 7.73 | Entropy loss: -0.0006  | Total Loss: 7.92 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 440/129000  | Episode Reward: -2  | Average Reward 6.34  | Actor loss: -1.08 | Critic loss: 12.70 | Entropy loss: -0.0060  | Total Loss: 11.61 | Total Steps: 58\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 441/129000  | Episode Reward: 10  | Average Reward 6.36  | Actor loss: 0.23 | Critic loss: 4.55 | Entropy loss: -0.0008  | Total Loss: 4.78 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 442/129000  | Episode Reward: 7  | Average Reward 6.34  | Actor loss: 0.45 | Critic loss: 7.38 | Entropy loss: -0.0019  | Total Loss: 7.83 | Total Steps: 32\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 443/129000  | Episode Reward: 4  | Average Reward 6.33  | Actor loss: -0.03 | Critic loss: 6.07 | Entropy loss: -0.0005  | Total Loss: 6.05 | Total Steps: 43\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 444/129000  | Episode Reward: 4  | Average Reward 6.32  | Actor loss: -0.07 | Critic loss: 8.08 | Entropy loss: -0.0015  | Total Loss: 8.01 | Total Steps: 52\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 445/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.16 | Critic loss: 7.88 | Entropy loss: -0.0023  | Total Loss: 7.71 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 446/129000  | Episode Reward: 4  | Average Reward 6.27  | Actor loss: 0.02 | Critic loss: 8.20 | Entropy loss: -0.0006  | Total Loss: 8.22 | Total Steps: 47\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 447/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.16 | Critic loss: 3.41 | Entropy loss: -0.0007  | Total Loss: 3.57 | Total Steps: 34\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 448/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.23 | Critic loss: 8.29 | Entropy loss: -0.0001  | Total Loss: 8.53 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 449/129000  | Episode Reward: 4  | Average Reward 6.29  | Actor loss: -0.90 | Critic loss: 6.95 | Entropy loss: -0.0091  | Total Loss: 6.04 | Total Steps: 69\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 450/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: -0.08 | Critic loss: 7.08 | Entropy loss: -0.0005  | Total Loss: 7.00 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 451/129000  | Episode Reward: 10  | Average Reward 6.25  | Actor loss: 0.01 | Critic loss: 4.28 | Entropy loss: -0.0000  | Total Loss: 4.29 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 452/129000  | Episode Reward: 7  | Average Reward 6.24  | Actor loss: 0.15 | Critic loss: 5.35 | Entropy loss: -0.0005  | Total Loss: 5.50 | Total Steps: 29\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 453/129000  | Episode Reward: 10  | Average Reward 6.27  | Actor loss: 0.04 | Critic loss: 3.98 | Entropy loss: -0.0000  | Total Loss: 4.01 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 454/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.02 | Critic loss: 2.76 | Entropy loss: -0.0000  | Total Loss: 2.78 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 455/129000  | Episode Reward: 7  | Average Reward 6.33  | Actor loss: 0.23 | Critic loss: 6.40 | Entropy loss: -0.0011  | Total Loss: 6.64 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 456/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: 0.02 | Critic loss: 6.02 | Entropy loss: -0.0010  | Total Loss: 6.03 | Total Steps: 53\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 457/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.09 | Critic loss: 1.45 | Entropy loss: -0.0011  | Total Loss: 1.55 | Total Steps: 47\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 458/129000  | Episode Reward: 7  | Average Reward 6.33  | Actor loss: 0.05 | Critic loss: 6.06 | Entropy loss: -0.0002  | Total Loss: 6.11 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 459/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.64 | Critic loss: 7.90 | Entropy loss: -0.0030  | Total Loss: 7.26 | Total Steps: 45\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 460/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: 0.02 | Critic loss: 3.82 | Entropy loss: -0.0003  | Total Loss: 3.83 | Total Steps: 34\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Step: 250\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 461/129000  | Episode Reward: -95  | Average Reward 5.78  | Actor loss: -0.30 | Critic loss: 26.32 | Entropy loss: -0.0092  | Total Loss: 26.01 | Total Steps: 497\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 462/129000  | Episode Reward: 1  | Average Reward 5.74  | Actor loss: -0.05 | Critic loss: 6.37 | Entropy loss: -0.0005  | Total Loss: 6.32 | Total Steps: 53\n",
      "---capsule---\n",
      "Step: 250\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 463/129000  | Episode Reward: 7  | Average Reward 5.74  | Actor loss: -0.04 | Critic loss: 2.64 | Entropy loss: -0.0027  | Total Loss: 2.59 | Total Steps: 496\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 464/129000  | Episode Reward: 10  | Average Reward 5.75  | Actor loss: 0.01 | Critic loss: 3.47 | Entropy loss: -0.0000  | Total Loss: 3.48 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 465/129000  | Episode Reward: 10  | Average Reward 5.88  | Actor loss: 0.01 | Critic loss: 5.13 | Entropy loss: -0.0001  | Total Loss: 5.14 | Total Steps: 31\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 466/129000  | Episode Reward: 4  | Average Reward 5.87  | Actor loss: -0.26 | Critic loss: 4.50 | Entropy loss: -0.0032  | Total Loss: 4.23 | Total Steps: 47\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 467/129000  | Episode Reward: 10  | Average Reward 5.87  | Actor loss: 0.03 | Critic loss: 13.44 | Entropy loss: -0.0000  | Total Loss: 13.47 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Step: 250\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 468/129000  | Episode Reward: -53  | Average Reward 5.57  | Actor loss: -0.42 | Critic loss: 16.11 | Entropy loss: -0.0041  | Total Loss: 15.68 | Total Steps: 345\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 469/129000  | Episode Reward: 7  | Average Reward 5.55  | Actor loss: 0.03 | Critic loss: 8.48 | Entropy loss: -0.0001  | Total Loss: 8.51 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 470/129000  | Episode Reward: 1  | Average Reward 5.51  | Actor loss: 0.09 | Critic loss: 3.99 | Entropy loss: -0.0023  | Total Loss: 4.08 | Total Steps: 50\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 471/129000  | Episode Reward: 10  | Average Reward 5.51  | Actor loss: 0.23 | Critic loss: 6.11 | Entropy loss: -0.0007  | Total Loss: 6.35 | Total Steps: 36\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 472/129000  | Episode Reward: 10  | Average Reward 5.51  | Actor loss: 0.09 | Critic loss: 6.38 | Entropy loss: -0.0003  | Total Loss: 6.47 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 473/129000  | Episode Reward: 4  | Average Reward 5.48  | Actor loss: -0.06 | Critic loss: 3.85 | Entropy loss: -0.0004  | Total Loss: 3.79 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 474/129000  | Episode Reward: 7  | Average Reward 5.48  | Actor loss: 0.01 | Critic loss: 5.89 | Entropy loss: -0.0002  | Total Loss: 5.90 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 475/129000  | Episode Reward: 10  | Average Reward 5.48  | Actor loss: 0.02 | Critic loss: 3.15 | Entropy loss: -0.0000  | Total Loss: 3.17 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 476/129000  | Episode Reward: 1  | Average Reward 5.43  | Actor loss: -0.63 | Critic loss: 6.54 | Entropy loss: -0.0033  | Total Loss: 5.90 | Total Steps: 50\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 477/129000  | Episode Reward: 10  | Average Reward 5.46  | Actor loss: 0.01 | Critic loss: 4.53 | Entropy loss: -0.0000  | Total Loss: 4.54 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 478/129000  | Episode Reward: 10  | Average Reward 5.50  | Actor loss: 0.18 | Critic loss: 4.99 | Entropy loss: -0.0005  | Total Loss: 5.17 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 479/129000  | Episode Reward: 7  | Average Reward 5.48  | Actor loss: 0.29 | Critic loss: 6.02 | Entropy loss: -0.0008  | Total Loss: 6.31 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 480/129000  | Episode Reward: 1  | Average Reward 5.46  | Actor loss: -0.19 | Critic loss: 6.27 | Entropy loss: -0.0011  | Total Loss: 6.08 | Total Steps: 52\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 481/129000  | Episode Reward: 4  | Average Reward 5.50  | Actor loss: -0.08 | Critic loss: 7.24 | Entropy loss: -0.0005  | Total Loss: 7.16 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 482/129000  | Episode Reward: 4  | Average Reward 5.46  | Actor loss: -0.06 | Critic loss: 7.54 | Entropy loss: -0.0007  | Total Loss: 7.47 | Total Steps: 47\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 483/129000  | Episode Reward: -8  | Average Reward 5.39  | Actor loss: -0.48 | Critic loss: 11.99 | Entropy loss: -0.0051  | Total Loss: 11.51 | Total Steps: 122\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 484/129000  | Episode Reward: 10  | Average Reward 5.42  | Actor loss: 0.01 | Critic loss: 4.68 | Entropy loss: -0.0000  | Total Loss: 4.68 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 485/129000  | Episode Reward: 1  | Average Reward 5.38  | Actor loss: -0.11 | Critic loss: 10.03 | Entropy loss: -0.0009  | Total Loss: 9.91 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 486/129000  | Episode Reward: -5  | Average Reward 5.34  | Actor loss: -0.34 | Critic loss: 14.48 | Entropy loss: -0.0031  | Total Loss: 14.14 | Total Steps: 103\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 487/129000  | Episode Reward: 7  | Average Reward 5.33  | Actor loss: -0.01 | Critic loss: 4.07 | Entropy loss: -0.0002  | Total Loss: 4.06 | Total Steps: 42\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 488/129000  | Episode Reward: 10  | Average Reward 5.33  | Actor loss: 0.00 | Critic loss: 2.52 | Entropy loss: -0.0000  | Total Loss: 2.53 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 489/129000  | Episode Reward: 7  | Average Reward 5.33  | Actor loss: 0.14 | Critic loss: 4.50 | Entropy loss: -0.0007  | Total Loss: 4.64 | Total Steps: 30\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 490/129000  | Episode Reward: 10  | Average Reward 5.33  | Actor loss: -0.05 | Critic loss: 3.84 | Entropy loss: -0.0013  | Total Loss: 3.78 | Total Steps: 31\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 491/129000  | Episode Reward: 10  | Average Reward 5.33  | Actor loss: 0.01 | Critic loss: 10.25 | Entropy loss: -0.0000  | Total Loss: 10.26 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 492/129000  | Episode Reward: 4  | Average Reward 5.32  | Actor loss: -0.90 | Critic loss: 6.54 | Entropy loss: -0.0040  | Total Loss: 5.63 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 493/129000  | Episode Reward: 7  | Average Reward 5.32  | Actor loss: 0.12 | Critic loss: 6.70 | Entropy loss: -0.0003  | Total Loss: 6.82 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 494/129000  | Episode Reward: 4  | Average Reward 5.29  | Actor loss: -1.01 | Critic loss: 6.73 | Entropy loss: -0.0023  | Total Loss: 5.71 | Total Steps: 42\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 495/129000  | Episode Reward: 4  | Average Reward 5.25  | Actor loss: -0.69 | Critic loss: 4.52 | Entropy loss: -0.0066  | Total Loss: 3.82 | Total Steps: 55\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 496/129000  | Episode Reward: -2  | Average Reward 5.22  | Actor loss: -0.77 | Critic loss: 11.84 | Entropy loss: -0.0030  | Total Loss: 11.07 | Total Steps: 58\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 497/129000  | Episode Reward: 10  | Average Reward 5.27  | Actor loss: 0.04 | Critic loss: 15.78 | Entropy loss: -0.0000  | Total Loss: 15.83 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 498/129000  | Episode Reward: 10  | Average Reward 5.32  | Actor loss: 0.85 | Critic loss: 3.48 | Entropy loss: -0.0015  | Total Loss: 4.33 | Total Steps: 12\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 499/129000  | Episode Reward: 4  | Average Reward 5.34  | Actor loss: -0.02 | Critic loss: 3.55 | Entropy loss: -0.0003  | Total Loss: 3.53 | Total Steps: 42\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 500/129000  | Episode Reward: 10  | Average Reward 5.38  | Actor loss: 0.11 | Critic loss: 3.61 | Entropy loss: -0.0001  | Total Loss: 3.72 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 501/129000  | Episode Reward: 10  | Average Reward 5.38  | Actor loss: 0.13 | Critic loss: 1.49 | Entropy loss: -0.0011  | Total Loss: 1.62 | Total Steps: 47\n",
      "Model has been saved\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 7  | Average Reward -0.98  | Actor loss: 0.00 | Critic loss: 4.54 | Entropy loss: -0.0048  | Total Loss: 4.53 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Average Reward -0.98  | Actor loss: 0.01 | Critic loss: 0.61 | Entropy loss: -0.0243  | Total Loss: 0.59 | Total Steps: 13\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 4  | Average Reward -1.01  | Actor loss: 0.00 | Critic loss: 3.76 | Entropy loss: -0.0060  | Total Loss: 3.76 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 4  | Average Reward -1.03  | Actor loss: 0.00 | Critic loss: 4.37 | Entropy loss: -0.0010  | Total Loss: 4.37 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 5/100  | Episode Reward: -100  | Average Reward -1.56  | Actor loss: -1.72 | Critic loss: 111.70 | Entropy loss: -0.0372  | Total Loss: 109.94 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 10  | Average Reward -1.56  | Actor loss: 0.02 | Critic loss: 13.56 | Entropy loss: -0.0078  | Total Loss: 13.57 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 4  | Average Reward -1.56  | Actor loss: 0.05 | Critic loss: 3.73 | Entropy loss: -0.0004  | Total Loss: 3.77 | Total Steps: 46\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward -1.56  | Actor loss: 0.00 | Critic loss: 1.95 | Entropy loss: -0.0020  | Total Loss: 1.96 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 7  | Average Reward -1.56  | Actor loss: 0.07 | Critic loss: 4.23 | Entropy loss: -0.0025  | Total Loss: 4.30 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 1  | Average Reward -1.61  | Actor loss: 0.00 | Critic loss: 4.52 | Entropy loss: -0.0061  | Total Loss: 4.52 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward -1.61  | Actor loss: 0.00 | Critic loss: 2.75 | Entropy loss: -0.0015  | Total Loss: 2.76 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 12/100  | Episode Reward: -85  | Average Reward -2.06  | Actor loss: -7.01 | Critic loss: 84.86 | Entropy loss: -0.0331  | Total Loss: 77.82 | Total Steps: 500\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward -2.06  | Actor loss: 0.07 | Critic loss: 3.81 | Entropy loss: -0.0041  | Total Loss: 3.87 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 4  | Average Reward -2.04  | Actor loss: 0.00 | Critic loss: 4.69 | Entropy loss: -0.0060  | Total Loss: 4.68 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward -2.00  | Actor loss: 0.02 | Critic loss: 13.09 | Entropy loss: -0.0025  | Total Loss: 13.11 | Total Steps: 6\n",
      "TEST: ---red---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 1  | Average Reward -2.04  | Actor loss: 0.00 | Critic loss: 2.87 | Entropy loss: -0.0221  | Total Loss: 2.86 | Total Steps: 51\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward -2.00  | Actor loss: 0.04 | Critic loss: 7.26 | Entropy loss: -0.0054  | Total Loss: 7.30 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 4  | Average Reward -1.97  | Actor loss: 0.00 | Critic loss: 2.60 | Entropy loss: -0.0151  | Total Loss: 2.59 | Total Steps: 45\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Average Reward -1.98  | Actor loss: 0.00 | Critic loss: 2.64 | Entropy loss: -0.0046  | Total Loss: 2.64 | Total Steps: 38\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward -1.98  | Actor loss: 0.00 | Critic loss: 1.90 | Entropy loss: -0.0014  | Total Loss: 1.90 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward -1.98  | Actor loss: 0.03 | Critic loss: 1.33 | Entropy loss: -0.0098  | Total Loss: 1.36 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 1  | Average Reward -2.02  | Actor loss: 0.01 | Critic loss: 3.02 | Entropy loss: -0.0469  | Total Loss: 2.98 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 1  | Average Reward -1.86  | Actor loss: 0.07 | Critic loss: 4.75 | Entropy loss: -0.0019  | Total Loss: 4.81 | Total Steps: 50\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Average Reward -1.83  | Actor loss: 0.01 | Critic loss: 1.94 | Entropy loss: -0.0041  | Total Loss: 1.95 | Total Steps: 49\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 10  | Average Reward -1.81  | Actor loss: 0.00 | Critic loss: 2.44 | Entropy loss: -0.0012  | Total Loss: 2.45 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward -1.80  | Actor loss: 0.03 | Critic loss: 11.88 | Entropy loss: -0.0018  | Total Loss: 11.90 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 4  | Average Reward -1.80  | Actor loss: 0.94 | Critic loss: 12.65 | Entropy loss: -0.0182  | Total Loss: 13.57 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward -1.80  | Actor loss: 0.94 | Critic loss: 5.70 | Entropy loss: -0.0019  | Total Loss: 6.64 | Total Steps: 47\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 3  | Average Reward -1.79  | Actor loss: 0.02 | Critic loss: 1.67 | Entropy loss: -0.0210  | Total Loss: 1.66 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward -1.80  | Actor loss: 0.00 | Critic loss: 6.00 | Entropy loss: -0.0014  | Total Loss: 6.00 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward -1.80  | Actor loss: 0.01 | Critic loss: 3.16 | Entropy loss: -0.0004  | Total Loss: 3.17 | Total Steps: 38\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward -1.79  | Actor loss: 0.00 | Critic loss: 1.76 | Entropy loss: -0.0004  | Total Loss: 1.77 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 7  | Average Reward -1.80  | Actor loss: 0.00 | Critic loss: 5.70 | Entropy loss: -0.0171  | Total Loss: 5.69 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 34/100  | Episode Reward: -11  | Average Reward -1.91  | Actor loss: -0.00 | Critic loss: 75.32 | Entropy loss: -0.0033  | Total Loss: 75.31 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 4  | Average Reward -1.94  | Actor loss: 0.00 | Critic loss: 5.01 | Entropy loss: -0.0111  | Total Loss: 5.00 | Total Steps: 46\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 7  | Average Reward -1.96  | Actor loss: 0.01 | Critic loss: 1.23 | Entropy loss: -0.0298  | Total Loss: 1.22 | Total Steps: 65\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 1  | Average Reward -1.97  | Actor loss: 0.01 | Critic loss: 1.71 | Entropy loss: -0.0036  | Total Loss: 1.72 | Total Steps: 51\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 7  | Average Reward -1.93  | Actor loss: 0.01 | Critic loss: 4.28 | Entropy loss: -0.0022  | Total Loss: 4.28 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward -1.91  | Actor loss: 2.46 | Critic loss: 14.31 | Entropy loss: -0.0104  | Total Loss: 16.76 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 4  | Average Reward -1.94  | Actor loss: 0.28 | Critic loss: 5.14 | Entropy loss: -0.0193  | Total Loss: 5.40 | Total Steps: 46\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 1  | Average Reward -1.94  | Actor loss: 0.07 | Critic loss: 1.90 | Entropy loss: -0.0072  | Total Loss: 1.96 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 10  | Average Reward -1.93  | Actor loss: 0.02 | Critic loss: 1.85 | Entropy loss: -0.0020  | Total Loss: 1.87 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 4  | Average Reward -1.96  | Actor loss: 0.94 | Critic loss: 5.61 | Entropy loss: -0.0014  | Total Loss: 6.55 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward -1.96  | Actor loss: 0.00 | Critic loss: 6.54 | Entropy loss: -0.0006  | Total Loss: 6.54 | Total Steps: 34\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 4  | Average Reward -1.92  | Actor loss: 0.02 | Critic loss: 2.30 | Entropy loss: -0.0042  | Total Loss: 2.31 | Total Steps: 49\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward -1.92  | Actor loss: 0.01 | Critic loss: 2.05 | Entropy loss: -0.0010  | Total Loss: 2.05 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward -1.87  | Actor loss: 0.01 | Critic loss: 3.97 | Entropy loss: -0.0002  | Total Loss: 3.98 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward -1.85  | Actor loss: 6.06 | Critic loss: 16.96 | Entropy loss: -0.0194  | Total Loss: 23.00 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward -1.85  | Actor loss: 0.00 | Critic loss: 3.20 | Entropy loss: -0.0255  | Total Loss: 3.18 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward -1.85  | Actor loss: 0.22 | Critic loss: 9.45 | Entropy loss: -0.0384  | Total Loss: 9.63 | Total Steps: 48\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Average Reward -1.82  | Actor loss: 0.01 | Critic loss: 4.39 | Entropy loss: -0.0149  | Total Loss: 4.39 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: -2  | Average Reward -1.85  | Actor loss: 0.03 | Critic loss: 3.63 | Entropy loss: -0.0245  | Total Loss: 3.63 | Total Steps: 46\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 10  | Average Reward -1.82  | Actor loss: 0.00 | Critic loss: 1.91 | Entropy loss: -0.0044  | Total Loss: 1.91 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 4  | Average Reward -1.85  | Actor loss: 0.04 | Critic loss: 1.61 | Entropy loss: -0.0318  | Total Loss: 1.62 | Total Steps: 59\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: -2  | Average Reward -1.92  | Actor loss: 0.00 | Critic loss: 10.95 | Entropy loss: -0.0124  | Total Loss: 10.94 | Total Steps: 74\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward -1.92  | Actor loss: 0.03 | Critic loss: 1.13 | Entropy loss: -0.0347  | Total Loss: 1.12 | Total Steps: 9\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward -1.85  | Actor loss: 0.01 | Critic loss: 4.06 | Entropy loss: -0.0014  | Total Loss: 4.06 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward -1.85  | Actor loss: 0.04 | Critic loss: 1.58 | Entropy loss: -0.0293  | Total Loss: 1.59 | Total Steps: 12\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward -1.85  | Actor loss: 0.01 | Critic loss: 4.05 | Entropy loss: -0.0012  | Total Loss: 4.05 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: -1  | Average Reward -1.88  | Actor loss: 0.12 | Critic loss: 3.68 | Entropy loss: -0.0266  | Total Loss: 3.78 | Total Steps: 70\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward -1.85  | Actor loss: 0.01 | Critic loss: 3.05 | Entropy loss: -0.0010  | Total Loss: 3.06 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward -1.83  | Actor loss: 0.01 | Critic loss: 1.83 | Entropy loss: -0.0318  | Total Loss: 1.81 | Total Steps: 66\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 1  | Average Reward -1.86  | Actor loss: 0.35 | Critic loss: 3.22 | Entropy loss: -0.0124  | Total Loss: 3.55 | Total Steps: 54\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 4  | Average Reward -1.90  | Actor loss: 0.01 | Critic loss: 3.07 | Entropy loss: -0.0010  | Total Loss: 3.08 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 10  | Average Reward -1.79  | Actor loss: 0.05 | Critic loss: 6.30 | Entropy loss: -0.0018  | Total Loss: 6.34 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward -1.81  | Actor loss: 0.01 | Critic loss: 4.31 | Entropy loss: -0.0021  | Total Loss: 4.31 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 7  | Average Reward -1.78  | Actor loss: 0.00 | Critic loss: 5.69 | Entropy loss: -0.0072  | Total Loss: 5.69 | Total Steps: 66\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 7  | Average Reward -1.77  | Actor loss: 0.00 | Critic loss: 4.91 | Entropy loss: -0.0094  | Total Loss: 4.90 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward -1.80  | Actor loss: 0.59 | Critic loss: 5.80 | Entropy loss: -0.0110  | Total Loss: 6.38 | Total Steps: 52\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 70/100  | Episode Reward: -85  | Average Reward -2.27  | Actor loss: -1.09 | Critic loss: 117.40 | Entropy loss: -0.0345  | Total Loss: 116.27 | Total Steps: 500\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 10  | Average Reward -2.27  | Actor loss: 0.03 | Critic loss: 1.27 | Entropy loss: -0.0139  | Total Loss: 1.29 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 10  | Average Reward -2.24  | Actor loss: 0.00 | Critic loss: 2.58 | Entropy loss: -0.0029  | Total Loss: 2.58 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 3  | Average Reward -2.27  | Actor loss: 0.01 | Critic loss: 2.58 | Entropy loss: -0.0439  | Total Loss: 2.54 | Total Steps: 77\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward -2.27  | Actor loss: 0.02 | Critic loss: 1.86 | Entropy loss: -0.0080  | Total Loss: 1.87 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward -2.27  | Actor loss: 0.01 | Critic loss: 0.85 | Entropy loss: -0.0204  | Total Loss: 0.84 | Total Steps: 13\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Average Reward -2.26  | Actor loss: 0.09 | Critic loss: 2.40 | Entropy loss: -0.0136  | Total Loss: 2.48 | Total Steps: 49\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 4  | Average Reward -2.27  | Actor loss: 0.00 | Critic loss: 4.19 | Entropy loss: -0.0014  | Total Loss: 4.19 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 78/100  | Episode Reward: -79  | Average Reward -2.72  | Actor loss: -0.01 | Critic loss: 62.94 | Entropy loss: -0.0347  | Total Loss: 62.89 | Total Steps: 500\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 1  | Average Reward -2.75  | Actor loss: 0.03 | Critic loss: 1.88 | Entropy loss: -0.0015  | Total Loss: 1.91 | Total Steps: 50\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 1  | Average Reward -2.79  | Actor loss: 0.01 | Critic loss: 1.37 | Entropy loss: -0.0089  | Total Loss: 1.37 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward -2.79  | Actor loss: 0.00 | Critic loss: 3.12 | Entropy loss: -0.0024  | Total Loss: 3.12 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 0  | Average Reward -2.80  | Actor loss: 0.01 | Critic loss: 0.94 | Entropy loss: -0.0467  | Total Loss: 0.90 | Total Steps: 70\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 7  | Average Reward -2.81  | Actor loss: 0.00 | Critic loss: 7.37 | Entropy loss: -0.0092  | Total Loss: 7.36 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 7  | Average Reward -2.79  | Actor loss: 0.13 | Critic loss: 8.09 | Entropy loss: -0.0429  | Total Loss: 8.18 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 3  | Average Reward -2.81  | Actor loss: 0.01 | Critic loss: 0.56 | Entropy loss: -0.0134  | Total Loss: 0.56 | Total Steps: 45\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward -2.81  | Actor loss: 0.01 | Critic loss: 3.09 | Entropy loss: -0.0009  | Total Loss: 3.10 | Total Steps: 34\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 4  | Average Reward -2.83  | Actor loss: 0.00 | Critic loss: 1.80 | Entropy loss: -0.0071  | Total Loss: 1.80 | Total Steps: 46\n",
      "TEST: ---green---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 7  | Average Reward -2.82  | Actor loss: 0.00 | Critic loss: 2.91 | Entropy loss: -0.0028  | Total Loss: 2.91 | Total Steps: 128\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward -2.79  | Actor loss: 0.02 | Critic loss: 10.70 | Entropy loss: -0.0018  | Total Loss: 10.72 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward -2.77  | Actor loss: 0.00 | Critic loss: 3.21 | Entropy loss: -0.0254  | Total Loss: 3.19 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Average Reward -2.77  | Actor loss: 0.00 | Critic loss: 3.38 | Entropy loss: -0.0049  | Total Loss: 3.38 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 7  | Average Reward -2.50  | Actor loss: 0.00 | Critic loss: 6.50 | Entropy loss: -0.0086  | Total Loss: 6.50 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: -5  | Average Reward -2.54  | Actor loss: 10.14 | Critic loss: 8.04 | Entropy loss: -0.0369  | Total Loss: 18.14 | Total Steps: 109\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 7  | Average Reward -2.53  | Actor loss: 0.01 | Critic loss: 3.16 | Entropy loss: -0.0014  | Total Loss: 3.17 | Total Steps: 38\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 10  | Average Reward -2.23  | Actor loss: 0.00 | Critic loss: 2.74 | Entropy loss: -0.0007  | Total Loss: 2.75 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: -41  | Average Reward -2.49  | Actor loss: 7.06 | Critic loss: 16.61 | Entropy loss: -0.0331  | Total Loss: 23.64 | Total Steps: 468\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 7  | Average Reward -2.48  | Actor loss: 0.01 | Critic loss: 4.55 | Entropy loss: -0.0033  | Total Loss: 4.56 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 4  | Average Reward -2.48  | Actor loss: 0.00 | Critic loss: 2.05 | Entropy loss: -0.0034  | Total Loss: 2.05 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 1  | Average Reward -2.50  | Actor loss: 0.02 | Critic loss: 3.91 | Entropy loss: -0.0329  | Total Loss: 3.90 | Total Steps: 45\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward -2.50  | Actor loss: 0.00 | Critic loss: 11.79 | Entropy loss: -0.0012  | Total Loss: 11.79 | Total Steps: 38\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 502/129000  | Episode Reward: 7  | Average Reward 5.36  | Actor loss: 0.10 | Critic loss: 8.74 | Entropy loss: -0.0004  | Total Loss: 8.84 | Total Steps: 32\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 503/129000  | Episode Reward: 1  | Average Reward 5.38  | Actor loss: -0.74 | Critic loss: 11.24 | Entropy loss: -0.0031  | Total Loss: 10.49 | Total Steps: 47\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 504/129000  | Episode Reward: 7  | Average Reward 5.36  | Actor loss: 0.07 | Critic loss: 2.76 | Entropy loss: -0.0023  | Total Loss: 2.83 | Total Steps: 42\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 505/129000  | Episode Reward: 7  | Average Reward 5.36  | Actor loss: 0.10 | Critic loss: 6.16 | Entropy loss: -0.0003  | Total Loss: 6.26 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 506/129000  | Episode Reward: 7  | Average Reward 5.38  | Actor loss: -0.06 | Critic loss: 6.30 | Entropy loss: -0.0005  | Total Loss: 6.24 | Total Steps: 30\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 507/129000  | Episode Reward: 10  | Average Reward 5.41  | Actor loss: 0.02 | Critic loss: 3.58 | Entropy loss: -0.0000  | Total Loss: 3.60 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 508/129000  | Episode Reward: 7  | Average Reward 5.41  | Actor loss: -0.81 | Critic loss: 3.22 | Entropy loss: -0.0039  | Total Loss: 2.40 | Total Steps: 50\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 509/129000  | Episode Reward: 7  | Average Reward 5.43  | Actor loss: 0.27 | Critic loss: 4.43 | Entropy loss: -0.0017  | Total Loss: 4.69 | Total Steps: 31\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 510/129000  | Episode Reward: 4  | Average Reward 5.45  | Actor loss: -0.72 | Critic loss: 4.33 | Entropy loss: -0.0028  | Total Loss: 3.60 | Total Steps: 43\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 511/129000  | Episode Reward: 10  | Average Reward 5.48  | Actor loss: 0.01 | Critic loss: 4.11 | Entropy loss: -0.0000  | Total Loss: 4.12 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 512/129000  | Episode Reward: -11  | Average Reward 5.41  | Actor loss: -1.65 | Critic loss: 21.44 | Entropy loss: -0.0111  | Total Loss: 19.78 | Total Steps: 107\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 513/129000  | Episode Reward: 4  | Average Reward 5.39  | Actor loss: -0.33 | Critic loss: 5.69 | Entropy loss: -0.0019  | Total Loss: 5.36 | Total Steps: 46\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 514/129000  | Episode Reward: 10  | Average Reward 5.39  | Actor loss: 0.03 | Critic loss: 2.65 | Entropy loss: -0.0000  | Total Loss: 2.68 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 515/129000  | Episode Reward: 4  | Average Reward 5.36  | Actor loss: -0.33 | Critic loss: 10.36 | Entropy loss: -0.0129  | Total Loss: 10.02 | Total Steps: 99\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 516/129000  | Episode Reward: 10  | Average Reward 5.39  | Actor loss: 0.02 | Critic loss: 2.86 | Entropy loss: -0.0001  | Total Loss: 2.88 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 517/129000  | Episode Reward: 4  | Average Reward 5.40  | Actor loss: -0.08 | Critic loss: 4.04 | Entropy loss: -0.0008  | Total Loss: 3.96 | Total Steps: 47\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 518/129000  | Episode Reward: -8  | Average Reward 5.31  | Actor loss: -0.81 | Critic loss: 20.51 | Entropy loss: -0.0021  | Total Loss: 19.70 | Total Steps: 57\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 519/129000  | Episode Reward: 4  | Average Reward 5.28  | Actor loss: 0.08 | Critic loss: 4.25 | Entropy loss: -0.0015  | Total Loss: 4.33 | Total Steps: 44\n",
      "---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 520/129000  | Episode Reward: 4  | Average Reward 5.26  | Actor loss: -0.08 | Critic loss: 6.56 | Entropy loss: -0.0024  | Total Loss: 6.48 | Total Steps: 115\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 521/129000  | Episode Reward: 10  | Average Reward 5.26  | Actor loss: 0.04 | Critic loss: 4.23 | Entropy loss: -0.0002  | Total Loss: 4.27 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 522/129000  | Episode Reward: 7  | Average Reward 5.26  | Actor loss: 0.00 | Critic loss: 2.96 | Entropy loss: -0.0002  | Total Loss: 2.97 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 523/129000  | Episode Reward: 7  | Average Reward 5.25  | Actor loss: -0.01 | Critic loss: 4.78 | Entropy loss: -0.0002  | Total Loss: 4.77 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 524/129000  | Episode Reward: 4  | Average Reward 5.25  | Actor loss: -0.05 | Critic loss: 5.97 | Entropy loss: -0.0004  | Total Loss: 5.91 | Total Steps: 43\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 525/129000  | Episode Reward: 10  | Average Reward 5.25  | Actor loss: 0.00 | Critic loss: 2.73 | Entropy loss: -0.0000  | Total Loss: 2.74 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 526/129000  | Episode Reward: 1  | Average Reward 5.21  | Actor loss: -0.58 | Critic loss: 11.17 | Entropy loss: -0.0041  | Total Loss: 10.59 | Total Steps: 55\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 527/129000  | Episode Reward: 1  | Average Reward 5.17  | Actor loss: -0.06 | Critic loss: 6.36 | Entropy loss: -0.0004  | Total Loss: 6.30 | Total Steps: 50\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 528/129000  | Episode Reward: 4  | Average Reward 5.13  | Actor loss: -0.01 | Critic loss: 3.79 | Entropy loss: -0.0002  | Total Loss: 3.78 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 529/129000  | Episode Reward: 10  | Average Reward 5.17  | Actor loss: 1.72 | Critic loss: 2.96 | Entropy loss: -0.0040  | Total Loss: 4.67 | Total Steps: 13\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 530/129000  | Episode Reward: 4  | Average Reward 5.13  | Actor loss: -0.61 | Critic loss: 4.40 | Entropy loss: -0.0033  | Total Loss: 3.79 | Total Steps: 43\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 531/129000  | Episode Reward: 4  | Average Reward 5.13  | Actor loss: -0.02 | Critic loss: 6.98 | Entropy loss: -0.0003  | Total Loss: 6.96 | Total Steps: 42\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 532/129000  | Episode Reward: 10  | Average Reward 5.13  | Actor loss: 0.02 | Critic loss: 12.49 | Entropy loss: -0.0000  | Total Loss: 12.52 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 533/129000  | Episode Reward: 4  | Average Reward 5.12  | Actor loss: -0.14 | Critic loss: 8.13 | Entropy loss: -0.0021  | Total Loss: 8.00 | Total Steps: 42\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 534/129000  | Episode Reward: 3  | Average Reward 5.08  | Actor loss: -0.39 | Critic loss: 6.26 | Entropy loss: -0.0043  | Total Loss: 5.87 | Total Steps: 55\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 535/129000  | Episode Reward: 10  | Average Reward 5.08  | Actor loss: -0.07 | Critic loss: 5.09 | Entropy loss: -0.0007  | Total Loss: 5.01 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 536/129000  | Episode Reward: 7  | Average Reward 5.08  | Actor loss: -0.06 | Critic loss: 1.67 | Entropy loss: -0.0021  | Total Loss: 1.61 | Total Steps: 39\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 537/129000  | Episode Reward: 10  | Average Reward 5.12  | Actor loss: 0.00 | Critic loss: 3.35 | Entropy loss: -0.0000  | Total Loss: 3.36 | Total Steps: 31\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 538/129000  | Episode Reward: 10  | Average Reward 5.14  | Actor loss: 0.64 | Critic loss: 3.46 | Entropy loss: -0.0010  | Total Loss: 4.09 | Total Steps: 11\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 539/129000  | Episode Reward: 4  | Average Reward 5.12  | Actor loss: -0.57 | Critic loss: 4.50 | Entropy loss: -0.0027  | Total Loss: 3.92 | Total Steps: 41\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 540/129000  | Episode Reward: 4  | Average Reward 5.11  | Actor loss: -0.82 | Critic loss: 4.71 | Entropy loss: -0.0056  | Total Loss: 3.88 | Total Steps: 58\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 541/129000  | Episode Reward: 4  | Average Reward 5.08  | Actor loss: 0.01 | Critic loss: 4.19 | Entropy loss: -0.0003  | Total Loss: 4.20 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 542/129000  | Episode Reward: 7  | Average Reward 5.07  | Actor loss: 0.06 | Critic loss: 4.52 | Entropy loss: -0.0004  | Total Loss: 4.59 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 543/129000  | Episode Reward: 7  | Average Reward 5.08  | Actor loss: 0.13 | Critic loss: 5.18 | Entropy loss: -0.0006  | Total Loss: 5.32 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 544/129000  | Episode Reward: 4  | Average Reward 5.12  | Actor loss: 0.04 | Critic loss: 3.76 | Entropy loss: -0.0008  | Total Loss: 3.80 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 545/129000  | Episode Reward: 4  | Average Reward 5.12  | Actor loss: 0.02 | Critic loss: 7.59 | Entropy loss: -0.0008  | Total Loss: 7.60 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 546/129000  | Episode Reward: 10  | Average Reward 5.12  | Actor loss: 0.02 | Critic loss: 9.17 | Entropy loss: -0.0000  | Total Loss: 9.18 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 547/129000  | Episode Reward: 1  | Average Reward 5.07  | Actor loss: 0.14 | Critic loss: 10.06 | Entropy loss: -0.0023  | Total Loss: 10.19 | Total Steps: 53\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 548/129000  | Episode Reward: 10  | Average Reward 5.13  | Actor loss: 0.47 | Critic loss: 3.85 | Entropy loss: -0.0112  | Total Loss: 4.31 | Total Steps: 54\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 549/129000  | Episode Reward: -1  | Average Reward 5.08  | Actor loss: -2.30 | Critic loss: 9.54 | Entropy loss: -0.0166  | Total Loss: 7.22 | Total Steps: 76\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 550/129000  | Episode Reward: 4  | Average Reward 5.06  | Actor loss: -0.01 | Critic loss: 9.13 | Entropy loss: -0.0022  | Total Loss: 9.12 | Total Steps: 53\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 551/129000  | Episode Reward: 10  | Average Reward 5.06  | Actor loss: 0.01 | Critic loss: 3.35 | Entropy loss: -0.0000  | Total Loss: 3.36 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 552/129000  | Episode Reward: 7  | Average Reward 5.08  | Actor loss: 0.12 | Critic loss: 7.06 | Entropy loss: -0.0004  | Total Loss: 7.18 | Total Steps: 29\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 553/129000  | Episode Reward: 4  | Average Reward 5.06  | Actor loss: -0.13 | Critic loss: 3.13 | Entropy loss: -0.0006  | Total Loss: 3.00 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 554/129000  | Episode Reward: 4  | Average Reward 5.04  | Actor loss: -0.09 | Critic loss: 4.30 | Entropy loss: -0.0011  | Total Loss: 4.21 | Total Steps: 79\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 555/129000  | Episode Reward: 7  | Average Reward 5.08  | Actor loss: -0.15 | Critic loss: 3.23 | Entropy loss: -0.0009  | Total Loss: 3.08 | Total Steps: 44\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 556/129000  | Episode Reward: 1  | Average Reward 5.04  | Actor loss: -0.17 | Critic loss: 10.68 | Entropy loss: -0.0025  | Total Loss: 10.51 | Total Steps: 131\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 557/129000  | Episode Reward: 4  | Average Reward 5.01  | Actor loss: -0.71 | Critic loss: 4.71 | Entropy loss: -0.0055  | Total Loss: 4.00 | Total Steps: 52\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 558/129000  | Episode Reward: 10  | Average Reward 5.06  | Actor loss: 0.26 | Critic loss: 2.57 | Entropy loss: -0.0040  | Total Loss: 2.83 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 559/129000  | Episode Reward: 10  | Average Reward 5.08  | Actor loss: 0.48 | Critic loss: 2.26 | Entropy loss: -0.0009  | Total Loss: 2.74 | Total Steps: 11\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 560/129000  | Episode Reward: 4  | Average Reward 5.06  | Actor loss: -0.02 | Critic loss: 6.41 | Entropy loss: -0.0003  | Total Loss: 6.39 | Total Steps: 47\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 561/129000  | Episode Reward: -2  | Average Reward 5.02  | Actor loss: -0.57 | Critic loss: 8.21 | Entropy loss: -0.0043  | Total Loss: 7.63 | Total Steps: 58\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 562/129000  | Episode Reward: 4  | Average Reward 4.99  | Actor loss: 0.03 | Critic loss: 3.80 | Entropy loss: -0.0022  | Total Loss: 3.82 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 563/129000  | Episode Reward: 10  | Average Reward 5.04  | Actor loss: 0.42 | Critic loss: 3.37 | Entropy loss: -0.0007  | Total Loss: 3.79 | Total Steps: 11\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 564/129000  | Episode Reward: 10  | Average Reward 5.05  | Actor loss: 0.00 | Critic loss: 2.29 | Entropy loss: -0.0000  | Total Loss: 2.30 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 565/129000  | Episode Reward: 1  | Average Reward 5.04  | Actor loss: -0.06 | Critic loss: 6.07 | Entropy loss: -0.0004  | Total Loss: 6.01 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 566/129000  | Episode Reward: 7  | Average Reward 5.07  | Actor loss: 0.15 | Critic loss: 9.28 | Entropy loss: -0.0005  | Total Loss: 9.44 | Total Steps: 29\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 567/129000  | Episode Reward: 10  | Average Reward 5.07  | Actor loss: 0.02 | Critic loss: 7.28 | Entropy loss: -0.0000  | Total Loss: 7.29 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 568/129000  | Episode Reward: 4  | Average Reward 5.04  | Actor loss: -0.29 | Critic loss: 3.99 | Entropy loss: -0.0026  | Total Loss: 3.70 | Total Steps: 79\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 569/129000  | Episode Reward: 4  | Average Reward 5.00  | Actor loss: -0.03 | Critic loss: 6.34 | Entropy loss: -0.0003  | Total Loss: 6.31 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 570/129000  | Episode Reward: 10  | Average Reward 5.00  | Actor loss: 0.01 | Critic loss: 1.37 | Entropy loss: -0.0000  | Total Loss: 1.38 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 571/129000  | Episode Reward: -2  | Average Reward 4.97  | Actor loss: -1.26 | Critic loss: 10.59 | Entropy loss: -0.0051  | Total Loss: 9.33 | Total Steps: 57\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 572/129000  | Episode Reward: 4  | Average Reward 4.97  | Actor loss: -0.02 | Critic loss: 4.24 | Entropy loss: -0.0003  | Total Loss: 4.21 | Total Steps: 42\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 573/129000  | Episode Reward: 10  | Average Reward 4.99  | Actor loss: 0.01 | Critic loss: 3.99 | Entropy loss: -0.0002  | Total Loss: 4.00 | Total Steps: 31\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 574/129000  | Episode Reward: 10  | Average Reward 5.00  | Actor loss: 0.01 | Critic loss: 1.43 | Entropy loss: -0.0000  | Total Loss: 1.44 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 575/129000  | Episode Reward: 4  | Average Reward 5.00  | Actor loss: -1.29 | Critic loss: 6.17 | Entropy loss: -0.0107  | Total Loss: 4.87 | Total Steps: 68\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 576/129000  | Episode Reward: 10  | Average Reward 5.00  | Actor loss: 0.01 | Critic loss: 1.34 | Entropy loss: -0.0001  | Total Loss: 1.34 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 577/129000  | Episode Reward: 7  | Average Reward 5.04  | Actor loss: 0.03 | Critic loss: 7.02 | Entropy loss: -0.0002  | Total Loss: 7.06 | Total Steps: 30\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 578/129000  | Episode Reward: 1  | Average Reward 4.99  | Actor loss: -0.17 | Critic loss: 8.81 | Entropy loss: -0.0021  | Total Loss: 8.64 | Total Steps: 53\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 579/129000  | Episode Reward: 10  | Average Reward 4.99  | Actor loss: 0.01 | Critic loss: 1.27 | Entropy loss: -0.0000  | Total Loss: 1.28 | Total Steps: 6\n",
      "---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 580/129000  | Episode Reward: 10  | Average Reward 5.02  | Actor loss: 0.01 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 581/129000  | Episode Reward: 10  | Average Reward 5.02  | Actor loss: 0.01 | Critic loss: 0.89 | Entropy loss: -0.0000  | Total Loss: 0.90 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 582/129000  | Episode Reward: 10  | Average Reward 5.07  | Actor loss: 0.51 | Critic loss: 3.68 | Entropy loss: -0.0006  | Total Loss: 4.19 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 583/129000  | Episode Reward: 4  | Average Reward 5.05  | Actor loss: -0.04 | Critic loss: 5.78 | Entropy loss: -0.0003  | Total Loss: 5.74 | Total Steps: 43\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 584/129000  | Episode Reward: 4  | Average Reward 5.02  | Actor loss: -0.10 | Critic loss: 5.26 | Entropy loss: -0.0012  | Total Loss: 5.16 | Total Steps: 49\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 585/129000  | Episode Reward: 10  | Average Reward 5.02  | Actor loss: 0.01 | Critic loss: 8.82 | Entropy loss: -0.0000  | Total Loss: 8.83 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 586/129000  | Episode Reward: 10  | Average Reward 5.02  | Actor loss: 0.01 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.05 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 587/129000  | Episode Reward: 10  | Average Reward 5.02  | Actor loss: 0.02 | Critic loss: 3.76 | Entropy loss: -0.0001  | Total Loss: 3.78 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 588/129000  | Episode Reward: 1  | Average Reward 5.00  | Actor loss: -2.10 | Critic loss: 8.76 | Entropy loss: -0.0167  | Total Loss: 6.64 | Total Steps: 69\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 589/129000  | Episode Reward: -2  | Average Reward 4.95  | Actor loss: -0.51 | Critic loss: 14.44 | Entropy loss: -0.0052  | Total Loss: 13.92 | Total Steps: 52\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 590/129000  | Episode Reward: 10  | Average Reward 4.95  | Actor loss: 0.06 | Critic loss: 2.78 | Entropy loss: -0.0048  | Total Loss: 2.83 | Total Steps: 50\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 591/129000  | Episode Reward: 4  | Average Reward 4.96  | Actor loss: -0.01 | Critic loss: 8.45 | Entropy loss: -0.0002  | Total Loss: 8.44 | Total Steps: 47\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 592/129000  | Episode Reward: 10  | Average Reward 4.96  | Actor loss: 0.01 | Critic loss: 0.53 | Entropy loss: -0.0000  | Total Loss: 0.54 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 593/129000  | Episode Reward: 10  | Average Reward 4.97  | Actor loss: 0.01 | Critic loss: 1.56 | Entropy loss: -0.0000  | Total Loss: 1.57 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 594/129000  | Episode Reward: 7  | Average Reward 4.96  | Actor loss: 0.02 | Critic loss: 7.50 | Entropy loss: -0.0001  | Total Loss: 7.52 | Total Steps: 34\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 595/129000  | Episode Reward: 7  | Average Reward 4.99  | Actor loss: -0.42 | Critic loss: 3.99 | Entropy loss: -0.0092  | Total Loss: 3.56 | Total Steps: 73\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 596/129000  | Episode Reward: 10  | Average Reward 5.02  | Actor loss: 0.08 | Critic loss: 3.51 | Entropy loss: -0.0006  | Total Loss: 3.59 | Total Steps: 30\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 597/129000  | Episode Reward: 10  | Average Reward 5.04  | Actor loss: 0.02 | Critic loss: 2.87 | Entropy loss: -0.0000  | Total Loss: 2.89 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 598/129000  | Episode Reward: 7  | Average Reward 5.04  | Actor loss: -0.05 | Critic loss: 4.89 | Entropy loss: -0.0006  | Total Loss: 4.85 | Total Steps: 43\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 599/129000  | Episode Reward: 7  | Average Reward 5.07  | Actor loss: -0.34 | Critic loss: 2.25 | Entropy loss: -0.0076  | Total Loss: 1.90 | Total Steps: 70\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 600/129000  | Episode Reward: 10  | Average Reward 5.08  | Actor loss: 0.02 | Critic loss: 0.50 | Entropy loss: -0.0001  | Total Loss: 0.52 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 601/129000  | Episode Reward: 7  | Average Reward 5.09  | Actor loss: 0.02 | Critic loss: 8.76 | Entropy loss: -0.0003  | Total Loss: 8.78 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 1/100  | Episode Reward: -115  | Average Reward -3.07  | Actor loss: -1.40 | Critic loss: 87.95 | Entropy loss: -0.0208  | Total Loss: 86.53 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 2/100  | Episode Reward: -76  | Average Reward -3.48  | Actor loss: -2.71 | Critic loss: 87.54 | Entropy loss: -0.0342  | Total Loss: 84.79 | Total Steps: 500\n",
      "TEST: ---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 7  | Average Reward -3.46  | Actor loss: 0.07 | Critic loss: 8.43 | Entropy loss: -0.0044  | Total Loss: 8.50 | Total Steps: 95\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 1  | Average Reward -3.46  | Actor loss: 0.33 | Critic loss: 5.69 | Entropy loss: -0.0046  | Total Loss: 6.02 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 1  | Average Reward -3.42  | Actor loss: 0.00 | Critic loss: 2.82 | Entropy loss: -0.0019  | Total Loss: 2.82 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 6/100  | Episode Reward: -103  | Average Reward -3.38  | Actor loss: -1.56 | Critic loss: 95.12 | Entropy loss: -0.0278  | Total Loss: 93.52 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 4  | Average Reward -3.41  | Actor loss: 0.00 | Critic loss: 1.27 | Entropy loss: -0.0045  | Total Loss: 1.27 | Total Steps: 42\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 7  | Average Reward -3.40  | Actor loss: 2.80 | Critic loss: 18.20 | Entropy loss: -0.0481  | Total Loss: 20.95 | Total Steps: 37\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward -3.37  | Actor loss: 0.01 | Critic loss: 11.46 | Entropy loss: -0.0083  | Total Loss: 11.46 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward -3.35  | Actor loss: 0.00 | Critic loss: 1.99 | Entropy loss: -0.0021  | Total Loss: 1.99 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 7  | Average Reward -3.32  | Actor loss: 0.01 | Critic loss: 14.88 | Entropy loss: -0.0011  | Total Loss: 14.89 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 4  | Average Reward -3.35  | Actor loss: 0.00 | Critic loss: 5.09 | Entropy loss: -0.0072  | Total Loss: 5.09 | Total Steps: 49\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 1  | Average Reward -2.69  | Actor loss: 0.35 | Critic loss: 12.31 | Entropy loss: -0.0221  | Total Loss: 12.64 | Total Steps: 73\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 7  | Average Reward -2.69  | Actor loss: 0.00 | Critic loss: 1.35 | Entropy loss: -0.0010  | Total Loss: 1.36 | Total Steps: 38\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 4  | Average Reward -2.66  | Actor loss: 0.01 | Critic loss: 0.89 | Entropy loss: -0.0054  | Total Loss: 0.89 | Total Steps: 44\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 1  | Average Reward -2.71  | Actor loss: 0.02 | Critic loss: 4.41 | Entropy loss: -0.0117  | Total Loss: 4.42 | Total Steps: 61\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 7  | Average Reward -2.71  | Actor loss: 0.02 | Critic loss: 3.97 | Entropy loss: -0.0020  | Total Loss: 3.99 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward -2.71  | Actor loss: 3.81 | Critic loss: 34.09 | Entropy loss: -0.0205  | Total Loss: 37.88 | Total Steps: 36\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 1  | Average Reward -2.72  | Actor loss: 10.35 | Critic loss: 23.35 | Entropy loss: -0.0108  | Total Loss: 33.69 | Total Steps: 58\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 1  | Average Reward -2.75  | Actor loss: 1.33 | Critic loss: 15.89 | Entropy loss: -0.0197  | Total Loss: 17.21 | Total Steps: 59\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 7  | Average Reward -2.77  | Actor loss: 0.28 | Critic loss: 11.35 | Entropy loss: -0.0230  | Total Loss: 11.61 | Total Steps: 32\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 7  | Average Reward -2.46  | Actor loss: 2.47 | Critic loss: 29.89 | Entropy loss: -0.0126  | Total Loss: 32.35 | Total Steps: 45\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 4  | Average Reward -2.48  | Actor loss: 0.01 | Critic loss: 0.76 | Entropy loss: -0.0085  | Total Loss: 0.76 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Average Reward -2.51  | Actor loss: 0.04 | Critic loss: 2.93 | Entropy loss: -0.0065  | Total Loss: 2.97 | Total Steps: 47\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: -8  | Average Reward -2.57  | Actor loss: 0.01 | Critic loss: 1.06 | Entropy loss: -0.0266  | Total Loss: 1.04 | Total Steps: 115\n",
      "TEST: ---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward -2.57  | Actor loss: 0.19 | Critic loss: 11.60 | Entropy loss: -0.0015  | Total Loss: 11.79 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward -2.58  | Actor loss: 1.69 | Critic loss: 15.31 | Entropy loss: -0.0037  | Total Loss: 17.00 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 7  | Average Reward -2.57  | Actor loss: 2.00 | Critic loss: 13.45 | Entropy loss: -0.0067  | Total Loss: 15.44 | Total Steps: 40\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 7  | Average Reward -2.56  | Actor loss: 0.01 | Critic loss: 14.55 | Entropy loss: -0.0059  | Total Loss: 14.55 | Total Steps: 73\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 1  | Average Reward -2.57  | Actor loss: 0.01 | Critic loss: 7.36 | Entropy loss: -0.0067  | Total Loss: 7.36 | Total Steps: 62\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward -2.54  | Actor loss: 0.00 | Critic loss: 3.11 | Entropy loss: -0.0029  | Total Loss: 3.11 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 1  | Average Reward -2.57  | Actor loss: 0.00 | Critic loss: 2.42 | Entropy loss: -0.0022  | Total Loss: 2.42 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward -2.57  | Actor loss: 2.22 | Critic loss: 32.15 | Entropy loss: -0.0335  | Total Loss: 34.34 | Total Steps: 39\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 34/100  | Episode Reward: -103  | Average Reward -3.12  | Actor loss: -1.66 | Critic loss: 100.64 | Entropy loss: -0.0243  | Total Loss: 98.95 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 7  | Average Reward -3.13  | Actor loss: 1.25 | Critic loss: 24.78 | Entropy loss: -0.0446  | Total Loss: 25.98 | Total Steps: 56\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 7  | Average Reward -3.15  | Actor loss: 0.00 | Critic loss: 1.35 | Entropy loss: -0.0016  | Total Loss: 1.36 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 3  | Average Reward -3.15  | Actor loss: 0.01 | Critic loss: 0.88 | Entropy loss: -0.0184  | Total Loss: 0.86 | Total Steps: 45\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 4  | Average Reward -3.14  | Actor loss: 1.60 | Critic loss: 4.27 | Entropy loss: -0.0042  | Total Loss: 5.87 | Total Steps: 49\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 4  | Average Reward -3.15  | Actor loss: 0.01 | Critic loss: 0.82 | Entropy loss: -0.0114  | Total Loss: 0.83 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 7  | Average Reward -3.08  | Actor loss: 0.00 | Critic loss: 7.07 | Entropy loss: -0.0019  | Total Loss: 7.08 | Total Steps: 36\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 41/100  | Episode Reward: -91  | Average Reward -3.57  | Actor loss: -0.01 | Critic loss: 64.77 | Entropy loss: -0.0286  | Total Loss: 64.72 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 7  | Average Reward -3.56  | Actor loss: 5.63 | Critic loss: 29.13 | Entropy loss: -0.0381  | Total Loss: 34.72 | Total Steps: 35\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 4  | Average Reward -3.58  | Actor loss: 0.11 | Critic loss: 8.16 | Entropy loss: -0.0059  | Total Loss: 8.27 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 1  | Average Reward -3.60  | Actor loss: 0.51 | Critic loss: 8.06 | Entropy loss: -0.0044  | Total Loss: 8.56 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 7  | Average Reward -3.58  | Actor loss: 0.01 | Critic loss: 14.94 | Entropy loss: -0.0009  | Total Loss: 14.95 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 4  | Average Reward -3.60  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0036  | Total Loss: 1.38 | Total Steps: 49\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: -15  | Average Reward -3.73  | Actor loss: 3.19 | Critic loss: 19.44 | Entropy loss: -0.0432  | Total Loss: 22.59 | Total Steps: 166\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward -3.74  | Actor loss: 0.00 | Critic loss: 4.27 | Entropy loss: -0.0015  | Total Loss: 4.27 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 1  | Average Reward -3.79  | Actor loss: 1.08 | Critic loss: 15.02 | Entropy loss: -0.0330  | Total Loss: 16.08 | Total Steps: 55\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward -3.75  | Actor loss: 1.02 | Critic loss: 17.76 | Entropy loss: -0.0107  | Total Loss: 18.77 | Total Steps: 37\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 1  | Average Reward -3.16  | Actor loss: 0.02 | Critic loss: 0.64 | Entropy loss: -0.0084  | Total Loss: 0.64 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward -3.13  | Actor loss: 21.64 | Critic loss: 28.88 | Entropy loss: -0.0535  | Total Loss: 50.47 | Total Steps: 9\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 7  | Average Reward -3.12  | Actor loss: 2.33 | Critic loss: 25.22 | Entropy loss: -0.0294  | Total Loss: 27.53 | Total Steps: 37\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward -3.12  | Actor loss: 0.01 | Critic loss: 12.28 | Entropy loss: -0.0052  | Total Loss: 12.28 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 10  | Average Reward -3.08  | Actor loss: 0.55 | Critic loss: 15.68 | Entropy loss: -0.0106  | Total Loss: 16.22 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 1  | Average Reward -3.08  | Actor loss: 0.07 | Critic loss: 2.44 | Entropy loss: -0.0029  | Total Loss: 2.51 | Total Steps: 53\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward -2.98  | Actor loss: 0.01 | Critic loss: 2.51 | Entropy loss: -0.0008  | Total Loss: 2.52 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward -2.98  | Actor loss: 22.20 | Critic loss: 29.80 | Entropy loss: -0.0543  | Total Loss: 51.95 | Total Steps: 22\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 1  | Average Reward -2.43  | Actor loss: 0.56 | Critic loss: 12.64 | Entropy loss: -0.0261  | Total Loss: 13.17 | Total Steps: 79\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 60/100  | Episode Reward: -10  | Average Reward -2.53  | Actor loss: -0.00 | Critic loss: 77.34 | Entropy loss: -0.0003  | Total Loss: 77.34 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Average Reward -2.50  | Actor loss: 0.01 | Critic loss: 2.09 | Entropy loss: -0.0208  | Total Loss: 2.08 | Total Steps: 8\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward -2.50  | Actor loss: 0.00 | Critic loss: 4.86 | Entropy loss: -0.0007  | Total Loss: 4.86 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward -2.47  | Actor loss: 13.03 | Critic loss: 17.69 | Entropy loss: -0.0698  | Total Loss: 30.65 | Total Steps: 11\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 7  | Average Reward -2.44  | Actor loss: 1.50 | Critic loss: 23.50 | Entropy loss: -0.0336  | Total Loss: 24.97 | Total Steps: 27\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward -2.44  | Actor loss: 0.01 | Critic loss: 1.63 | Entropy loss: -0.0212  | Total Loss: 1.63 | Total Steps: 44\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 4  | Average Reward -2.47  | Actor loss: 0.00 | Critic loss: 3.62 | Entropy loss: -0.0012  | Total Loss: 3.62 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 4  | Average Reward -2.48  | Actor loss: 0.42 | Critic loss: 11.49 | Entropy loss: -0.0297  | Total Loss: 11.89 | Total Steps: 60\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 4  | Average Reward -2.48  | Actor loss: 0.01 | Critic loss: 0.70 | Entropy loss: -0.0112  | Total Loss: 0.69 | Total Steps: 44\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 1  | Average Reward -2.53  | Actor loss: 0.07 | Critic loss: 1.14 | Entropy loss: -0.0069  | Total Loss: 1.20 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 70/100  | Episode Reward: -109  | Average Reward -3.10  | Actor loss: -0.01 | Critic loss: 59.88 | Entropy loss: -0.0217  | Total Loss: 59.85 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 4  | Average Reward -2.50  | Actor loss: 0.00 | Critic loss: 1.13 | Entropy loss: -0.0017  | Total Loss: 1.13 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 1  | Average Reward -2.52  | Actor loss: 0.01 | Critic loss: 0.90 | Entropy loss: -0.0367  | Total Loss: 0.87 | Total Steps: 55\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 73/100  | Episode Reward: -106  | Average Reward -3.10  | Actor loss: -0.02 | Critic loss: 77.57 | Entropy loss: -0.0249  | Total Loss: 77.53 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 4  | Average Reward -3.12  | Actor loss: 0.01 | Critic loss: 3.30 | Entropy loss: -0.0010  | Total Loss: 3.30 | Total Steps: 46\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 1  | Average Reward -3.17  | Actor loss: 0.01 | Critic loss: 0.71 | Entropy loss: -0.0257  | Total Loss: 0.70 | Total Steps: 56\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 1  | Average Reward -2.93  | Actor loss: 0.00 | Critic loss: 2.42 | Entropy loss: -0.0026  | Total Loss: 2.42 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 77/100  | Episode Reward: -109  | Average Reward -3.50  | Actor loss: -20.68 | Critic loss: 98.10 | Entropy loss: -0.0167  | Total Loss: 77.40 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward -3.46  | Actor loss: 0.00 | Critic loss: 1.98 | Entropy loss: -0.0033  | Total Loss: 1.98 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 7  | Average Reward -2.78  | Actor loss: 0.01 | Critic loss: 13.09 | Entropy loss: -0.0080  | Total Loss: 13.09 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 1  | Average Reward -2.78  | Actor loss: 0.33 | Critic loss: 5.12 | Entropy loss: -0.0038  | Total Loss: 5.44 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 4  | Average Reward -2.78  | Actor loss: 0.01 | Critic loss: 1.45 | Entropy loss: -0.0009  | Total Loss: 1.46 | Total Steps: 49\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward -2.78  | Actor loss: 0.01 | Critic loss: 15.13 | Entropy loss: -0.0004  | Total Loss: 15.14 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: -8  | Average Reward -2.81  | Actor loss: 2.85 | Critic loss: 28.97 | Entropy loss: -0.0368  | Total Loss: 31.79 | Total Steps: 97\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 7  | Average Reward -2.81  | Actor loss: 0.00 | Critic loss: 7.05 | Entropy loss: -0.0033  | Total Loss: 7.05 | Total Steps: 36\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 10  | Average Reward -2.20  | Actor loss: 0.00 | Critic loss: 1.91 | Entropy loss: -0.0012  | Total Loss: 1.91 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward -2.14  | Actor loss: 0.00 | Critic loss: 5.43 | Entropy loss: -0.0093  | Total Loss: 5.42 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward -2.14  | Actor loss: 0.01 | Critic loss: 2.45 | Entropy loss: -0.0134  | Total Loss: 2.45 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward -2.11  | Actor loss: 4.75 | Critic loss: 20.52 | Entropy loss: -0.0623  | Total Loss: 25.20 | Total Steps: 18\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward -2.01  | Actor loss: 0.00 | Critic loss: 3.32 | Entropy loss: -0.0024  | Total Loss: 3.32 | Total Steps: 31\n",
      "TEST: ---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward -2.01  | Actor loss: 0.18 | Critic loss: 11.59 | Entropy loss: -0.0010  | Total Loss: 11.78 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 91/100  | Episode Reward: -103  | Average Reward -2.54  | Actor loss: -9.37 | Critic loss: 88.41 | Entropy loss: -0.0306  | Total Loss: 79.01 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward -2.54  | Actor loss: 4.79 | Critic loss: 14.97 | Entropy loss: -0.0299  | Total Loss: 19.72 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 93/100  | Episode Reward: -94  | Average Reward -3.05  | Actor loss: -0.61 | Critic loss: 114.56 | Entropy loss: -0.0344  | Total Loss: 113.92 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 10  | Average Reward -2.38  | Actor loss: 0.00 | Critic loss: 1.50 | Entropy loss: -0.0062  | Total Loss: 1.50 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward -2.38  | Actor loss: 2.46 | Critic loss: 29.54 | Entropy loss: -0.0285  | Total Loss: 31.97 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 7  | Average Reward -2.40  | Actor loss: 0.77 | Critic loss: 17.44 | Entropy loss: -0.0091  | Total Loss: 18.20 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 10  | Average Reward -1.86  | Actor loss: 0.00 | Critic loss: 1.75 | Entropy loss: -0.0019  | Total Loss: 1.75 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward -1.86  | Actor loss: 0.00 | Critic loss: 3.43 | Entropy loss: -0.0055  | Total Loss: 3.43 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: -11  | Average Reward -1.94  | Actor loss: 0.60 | Critic loss: 15.20 | Entropy loss: -0.0409  | Total Loss: 15.76 | Total Steps: 155\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward -1.94  | Actor loss: 4.65 | Critic loss: 4.19 | Entropy loss: -0.0561  | Total Loss: 8.78 | Total Steps: 69\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 602/129000  | Episode Reward: 10  | Average Reward 5.11  | Actor loss: 0.01 | Critic loss: 0.91 | Entropy loss: -0.0000  | Total Loss: 0.92 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 603/129000  | Episode Reward: 1  | Average Reward 5.08  | Actor loss: -0.03 | Critic loss: 13.24 | Entropy loss: -0.0026  | Total Loss: 13.21 | Total Steps: 54\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 604/129000  | Episode Reward: 7  | Average Reward 5.09  | Actor loss: 0.19 | Critic loss: 8.53 | Entropy loss: -0.0008  | Total Loss: 8.72 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 605/129000  | Episode Reward: 0  | Average Reward 5.09  | Actor loss: -0.87 | Critic loss: 10.31 | Entropy loss: -0.0052  | Total Loss: 9.43 | Total Steps: 50\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 606/129000  | Episode Reward: 10  | Average Reward 5.09  | Actor loss: 0.00 | Critic loss: 1.01 | Entropy loss: -0.0000  | Total Loss: 1.01 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 607/129000  | Episode Reward: 4  | Average Reward 5.06  | Actor loss: -0.07 | Critic loss: 7.36 | Entropy loss: -0.0010  | Total Loss: 7.29 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 608/129000  | Episode Reward: 7  | Average Reward 5.06  | Actor loss: 0.34 | Critic loss: 5.36 | Entropy loss: -0.0039  | Total Loss: 5.69 | Total Steps: 50\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 609/129000  | Episode Reward: 7  | Average Reward 5.11  | Actor loss: -0.17 | Critic loss: 5.99 | Entropy loss: -0.0055  | Total Loss: 5.82 | Total Steps: 50\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 610/129000  | Episode Reward: 7  | Average Reward 5.12  | Actor loss: -0.22 | Critic loss: 11.92 | Entropy loss: -0.0020  | Total Loss: 11.70 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 611/129000  | Episode Reward: 7  | Average Reward 5.11  | Actor loss: 0.71 | Critic loss: 7.58 | Entropy loss: -0.0029  | Total Loss: 8.29 | Total Steps: 31\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 612/129000  | Episode Reward: 10  | Average Reward 5.12  | Actor loss: 0.00 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.36 | Total Steps: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 613/129000  | Episode Reward: 10  | Average Reward 5.13  | Actor loss: 0.31 | Critic loss: 5.21 | Entropy loss: -0.0011  | Total Loss: 5.52 | Total Steps: 32\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 614/129000  | Episode Reward: -9  | Average Reward 5.05  | Actor loss: -0.97 | Critic loss: 20.98 | Entropy loss: -0.0106  | Total Loss: 19.99 | Total Steps: 142\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 615/129000  | Episode Reward: 10  | Average Reward 5.07  | Actor loss: 0.95 | Critic loss: 1.19 | Entropy loss: -0.0040  | Total Loss: 2.14 | Total Steps: 15\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 616/129000  | Episode Reward: 10  | Average Reward 5.08  | Actor loss: 0.31 | Critic loss: 6.21 | Entropy loss: -0.0104  | Total Loss: 6.52 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 617/129000  | Episode Reward: 10  | Average Reward 5.12  | Actor loss: 0.85 | Critic loss: 2.86 | Entropy loss: -0.0018  | Total Loss: 3.71 | Total Steps: 10\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 618/129000  | Episode Reward: 4  | Average Reward 5.10  | Actor loss: -0.03 | Critic loss: 8.14 | Entropy loss: -0.0003  | Total Loss: 8.10 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 619/129000  | Episode Reward: 1  | Average Reward 5.10  | Actor loss: -0.23 | Critic loss: 7.22 | Entropy loss: -0.0017  | Total Loss: 6.98 | Total Steps: 54\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 620/129000  | Episode Reward: 1  | Average Reward 5.05  | Actor loss: -0.59 | Critic loss: 11.78 | Entropy loss: -0.0030  | Total Loss: 11.19 | Total Steps: 52\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 621/129000  | Episode Reward: 4  | Average Reward 5.07  | Actor loss: -0.17 | Critic loss: 5.51 | Entropy loss: -0.0021  | Total Loss: 5.34 | Total Steps: 51\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 622/129000  | Episode Reward: 10  | Average Reward 5.07  | Actor loss: 0.00 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.36 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 623/129000  | Episode Reward: 4  | Average Reward 5.05  | Actor loss: -0.04 | Critic loss: 8.13 | Entropy loss: -0.0005  | Total Loss: 8.10 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 624/129000  | Episode Reward: 4  | Average Reward 5.04  | Actor loss: -0.10 | Critic loss: 8.44 | Entropy loss: -0.0004  | Total Loss: 8.33 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 625/129000  | Episode Reward: 7  | Average Reward 5.07  | Actor loss: -1.07 | Critic loss: 4.14 | Entropy loss: -0.0080  | Total Loss: 3.05 | Total Steps: 54\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 626/129000  | Episode Reward: 4  | Average Reward 5.10  | Actor loss: 0.02 | Critic loss: 8.81 | Entropy loss: -0.0010  | Total Loss: 8.83 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 627/129000  | Episode Reward: 1  | Average Reward 5.05  | Actor loss: -0.36 | Critic loss: 8.61 | Entropy loss: -0.0050  | Total Loss: 8.25 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 628/129000  | Episode Reward: -11  | Average Reward 5.00  | Actor loss: -0.84 | Critic loss: 17.94 | Entropy loss: -0.0109  | Total Loss: 17.08 | Total Steps: 142\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 629/129000  | Episode Reward: 10  | Average Reward 5.04  | Actor loss: 0.69 | Critic loss: 6.72 | Entropy loss: -0.0006  | Total Loss: 7.41 | Total Steps: 8\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 630/129000  | Episode Reward: 1  | Average Reward 5.01  | Actor loss: -1.00 | Critic loss: 10.73 | Entropy loss: -0.0030  | Total Loss: 9.73 | Total Steps: 45\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 631/129000  | Episode Reward: 7  | Average Reward 5.03  | Actor loss: 0.26 | Critic loss: 5.14 | Entropy loss: -0.0011  | Total Loss: 5.40 | Total Steps: 32\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 632/129000  | Episode Reward: 10  | Average Reward 5.05  | Actor loss: 0.09 | Critic loss: 5.64 | Entropy loss: -0.0005  | Total Loss: 5.73 | Total Steps: 31\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 633/129000  | Episode Reward: 1  | Average Reward 5.01  | Actor loss: -0.87 | Critic loss: 5.96 | Entropy loss: -0.0065  | Total Loss: 5.09 | Total Steps: 64\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 634/129000  | Episode Reward: 4  | Average Reward 5.01  | Actor loss: -1.45 | Critic loss: 5.23 | Entropy loss: -0.0082  | Total Loss: 3.77 | Total Steps: 53\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 635/129000  | Episode Reward: 4  | Average Reward 5.03  | Actor loss: -0.03 | Critic loss: 6.82 | Entropy loss: -0.0006  | Total Loss: 6.79 | Total Steps: 43\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 636/129000  | Episode Reward: 10  | Average Reward 5.04  | Actor loss: 1.84 | Critic loss: 7.57 | Entropy loss: -0.0040  | Total Loss: 9.40 | Total Steps: 16\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 637/129000  | Episode Reward: 10  | Average Reward 5.04  | Actor loss: 0.01 | Critic loss: 2.10 | Entropy loss: -0.0000  | Total Loss: 2.10 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 638/129000  | Episode Reward: 1  | Average Reward 5.05  | Actor loss: -0.32 | Critic loss: 10.02 | Entropy loss: -0.0037  | Total Loss: 9.70 | Total Steps: 73\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 639/129000  | Episode Reward: 10  | Average Reward 5.07  | Actor loss: 0.42 | Critic loss: 3.81 | Entropy loss: -0.0005  | Total Loss: 4.23 | Total Steps: 8\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 640/129000  | Episode Reward: 7  | Average Reward 5.12  | Actor loss: 0.65 | Critic loss: 6.67 | Entropy loss: -0.0070  | Total Loss: 7.32 | Total Steps: 52\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 641/129000  | Episode Reward: 10  | Average Reward 5.12  | Actor loss: 1.03 | Critic loss: 7.24 | Entropy loss: -0.0007  | Total Loss: 8.28 | Total Steps: 7\n",
      "---capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 642/129000  | Episode Reward: 10  | Average Reward 5.13  | Actor loss: 0.20 | Critic loss: 5.51 | Entropy loss: -0.0007  | Total Loss: 5.71 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 643/129000  | Episode Reward: 7  | Average Reward 5.14  | Actor loss: 0.04 | Critic loss: 9.36 | Entropy loss: -0.0002  | Total Loss: 9.40 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 644/129000  | Episode Reward: 7  | Average Reward 5.16  | Actor loss: 0.12 | Critic loss: 8.87 | Entropy loss: -0.0005  | Total Loss: 8.98 | Total Steps: 32\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 645/129000  | Episode Reward: 10  | Average Reward 5.19  | Actor loss: 0.01 | Critic loss: 4.03 | Entropy loss: -0.0000  | Total Loss: 4.03 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 646/129000  | Episode Reward: 10  | Average Reward 5.22  | Actor loss: 0.07 | Critic loss: 15.13 | Entropy loss: -0.0000  | Total Loss: 15.20 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 647/129000  | Episode Reward: 1  | Average Reward 5.19  | Actor loss: -0.24 | Critic loss: 11.18 | Entropy loss: -0.0028  | Total Loss: 10.94 | Total Steps: 55\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 648/129000  | Episode Reward: 7  | Average Reward 5.17  | Actor loss: 0.10 | Critic loss: 8.96 | Entropy loss: -0.0008  | Total Loss: 9.06 | Total Steps: 32\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 649/129000  | Episode Reward: 7  | Average Reward 5.19  | Actor loss: 0.30 | Critic loss: 6.83 | Entropy loss: -0.0018  | Total Loss: 7.13 | Total Steps: 45\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 650/129000  | Episode Reward: 7  | Average Reward 5.21  | Actor loss: 0.27 | Critic loss: 8.73 | Entropy loss: -0.0012  | Total Loss: 9.00 | Total Steps: 29\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 651/129000  | Episode Reward: 10  | Average Reward 5.21  | Actor loss: 0.06 | Critic loss: 13.53 | Entropy loss: -0.0000  | Total Loss: 13.59 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 652/129000  | Episode Reward: -2  | Average Reward 5.16  | Actor loss: -1.05 | Critic loss: 15.98 | Entropy loss: -0.0035  | Total Loss: 14.92 | Total Steps: 55\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 653/129000  | Episode Reward: 10  | Average Reward 5.16  | Actor loss: 0.53 | Critic loss: 5.21 | Entropy loss: -0.0042  | Total Loss: 5.73 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 654/129000  | Episode Reward: 4  | Average Reward 5.13  | Actor loss: -0.01 | Critic loss: 8.16 | Entropy loss: -0.0002  | Total Loss: 8.15 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 655/129000  | Episode Reward: 7  | Average Reward 5.13  | Actor loss: 0.35 | Critic loss: 7.44 | Entropy loss: -0.0015  | Total Loss: 7.79 | Total Steps: 31\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 656/129000  | Episode Reward: 10  | Average Reward 5.16  | Actor loss: 0.02 | Critic loss: 10.70 | Entropy loss: -0.0000  | Total Loss: 10.72 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 657/129000  | Episode Reward: 10  | Average Reward 5.16  | Actor loss: 0.21 | Critic loss: 6.27 | Entropy loss: -0.0002  | Total Loss: 6.48 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 658/129000  | Episode Reward: 10  | Average Reward 5.17  | Actor loss: 0.02 | Critic loss: 2.64 | Entropy loss: -0.0006  | Total Loss: 2.66 | Total Steps: 31\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 659/129000  | Episode Reward: 7  | Average Reward 5.19  | Actor loss: -0.26 | Critic loss: 6.02 | Entropy loss: -0.0026  | Total Loss: 5.76 | Total Steps: 55\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 660/129000  | Episode Reward: 4  | Average Reward 5.17  | Actor loss: -0.14 | Critic loss: 8.02 | Entropy loss: -0.0008  | Total Loss: 7.87 | Total Steps: 43\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 661/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: 0.39 | Critic loss: 7.73 | Entropy loss: -0.0019  | Total Loss: 8.12 | Total Steps: 31\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 662/129000  | Episode Reward: 4  | Average Reward 5.70  | Actor loss: -0.04 | Critic loss: 10.21 | Entropy loss: -0.0005  | Total Loss: 10.16 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 663/129000  | Episode Reward: -8  | Average Reward 5.62  | Actor loss: -0.40 | Critic loss: 24.99 | Entropy loss: -0.0019  | Total Loss: 24.60 | Total Steps: 87\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 664/129000  | Episode Reward: 4  | Average Reward 5.59  | Actor loss: -0.08 | Critic loss: 5.31 | Entropy loss: -0.0006  | Total Loss: 5.23 | Total Steps: 52\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 665/129000  | Episode Reward: -2  | Average Reward 5.54  | Actor loss: -0.55 | Critic loss: 11.88 | Entropy loss: -0.0017  | Total Loss: 11.33 | Total Steps: 57\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 666/129000  | Episode Reward: 4  | Average Reward 5.54  | Actor loss: -0.05 | Critic loss: 8.65 | Entropy loss: -0.0003  | Total Loss: 8.60 | Total Steps: 52\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 667/129000  | Episode Reward: 10  | Average Reward 5.54  | Actor loss: 0.11 | Critic loss: 5.04 | Entropy loss: -0.0005  | Total Loss: 5.15 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 668/129000  | Episode Reward: 4  | Average Reward 5.82  | Actor loss: -0.14 | Critic loss: 6.90 | Entropy loss: -0.0010  | Total Loss: 6.76 | Total Steps: 43\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 669/129000  | Episode Reward: 10  | Average Reward 5.83  | Actor loss: 0.01 | Critic loss: 2.14 | Entropy loss: -0.0000  | Total Loss: 2.15 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 670/129000  | Episode Reward: -6  | Average Reward 5.80  | Actor loss: -1.01 | Critic loss: 19.50 | Entropy loss: -0.0063  | Total Loss: 18.49 | Total Steps: 103\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  | Episode: 671/129000  | Episode Reward: 10  | Average Reward 5.80  | Actor loss: 0.01 | Critic loss: 2.83 | Entropy loss: -0.0000  | Total Loss: 2.84 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 672/129000  | Episode Reward: 7  | Average Reward 5.79  | Actor loss: 0.07 | Critic loss: 5.23 | Entropy loss: -0.0019  | Total Loss: 5.29 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 673/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: 0.18 | Critic loss: 7.55 | Entropy loss: -0.0007  | Total Loss: 7.73 | Total Steps: 31\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 674/129000  | Episode Reward: 4  | Average Reward 5.79  | Actor loss: -0.68 | Critic loss: 9.56 | Entropy loss: -0.0113  | Total Loss: 8.87 | Total Steps: 89\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 675/129000  | Episode Reward: 7  | Average Reward 5.77  | Actor loss: 0.06 | Critic loss: 7.00 | Entropy loss: -0.0002  | Total Loss: 7.06 | Total Steps: 29\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 676/129000  | Episode Reward: 10  | Average Reward 5.82  | Actor loss: 0.01 | Critic loss: 1.98 | Entropy loss: -0.0000  | Total Loss: 1.99 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 677/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: -0.15 | Critic loss: 6.89 | Entropy loss: -0.0019  | Total Loss: 6.73 | Total Steps: 40\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 678/129000  | Episode Reward: 10  | Average Reward 5.80  | Actor loss: 0.01 | Critic loss: 5.04 | Entropy loss: -0.0000  | Total Loss: 5.05 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 679/129000  | Episode Reward: 1  | Average Reward 5.77  | Actor loss: -0.60 | Critic loss: 9.61 | Entropy loss: -0.0071  | Total Loss: 9.01 | Total Steps: 55\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 680/129000  | Episode Reward: 4  | Average Reward 5.79  | Actor loss: -0.19 | Critic loss: 7.71 | Entropy loss: -0.0067  | Total Loss: 7.52 | Total Steps: 58\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 681/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: 0.11 | Critic loss: 7.71 | Entropy loss: -0.0003  | Total Loss: 7.82 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 682/129000  | Episode Reward: 1  | Average Reward 5.79  | Actor loss: -0.04 | Critic loss: 8.56 | Entropy loss: -0.0031  | Total Loss: 8.51 | Total Steps: 54\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 683/129000  | Episode Reward: 4  | Average Reward 5.84  | Actor loss: -0.09 | Critic loss: 4.62 | Entropy loss: -0.0005  | Total Loss: 4.53 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 684/129000  | Episode Reward: 4  | Average Reward 5.82  | Actor loss: 0.07 | Critic loss: 5.53 | Entropy loss: -0.0013  | Total Loss: 5.60 | Total Steps: 44\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 685/129000  | Episode Reward: 10  | Average Reward 5.86  | Actor loss: 0.02 | Critic loss: 2.63 | Entropy loss: -0.0000  | Total Loss: 2.65 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 686/129000  | Episode Reward: 10  | Average Reward 5.93  | Actor loss: 0.01 | Critic loss: 1.54 | Entropy loss: -0.0000  | Total Loss: 1.55 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 687/129000  | Episode Reward: 10  | Average Reward 5.95  | Actor loss: 0.19 | Critic loss: 3.35 | Entropy loss: -0.0002  | Total Loss: 3.54 | Total Steps: 8\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 688/129000  | Episode Reward: 10  | Average Reward 5.95  | Actor loss: 0.07 | Critic loss: 3.88 | Entropy loss: -0.0001  | Total Loss: 3.95 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 689/129000  | Episode Reward: 7  | Average Reward 5.95  | Actor loss: 0.10 | Critic loss: 7.40 | Entropy loss: -0.0005  | Total Loss: 7.50 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 690/129000  | Episode Reward: 4  | Average Reward 5.92  | Actor loss: -0.04 | Critic loss: 6.06 | Entropy loss: -0.0005  | Total Loss: 6.02 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 691/129000  | Episode Reward: 7  | Average Reward 5.91  | Actor loss: -0.05 | Critic loss: 4.47 | Entropy loss: -0.0004  | Total Loss: 4.42 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 692/129000  | Episode Reward: 7  | Average Reward 5.92  | Actor loss: -0.02 | Critic loss: 2.94 | Entropy loss: -0.0005  | Total Loss: 2.92 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 693/129000  | Episode Reward: 10  | Average Reward 5.93  | Actor loss: 0.22 | Critic loss: 3.43 | Entropy loss: -0.0003  | Total Loss: 3.65 | Total Steps: 8\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 694/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.27 | Critic loss: 16.52 | Entropy loss: -0.0001  | Total Loss: 16.79 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 695/129000  | Episode Reward: 4  | Average Reward 5.96  | Actor loss: 0.01 | Critic loss: 7.37 | Entropy loss: -0.0014  | Total Loss: 7.38 | Total Steps: 45\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 696/129000  | Episode Reward: 1  | Average Reward 5.98  | Actor loss: -0.02 | Critic loss: 7.11 | Entropy loss: -0.0014  | Total Loss: 7.09 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 697/129000  | Episode Reward: 7  | Average Reward 5.96  | Actor loss: 0.01 | Critic loss: 3.35 | Entropy loss: -0.0004  | Total Loss: 3.36 | Total Steps: 42\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 698/129000  | Episode Reward: 1  | Average Reward 5.92  | Actor loss: -0.12 | Critic loss: 8.47 | Entropy loss: -0.0016  | Total Loss: 8.35 | Total Steps: 54\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 699/129000  | Episode Reward: 4  | Average Reward 5.92  | Actor loss: -0.05 | Critic loss: 8.03 | Entropy loss: -0.0004  | Total Loss: 7.99 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 700/129000  | Episode Reward: 7  | Average Reward 5.91  | Actor loss: -0.00 | Critic loss: 5.43 | Entropy loss: -0.0025  | Total Loss: 5.43 | Total Steps: 54\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 701/129000  | Episode Reward: 4  | Average Reward 5.88  | Actor loss: -0.17 | Critic loss: 4.69 | Entropy loss: -0.0026  | Total Loss: 4.52 | Total Steps: 51\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 7  | Average Reward -1.94  | Actor loss: 0.17 | Critic loss: 13.75 | Entropy loss: -0.0316  | Total Loss: 13.88 | Total Steps: 105\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 7  | Average Reward -1.95  | Actor loss: 0.00 | Critic loss: 3.74 | Entropy loss: -0.0017  | Total Loss: 3.74 | Total Steps: 365\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 1  | Average Reward -1.97  | Actor loss: 0.02 | Critic loss: 2.02 | Entropy loss: -0.0043  | Total Loss: 2.03 | Total Steps: 53\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: -2  | Average Reward -2.00  | Actor loss: 0.08 | Critic loss: 2.60 | Entropy loss: -0.0245  | Total Loss: 2.65 | Total Steps: 55\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: -2  | Average Reward -1.50  | Actor loss: 8.58 | Critic loss: 7.21 | Entropy loss: -0.0326  | Total Loss: 15.76 | Total Steps: 241\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 4  | Average Reward -1.53  | Actor loss: 0.00 | Critic loss: 2.46 | Entropy loss: -0.0031  | Total Loss: 2.46 | Total Steps: 42\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 10  | Average Reward -1.50  | Actor loss: 0.02 | Critic loss: 2.13 | Entropy loss: -0.0013  | Total Loss: 2.14 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 10  | Average Reward -1.48  | Actor loss: 0.00 | Critic loss: 2.98 | Entropy loss: -0.0014  | Total Loss: 2.98 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward -1.46  | Actor loss: 0.00 | Critic loss: 3.86 | Entropy loss: -0.0017  | Total Loss: 3.86 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 7  | Average Reward -1.43  | Actor loss: 1.54 | Critic loss: 8.03 | Entropy loss: -0.0279  | Total Loss: 9.54 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward -1.46  | Actor loss: 0.01 | Critic loss: 1.06 | Entropy loss: -0.0247  | Total Loss: 1.04 | Total Steps: 49\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 10  | Average Reward -0.98  | Actor loss: 0.01 | Critic loss: 7.12 | Entropy loss: -0.0047  | Total Loss: 7.13 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward -0.98  | Actor loss: 0.00 | Critic loss: 2.45 | Entropy loss: -0.0046  | Total Loss: 2.45 | Total Steps: 39\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: -5  | Average Reward -1.03  | Actor loss: 0.01 | Critic loss: 1.16 | Entropy loss: -0.0123  | Total Loss: 1.16 | Total Steps: 93\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward -1.03  | Actor loss: 0.69 | Critic loss: 13.30 | Entropy loss: -0.0055  | Total Loss: 13.98 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 7  | Average Reward -1.00  | Actor loss: 0.00 | Critic loss: 2.48 | Entropy loss: -0.0016  | Total Loss: 2.48 | Total Steps: 34\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 4  | Average Reward -1.03  | Actor loss: 0.00 | Critic loss: 7.16 | Entropy loss: -0.0078  | Total Loss: 7.15 | Total Steps: 47\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 1  | Average Reward -1.04  | Actor loss: 0.18 | Critic loss: 2.76 | Entropy loss: -0.0046  | Total Loss: 2.94 | Total Steps: 54\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward -1.03  | Actor loss: 0.01 | Critic loss: 10.13 | Entropy loss: -0.0169  | Total Loss: 10.12 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Average Reward -1.04  | Actor loss: 0.07 | Critic loss: 3.72 | Entropy loss: -0.0039  | Total Loss: 3.78 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 7  | Average Reward -1.06  | Actor loss: 0.00 | Critic loss: 8.12 | Entropy loss: -0.0005  | Total Loss: 8.12 | Total Steps: 34\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 2  | Average Reward -1.05  | Actor loss: 0.01 | Critic loss: 3.56 | Entropy loss: -0.0470  | Total Loss: 3.53 | Total Steps: 59\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 1  | Average Reward -1.05  | Actor loss: 0.02 | Critic loss: 2.00 | Entropy loss: -0.0042  | Total Loss: 2.02 | Total Steps: 53\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 2  | Average Reward -1.06  | Actor loss: 0.06 | Critic loss: 2.00 | Entropy loss: -0.0268  | Total Loss: 2.04 | Total Steps: 58\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 10  | Average Reward -1.06  | Actor loss: 0.00 | Critic loss: 3.61 | Entropy loss: -0.0011  | Total Loss: 3.61 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward -1.08  | Actor loss: 0.07 | Critic loss: 3.72 | Entropy loss: -0.0033  | Total Loss: 3.78 | Total Steps: 30\n",
      "TEST: ---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward -1.06  | Actor loss: 0.00 | Critic loss: 5.48 | Entropy loss: -0.0004  | Total Loss: 5.48 | Total Steps: 38\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 7  | Average Reward -1.05  | Actor loss: 0.05 | Critic loss: 2.67 | Entropy loss: -0.0192  | Total Loss: 2.70 | Total Steps: 50\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward -1.01  | Actor loss: 0.24 | Critic loss: 14.55 | Entropy loss: -0.0057  | Total Loss: 14.78 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward -1.01  | Actor loss: 0.04 | Critic loss: 2.78 | Entropy loss: -0.0110  | Total Loss: 2.80 | Total Steps: 35\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Average Reward -1.00  | Actor loss: 0.01 | Critic loss: 3.87 | Entropy loss: -0.0074  | Total Loss: 3.87 | Total Steps: 8\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward -1.00  | Actor loss: 0.01 | Critic loss: 16.61 | Entropy loss: -0.0019  | Total Loss: 16.62 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: -11  | Average Reward -1.09  | Actor loss: 9.05 | Critic loss: 8.69 | Entropy loss: -0.0343  | Total Loss: 17.71 | Total Steps: 292\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Average Reward -0.98  | Actor loss: 0.01 | Critic loss: 15.18 | Entropy loss: -0.0063  | Total Loss: 15.19 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 7  | Average Reward -0.97  | Actor loss: 0.00 | Critic loss: 2.69 | Entropy loss: -0.0020  | Total Loss: 2.69 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward -0.95  | Actor loss: 0.29 | Critic loss: 11.30 | Entropy loss: -0.0183  | Total Loss: 11.57 | Total Steps: 20\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward -0.94  | Actor loss: 0.02 | Critic loss: 2.15 | Entropy loss: -0.0261  | Total Loss: 2.15 | Total Steps: 49\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 4  | Average Reward -0.95  | Actor loss: 0.01 | Critic loss: 2.08 | Entropy loss: -0.0066  | Total Loss: 2.08 | Total Steps: 44\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 4  | Average Reward -0.98  | Actor loss: 2.67 | Critic loss: 6.08 | Entropy loss: -0.0086  | Total Loss: 8.75 | Total Steps: 51\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: -29  | Average Reward -1.15  | Actor loss: 0.34 | Critic loss: 10.94 | Entropy loss: -0.0347  | Total Loss: 11.24 | Total Steps: 390\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 10  | Average Reward -1.10  | Actor loss: 0.01 | Critic loss: 16.03 | Entropy loss: -0.0064  | Total Loss: 16.03 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 1  | Average Reward -1.15  | Actor loss: 0.00 | Critic loss: 2.82 | Entropy loss: -0.0167  | Total Loss: 2.80 | Total Steps: 44\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 1  | Average Reward -1.17  | Actor loss: 0.41 | Critic loss: 1.95 | Entropy loss: -0.0047  | Total Loss: 2.36 | Total Steps: 52\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward -1.17  | Actor loss: 0.07 | Critic loss: 5.36 | Entropy loss: -0.0033  | Total Loss: 5.42 | Total Steps: 30\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 4  | Average Reward -1.17  | Actor loss: 0.07 | Critic loss: 1.83 | Entropy loss: -0.0082  | Total Loss: 1.90 | Total Steps: 49\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 7  | Average Reward -1.18  | Actor loss: 0.01 | Critic loss: 15.97 | Entropy loss: -0.0010  | Total Loss: 15.98 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Average Reward -1.21  | Actor loss: 0.47 | Critic loss: 2.15 | Entropy loss: -0.0201  | Total Loss: 2.60 | Total Steps: 44\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 1  | Average Reward -1.25  | Actor loss: 0.08 | Critic loss: 2.29 | Entropy loss: -0.0053  | Total Loss: 2.36 | Total Steps: 53\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 7  | Average Reward -1.27  | Actor loss: 0.15 | Critic loss: 5.47 | Entropy loss: -0.0092  | Total Loss: 5.61 | Total Steps: 32\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 1  | Average Reward -1.30  | Actor loss: 0.16 | Critic loss: 2.22 | Entropy loss: -0.0121  | Total Loss: 2.37 | Total Steps: 52\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 1  | Average Reward -1.34  | Actor loss: 0.01 | Critic loss: 4.17 | Entropy loss: -0.0013  | Total Loss: 4.18 | Total Steps: 50\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward -1.28  | Actor loss: 0.01 | Critic loss: 15.55 | Entropy loss: -0.0009  | Total Loss: 15.56 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 10  | Average Reward -1.28  | Actor loss: 0.00 | Critic loss: 2.80 | Entropy loss: -0.0007  | Total Loss: 2.81 | Total Steps: 6\n",
      "TEST: ---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward -1.25  | Actor loss: 0.04 | Critic loss: 2.61 | Entropy loss: -0.0268  | Total Loss: 2.62 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 1  | Average Reward -1.24  | Actor loss: 0.01 | Critic loss: 1.30 | Entropy loss: -0.0073  | Total Loss: 1.29 | Total Steps: 54\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward -1.24  | Actor loss: 0.00 | Critic loss: 1.80 | Entropy loss: -0.0016  | Total Loss: 1.80 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 1  | Average Reward -1.28  | Actor loss: 0.00 | Critic loss: 3.54 | Entropy loss: -0.0151  | Total Loss: 3.53 | Total Steps: 54\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward -1.28  | Actor loss: 0.01 | Critic loss: 8.40 | Entropy loss: -0.0031  | Total Loss: 8.40 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 7  | Average Reward -1.30  | Actor loss: 0.10 | Critic loss: 4.57 | Entropy loss: -0.0094  | Total Loss: 4.66 | Total Steps: 51\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 10  | Average Reward -1.25  | Actor loss: 0.00 | Critic loss: 3.70 | Entropy loss: -0.0081  | Total Loss: 3.69 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 4  | Average Reward -1.26  | Actor loss: 0.01 | Critic loss: 2.17 | Entropy loss: -0.0074  | Total Loss: 2.17 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward -1.26  | Actor loss: 0.00 | Critic loss: 4.74 | Entropy loss: -0.0007  | Total Loss: 4.74 | Total Steps: 36\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward -1.22  | Actor loss: 0.01 | Critic loss: 13.38 | Entropy loss: -0.0007  | Total Loss: 13.39 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 2  | Average Reward -1.23  | Actor loss: 0.02 | Critic loss: 2.39 | Entropy loss: -0.0249  | Total Loss: 2.38 | Total Steps: 46\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward -1.25  | Actor loss: 0.00 | Critic loss: 2.63 | Entropy loss: -0.0144  | Total Loss: 2.62 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 4  | Average Reward -1.27  | Actor loss: 0.00 | Critic loss: 7.36 | Entropy loss: -0.0025  | Total Loss: 7.36 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 3  | Average Reward -1.29  | Actor loss: 0.01 | Critic loss: 1.10 | Entropy loss: -0.0252  | Total Loss: 1.09 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 4  | Average Reward -1.30  | Actor loss: 0.01 | Critic loss: 3.38 | Entropy loss: -0.0095  | Total Loss: 3.38 | Total Steps: 49\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 69/100  | Episode Reward: -85  | Average Reward -1.75  | Actor loss: -0.58 | Critic loss: 82.30 | Entropy loss: -0.0289  | Total Loss: 81.69 | Total Steps: 500\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 4  | Average Reward -1.30  | Actor loss: 0.00 | Critic loss: 2.41 | Entropy loss: -0.0032  | Total Loss: 2.40 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 3  | Average Reward -1.34  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0380  | Total Loss: 0.93 | Total Steps: 67\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: -38  | Average Reward -1.58  | Actor loss: 5.86 | Critic loss: 7.71 | Entropy loss: -0.0309  | Total Loss: 13.54 | Total Steps: 328\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 4  | Average Reward -1.57  | Actor loss: 0.03 | Critic loss: 2.31 | Entropy loss: -0.0096  | Total Loss: 2.33 | Total Steps: 54\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward -1.57  | Actor loss: 0.00 | Critic loss: 3.76 | Entropy loss: -0.0094  | Total Loss: 3.75 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward -1.57  | Actor loss: 0.01 | Critic loss: 3.87 | Entropy loss: -0.0072  | Total Loss: 3.87 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 7  | Average Reward -1.56  | Actor loss: 2.16 | Critic loss: 10.71 | Entropy loss: -0.0134  | Total Loss: 12.86 | Total Steps: 55\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Average Reward -1.53  | Actor loss: 0.00 | Critic loss: 3.84 | Entropy loss: -0.0034  | Total Loss: 3.84 | Total Steps: 6\n",
      "TEST: ---red---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward -1.11  | Actor loss: 0.00 | Critic loss: 3.28 | Entropy loss: -0.0158  | Total Loss: 3.27 | Total Steps: 39\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 1  | Average Reward -1.11  | Actor loss: 0.01 | Critic loss: 1.40 | Entropy loss: -0.0020  | Total Loss: 1.41 | Total Steps: 50\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 4  | Average Reward -1.10  | Actor loss: 0.68 | Critic loss: 2.55 | Entropy loss: -0.0141  | Total Loss: 3.22 | Total Steps: 56\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward -1.10  | Actor loss: 0.00 | Critic loss: 3.94 | Entropy loss: -0.0028  | Total Loss: 3.95 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward -1.05  | Actor loss: 0.01 | Critic loss: 13.38 | Entropy loss: -0.0011  | Total Loss: 13.39 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 10  | Average Reward -1.03  | Actor loss: 0.01 | Critic loss: 0.78 | Entropy loss: -0.0369  | Total Loss: 0.75 | Total Steps: 13\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Average Reward -1.02  | Actor loss: 0.00 | Critic loss: 3.64 | Entropy loss: -0.0077  | Total Loss: 3.64 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 7  | Average Reward -1.00  | Actor loss: 0.01 | Critic loss: 4.76 | Entropy loss: -0.0025  | Total Loss: 4.76 | Total Steps: 30\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward -1.00  | Actor loss: 0.01 | Critic loss: 4.93 | Entropy loss: -0.0023  | Total Loss: 4.94 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward -0.98  | Actor loss: 0.00 | Critic loss: 4.42 | Entropy loss: -0.0006  | Total Loss: 4.42 | Total Steps: 36\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward -0.97  | Actor loss: 0.01 | Critic loss: 7.14 | Entropy loss: -0.0086  | Total Loss: 7.14 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward -0.97  | Actor loss: 0.00 | Critic loss: 3.64 | Entropy loss: -0.0006  | Total Loss: 3.64 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 90/100  | Episode Reward: -70  | Average Reward -1.37  | Actor loss: -0.28 | Critic loss: 78.87 | Entropy loss: -0.0342  | Total Loss: 78.55 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Average Reward -1.37  | Actor loss: 0.01 | Critic loss: 14.86 | Entropy loss: -0.0064  | Total Loss: 14.86 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 7  | Average Reward -1.37  | Actor loss: 4.05 | Critic loss: 13.00 | Entropy loss: -0.0470  | Total Loss: 17.01 | Total Steps: 70\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 10  | Average Reward -1.29  | Actor loss: 0.00 | Critic loss: 3.03 | Entropy loss: -0.0085  | Total Loss: 3.02 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 10  | Average Reward -1.28  | Actor loss: 0.01 | Critic loss: 16.22 | Entropy loss: -0.0021  | Total Loss: 16.23 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward -1.29  | Actor loss: 0.01 | Critic loss: 16.89 | Entropy loss: -0.0008  | Total Loss: 16.90 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 10  | Average Reward -1.04  | Actor loss: 0.00 | Critic loss: 3.57 | Entropy loss: -0.0009  | Total Loss: 3.57 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 4  | Average Reward -1.05  | Actor loss: 0.01 | Critic loss: 4.73 | Entropy loss: -0.0317  | Total Loss: 4.70 | Total Steps: 32\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward -1.04  | Actor loss: 0.01 | Critic loss: 15.97 | Entropy loss: -0.0018  | Total Loss: 15.98 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward -1.01  | Actor loss: 0.00 | Critic loss: 4.74 | Entropy loss: -0.0006  | Total Loss: 4.74 | Total Steps: 36\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Average Reward -0.99  | Actor loss: 1.73 | Critic loss: 16.53 | Entropy loss: -0.0522  | Total Loss: 18.20 | Total Steps: 10\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 702/129000  | Episode Reward: 10  | Average Reward 5.89  | Actor loss: 0.15 | Critic loss: 2.50 | Entropy loss: -0.0002  | Total Loss: 2.65 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 703/129000  | Episode Reward: 10  | Average Reward 5.93  | Actor loss: 0.02 | Critic loss: 2.40 | Entropy loss: -0.0000  | Total Loss: 2.42 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 704/129000  | Episode Reward: 4  | Average Reward 5.92  | Actor loss: -0.47 | Critic loss: 11.52 | Entropy loss: -0.0035  | Total Loss: 11.05 | Total Steps: 46\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 705/129000  | Episode Reward: 7  | Average Reward 5.92  | Actor loss: 0.03 | Critic loss: 4.35 | Entropy loss: -0.0001  | Total Loss: 4.38 | Total Steps: 34\n",
      "---black---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 706/129000  | Episode Reward: 4  | Average Reward 5.91  | Actor loss: -0.02 | Critic loss: 4.36 | Entropy loss: -0.0003  | Total Loss: 4.35 | Total Steps: 53\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 707/129000  | Episode Reward: 10  | Average Reward 5.91  | Actor loss: 0.07 | Critic loss: 2.03 | Entropy loss: -0.0001  | Total Loss: 2.10 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 708/129000  | Episode Reward: 1  | Average Reward 5.88  | Actor loss: 0.10 | Critic loss: 7.89 | Entropy loss: -0.0016  | Total Loss: 7.99 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 709/129000  | Episode Reward: 10  | Average Reward 5.89  | Actor loss: -0.00 | Critic loss: 1.82 | Entropy loss: -0.0003  | Total Loss: 1.82 | Total Steps: 47\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 710/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.01 | Critic loss: 1.10 | Entropy loss: -0.0000  | Total Loss: 1.11 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 711/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.01 | Critic loss: 1.39 | Entropy loss: -0.0000  | Total Loss: 1.40 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 712/129000  | Episode Reward: 7  | Average Reward 6.01  | Actor loss: 0.05 | Critic loss: 3.33 | Entropy loss: -0.0018  | Total Loss: 3.39 | Total Steps: 42\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 713/129000  | Episode Reward: 10  | Average Reward 6.04  | Actor loss: 0.01 | Critic loss: 7.56 | Entropy loss: -0.0000  | Total Loss: 7.57 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 714/129000  | Episode Reward: 1  | Average Reward 6.00  | Actor loss: 0.01 | Critic loss: 9.11 | Entropy loss: -0.0016  | Total Loss: 9.11 | Total Steps: 54\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 715/129000  | Episode Reward: 10  | Average Reward 6.03  | Actor loss: 0.04 | Critic loss: 0.92 | Entropy loss: -0.0001  | Total Loss: 0.96 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 716/129000  | Episode Reward: 7  | Average Reward 6.01  | Actor loss: 0.11 | Critic loss: 7.38 | Entropy loss: -0.0005  | Total Loss: 7.49 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 717/129000  | Episode Reward: 4  | Average Reward 6.01  | Actor loss: -0.01 | Critic loss: 7.79 | Entropy loss: -0.0002  | Total Loss: 7.77 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 718/129000  | Episode Reward: 7  | Average Reward 6.08  | Actor loss: 0.00 | Critic loss: 6.08 | Entropy loss: -0.0002  | Total Loss: 6.08 | Total Steps: 42\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 719/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: -0.03 | Critic loss: 1.82 | Entropy loss: -0.0012  | Total Loss: 1.78 | Total Steps: 64\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 720/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.19 | Critic loss: 5.80 | Entropy loss: -0.0016  | Total Loss: 5.61 | Total Steps: 47\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 721/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.01 | Critic loss: 1.08 | Entropy loss: -0.0000  | Total Loss: 1.09 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 722/129000  | Episode Reward: 1  | Average Reward 6.08  | Actor loss: -0.77 | Critic loss: 10.21 | Entropy loss: -0.0066  | Total Loss: 9.44 | Total Steps: 62\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 723/129000  | Episode Reward: 4  | Average Reward 6.07  | Actor loss: -0.20 | Critic loss: 7.34 | Entropy loss: -0.0023  | Total Loss: 7.13 | Total Steps: 52\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 724/129000  | Episode Reward: 4  | Average Reward 6.07  | Actor loss: -0.74 | Critic loss: 6.87 | Entropy loss: -0.0078  | Total Loss: 6.12 | Total Steps: 62\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 725/129000  | Episode Reward: 1  | Average Reward 6.03  | Actor loss: -1.07 | Critic loss: 10.62 | Entropy loss: -0.0026  | Total Loss: 9.55 | Total Steps: 33\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 726/129000  | Episode Reward: 9  | Average Reward 6.07  | Actor loss: -0.76 | Critic loss: 2.36 | Entropy loss: -0.0080  | Total Loss: 1.59 | Total Steps: 59\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 727/129000  | Episode Reward: 7  | Average Reward 6.09  | Actor loss: 0.04 | Critic loss: 3.82 | Entropy loss: -0.0004  | Total Loss: 3.85 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 728/129000  | Episode Reward: 7  | Average Reward 6.11  | Actor loss: -0.29 | Critic loss: 3.26 | Entropy loss: -0.0052  | Total Loss: 2.96 | Total Steps: 47\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 729/129000  | Episode Reward: 7  | Average Reward 6.09  | Actor loss: 0.03 | Critic loss: 7.34 | Entropy loss: -0.0007  | Total Loss: 7.36 | Total Steps: 32\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 730/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.02 | Critic loss: 1.90 | Entropy loss: -0.0000  | Total Loss: 1.91 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 731/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.32 | Critic loss: 5.42 | Entropy loss: -0.0026  | Total Loss: 5.74 | Total Steps: 19\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 732/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: -0.49 | Critic loss: 4.42 | Entropy loss: -0.0089  | Total Loss: 3.92 | Total Steps: 58\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 733/129000  | Episode Reward: 0  | Average Reward 6.12  | Actor loss: -1.90 | Critic loss: 10.29 | Entropy loss: -0.0103  | Total Loss: 8.38 | Total Steps: 65\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 734/129000  | Episode Reward: 3  | Average Reward 6.12  | Actor loss: -0.04 | Critic loss: 7.90 | Entropy loss: -0.0022  | Total Loss: 7.86 | Total Steps: 46\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 735/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.40 | Critic loss: 2.30 | Entropy loss: -0.0010  | Total Loss: 2.70 | Total Steps: 11\n",
      "---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 736/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.11 | Critic loss: 3.73 | Entropy loss: -0.0005  | Total Loss: 3.84 | Total Steps: 29\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 737/129000  | Episode Reward: 1  | Average Reward 6.09  | Actor loss: -0.52 | Critic loss: 8.54 | Entropy loss: -0.0035  | Total Loss: 8.01 | Total Steps: 53\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 738/129000  | Episode Reward: 7  | Average Reward 6.08  | Actor loss: 0.53 | Critic loss: 6.14 | Entropy loss: -0.0018  | Total Loss: 6.67 | Total Steps: 31\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 739/129000  | Episode Reward: 10  | Average Reward 6.11  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0000  | Total Loss: 1.39 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 740/129000  | Episode Reward: 4  | Average Reward 6.11  | Actor loss: -0.01 | Critic loss: 3.46 | Entropy loss: -0.0012  | Total Loss: 3.44 | Total Steps: 53\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 741/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.74 | Critic loss: 2.20 | Entropy loss: -0.0014  | Total Loss: 2.93 | Total Steps: 11\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 742/129000  | Episode Reward: 1  | Average Reward 6.11  | Actor loss: 0.02 | Critic loss: 10.29 | Entropy loss: -0.0039  | Total Loss: 10.30 | Total Steps: 56\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 743/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.02 | Critic loss: 9.28 | Entropy loss: -0.0000  | Total Loss: 9.29 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 744/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: -0.39 | Critic loss: 4.60 | Entropy loss: -0.0029  | Total Loss: 4.20 | Total Steps: 35\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 745/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.00 | Critic loss: 1.81 | Entropy loss: -0.0000  | Total Loss: 1.81 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 746/129000  | Episode Reward: -2  | Average Reward 6.11  | Actor loss: -0.40 | Critic loss: 12.86 | Entropy loss: -0.0017  | Total Loss: 12.46 | Total Steps: 71\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 747/129000  | Episode Reward: 2  | Average Reward 6.11  | Actor loss: -0.87 | Critic loss: 10.45 | Entropy loss: -0.0077  | Total Loss: 9.58 | Total Steps: 63\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 748/129000  | Episode Reward: 10  | Average Reward 6.11  | Actor loss: -0.07 | Critic loss: 1.77 | Entropy loss: -0.0008  | Total Loss: 1.70 | Total Steps: 38\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 749/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.02 | Critic loss: 0.90 | Entropy loss: -0.0000  | Total Loss: 0.92 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 750/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.84 | Critic loss: 2.37 | Entropy loss: -0.0032  | Total Loss: 3.20 | Total Steps: 15\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 751/129000  | Episode Reward: 7  | Average Reward 6.18  | Actor loss: 0.16 | Critic loss: 9.10 | Entropy loss: -0.0006  | Total Loss: 9.27 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 752/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.07 | Critic loss: 9.32 | Entropy loss: -0.0007  | Total Loss: 9.25 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 753/129000  | Episode Reward: 1  | Average Reward 6.15  | Actor loss: -0.01 | Critic loss: 7.08 | Entropy loss: -0.0003  | Total Loss: 7.06 | Total Steps: 53\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 754/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: -0.22 | Critic loss: 4.52 | Entropy loss: -0.0026  | Total Loss: 4.29 | Total Steps: 45\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 755/129000  | Episode Reward: 4  | Average Reward 6.15  | Actor loss: -0.39 | Critic loss: 5.04 | Entropy loss: -0.0016  | Total Loss: 4.66 | Total Steps: 43\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 756/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.00 | Critic loss: 8.27 | Entropy loss: -0.0016  | Total Loss: 8.27 | Total Steps: 45\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 757/129000  | Episode Reward: 0  | Average Reward 6.14  | Actor loss: -0.91 | Critic loss: 9.92 | Entropy loss: -0.0072  | Total Loss: 9.00 | Total Steps: 55\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 758/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.90 | Critic loss: 7.56 | Entropy loss: -0.0062  | Total Loss: 6.65 | Total Steps: 59\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 759/129000  | Episode Reward: 7  | Average Reward 6.10  | Actor loss: 0.41 | Critic loss: 6.04 | Entropy loss: -0.0022  | Total Loss: 6.45 | Total Steps: 35\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 760/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.26 | Critic loss: 2.61 | Entropy loss: -0.0003  | Total Loss: 2.87 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 761/129000  | Episode Reward: 10  | Average Reward 6.19  | Actor loss: 0.23 | Critic loss: 4.79 | Entropy loss: -0.0051  | Total Loss: 5.02 | Total Steps: 42\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 762/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: -0.25 | Critic loss: 2.45 | Entropy loss: -0.0048  | Total Loss: 2.20 | Total Steps: 51\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 763/129000  | Episode Reward: 7  | Average Reward 6.19  | Actor loss: 0.08 | Critic loss: 5.72 | Entropy loss: -0.0003  | Total Loss: 5.79 | Total Steps: 31\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 764/129000  | Episode Reward: 1  | Average Reward 6.14  | Actor loss: -0.45 | Critic loss: 6.44 | Entropy loss: -0.0047  | Total Loss: 5.98 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 765/129000  | Episode Reward: 10  | Average Reward 6.19  | Actor loss: 0.00 | Critic loss: 2.25 | Entropy loss: -0.0000  | Total Loss: 2.26 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 766/129000  | Episode Reward: 7  | Average Reward 6.19  | Actor loss: 0.07 | Critic loss: 5.71 | Entropy loss: -0.0003  | Total Loss: 5.78 | Total Steps: 31\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 767/129000  | Episode Reward: 10  | Average Reward 6.19  | Actor loss: 0.01 | Critic loss: 6.42 | Entropy loss: -0.0000  | Total Loss: 6.43 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 768/129000  | Episode Reward: 4  | Average Reward 6.19  | Actor loss: -0.11 | Critic loss: 4.75 | Entropy loss: -0.0007  | Total Loss: 4.64 | Total Steps: 42\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 769/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.04 | Critic loss: 1.59 | Entropy loss: -0.0000  | Total Loss: 1.63 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 770/129000  | Episode Reward: 4  | Average Reward 6.19  | Actor loss: -0.45 | Critic loss: 5.77 | Entropy loss: -0.0017  | Total Loss: 5.32 | Total Steps: 43\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 771/129000  | Episode Reward: 4  | Average Reward 6.22  | Actor loss: -0.07 | Critic loss: 4.33 | Entropy loss: -0.0005  | Total Loss: 4.26 | Total Steps: 43\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 772/129000  | Episode Reward: 10  | Average Reward 6.25  | Actor loss: 0.00 | Critic loss: 0.91 | Entropy loss: -0.0000  | Total Loss: 0.92 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 773/129000  | Episode Reward: 7  | Average Reward 6.24  | Actor loss: 0.23 | Critic loss: 6.65 | Entropy loss: -0.0008  | Total Loss: 6.88 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 774/129000  | Episode Reward: 7  | Average Reward 6.22  | Actor loss: -1.18 | Critic loss: 3.90 | Entropy loss: -0.0106  | Total Loss: 2.71 | Total Steps: 64\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 775/129000  | Episode Reward: 4  | Average Reward 6.22  | Actor loss: 0.08 | Critic loss: 5.26 | Entropy loss: -0.0018  | Total Loss: 5.34 | Total Steps: 50\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 776/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.01 | Critic loss: 0.82 | Entropy loss: -0.0000  | Total Loss: 0.84 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 777/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -0.08 | Critic loss: 7.19 | Entropy loss: -0.0008  | Total Loss: 7.12 | Total Steps: 42\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 778/129000  | Episode Reward: 7  | Average Reward 6.24  | Actor loss: 0.15 | Critic loss: 6.68 | Entropy loss: -0.0011  | Total Loss: 6.84 | Total Steps: 34\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 779/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.02 | Critic loss: 2.61 | Entropy loss: -0.0000  | Total Loss: 2.63 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 780/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.07 | Critic loss: 1.13 | Entropy loss: -0.0001  | Total Loss: 1.20 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 781/129000  | Episode Reward: 5  | Average Reward 6.21  | Actor loss: -0.60 | Critic loss: 6.27 | Entropy loss: -0.0059  | Total Loss: 5.66 | Total Steps: 60\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 782/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.00 | Critic loss: 3.36 | Entropy loss: -0.0000  | Total Loss: 3.36 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 783/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -1.76 | Critic loss: 8.19 | Entropy loss: -0.0129  | Total Loss: 6.42 | Total Steps: 67\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 784/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.02 | Critic loss: 1.11 | Entropy loss: -0.0001  | Total Loss: 1.12 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 785/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.07 | Critic loss: 4.77 | Entropy loss: -0.0003  | Total Loss: 4.84 | Total Steps: 29\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 786/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.01 | Critic loss: 0.56 | Entropy loss: -0.0000  | Total Loss: 0.57 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 787/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.01 | Critic loss: 0.48 | Entropy loss: -0.0000  | Total Loss: 0.49 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 788/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: -0.46 | Critic loss: 3.89 | Entropy loss: -0.0025  | Total Loss: 3.43 | Total Steps: 54\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 789/129000  | Episode Reward: 4  | Average Reward 6.29  | Actor loss: -0.15 | Critic loss: 3.73 | Entropy loss: -0.0011  | Total Loss: 3.58 | Total Steps: 43\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 790/129000  | Episode Reward: 7  | Average Reward 6.27  | Actor loss: 0.07 | Critic loss: 7.77 | Entropy loss: -0.0004  | Total Loss: 7.84 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 791/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 0.57 | Entropy loss: -0.0000  | Total Loss: 0.58 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 792/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 2.81 | Entropy loss: -0.0000  | Total Loss: 2.82 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 793/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 794/129000  | Episode Reward: 7  | Average Reward 6.30  | Actor loss: -0.72 | Critic loss: 4.07 | Entropy loss: -0.0065  | Total Loss: 3.34 | Total Steps: 46\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 795/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.03 | Critic loss: 0.44 | Entropy loss: -0.0027  | Total Loss: 0.47 | Total Steps: 7\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 796/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.04 | Critic loss: 3.42 | Entropy loss: -0.0003  | Total Loss: 3.45 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  | Episode: 797/129000  | Episode Reward: 4  | Average Reward 6.29  | Actor loss: -0.41 | Critic loss: 6.38 | Entropy loss: -0.0015  | Total Loss: 5.97 | Total Steps: 34\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 798/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.52 | Critic loss: 0.70 | Entropy loss: -0.0016  | Total Loss: 1.23 | Total Steps: 7\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 799/129000  | Episode Reward: 7  | Average Reward 6.30  | Actor loss: -0.29 | Critic loss: 3.88 | Entropy loss: -0.0023  | Total Loss: 3.59 | Total Steps: 47\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 800/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: 0.05 | Critic loss: 5.47 | Entropy loss: -0.0003  | Total Loss: 5.51 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 801/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: 0.11 | Critic loss: 4.67 | Entropy loss: -0.0005  | Total Loss: 4.78 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 4  | Average Reward -0.40  | Actor loss: 0.26 | Critic loss: 9.40 | Entropy loss: -0.0196  | Total Loss: 9.64 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 1  | Average Reward -0.01  | Actor loss: 0.02 | Critic loss: 6.90 | Entropy loss: -0.0091  | Total Loss: 6.91 | Total Steps: 60\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 7  | Average Reward -0.01  | Actor loss: 0.03 | Critic loss: 3.19 | Entropy loss: -0.0202  | Total Loss: 3.20 | Total Steps: 39\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: -2  | Average Reward -0.03  | Actor loss: 0.01 | Critic loss: 1.44 | Entropy loss: -0.0273  | Total Loss: 1.43 | Total Steps: 65\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 4  | Average Reward -0.01  | Actor loss: 0.00 | Critic loss: 1.48 | Entropy loss: -0.0066  | Total Loss: 1.47 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 4  | Average Reward 0.52  | Actor loss: 0.09 | Critic loss: 6.81 | Entropy loss: -0.0107  | Total Loss: 6.90 | Total Steps: 61\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 4  | Average Reward 0.52  | Actor loss: 0.50 | Critic loss: 13.34 | Entropy loss: -0.0121  | Total Loss: 13.83 | Total Steps: 47\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 8/100  | Episode Reward: -112  | Average Reward -0.07  | Actor loss: -0.01 | Critic loss: 72.03 | Entropy loss: -0.0230  | Total Loss: 72.00 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 9/100  | Episode Reward: -10  | Average Reward -0.17  | Actor loss: -0.00 | Critic loss: 80.73 | Entropy loss: -0.0003  | Total Loss: 80.73 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward -0.17  | Actor loss: 0.00 | Critic loss: 2.87 | Entropy loss: -0.0006  | Total Loss: 2.87 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward -0.19  | Actor loss: 0.44 | Critic loss: 12.31 | Entropy loss: -0.0138  | Total Loss: 12.74 | Total Steps: 40\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 4  | Average Reward -0.19  | Actor loss: 0.03 | Critic loss: 4.96 | Entropy loss: -0.0019  | Total Loss: 4.99 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 10  | Average Reward -0.14  | Actor loss: 0.01 | Critic loss: 12.35 | Entropy loss: -0.0003  | Total Loss: 12.36 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 4  | Average Reward -0.16  | Actor loss: 0.01 | Critic loss: 2.52 | Entropy loss: -0.0032  | Total Loss: 2.53 | Total Steps: 44\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 4  | Average Reward -0.16  | Actor loss: 0.25 | Critic loss: 1.19 | Entropy loss: -0.0046  | Total Loss: 1.44 | Total Steps: 46\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 16/100  | Episode Reward: -124  | Average Reward -0.79  | Actor loss: -0.03 | Critic loss: 61.81 | Entropy loss: -0.0175  | Total Loss: 61.76 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 4  | Average Reward -0.80  | Actor loss: 0.03 | Critic loss: 5.38 | Entropy loss: -0.0020  | Total Loss: 5.41 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 7  | Average Reward -0.81  | Actor loss: 0.00 | Critic loss: 1.54 | Entropy loss: -0.0004  | Total Loss: 1.54 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 19/100  | Episode Reward: -130  | Average Reward -1.47  | Actor loss: -0.02 | Critic loss: 114.18 | Entropy loss: -0.0154  | Total Loss: 114.15 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Average Reward -1.44  | Actor loss: 0.09 | Critic loss: 12.46 | Entropy loss: -0.0214  | Total Loss: 12.53 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward -1.43  | Actor loss: 0.01 | Critic loss: 10.86 | Entropy loss: -0.0039  | Total Loss: 10.87 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 10  | Average Reward -1.41  | Actor loss: 1.27 | Critic loss: 11.80 | Entropy loss: -0.0083  | Total Loss: 13.07 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward -1.38  | Actor loss: 0.22 | Critic loss: 6.67 | Entropy loss: -0.0111  | Total Loss: 6.88 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Average Reward -1.35  | Actor loss: 0.00 | Critic loss: 1.87 | Entropy loss: -0.0022  | Total Loss: 1.87 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 1  | Average Reward -1.30  | Actor loss: 0.00 | Critic loss: 6.82 | Entropy loss: -0.0103  | Total Loss: 6.82 | Total Steps: 52\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward -1.32  | Actor loss: 0.07 | Critic loss: 6.96 | Entropy loss: -0.0127  | Total Loss: 7.02 | Total Steps: 35\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward -1.32  | Actor loss: 0.00 | Critic loss: 5.36 | Entropy loss: -0.0059  | Total Loss: 5.35 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: -8  | Average Reward -1.40  | Actor loss: 0.03 | Critic loss: 7.27 | Entropy loss: -0.0173  | Total Loss: 7.28 | Total Steps: 111\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 3  | Average Reward -1.42  | Actor loss: 1.35 | Critic loss: 13.41 | Entropy loss: -0.0215  | Total Loss: 14.74 | Total Steps: 46\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 30/100  | Episode Reward: -115  | Average Reward -2.00  | Actor loss: -0.04 | Critic loss: 84.43 | Entropy loss: -0.0133  | Total Loss: 84.37 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward -2.00  | Actor loss: 0.00 | Critic loss: 2.56 | Entropy loss: -0.0020  | Total Loss: 2.56 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward -1.95  | Actor loss: 0.00 | Critic loss: 1.81 | Entropy loss: -0.0007  | Total Loss: 1.81 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: -57  | Average Reward -2.29  | Actor loss: 5.99 | Critic loss: 4.58 | Entropy loss: -0.0148  | Total Loss: 10.56 | Total Steps: 324\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 34/100  | Episode Reward: -133  | Average Reward -2.44  | Actor loss: -0.10 | Critic loss: 127.29 | Entropy loss: -0.0178  | Total Loss: 127.18 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 1  | Average Reward -2.46  | Actor loss: 0.01 | Critic loss: 4.96 | Entropy loss: -0.0088  | Total Loss: 4.96 | Total Steps: 53\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward -2.45  | Actor loss: 0.01 | Critic loss: 13.89 | Entropy loss: -0.0006  | Total Loss: 13.90 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 10  | Average Reward -2.42  | Actor loss: 0.00 | Critic loss: 2.08 | Entropy loss: -0.0004  | Total Loss: 2.08 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 1  | Average Reward -2.43  | Actor loss: 0.09 | Critic loss: 1.64 | Entropy loss: -0.0131  | Total Loss: 1.71 | Total Steps: 54\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 1  | Average Reward -2.44  | Actor loss: 0.04 | Critic loss: 0.71 | Entropy loss: -0.0033  | Total Loss: 0.74 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 40/100  | Episode Reward: -97  | Average Reward -2.96  | Actor loss: -0.39 | Critic loss: 104.42 | Entropy loss: -0.0158  | Total Loss: 104.02 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: -3  | Average Reward -2.52  | Actor loss: 0.41 | Critic loss: 10.39 | Entropy loss: -0.0207  | Total Loss: 10.77 | Total Steps: 87\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 4  | Average Reward -2.54  | Actor loss: 0.00 | Critic loss: 1.68 | Entropy loss: -0.0045  | Total Loss: 1.68 | Total Steps: 49\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward -2.52  | Actor loss: 0.07 | Critic loss: 7.34 | Entropy loss: -0.0052  | Total Loss: 7.40 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 4  | Average Reward -2.51  | Actor loss: 0.00 | Critic loss: 7.91 | Entropy loss: -0.0033  | Total Loss: 7.91 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 1  | Average Reward -2.54  | Actor loss: 0.01 | Critic loss: 9.89 | Entropy loss: -0.0026  | Total Loss: 9.90 | Total Steps: 50\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: -65  | Average Reward -2.88  | Actor loss: 0.02 | Critic loss: 2.05 | Entropy loss: -0.0199  | Total Loss: 2.05 | Total Steps: 383\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward -2.76  | Actor loss: 1.38 | Critic loss: 10.07 | Entropy loss: -0.0144  | Total Loss: 11.44 | Total Steps: 6\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 1  | Average Reward -2.79  | Actor loss: 0.65 | Critic loss: 11.22 | Entropy loss: -0.0062  | Total Loss: 11.86 | Total Steps: 52\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 7  | Average Reward -2.76  | Actor loss: 0.02 | Critic loss: 3.36 | Entropy loss: -0.0053  | Total Loss: 3.38 | Total Steps: 34\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward -2.76  | Actor loss: 0.00 | Critic loss: 2.59 | Entropy loss: -0.0018  | Total Loss: 2.59 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Average Reward -2.71  | Actor loss: 0.00 | Critic loss: 1.44 | Entropy loss: -0.0006  | Total Loss: 1.44 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 1  | Average Reward -2.76  | Actor loss: 0.43 | Critic loss: 13.32 | Entropy loss: -0.0143  | Total Loss: 13.73 | Total Steps: 51\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "TEST: Step: 100\n",
      "Testing  | Episode: 53/100  | Episode Reward: -9  | Average Reward -2.84  | Actor loss: 19.19 | Critic loss: 10.16 | Entropy loss: -0.0154  | Total Loss: 29.34 | Total Steps: 100\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 1  | Average Reward -2.88  | Actor loss: 0.04 | Critic loss: 0.61 | Entropy loss: -0.0040  | Total Loss: 0.64 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: -5  | Average Reward -2.96  | Actor loss: 1.16 | Critic loss: 13.64 | Entropy loss: -0.0387  | Total Loss: 14.77 | Total Steps: 101\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 4  | Average Reward -2.94  | Actor loss: 0.00 | Critic loss: 1.40 | Entropy loss: -0.0020  | Total Loss: 1.40 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -5  | Average Reward -3.02  | Actor loss: 0.10 | Critic loss: 4.71 | Entropy loss: -0.0226  | Total Loss: 4.78 | Total Steps: 91\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 4  | Average Reward -3.05  | Actor loss: 2.16 | Critic loss: 28.62 | Entropy loss: -0.0418  | Total Loss: 30.74 | Total Steps: 61\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 59/100  | Episode Reward: -112  | Average Reward -3.62  | Actor loss: -1.20 | Critic loss: 88.82 | Entropy loss: -0.0207  | Total Loss: 87.60 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 1  | Average Reward -3.56  | Actor loss: 4.65 | Critic loss: 3.70 | Entropy loss: -0.0111  | Total Loss: 8.35 | Total Steps: 58\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 1  | Average Reward -3.60  | Actor loss: 0.16 | Critic loss: 5.84 | Entropy loss: -0.0053  | Total Loss: 5.99 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: -2  | Average Reward -3.65  | Actor loss: 1.51 | Critic loss: 23.04 | Entropy loss: -0.0132  | Total Loss: 24.54 | Total Steps: 59\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward -3.65  | Actor loss: 0.00 | Critic loss: 1.81 | Entropy loss: -0.0026  | Total Loss: 1.81 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward -3.63  | Actor loss: 0.42 | Critic loss: 7.41 | Entropy loss: -0.0154  | Total Loss: 7.81 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: -3  | Average Reward -3.67  | Actor loss: 0.32 | Critic loss: 10.16 | Entropy loss: -0.0371  | Total Loss: 10.44 | Total Steps: 82\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward -3.65  | Actor loss: 0.00 | Critic loss: 1.54 | Entropy loss: -0.0013  | Total Loss: 1.54 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 4  | Average Reward -3.65  | Actor loss: 0.00 | Critic loss: 9.40 | Entropy loss: -0.0120  | Total Loss: 9.39 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 68/100  | Episode Reward: -97  | Average Reward -4.16  | Actor loss: -2.15 | Critic loss: 87.40 | Entropy loss: -0.0269  | Total Loss: 85.22 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward -4.14  | Actor loss: 0.68 | Critic loss: 18.87 | Entropy loss: -0.0393  | Total Loss: 19.51 | Total Steps: 40\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward -3.55  | Actor loss: 0.03 | Critic loss: 7.30 | Entropy loss: -0.0018  | Total Loss: 7.33 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 4  | Average Reward -3.55  | Actor loss: 0.00 | Critic loss: 1.14 | Entropy loss: -0.0067  | Total Loss: 1.14 | Total Steps: 44\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 7  | Average Reward -3.52  | Actor loss: 0.00 | Critic loss: 2.08 | Entropy loss: -0.0032  | Total Loss: 2.08 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: -14  | Average Reward -3.06  | Actor loss: 0.01 | Critic loss: 15.44 | Entropy loss: -0.0056  | Total Loss: 15.45 | Total Steps: 161\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 7  | Average Reward -3.04  | Actor loss: 0.01 | Critic loss: 2.35 | Entropy loss: -0.0180  | Total Loss: 2.34 | Total Steps: 35\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 4  | Average Reward -3.03  | Actor loss: 0.03 | Critic loss: 4.95 | Entropy loss: -0.0020  | Total Loss: 4.98 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 1  | Average Reward -3.03  | Actor loss: 0.03 | Critic loss: 0.91 | Entropy loss: -0.0072  | Total Loss: 0.93 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 7  | Average Reward -2.45  | Actor loss: 0.00 | Critic loss: 9.90 | Entropy loss: -0.0004  | Total Loss: 9.90 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 7  | Average Reward -2.46  | Actor loss: 0.01 | Critic loss: 14.06 | Entropy loss: -0.0085  | Total Loss: 14.06 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 4  | Average Reward -2.48  | Actor loss: 0.01 | Critic loss: 0.80 | Entropy loss: -0.0014  | Total Loss: 0.80 | Total Steps: 49\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 80/100  | Episode Reward: -115  | Average Reward -3.06  | Actor loss: -0.89 | Critic loss: 130.86 | Entropy loss: -0.0167  | Total Loss: 129.96 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 7  | Average Reward -3.04  | Actor loss: 0.01 | Critic loss: 14.45 | Entropy loss: -0.0013  | Total Loss: 14.46 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward -3.04  | Actor loss: 0.04 | Critic loss: 7.57 | Entropy loss: -0.0007  | Total Loss: 7.61 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 1  | Average Reward -3.00  | Actor loss: 0.02 | Critic loss: 3.36 | Entropy loss: -0.0065  | Total Loss: 3.37 | Total Steps: 53\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 4  | Average Reward -3.02  | Actor loss: 0.64 | Critic loss: 16.67 | Entropy loss: -0.0393  | Total Loss: 17.27 | Total Steps: 126\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 10  | Average Reward -3.02  | Actor loss: 0.00 | Critic loss: 1.86 | Entropy loss: -0.0069  | Total Loss: 1.86 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 1  | Average Reward -3.04  | Actor loss: 0.05 | Critic loss: 6.73 | Entropy loss: -0.0099  | Total Loss: 6.78 | Total Steps: 63\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 1  | Average Reward -3.08  | Actor loss: 0.01 | Critic loss: 0.94 | Entropy loss: -0.0023  | Total Loss: 0.95 | Total Steps: 53\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward -3.08  | Actor loss: 0.01 | Critic loss: 17.59 | Entropy loss: -0.0007  | Total Loss: 17.60 | Total Steps: 6\n",
      "TEST: ---capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: -2  | Average Reward -3.13  | Actor loss: 0.00 | Critic loss: 9.14 | Entropy loss: -0.0118  | Total Loss: 9.13 | Total Steps: 80\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 90/100  | Episode Reward: -124  | Average Reward -3.81  | Actor loss: -0.26 | Critic loss: 133.52 | Entropy loss: -0.0234  | Total Loss: 133.24 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward -3.27  | Actor loss: 0.18 | Critic loss: 13.10 | Entropy loss: -0.0164  | Total Loss: 13.27 | Total Steps: 45\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 4  | Average Reward -3.30  | Actor loss: 0.01 | Critic loss: 1.02 | Entropy loss: -0.0017  | Total Loss: 1.02 | Total Steps: 49\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 4  | Average Reward -2.81  | Actor loss: 0.02 | Critic loss: 4.08 | Entropy loss: -0.0043  | Total Loss: 4.10 | Total Steps: 43\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 10  | Average Reward -2.81  | Actor loss: 0.00 | Critic loss: 2.42 | Entropy loss: -0.0011  | Total Loss: 2.42 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 1  | Average Reward -2.84  | Actor loss: 0.01 | Critic loss: 1.99 | Entropy loss: -0.0042  | Total Loss: 1.99 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Average Reward -2.85  | Actor loss: 0.02 | Critic loss: 3.76 | Entropy loss: -0.0057  | Total Loss: 3.78 | Total Steps: 43\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 7  | Average Reward -2.87  | Actor loss: 0.00 | Critic loss: 2.59 | Entropy loss: -0.0018  | Total Loss: 2.59 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 1  | Average Reward -2.90  | Actor loss: 0.29 | Critic loss: 10.49 | Entropy loss: -0.0084  | Total Loss: 10.78 | Total Steps: 52\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 10  | Average Reward -2.79  | Actor loss: 0.02 | Critic loss: 5.51 | Entropy loss: -0.0009  | Total Loss: 5.52 | Total Steps: 31\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward -2.79  | Actor loss: 0.01 | Critic loss: 12.99 | Entropy loss: -0.0010  | Total Loss: 13.00 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 802/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: 0.11 | Critic loss: 8.97 | Entropy loss: -0.0064  | Total Loss: 9.08 | Total Steps: 34\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 803/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.25 | Critic loss: 3.81 | Entropy loss: -0.0036  | Total Loss: 4.06 | Total Steps: 45\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 804/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.01 | Critic loss: 2.57 | Entropy loss: -0.0000  | Total Loss: 2.57 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 805/129000  | Episode Reward: 4  | Average Reward 6.33  | Actor loss: -1.21 | Critic loss: 7.64 | Entropy loss: -0.0072  | Total Loss: 6.42 | Total Steps: 60\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 806/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: 0.18 | Critic loss: 10.69 | Entropy loss: -0.0033  | Total Loss: 10.87 | Total Steps: 44\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 807/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.07 | Critic loss: 6.23 | Entropy loss: -0.0006  | Total Loss: 6.15 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 808/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.01 | Critic loss: 0.80 | Entropy loss: -0.0000  | Total Loss: 0.81 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 809/129000  | Episode Reward: 1  | Average Reward 6.29  | Actor loss: -0.22 | Critic loss: 10.26 | Entropy loss: -0.0032  | Total Loss: 10.04 | Total Steps: 44\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 810/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 1.25 | Entropy loss: -0.0000  | Total Loss: 1.26 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 811/129000  | Episode Reward: 1  | Average Reward 6.28  | Actor loss: -0.13 | Critic loss: 11.76 | Entropy loss: -0.0006  | Total Loss: 11.63 | Total Steps: 50\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 812/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.04 | Critic loss: 2.92 | Entropy loss: -0.0000  | Total Loss: 2.96 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 813/129000  | Episode Reward: 7  | Average Reward 6.26  | Actor loss: 0.08 | Critic loss: 8.35 | Entropy loss: -0.0002  | Total Loss: 8.43 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 814/129000  | Episode Reward: 4  | Average Reward 6.33  | Actor loss: -0.23 | Critic loss: 5.79 | Entropy loss: -0.0021  | Total Loss: 5.55 | Total Steps: 46\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 815/129000  | Episode Reward: 7  | Average Reward 6.31  | Actor loss: -0.71 | Critic loss: 9.15 | Entropy loss: -0.0028  | Total Loss: 8.44 | Total Steps: 33\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 816/129000  | Episode Reward: 10  | Average Reward 6.31  | Actor loss: 0.00 | Critic loss: 0.97 | Entropy loss: -0.0000  | Total Loss: 0.97 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 817/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: 0.07 | Critic loss: 5.78 | Entropy loss: -0.0003  | Total Loss: 5.84 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 818/129000  | Episode Reward: 1  | Average Reward 6.28  | Actor loss: -0.17 | Critic loss: 11.39 | Entropy loss: -0.0009  | Total Loss: 11.21 | Total Steps: 53\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 819/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.01 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 820/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.00 | Critic loss: 2.51 | Entropy loss: -0.0000  | Total Loss: 2.51 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 821/129000  | Episode Reward: 10  | Average Reward 6.40  | Actor loss: 0.01 | Critic loss: 6.47 | Entropy loss: -0.0000  | Total Loss: 6.48 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 822/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: 0.07 | Critic loss: 7.85 | Entropy loss: -0.0004  | Total Loss: 7.92 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 823/129000  | Episode Reward: 4  | Average Reward 6.38  | Actor loss: -0.45 | Critic loss: 5.01 | Entropy loss: -0.0017  | Total Loss: 4.56 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 824/129000  | Episode Reward: 7  | Average Reward 6.40  | Actor loss: 0.34 | Critic loss: 4.90 | Entropy loss: -0.0011  | Total Loss: 5.24 | Total Steps: 29\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 825/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.00 | Critic loss: 0.61 | Entropy loss: -0.0000  | Total Loss: 0.61 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 826/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.01 | Critic loss: 0.91 | Entropy loss: -0.0000  | Total Loss: 0.92 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 827/129000  | Episode Reward: 1  | Average Reward 6.45  | Actor loss: -0.08 | Critic loss: 8.77 | Entropy loss: -0.0006  | Total Loss: 8.68 | Total Steps: 53\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 828/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.02 | Critic loss: 1.20 | Entropy loss: -0.0000  | Total Loss: 1.21 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 829/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.00 | Critic loss: 0.59 | Entropy loss: -0.0000  | Total Loss: 0.60 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.595. Model has been saved-----\n",
      "Training  | Episode: 830/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 0.03 | Critic loss: 10.98 | Entropy loss: -0.0000  | Total Loss: 11.01 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 831/129000  | Episode Reward: 4  | Average Reward 6.58  | Actor loss: -0.20 | Critic loss: 11.11 | Entropy loss: -0.0010  | Total Loss: 10.91 | Total Steps: 51\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 832/129000  | Episode Reward: 4  | Average Reward 6.55  | Actor loss: -0.38 | Critic loss: 10.18 | Entropy loss: -0.0027  | Total Loss: 9.80 | Total Steps: 49\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 833/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.05 | Critic loss: 4.74 | Entropy loss: -0.0004  | Total Loss: 4.79 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 834/129000  | Episode Reward: 4  | Average Reward 6.58  | Actor loss: -0.08 | Critic loss: 7.99 | Entropy loss: -0.0003  | Total Loss: 7.91 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 835/129000  | Episode Reward: -8  | Average Reward 6.52  | Actor loss: -0.62 | Critic loss: 20.26 | Entropy loss: -0.0068  | Total Loss: 19.64 | Total Steps: 149\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 836/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.05 | Critic loss: 8.41 | Entropy loss: -0.0006  | Total Loss: 8.36 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 837/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: -0.74 | Critic loss: 6.20 | Entropy loss: -0.0071  | Total Loss: 5.45 | Total Steps: 52\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 838/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: -0.06 | Critic loss: 3.29 | Entropy loss: -0.0010  | Total Loss: 3.23 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 839/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: 0.05 | Critic loss: 5.38 | Entropy loss: -0.0003  | Total Loss: 5.43 | Total Steps: 34\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 840/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.09 | Critic loss: 4.50 | Entropy loss: -0.0007  | Total Loss: 4.41 | Total Steps: 47\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 841/129000  | Episode Reward: 1  | Average Reward 6.45  | Actor loss: -0.76 | Critic loss: 7.31 | Entropy loss: -0.0033  | Total Loss: 6.54 | Total Steps: 37\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 842/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.01 | Critic loss: 9.76 | Entropy loss: -0.0000  | Total Loss: 9.77 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 843/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: 0.05 | Critic loss: 5.53 | Entropy loss: -0.0002  | Total Loss: 5.58 | Total Steps: 29\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 844/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.14 | Critic loss: 3.16 | Entropy loss: -0.0076  | Total Loss: 3.30 | Total Steps: 63\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 845/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: 0.08 | Critic loss: 8.12 | Entropy loss: -0.0005  | Total Loss: 8.20 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 846/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -0.05 | Critic loss: 8.96 | Entropy loss: -0.0010  | Total Loss: 8.91 | Total Steps: 46\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 847/129000  | Episode Reward: 4  | Average Reward 6.43  | Actor loss: -0.03 | Critic loss: 4.13 | Entropy loss: -0.0002  | Total Loss: 4.11 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 848/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: 0.16 | Critic loss: 6.16 | Entropy loss: -0.0035  | Total Loss: 6.32 | Total Steps: 45\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 849/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.25 | Critic loss: 7.45 | Entropy loss: -0.0013  | Total Loss: 7.70 | Total Steps: 32\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 850/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.10 | Critic loss: 7.35 | Entropy loss: -0.0005  | Total Loss: 7.46 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 851/129000  | Episode Reward: 7  | Average Reward 6.40  | Actor loss: 0.07 | Critic loss: 4.30 | Entropy loss: -0.0048  | Total Loss: 4.37 | Total Steps: 45\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 852/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: 0.05 | Critic loss: 6.70 | Entropy loss: -0.0004  | Total Loss: 6.75 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 853/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.06 | Critic loss: 6.54 | Entropy loss: -0.0005  | Total Loss: 6.60 | Total Steps: 30\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 854/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.01 | Critic loss: 1.47 | Entropy loss: -0.0000  | Total Loss: 1.49 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 855/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: -0.04 | Critic loss: 5.50 | Entropy loss: -0.0003  | Total Loss: 5.46 | Total Steps: 52\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 856/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -0.09 | Critic loss: 5.67 | Entropy loss: -0.0007  | Total Loss: 5.58 | Total Steps: 47\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 857/129000  | Episode Reward: 1  | Average Reward 6.37  | Actor loss: -0.40 | Critic loss: 8.21 | Entropy loss: -0.0052  | Total Loss: 7.80 | Total Steps: 50\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 858/129000  | Episode Reward: 4  | Average Reward 6.34  | Actor loss: 0.25 | Critic loss: 5.55 | Entropy loss: -0.0015  | Total Loss: 5.80 | Total Steps: 48\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 859/129000  | Episode Reward: 7  | Average Reward 6.34  | Actor loss: 0.48 | Critic loss: 4.35 | Entropy loss: -0.0016  | Total Loss: 4.83 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 860/129000  | Episode Reward: 7  | Average Reward 6.36  | Actor loss: 0.04 | Critic loss: 3.43 | Entropy loss: -0.0002  | Total Loss: 3.47 | Total Steps: 34\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 861/129000  | Episode Reward: 7  | Average Reward 6.36  | Actor loss: 0.05 | Critic loss: 3.26 | Entropy loss: -0.0004  | Total Loss: 3.30 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 862/129000  | Episode Reward: 7  | Average Reward 6.37  | Actor loss: 0.04 | Critic loss: 5.74 | Entropy loss: -0.0013  | Total Loss: 5.78 | Total Steps: 45\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 863/129000  | Episode Reward: 4  | Average Reward 6.43  | Actor loss: -0.07 | Critic loss: 10.37 | Entropy loss: -0.0005  | Total Loss: 10.30 | Total Steps: 47\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 864/129000  | Episode Reward: 1  | Average Reward 6.42  | Actor loss: -0.16 | Critic loss: 8.61 | Entropy loss: -0.0006  | Total Loss: 8.46 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 865/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.05 | Critic loss: 5.75 | Entropy loss: -0.0002  | Total Loss: 5.79 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 866/129000  | Episode Reward: -14  | Average Reward 6.37  | Actor loss: -0.61 | Critic loss: 22.31 | Entropy loss: -0.0083  | Total Loss: 21.68 | Total Steps: 141\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 867/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.14 | Critic loss: 12.44 | Entropy loss: -0.0001  | Total Loss: 12.58 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 868/129000  | Episode Reward: 10  | Average Reward 6.40  | Actor loss: 0.06 | Critic loss: 2.96 | Entropy loss: -0.0003  | Total Loss: 3.02 | Total Steps: 34\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 869/129000  | Episode Reward: 4  | Average Reward 6.37  | Actor loss: -0.12 | Critic loss: 8.82 | Entropy loss: -0.0004  | Total Loss: 8.70 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 870/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.17 | Critic loss: 5.65 | Entropy loss: -0.0006  | Total Loss: 5.82 | Total Steps: 34\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 871/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: -0.01 | Critic loss: 1.76 | Entropy loss: -0.0002  | Total Loss: 1.75 | Total Steps: 38\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 872/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.02 | Critic loss: 10.39 | Entropy loss: -0.0000  | Total Loss: 10.41 | Total Steps: 6\n",
      "---cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 873/129000  | Episode Reward: 1  | Average Reward 6.41  | Actor loss: -0.33 | Critic loss: 12.48 | Entropy loss: -0.0020  | Total Loss: 12.15 | Total Steps: 84\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 874/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: -0.43 | Critic loss: 3.04 | Entropy loss: -0.0072  | Total Loss: 2.60 | Total Steps: 62\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 875/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.15 | Critic loss: 6.19 | Entropy loss: -0.0027  | Total Loss: 6.34 | Total Steps: 53\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 876/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.01 | Critic loss: 9.66 | Entropy loss: -0.0000  | Total Loss: 9.67 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 877/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: -0.04 | Critic loss: 4.87 | Entropy loss: -0.0003  | Total Loss: 4.84 | Total Steps: 42\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 878/129000  | Episode Reward: 4  | Average Reward 6.39  | Actor loss: -0.12 | Critic loss: 8.49 | Entropy loss: -0.0023  | Total Loss: 8.36 | Total Steps: 53\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 879/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.07 | Critic loss: 3.16 | Entropy loss: -0.0004  | Total Loss: 3.23 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 880/129000  | Episode Reward: 4  | Average Reward 6.43  | Actor loss: -0.24 | Critic loss: 9.00 | Entropy loss: -0.0008  | Total Loss: 8.76 | Total Steps: 52\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 881/129000  | Episode Reward: 10  | Average Reward 6.45  | Actor loss: 0.01 | Critic loss: 2.24 | Entropy loss: -0.0000  | Total Loss: 2.25 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 882/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.00 | Critic loss: 2.17 | Entropy loss: -0.0000  | Total Loss: 2.17 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 883/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.04 | Critic loss: 5.29 | Entropy loss: -0.0004  | Total Loss: 5.25 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 884/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.12 | Critic loss: 8.62 | Entropy loss: -0.0004  | Total Loss: 8.74 | Total Steps: 29\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 885/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.08 | Critic loss: 3.38 | Entropy loss: -0.0001  | Total Loss: 3.47 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 886/129000  | Episode Reward: 4  | Average Reward 6.48  | Actor loss: -0.19 | Critic loss: 8.16 | Entropy loss: -0.0009  | Total Loss: 7.96 | Total Steps: 42\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 887/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.55 | Critic loss: 3.39 | Entropy loss: -0.0024  | Total Loss: 3.94 | Total Steps: 43\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 888/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: -0.30 | Critic loss: 5.62 | Entropy loss: -0.0022  | Total Loss: 5.31 | Total Steps: 50\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 889/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: 0.08 | Critic loss: 5.90 | Entropy loss: -0.0002  | Total Loss: 5.98 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 890/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: -0.48 | Critic loss: 7.06 | Entropy loss: -0.0027  | Total Loss: 6.58 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 891/129000  | Episode Reward: 1  | Average Reward 6.42  | Actor loss: -0.24 | Critic loss: 9.22 | Entropy loss: -0.0015  | Total Loss: 8.98 | Total Steps: 45\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 892/129000  | Episode Reward: -8  | Average Reward 6.34  | Actor loss: -2.57 | Critic loss: 17.17 | Entropy loss: -0.0145  | Total Loss: 14.59 | Total Steps: 64\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 893/129000  | Episode Reward: 7  | Average Reward 6.33  | Actor loss: 0.31 | Critic loss: 3.65 | Entropy loss: -0.0013  | Total Loss: 3.96 | Total Steps: 32\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 894/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.54 | Critic loss: 6.32 | Entropy loss: -0.0046  | Total Loss: 5.78 | Total Steps: 51\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 895/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 1.68 | Critic loss: 8.67 | Entropy loss: -0.0037  | Total Loss: 10.35 | Total Steps: 20\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 896/129000  | Episode Reward: 7  | Average Reward 6.36  | Actor loss: -0.06 | Critic loss: 2.85 | Entropy loss: -0.0007  | Total Loss: 2.79 | Total Steps: 42\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 897/129000  | Episode Reward: 7  | Average Reward 6.36  | Actor loss: 0.07 | Critic loss: 7.73 | Entropy loss: -0.0002  | Total Loss: 7.80 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 898/129000  | Episode Reward: 1  | Average Reward 6.36  | Actor loss: -0.15 | Critic loss: 11.25 | Entropy loss: -0.0007  | Total Loss: 11.09 | Total Steps: 52\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 899/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: 0.21 | Critic loss: 5.02 | Entropy loss: -0.0008  | Total Loss: 5.24 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 900/129000  | Episode Reward: 1  | Average Reward 6.34  | Actor loss: -0.08 | Critic loss: 8.22 | Entropy loss: -0.0006  | Total Loss: 8.14 | Total Steps: 53\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 901/129000  | Episode Reward: 10  | Average Reward 6.38  | Actor loss: 0.01 | Critic loss: 2.41 | Entropy loss: -0.0000  | Total Loss: 2.42 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward -2.78  | Actor loss: 1.21 | Critic loss: 12.35 | Entropy loss: -0.0056  | Total Loss: 13.55 | Total Steps: 6\n",
      "TEST: ---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 7  | Average Reward -2.78  | Actor loss: 0.01 | Critic loss: 3.46 | Entropy loss: -0.0006  | Total Loss: 3.47 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 6  | Average Reward -2.75  | Actor loss: 0.01 | Critic loss: 3.24 | Entropy loss: -0.0375  | Total Loss: 3.21 | Total Steps: 67\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 4  | Average Reward -2.73  | Actor loss: 0.00 | Critic loss: 1.82 | Entropy loss: -0.0103  | Total Loss: 1.81 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 10  | Average Reward -2.67  | Actor loss: 0.72 | Critic loss: 12.17 | Entropy loss: -0.0058  | Total Loss: 12.88 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 4  | Average Reward -2.67  | Actor loss: 0.19 | Critic loss: 3.86 | Entropy loss: -0.0165  | Total Loss: 4.04 | Total Steps: 44\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward -2.71  | Actor loss: 0.20 | Critic loss: 13.71 | Entropy loss: -0.0365  | Total Loss: 13.88 | Total Steps: 91\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 1  | Average Reward -2.75  | Actor loss: 7.79 | Critic loss: 7.36 | Entropy loss: -0.0208  | Total Loss: 15.13 | Total Steps: 52\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward -2.75  | Actor loss: 0.00 | Critic loss: 3.45 | Entropy loss: -0.0263  | Total Loss: 3.43 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 7  | Average Reward -2.75  | Actor loss: 0.01 | Critic loss: 15.37 | Entropy loss: -0.0020  | Total Loss: 15.37 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward -2.75  | Actor loss: 0.01 | Critic loss: 3.37 | Entropy loss: -0.0010  | Total Loss: 3.38 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Average Reward -2.77  | Actor loss: 4.16 | Critic loss: 7.91 | Entropy loss: -0.0178  | Total Loss: 12.06 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 3  | Average Reward -2.79  | Actor loss: 0.01 | Critic loss: 1.42 | Entropy loss: -0.0240  | Total Loss: 1.41 | Total Steps: 66\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 1  | Average Reward -2.76  | Actor loss: 0.00 | Critic loss: 3.00 | Entropy loss: -0.0096  | Total Loss: 3.00 | Total Steps: 54\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 1  | Average Reward -2.81  | Actor loss: 9.48 | Critic loss: 6.67 | Entropy loss: -0.0363  | Total Loss: 16.12 | Total Steps: 130\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 4  | Average Reward -2.82  | Actor loss: 0.01 | Critic loss: 3.37 | Entropy loss: -0.0019  | Total Loss: 3.37 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 7  | Average Reward -2.81  | Actor loss: 0.00 | Critic loss: 2.51 | Entropy loss: -0.0016  | Total Loss: 2.51 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 18/100  | Episode Reward: -85  | Average Reward -3.23  | Actor loss: -21.41 | Critic loss: 92.87 | Entropy loss: -0.0331  | Total Loss: 71.42 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: -11  | Average Reward -3.34  | Actor loss: 0.00 | Critic loss: 17.14 | Entropy loss: -0.0105  | Total Loss: 17.13 | Total Steps: 114\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward -3.33  | Actor loss: 0.01 | Critic loss: 2.16 | Entropy loss: -0.0021  | Total Loss: 2.16 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward -3.31  | Actor loss: 0.01 | Critic loss: 12.10 | Entropy loss: -0.0015  | Total Loss: 12.12 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 10  | Average Reward -3.27  | Actor loss: 0.01 | Critic loss: 2.38 | Entropy loss: -0.0013  | Total Loss: 2.39 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward -3.23  | Actor loss: 0.00 | Critic loss: 2.69 | Entropy loss: -0.0009  | Total Loss: 2.69 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Average Reward -3.21  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0154  | Total Loss: 1.65 | Total Steps: 56\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward -3.23  | Actor loss: 0.00 | Critic loss: 2.74 | Entropy loss: -0.0016  | Total Loss: 2.74 | Total Steps: 34\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward -3.23  | Actor loss: 0.01 | Critic loss: 16.69 | Entropy loss: -0.0013  | Total Loss: 16.70 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward -3.23  | Actor loss: 0.01 | Critic loss: 2.80 | Entropy loss: -0.0227  | Total Loss: 2.79 | Total Steps: 67\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward -3.25  | Actor loss: 0.00 | Critic loss: 4.10 | Entropy loss: -0.0016  | Total Loss: 4.10 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: -8  | Average Reward -3.33  | Actor loss: 0.06 | Critic loss: 8.89 | Entropy loss: -0.0312  | Total Loss: 8.92 | Total Steps: 194\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward -3.33  | Actor loss: 0.16 | Critic loss: 3.50 | Entropy loss: -0.0055  | Total Loss: 3.66 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Average Reward -3.33  | Actor loss: 0.00 | Critic loss: 2.80 | Entropy loss: -0.0018  | Total Loss: 2.80 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: -4  | Average Reward -3.40  | Actor loss: 2.43 | Critic loss: 5.31 | Entropy loss: -0.0394  | Total Loss: 7.70 | Total Steps: 119\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 4  | Average Reward -3.33  | Actor loss: 10.52 | Critic loss: 8.02 | Entropy loss: -0.0357  | Total Loss: 18.50 | Total Steps: 55\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 0  | Average Reward -3.38  | Actor loss: 0.04 | Critic loss: 2.14 | Entropy loss: -0.0115  | Total Loss: 2.17 | Total Steps: 53\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Average Reward -3.37  | Actor loss: 0.00 | Critic loss: 4.60 | Entropy loss: -0.0041  | Total Loss: 4.60 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward -3.37  | Actor loss: 0.00 | Critic loss: 2.99 | Entropy loss: -0.0013  | Total Loss: 3.00 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 10  | Average Reward -3.33  | Actor loss: 0.00 | Critic loss: 7.31 | Entropy loss: -0.0389  | Total Loss: 7.27 | Total Steps: 10\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 1  | Average Reward -3.35  | Actor loss: 0.72 | Critic loss: 1.96 | Entropy loss: -0.0088  | Total Loss: 2.68 | Total Steps: 53\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 39/100  | Episode Reward: -64  | Average Reward -3.69  | Actor loss: -3.85 | Critic loss: 74.93 | Entropy loss: -0.0310  | Total Loss: 71.05 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 7  | Average Reward -3.51  | Actor loss: 0.00 | Critic loss: 3.96 | Entropy loss: -0.0047  | Total Loss: 3.96 | Total Steps: 68\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward -3.54  | Actor loss: 0.01 | Critic loss: 1.97 | Entropy loss: -0.0042  | Total Loss: 1.97 | Total Steps: 49\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 7  | Average Reward -3.51  | Actor loss: 0.01 | Critic loss: 3.65 | Entropy loss: -0.0033  | Total Loss: 3.65 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 1  | Average Reward -3.51  | Actor loss: 0.00 | Critic loss: 2.44 | Entropy loss: -0.0200  | Total Loss: 2.43 | Total Steps: 53\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: -2  | Average Reward -3.56  | Actor loss: 1.99 | Critic loss: 3.06 | Entropy loss: -0.0124  | Total Loss: 5.03 | Total Steps: 52\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward -3.52  | Actor loss: 0.03 | Critic loss: 1.50 | Entropy loss: -0.0107  | Total Loss: 1.52 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward -3.51  | Actor loss: 0.03 | Critic loss: 2.79 | Entropy loss: -0.0026  | Total Loss: 2.82 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Average Reward -3.51  | Actor loss: 0.25 | Critic loss: 5.35 | Entropy loss: -0.0088  | Total Loss: 5.60 | Total Steps: 47\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward -3.48  | Actor loss: 0.01 | Critic loss: 2.80 | Entropy loss: -0.0055  | Total Loss: 2.81 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 7  | Average Reward -3.48  | Actor loss: 0.16 | Critic loss: 3.66 | Entropy loss: -0.0046  | Total Loss: 3.81 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 10  | Average Reward -3.44  | Actor loss: 0.01 | Critic loss: 3.42 | Entropy loss: -0.0012  | Total Loss: 3.42 | Total Steps: 6\n",
      "TEST: ---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 4  | Average Reward -3.42  | Actor loss: 0.00 | Critic loss: 4.19 | Entropy loss: -0.0070  | Total Loss: 4.19 | Total Steps: 47\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward -3.42  | Actor loss: 0.03 | Critic loss: 2.13 | Entropy loss: -0.0490  | Total Loss: 2.12 | Total Steps: 12\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: -2  | Average Reward -3.48  | Actor loss: 0.01 | Critic loss: 2.50 | Entropy loss: -0.0204  | Total Loss: 2.49 | Total Steps: 56\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 1  | Average Reward -3.52  | Actor loss: 0.02 | Critic loss: 3.70 | Entropy loss: -0.0024  | Total Loss: 3.71 | Total Steps: 50\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 4  | Average Reward -3.51  | Actor loss: 0.00 | Critic loss: 1.93 | Entropy loss: -0.0019  | Total Loss: 1.93 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward -3.51  | Actor loss: 0.35 | Critic loss: 10.70 | Entropy loss: -0.0243  | Total Loss: 11.03 | Total Steps: 20\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 7  | Average Reward -3.48  | Actor loss: 0.00 | Critic loss: 5.94 | Entropy loss: -0.0012  | Total Loss: 5.94 | Total Steps: 34\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: -8  | Average Reward -3.57  | Actor loss: 0.03 | Critic loss: 9.28 | Entropy loss: -0.0332  | Total Loss: 9.28 | Total Steps: 242\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward -3.56  | Actor loss: 0.05 | Critic loss: 2.32 | Entropy loss: -0.0567  | Total Loss: 2.31 | Total Steps: 15\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 10  | Average Reward -3.56  | Actor loss: 1.34 | Critic loss: 12.73 | Entropy loss: -0.0344  | Total Loss: 14.03 | Total Steps: 12\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 1  | Average Reward -3.57  | Actor loss: 0.00 | Critic loss: 3.29 | Entropy loss: -0.0027  | Total Loss: 3.29 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 10  | Average Reward -3.56  | Actor loss: 0.00 | Critic loss: 2.80 | Entropy loss: -0.0013  | Total Loss: 2.80 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward -3.56  | Actor loss: 0.01 | Critic loss: 2.19 | Entropy loss: -0.0015  | Total Loss: 2.20 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward -3.52  | Actor loss: 0.01 | Critic loss: 6.13 | Entropy loss: -0.0013  | Total Loss: 6.14 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward -3.52  | Actor loss: 0.00 | Critic loss: 1.96 | Entropy loss: -0.0068  | Total Loss: 1.96 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 1  | Average Reward -3.53  | Actor loss: 2.37 | Critic loss: 2.71 | Entropy loss: -0.0488  | Total Loss: 5.04 | Total Steps: 57\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 1  | Average Reward -3.54  | Actor loss: 0.15 | Critic loss: 10.45 | Entropy loss: -0.0391  | Total Loss: 10.56 | Total Steps: 104\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 1  | Average Reward -3.56  | Actor loss: 0.00 | Critic loss: 7.07 | Entropy loss: -0.0017  | Total Loss: 7.07 | Total Steps: 53\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Average Reward -3.08  | Actor loss: 0.01 | Critic loss: 3.92 | Entropy loss: -0.0003  | Total Loss: 3.92 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 1  | Average Reward -3.10  | Actor loss: 0.08 | Critic loss: 3.03 | Entropy loss: -0.0305  | Total Loss: 3.08 | Total Steps: 54\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 1  | Average Reward -3.10  | Actor loss: 0.03 | Critic loss: 2.74 | Entropy loss: -0.0169  | Total Loss: 2.75 | Total Steps: 48\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 1  | Average Reward -2.91  | Actor loss: 0.01 | Critic loss: 2.46 | Entropy loss: -0.0070  | Total Loss: 2.46 | Total Steps: 53\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 7  | Average Reward -2.90  | Actor loss: 0.00 | Critic loss: 2.75 | Entropy loss: -0.0013  | Total Loss: 2.76 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 4  | Average Reward -2.92  | Actor loss: 0.01 | Critic loss: 3.33 | Entropy loss: -0.0048  | Total Loss: 3.33 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: -2  | Average Reward -2.98  | Actor loss: 0.01 | Critic loss: 2.17 | Entropy loss: -0.0323  | Total Loss: 2.15 | Total Steps: 58\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 7  | Average Reward -2.98  | Actor loss: 0.00 | Critic loss: 3.02 | Entropy loss: -0.0040  | Total Loss: 3.02 | Total Steps: 38\n",
      "TEST: ---red---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 7  | Average Reward -3.00  | Actor loss: 0.02 | Critic loss: 2.33 | Entropy loss: -0.0028  | Total Loss: 2.35 | Total Steps: 30\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward -3.00  | Actor loss: 0.23 | Critic loss: 2.93 | Entropy loss: -0.0064  | Total Loss: 3.15 | Total Steps: 51\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 4  | Average Reward -2.98  | Actor loss: 1.06 | Critic loss: 13.66 | Entropy loss: -0.0420  | Total Loss: 14.69 | Total Steps: 117\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 7  | Average Reward -2.97  | Actor loss: 0.43 | Critic loss: 3.45 | Entropy loss: -0.0115  | Total Loss: 3.87 | Total Steps: 44\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward -2.97  | Actor loss: 0.01 | Critic loss: 3.98 | Entropy loss: -0.0007  | Total Loss: 3.98 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 4  | Average Reward -3.00  | Actor loss: 0.01 | Critic loss: 1.94 | Entropy loss: -0.0069  | Total Loss: 1.95 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 4  | Average Reward -3.03  | Actor loss: 0.00 | Critic loss: 3.83 | Entropy loss: -0.0011  | Total Loss: 3.83 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Average Reward -3.03  | Actor loss: 0.00 | Critic loss: 2.68 | Entropy loss: -0.0008  | Total Loss: 2.68 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 4  | Average Reward -3.04  | Actor loss: 0.01 | Critic loss: 3.86 | Entropy loss: -0.0244  | Total Loss: 3.85 | Total Steps: 66\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Average Reward -3.03  | Actor loss: 0.06 | Critic loss: 3.37 | Entropy loss: -0.0259  | Total Loss: 3.40 | Total Steps: 11\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Average Reward -3.02  | Actor loss: 0.00 | Critic loss: 1.64 | Entropy loss: -0.0047  | Total Loss: 1.64 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 4  | Average Reward -3.04  | Actor loss: 0.00 | Critic loss: 3.28 | Entropy loss: -0.0031  | Total Loss: 3.28 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: -2  | Average Reward -3.10  | Actor loss: 0.22 | Critic loss: 12.24 | Entropy loss: -0.0275  | Total Loss: 12.43 | Total Steps: 150\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 7  | Average Reward -2.72  | Actor loss: 0.00 | Critic loss: 5.92 | Entropy loss: -0.0013  | Total Loss: 5.92 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward -2.75  | Actor loss: 0.01 | Critic loss: 2.41 | Entropy loss: -0.0018  | Total Loss: 2.41 | Total Steps: 49\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 4  | Average Reward -2.77  | Actor loss: 0.01 | Critic loss: 3.36 | Entropy loss: -0.0055  | Total Loss: 3.36 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 10  | Average Reward -2.77  | Actor loss: 0.00 | Critic loss: 3.37 | Entropy loss: -0.0022  | Total Loss: 3.37 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 1  | Average Reward -2.81  | Actor loss: 0.03 | Critic loss: 1.42 | Entropy loss: -0.0046  | Total Loss: 1.45 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 4  | Average Reward -2.83  | Actor loss: 0.01 | Critic loss: 3.10 | Entropy loss: -0.0031  | Total Loss: 3.11 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 7  | Average Reward -2.84  | Actor loss: 0.00 | Critic loss: 3.10 | Entropy loss: -0.0020  | Total Loss: 3.10 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: -2  | Average Reward -2.87  | Actor loss: 0.01 | Critic loss: 21.59 | Entropy loss: -0.0135  | Total Loss: 21.59 | Total Steps: 85\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward -2.85  | Actor loss: 0.01 | Critic loss: 3.68 | Entropy loss: -0.0007  | Total Loss: 3.69 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 4  | Average Reward -2.87  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0438  | Total Loss: 0.69 | Total Steps: 60\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: -8  | Average Reward -2.96  | Actor loss: 0.25 | Critic loss: 5.50 | Entropy loss: -0.0139  | Total Loss: 5.74 | Total Steps: 88\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 902/129000  | Episode Reward: 4  | Average Reward 6.34  | Actor loss: -0.35 | Critic loss: 6.02 | Entropy loss: -0.0038  | Total Loss: 5.67 | Total Steps: 107\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 903/129000  | Episode Reward: 3  | Average Reward 6.31  | Actor loss: -0.28 | Critic loss: 8.19 | Entropy loss: -0.0042  | Total Loss: 7.90 | Total Steps: 56\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 904/129000  | Episode Reward: 0  | Average Reward 6.29  | Actor loss: -2.25 | Critic loss: 9.92 | Entropy loss: -0.0089  | Total Loss: 7.65 | Total Steps: 58\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 905/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.86 | Critic loss: 4.08 | Entropy loss: -0.0009  | Total Loss: 4.95 | Total Steps: 8\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 906/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.01 | Critic loss: 3.12 | Entropy loss: -0.0000  | Total Loss: 3.13 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 907/129000  | Episode Reward: 7  | Average Reward 6.32  | Actor loss: 0.35 | Critic loss: 7.64 | Entropy loss: -0.0020  | Total Loss: 7.99 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 908/129000  | Episode Reward: 1  | Average Reward 6.32  | Actor loss: -0.95 | Critic loss: 13.49 | Entropy loss: -0.0073  | Total Loss: 12.54 | Total Steps: 92\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 909/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: -0.66 | Critic loss: 3.71 | Entropy loss: -0.0097  | Total Loss: 3.03 | Total Steps: 59\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 910/129000  | Episode Reward: -10  | Average Reward 6.22  | Actor loss: -1.56 | Critic loss: 18.02 | Entropy loss: -0.0157  | Total Loss: 16.45 | Total Steps: 151\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 911/129000  | Episode Reward: 4  | Average Reward 6.19  | Actor loss: 0.28 | Critic loss: 5.10 | Entropy loss: -0.0054  | Total Loss: 5.37 | Total Steps: 45\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 912/129000  | Episode Reward: 1  | Average Reward 6.16  | Actor loss: -0.13 | Critic loss: 6.11 | Entropy loss: -0.0006  | Total Loss: 5.99 | Total Steps: 50\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 913/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: -0.38 | Critic loss: 3.52 | Entropy loss: -0.0028  | Total Loss: 3.13 | Total Steps: 60\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 914/129000  | Episode Reward: 10  | Average Reward 6.19  | Actor loss: 0.02 | Critic loss: 2.95 | Entropy loss: -0.0022  | Total Loss: 2.97 | Total Steps: 50\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 915/129000  | Episode Reward: 0  | Average Reward 6.14  | Actor loss: -0.84 | Critic loss: 7.58 | Entropy loss: -0.0124  | Total Loss: 6.73 | Total Steps: 119\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 916/129000  | Episode Reward: 1  | Average Reward 6.11  | Actor loss: -0.04 | Critic loss: 7.10 | Entropy loss: -0.0008  | Total Loss: 7.07 | Total Steps: 51\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 917/129000  | Episode Reward: 4  | Average Reward 6.11  | Actor loss: -0.78 | Critic loss: 3.66 | Entropy loss: -0.0135  | Total Loss: 2.87 | Total Steps: 65\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 918/129000  | Episode Reward: 4  | Average Reward 6.09  | Actor loss: -0.20 | Critic loss: 6.03 | Entropy loss: -0.0023  | Total Loss: 5.83 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 919/129000  | Episode Reward: 7  | Average Reward 6.08  | Actor loss: -0.00 | Critic loss: 4.27 | Entropy loss: -0.0013  | Total Loss: 4.27 | Total Steps: 44\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 920/129000  | Episode Reward: 10  | Average Reward 6.11  | Actor loss: 0.05 | Critic loss: 1.64 | Entropy loss: -0.0001  | Total Loss: 1.69 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 921/129000  | Episode Reward: 1  | Average Reward 6.07  | Actor loss: -0.06 | Critic loss: 6.41 | Entropy loss: -0.0005  | Total Loss: 6.35 | Total Steps: 53\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 922/129000  | Episode Reward: 10  | Average Reward 6.11  | Actor loss: 0.01 | Critic loss: 3.83 | Entropy loss: -0.0000  | Total Loss: 3.84 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 923/129000  | Episode Reward: 7  | Average Reward 6.12  | Actor loss: 0.15 | Critic loss: 7.05 | Entropy loss: -0.0006  | Total Loss: 7.20 | Total Steps: 29\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 924/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: -0.46 | Critic loss: 2.70 | Entropy loss: -0.0025  | Total Loss: 2.23 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 925/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.12 | Critic loss: 1.71 | Entropy loss: -0.0001  | Total Loss: 1.83 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 926/129000  | Episode Reward: 10  | Average Reward 6.19  | Actor loss: 0.10 | Critic loss: 3.67 | Entropy loss: -0.0011  | Total Loss: 3.76 | Total Steps: 44\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 927/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.18 | Critic loss: 4.00 | Entropy loss: -0.0014  | Total Loss: 3.82 | Total Steps: 47\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 928/129000  | Episode Reward: 10  | Average Reward 6.19  | Actor loss: 0.07 | Critic loss: 11.95 | Entropy loss: -0.0000  | Total Loss: 12.03 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 929/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.01 | Critic loss: 1.80 | Entropy loss: -0.0000  | Total Loss: 1.81 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 930/129000  | Episode Reward: 7  | Average Reward 6.19  | Actor loss: -0.58 | Critic loss: 3.75 | Entropy loss: -0.0087  | Total Loss: 3.17 | Total Steps: 62\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 931/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.22 | Critic loss: 4.58 | Entropy loss: -0.0011  | Total Loss: 4.80 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 932/129000  | Episode Reward: 4  | Average Reward 6.16  | Actor loss: -0.85 | Critic loss: 5.01 | Entropy loss: -0.0087  | Total Loss: 4.15 | Total Steps: 74\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 933/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.10 | Critic loss: 0.96 | Entropy loss: -0.0002  | Total Loss: 1.06 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 934/129000  | Episode Reward: 7  | Average Reward 6.23  | Actor loss: 0.57 | Critic loss: 4.10 | Entropy loss: -0.0035  | Total Loss: 4.67 | Total Steps: 31\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 935/129000  | Episode Reward: 10  | Average Reward 6.23  | Actor loss: 0.01 | Critic loss: 0.95 | Entropy loss: -0.0000  | Total Loss: 0.96 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 936/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: 0.12 | Critic loss: 7.96 | Entropy loss: -0.0010  | Total Loss: 8.08 | Total Steps: 30\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 937/129000  | Episode Reward: 4  | Average Reward 6.23  | Actor loss: 0.08 | Critic loss: 7.65 | Entropy loss: -0.0015  | Total Loss: 7.73 | Total Steps: 53\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 938/129000  | Episode Reward: -2  | Average Reward 6.18  | Actor loss: -1.24 | Critic loss: 11.73 | Entropy loss: -0.0088  | Total Loss: 10.48 | Total Steps: 66\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 939/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.30 | Critic loss: 5.79 | Entropy loss: -0.0003  | Total Loss: 6.09 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 940/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.47 | Critic loss: 3.98 | Entropy loss: -0.0004  | Total Loss: 4.45 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 941/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: -0.04 | Critic loss: 4.43 | Entropy loss: -0.0002  | Total Loss: 4.39 | Total Steps: 47\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 942/129000  | Episode Reward: 1  | Average Reward 6.20  | Actor loss: -0.03 | Critic loss: 6.11 | Entropy loss: -0.0002  | Total Loss: 6.09 | Total Steps: 50\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 943/129000  | Episode Reward: 7  | Average Reward 6.18  | Actor loss: -0.03 | Critic loss: 1.35 | Entropy loss: -0.0003  | Total Loss: 1.32 | Total Steps: 38\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 944/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.43 | Critic loss: 3.31 | Entropy loss: -0.0004  | Total Loss: 3.74 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 945/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.01 | Critic loss: 5.36 | Entropy loss: -0.0023  | Total Loss: 5.35 | Total Steps: 32\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 946/129000  | Episode Reward: 10  | Average Reward 6.23  | Actor loss: 0.05 | Critic loss: 7.45 | Entropy loss: -0.0000  | Total Loss: 7.51 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 947/129000  | Episode Reward: 10  | Average Reward 6.27  | Actor loss: 0.07 | Critic loss: 2.05 | Entropy loss: -0.0001  | Total Loss: 2.12 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 948/129000  | Episode Reward: 10  | Average Reward 6.27  | Actor loss: 0.02 | Critic loss: 0.62 | Entropy loss: -0.0000  | Total Loss: 0.64 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 949/129000  | Episode Reward: 10  | Average Reward 6.27  | Actor loss: 0.04 | Critic loss: 2.02 | Entropy loss: -0.0000  | Total Loss: 2.06 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 950/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: -0.00 | Critic loss: 2.76 | Entropy loss: -0.0003  | Total Loss: 2.76 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 951/129000  | Episode Reward: 4  | Average Reward 6.24  | Actor loss: -0.05 | Critic loss: 3.66 | Entropy loss: -0.0003  | Total Loss: 3.60 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 952/129000  | Episode Reward: 4  | Average Reward 6.24  | Actor loss: -0.46 | Critic loss: 6.04 | Entropy loss: -0.0029  | Total Loss: 5.57 | Total Steps: 51\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 953/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 1.25 | Critic loss: 3.38 | Entropy loss: -0.0021  | Total Loss: 4.63 | Total Steps: 10\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 954/129000  | Episode Reward: 4  | Average Reward 6.27  | Actor loss: -0.15 | Critic loss: 6.26 | Entropy loss: -0.0030  | Total Loss: 6.11 | Total Steps: 56\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 955/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.05 | Critic loss: 1.32 | Entropy loss: -0.0001  | Total Loss: 1.37 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 956/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.28 | Critic loss: 12.23 | Entropy loss: -0.0016  | Total Loss: 11.95 | Total Steps: 45\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 957/129000  | Episode Reward: 10  | Average Reward 6.35  | Actor loss: 0.57 | Critic loss: 1.68 | Entropy loss: -0.0010  | Total Loss: 2.24 | Total Steps: 9\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 958/129000  | Episode Reward: 4  | Average Reward 6.35  | Actor loss: -0.98 | Critic loss: 9.24 | Entropy loss: -0.0058  | Total Loss: 8.25 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 959/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.00 | Critic loss: 0.74 | Entropy loss: -0.0000  | Total Loss: 0.75 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 960/129000  | Episode Reward: -11  | Average Reward 6.26  | Actor loss: -1.03 | Critic loss: 30.39 | Entropy loss: -0.0047  | Total Loss: 29.36 | Total Steps: 90\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 961/129000  | Episode Reward: 10  | Average Reward 6.26  | Actor loss: 0.00 | Critic loss: 0.86 | Entropy loss: -0.0000  | Total Loss: 0.86 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 962/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: 0.01 | Critic loss: 7.85 | Entropy loss: -0.0006  | Total Loss: 7.85 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 963/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.09 | Critic loss: 8.51 | Entropy loss: -0.0004  | Total Loss: 8.60 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 964/129000  | Episode Reward: 4  | Average Reward 6.26  | Actor loss: -0.04 | Critic loss: 7.11 | Entropy loss: -0.0004  | Total Loss: 7.06 | Total Steps: 53\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 965/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.46 | Critic loss: 6.55 | Entropy loss: -0.0015  | Total Loss: 7.02 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 966/129000  | Episode Reward: 4  | Average Reward 6.23  | Actor loss: -0.25 | Critic loss: 4.53 | Entropy loss: -0.0039  | Total Loss: 4.28 | Total Steps: 56\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 967/129000  | Episode Reward: 10  | Average Reward 6.23  | Actor loss: 0.03 | Critic loss: 3.16 | Entropy loss: -0.0008  | Total Loss: 3.19 | Total Steps: 44\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 968/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: -0.36 | Critic loss: 3.44 | Entropy loss: -0.0034  | Total Loss: 3.08 | Total Steps: 45\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 969/129000  | Episode Reward: 7  | Average Reward 6.23  | Actor loss: 0.06 | Critic loss: 3.45 | Entropy loss: -0.0013  | Total Loss: 3.50 | Total Steps: 43\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 970/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.08 | Critic loss: 3.63 | Entropy loss: -0.0008  | Total Loss: 3.72 | Total Steps: 45\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 971/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.01 | Critic loss: 5.41 | Entropy loss: -0.0000  | Total Loss: 5.43 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 972/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: -0.16 | Critic loss: 7.86 | Entropy loss: -0.0007  | Total Loss: 7.70 | Total Steps: 43\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 973/129000  | Episode Reward: 4  | Average Reward 6.23  | Actor loss: -0.67 | Critic loss: 7.53 | Entropy loss: -0.0065  | Total Loss: 6.85 | Total Steps: 44\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 974/129000  | Episode Reward: 7  | Average Reward 6.23  | Actor loss: -0.02 | Critic loss: 2.99 | Entropy loss: -0.0040  | Total Loss: 2.97 | Total Steps: 48\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 975/129000  | Episode Reward: 4  | Average Reward 6.23  | Actor loss: -0.16 | Critic loss: 5.92 | Entropy loss: -0.0022  | Total Loss: 5.76 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 976/129000  | Episode Reward: 10  | Average Reward 6.23  | Actor loss: 0.78 | Critic loss: 1.33 | Entropy loss: -0.0013  | Total Loss: 2.11 | Total Steps: 9\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 977/129000  | Episode Reward: 3  | Average Reward 6.22  | Actor loss: -0.31 | Critic loss: 8.37 | Entropy loss: -0.0053  | Total Loss: 8.06 | Total Steps: 57\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 978/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -0.76 | Critic loss: 9.73 | Entropy loss: -0.0047  | Total Loss: 8.97 | Total Steps: 48\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 979/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.30 | Critic loss: 6.01 | Entropy loss: -0.0017  | Total Loss: 6.31 | Total Steps: 34\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 980/129000  | Episode Reward: 0  | Average Reward 6.14  | Actor loss: -1.17 | Critic loss: 7.31 | Entropy loss: -0.0084  | Total Loss: 6.13 | Total Steps: 62\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 981/129000  | Episode Reward: 7  | Average Reward 6.16  | Actor loss: 0.02 | Critic loss: 6.76 | Entropy loss: -0.0007  | Total Loss: 6.78 | Total Steps: 32\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 982/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.27 | Critic loss: 5.39 | Entropy loss: -0.0015  | Total Loss: 5.66 | Total Steps: 32\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 983/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.55 | Critic loss: 2.93 | Entropy loss: -0.0009  | Total Loss: 3.48 | Total Steps: 10\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 984/129000  | Episode Reward: 9  | Average Reward 6.18  | Actor loss: -1.77 | Critic loss: 6.89 | Entropy loss: -0.0228  | Total Loss: 5.10 | Total Steps: 115\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 985/129000  | Episode Reward: 4  | Average Reward 6.15  | Actor loss: -0.07 | Critic loss: 5.69 | Entropy loss: -0.0007  | Total Loss: 5.61 | Total Steps: 51\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 986/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.07 | Critic loss: 3.78 | Entropy loss: -0.0004  | Total Loss: 3.85 | Total Steps: 30\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 987/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.01 | Critic loss: 2.27 | Entropy loss: -0.0000  | Total Loss: 2.28 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 988/129000  | Episode Reward: 6  | Average Reward 6.14  | Actor loss: -0.68 | Critic loss: 4.19 | Entropy loss: -0.0091  | Total Loss: 3.50 | Total Steps: 63\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 989/129000  | Episode Reward: -2  | Average Reward 6.12  | Actor loss: -0.33 | Critic loss: 15.89 | Entropy loss: -0.0031  | Total Loss: 15.56 | Total Steps: 85\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 990/129000  | Episode Reward: 7  | Average Reward 6.12  | Actor loss: -0.28 | Critic loss: 2.48 | Entropy loss: -0.0027  | Total Loss: 2.19 | Total Steps: 51\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 991/129000  | Episode Reward: 7  | Average Reward 6.10  | Actor loss: 0.10 | Critic loss: 5.86 | Entropy loss: -0.0005  | Total Loss: 5.96 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 992/129000  | Episode Reward: 7  | Average Reward 6.08  | Actor loss: 0.37 | Critic loss: 7.33 | Entropy loss: -0.0031  | Total Loss: 7.70 | Total Steps: 41\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 993/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 0.02 | Critic loss: 2.17 | Entropy loss: -0.0000  | Total Loss: 2.19 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 994/129000  | Episode Reward: 4  | Average Reward 6.07  | Actor loss: -0.04 | Critic loss: 7.22 | Entropy loss: -0.0005  | Total Loss: 7.19 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 995/129000  | Episode Reward: 7  | Average Reward 6.05  | Actor loss: -0.07 | Critic loss: 5.97 | Entropy loss: -0.0024  | Total Loss: 5.89 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 996/129000  | Episode Reward: 7  | Average Reward 6.04  | Actor loss: 0.33 | Critic loss: 6.90 | Entropy loss: -0.0015  | Total Loss: 7.22 | Total Steps: 29\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 997/129000  | Episode Reward: 10  | Average Reward 6.07  | Actor loss: 0.24 | Critic loss: 5.56 | Entropy loss: -0.0003  | Total Loss: 5.80 | Total Steps: 8\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 998/129000  | Episode Reward: 7  | Average Reward 6.05  | Actor loss: 0.22 | Critic loss: 6.10 | Entropy loss: -0.0018  | Total Loss: 6.32 | Total Steps: 31\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 999/129000  | Episode Reward: 4  | Average Reward 6.04  | Actor loss: -0.01 | Critic loss: 6.14 | Entropy loss: -0.0001  | Total Loss: 6.14 | Total Steps: 47\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1000/129000  | Episode Reward: 10  | Average Reward 6.05  | Actor loss: 0.01 | Critic loss: 1.99 | Entropy loss: -0.0000  | Total Loss: 1.99 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1001/129000  | Episode Reward: 10  | Average Reward 6.07  | Actor loss: 0.14 | Critic loss: 20.05 | Entropy loss: -0.0001  | Total Loss: 20.19 | Total Steps: 6\n",
      "Model has been saved\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 7  | Average Reward -2.94  | Actor loss: 0.66 | Critic loss: 15.45 | Entropy loss: -0.0231  | Total Loss: 16.08 | Total Steps: 40\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 1  | Average Reward -2.94  | Actor loss: 0.69 | Critic loss: 14.31 | Entropy loss: -0.0190  | Total Loss: 14.99 | Total Steps: 54\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 7  | Average Reward -2.94  | Actor loss: 0.05 | Critic loss: 3.35 | Entropy loss: -0.0118  | Total Loss: 3.39 | Total Steps: 66\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 4  | Average Reward -2.92  | Actor loss: 0.00 | Critic loss: 2.54 | Entropy loss: -0.0132  | Total Loss: 2.53 | Total Steps: 48\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 1  | Average Reward -2.93  | Actor loss: 1.20 | Critic loss: 15.50 | Entropy loss: -0.0333  | Total Loss: 16.67 | Total Steps: 83\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 1  | Average Reward -2.94  | Actor loss: 0.01 | Critic loss: 1.17 | Entropy loss: -0.0116  | Total Loss: 1.17 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 10  | Average Reward -2.92  | Actor loss: 0.49 | Critic loss: 15.30 | Entropy loss: -0.0027  | Total Loss: 15.79 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 1  | Average Reward -2.35  | Actor loss: 4.10 | Critic loss: 22.43 | Entropy loss: -0.0068  | Total Loss: 26.53 | Total Steps: 58\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 7  | Average Reward -2.27  | Actor loss: 0.11 | Critic loss: 11.14 | Entropy loss: -0.0069  | Total Loss: 11.24 | Total Steps: 30\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 10/100  | Episode Reward: -67  | Average Reward -2.65  | Actor loss: -0.92 | Critic loss: 108.04 | Entropy loss: -0.0282  | Total Loss: 107.10 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward -2.62  | Actor loss: 1.20 | Critic loss: 28.43 | Entropy loss: -0.0393  | Total Loss: 29.59 | Total Steps: 40\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: -2  | Average Reward -2.65  | Actor loss: 0.99 | Critic loss: 15.36 | Entropy loss: -0.0598  | Total Loss: 16.28 | Total Steps: 62\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward -2.67  | Actor loss: 0.00 | Critic loss: 2.63 | Entropy loss: -0.0036  | Total Loss: 2.63 | Total Steps: 30\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 10  | Average Reward -2.63  | Actor loss: 0.34 | Critic loss: 18.00 | Entropy loss: -0.0704  | Total Loss: 18.27 | Total Steps: 7\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward -2.60  | Actor loss: 1.33 | Critic loss: 28.03 | Entropy loss: -0.0468  | Total Loss: 29.31 | Total Steps: 16\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: -2  | Average Reward -2.00  | Actor loss: 1.18 | Critic loss: 22.60 | Entropy loss: -0.0174  | Total Loss: 23.77 | Total Steps: 81\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward -1.97  | Actor loss: 1.69 | Critic loss: 14.54 | Entropy loss: -0.0111  | Total Loss: 16.22 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward -1.95  | Actor loss: 0.01 | Critic loss: 11.00 | Entropy loss: -0.0014  | Total Loss: 11.01 | Total Steps: 31\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: -2  | Average Reward -1.31  | Actor loss: 0.81 | Critic loss: 18.70 | Entropy loss: -0.0368  | Total Loss: 19.47 | Total Steps: 115\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Average Reward -1.31  | Actor loss: 0.76 | Critic loss: 21.43 | Entropy loss: -0.0279  | Total Loss: 22.16 | Total Steps: 59\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 7  | Average Reward -1.32  | Actor loss: 0.36 | Critic loss: 17.17 | Entropy loss: -0.0081  | Total Loss: 17.53 | Total Steps: 27\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 4  | Average Reward -1.35  | Actor loss: 1.11 | Critic loss: 3.96 | Entropy loss: -0.0063  | Total Loss: 5.06 | Total Steps: 79\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward -1.35  | Actor loss: 4.61 | Critic loss: 16.70 | Entropy loss: -0.0167  | Total Loss: 21.29 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Average Reward -1.35  | Actor loss: 0.02 | Critic loss: 11.24 | Entropy loss: -0.0603  | Total Loss: 11.19 | Total Steps: 7\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward -1.32  | Actor loss: 0.00 | Critic loss: 2.49 | Entropy loss: -0.0042  | Total Loss: 2.48 | Total Steps: 34\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward -1.32  | Actor loss: 0.03 | Critic loss: 6.43 | Entropy loss: -0.0120  | Total Loss: 6.45 | Total Steps: 65\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: -2  | Average Reward -1.37  | Actor loss: 1.37 | Critic loss: 27.64 | Entropy loss: -0.0382  | Total Loss: 28.97 | Total Steps: 91\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 7  | Average Reward -1.29  | Actor loss: 0.42 | Critic loss: 15.11 | Entropy loss: -0.0080  | Total Loss: 15.53 | Total Steps: 37\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward -1.26  | Actor loss: 1.72 | Critic loss: 14.61 | Entropy loss: -0.0094  | Total Loss: 16.32 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: -2  | Average Reward -0.69  | Actor loss: 1.36 | Critic loss: 29.26 | Entropy loss: -0.0323  | Total Loss: 30.59 | Total Steps: 80\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 1  | Average Reward -0.72  | Actor loss: 0.04 | Critic loss: 7.06 | Entropy loss: -0.0118  | Total Loss: 7.09 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 32/100  | Episode Reward: -76  | Average Reward -1.16  | Actor loss: -0.04 | Critic loss: 80.07 | Entropy loss: -0.0355  | Total Loss: 80.00 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward -0.82  | Actor loss: 0.11 | Critic loss: 5.95 | Entropy loss: -0.0137  | Total Loss: 6.05 | Total Steps: 8\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: -2  | Average Reward -0.17  | Actor loss: 9.59 | Critic loss: 10.80 | Entropy loss: -0.0218  | Total Loss: 20.36 | Total Steps: 54\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Average Reward -0.12  | Actor loss: 0.00 | Critic loss: 7.62 | Entropy loss: -0.0038  | Total Loss: 7.62 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 1  | Average Reward -0.17  | Actor loss: 1.44 | Critic loss: 11.93 | Entropy loss: -0.0131  | Total Loss: 13.36 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 7  | Average Reward -0.18  | Actor loss: 0.37 | Critic loss: 3.22 | Entropy loss: -0.0241  | Total Loss: 3.56 | Total Steps: 50\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 7  | Average Reward -0.15  | Actor loss: 0.28 | Critic loss: 16.66 | Entropy loss: -0.0229  | Total Loss: 16.92 | Total Steps: 28\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 1  | Average Reward -0.15  | Actor loss: 0.03 | Critic loss: 3.55 | Entropy loss: -0.0059  | Total Loss: 3.58 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 4  | Average Reward 0.35  | Actor loss: 0.42 | Critic loss: 11.00 | Entropy loss: -0.0088  | Total Loss: 11.41 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 3  | Average Reward 0.39  | Actor loss: 0.01 | Critic loss: 2.23 | Entropy loss: -0.0192  | Total Loss: 2.22 | Total Steps: 46\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 10  | Average Reward 0.41  | Actor loss: 0.02 | Critic loss: 31.48 | Entropy loss: -0.0472  | Total Loss: 31.46 | Total Steps: 8\n",
      "TEST: ---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 1  | Average Reward 0.39  | Actor loss: 0.03 | Critic loss: 1.17 | Entropy loss: -0.0371  | Total Loss: 1.16 | Total Steps: 85\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 0.40  | Actor loss: 0.72 | Critic loss: 15.63 | Entropy loss: -0.0368  | Total Loss: 16.31 | Total Steps: 28\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 7  | Average Reward 0.43  | Actor loss: 1.70 | Critic loss: 25.68 | Entropy loss: -0.0187  | Total Loss: 27.36 | Total Steps: 33\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 0.81  | Actor loss: 0.01 | Critic loss: 16.25 | Entropy loss: -0.0034  | Total Loss: 16.26 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 1  | Average Reward 0.76  | Actor loss: 1.21 | Critic loss: 13.97 | Entropy loss: -0.0100  | Total Loss: 15.17 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward 0.79  | Actor loss: 0.10 | Critic loss: 5.57 | Entropy loss: -0.0317  | Total Loss: 5.64 | Total Steps: 44\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 4  | Average Reward 0.78  | Actor loss: 7.15 | Critic loss: 24.62 | Entropy loss: -0.0308  | Total Loss: 31.74 | Total Steps: 60\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward 0.78  | Actor loss: 0.01 | Critic loss: 13.19 | Entropy loss: -0.0096  | Total Loss: 13.18 | Total Steps: 96\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 1  | Average Reward 0.73  | Actor loss: 0.07 | Critic loss: 6.49 | Entropy loss: -0.0057  | Total Loss: 6.55 | Total Steps: 50\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward 0.78  | Actor loss: 0.00 | Critic loss: 2.25 | Entropy loss: -0.0017  | Total Loss: 2.26 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: -15  | Average Reward 0.74  | Actor loss: 4.71 | Critic loss: 20.93 | Entropy loss: -0.0302  | Total Loss: 25.61 | Total Steps: 170\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Average Reward 0.78  | Actor loss: 0.04 | Critic loss: 11.69 | Entropy loss: -0.0134  | Total Loss: 11.72 | Total Steps: 35\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 1  | Average Reward 0.81  | Actor loss: 0.00 | Critic loss: 2.10 | Entropy loss: -0.0157  | Total Loss: 2.09 | Total Steps: 54\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 7  | Average Reward 0.82  | Actor loss: 0.03 | Critic loss: 12.21 | Entropy loss: -0.0114  | Total Loss: 12.23 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -2  | Average Reward 0.83  | Actor loss: 0.79 | Critic loss: 25.29 | Entropy loss: -0.0189  | Total Loss: 26.06 | Total Steps: 103\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 7  | Average Reward 0.85  | Actor loss: 1.72 | Critic loss: 25.72 | Entropy loss: -0.0156  | Total Loss: 27.42 | Total Steps: 27\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 7  | Average Reward 1.45  | Actor loss: 0.48 | Critic loss: 18.46 | Entropy loss: -0.0165  | Total Loss: 18.91 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 7  | Average Reward 1.48  | Actor loss: 0.00 | Critic loss: 6.00 | Entropy loss: -0.0010  | Total Loss: 6.00 | Total Steps: 38\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward 1.50  | Actor loss: 0.28 | Critic loss: 16.55 | Entropy loss: -0.0156  | Total Loss: 16.82 | Total Steps: 33\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward 1.55  | Actor loss: 0.59 | Critic loss: 11.11 | Entropy loss: -0.0035  | Total Loss: 11.69 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward 1.55  | Actor loss: 0.01 | Critic loss: 2.06 | Entropy loss: -0.0847  | Total Loss: 1.99 | Total Steps: 19\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 7  | Average Reward 1.53  | Actor loss: 0.89 | Critic loss: 19.53 | Entropy loss: -0.0173  | Total Loss: 20.40 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 7  | Average Reward 1.58  | Actor loss: 0.01 | Critic loss: 13.25 | Entropy loss: -0.0013  | Total Loss: 13.26 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward 1.58  | Actor loss: 0.01 | Critic loss: 15.04 | Entropy loss: -0.0017  | Total Loss: 15.04 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 10  | Average Reward 1.61  | Actor loss: 0.01 | Critic loss: 9.34 | Entropy loss: -0.0003  | Total Loss: 9.34 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 7  | Average Reward 2.13  | Actor loss: 1.28 | Critic loss: 18.33 | Entropy loss: -0.0177  | Total Loss: 19.60 | Total Steps: 36\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 7  | Average Reward 2.15  | Actor loss: 2.79 | Critic loss: 19.01 | Entropy loss: -0.0306  | Total Loss: 21.77 | Total Steps: 76\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 0  | Average Reward 2.10  | Actor loss: 0.64 | Critic loss: 19.26 | Entropy loss: -0.0221  | Total Loss: 19.88 | Total Steps: 104\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: -2  | Average Reward 2.07  | Actor loss: 0.01 | Critic loss: 0.60 | Entropy loss: -0.0242  | Total Loss: 0.59 | Total Steps: 55\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 1  | Average Reward 2.04  | Actor loss: 0.04 | Critic loss: 4.28 | Entropy loss: -0.0080  | Total Loss: 4.31 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 4  | Average Reward 2.13  | Actor loss: 0.00 | Critic loss: 5.90 | Entropy loss: -0.0031  | Total Loss: 5.90 | Total Steps: 43\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 2.15  | Actor loss: 0.00 | Critic loss: 2.19 | Entropy loss: -0.0010  | Total Loss: 2.20 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 1  | Average Reward 2.13  | Actor loss: 0.01 | Critic loss: 1.11 | Entropy loss: -0.0256  | Total Loss: 1.10 | Total Steps: 64\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 10  | Average Reward 2.17  | Actor loss: 0.88 | Critic loss: 25.50 | Entropy loss: -0.0332  | Total Loss: 26.35 | Total Steps: 73\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Average Reward 2.19  | Actor loss: 0.38 | Critic loss: 16.93 | Entropy loss: -0.0085  | Total Loss: 17.30 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward 2.17  | Actor loss: 0.00 | Critic loss: 5.69 | Entropy loss: -0.0019  | Total Loss: 5.69 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 7.52 | Entropy loss: -0.0005  | Total Loss: 7.53 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 7  | Average Reward 2.81  | Actor loss: 1.16 | Critic loss: 19.54 | Entropy loss: -0.0159  | Total Loss: 20.69 | Total Steps: 34\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 3  | Average Reward 2.79  | Actor loss: 0.02 | Critic loss: 2.35 | Entropy loss: -0.0332  | Total Loss: 2.33 | Total Steps: 72\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 1  | Average Reward 2.75  | Actor loss: 0.00 | Critic loss: 1.53 | Entropy loss: -0.0074  | Total Loss: 1.53 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 10  | Average Reward 2.79  | Actor loss: 0.06 | Critic loss: 11.80 | Entropy loss: -0.0206  | Total Loss: 11.84 | Total Steps: 14\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: -5  | Average Reward 2.75  | Actor loss: 0.00 | Critic loss: 4.63 | Entropy loss: -0.0162  | Total Loss: 4.61 | Total Steps: 57\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: -1  | Average Reward 2.69  | Actor loss: 1.73 | Critic loss: 16.49 | Entropy loss: -0.0448  | Total Loss: 18.18 | Total Steps: 212\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: -11  | Average Reward 2.63  | Actor loss: 0.57 | Critic loss: 16.72 | Entropy loss: -0.0269  | Total Loss: 17.27 | Total Steps: 187\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward 2.67  | Actor loss: 1.30 | Critic loss: 21.11 | Entropy loss: -0.0220  | Total Loss: 22.39 | Total Steps: 37\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward 2.67  | Actor loss: 7.94 | Critic loss: 16.32 | Entropy loss: -0.0713  | Total Loss: 24.18 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward 2.73  | Actor loss: 0.25 | Critic loss: 14.42 | Entropy loss: -0.0227  | Total Loss: 14.65 | Total Steps: 33\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 1  | Average Reward 3.35  | Actor loss: 0.01 | Critic loss: 1.05 | Entropy loss: -0.0070  | Total Loss: 1.05 | Total Steps: 51\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward 3.35  | Actor loss: 0.00 | Critic loss: 5.89 | Entropy loss: -0.0070  | Total Loss: 5.89 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward 3.38  | Actor loss: 1.31 | Critic loss: 30.00 | Entropy loss: -0.0378  | Total Loss: 31.26 | Total Steps: 22\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 7  | Average Reward 3.40  | Actor loss: 0.43 | Critic loss: 15.30 | Entropy loss: -0.0077  | Total Loss: 15.71 | Total Steps: 37\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: -2  | Average Reward 3.33  | Actor loss: 0.54 | Critic loss: 10.47 | Entropy loss: -0.0304  | Total Loss: 10.98 | Total Steps: 52\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 1  | Average Reward 3.33  | Actor loss: 0.10 | Critic loss: 8.04 | Entropy loss: -0.0060  | Total Loss: 8.14 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 96/100  | Episode Reward: -85  | Average Reward 2.89  | Actor loss: -26.10 | Critic loss: 98.62 | Entropy loss: -0.0271  | Total Loss: 72.49 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 4  | Average Reward 2.88  | Actor loss: 1.00 | Critic loss: 17.26 | Entropy loss: -0.0371  | Total Loss: 18.22 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward 2.90  | Actor loss: 0.29 | Critic loss: 10.59 | Entropy loss: -0.0287  | Total Loss: 10.85 | Total Steps: 32\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 10  | Average Reward 2.90  | Actor loss: 0.48 | Critic loss: 15.28 | Entropy loss: -0.0041  | Total Loss: 15.76 | Total Steps: 31\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward 2.90  | Actor loss: 0.00 | Critic loss: 10.58 | Entropy loss: -0.0008  | Total Loss: 10.58 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1002/129000  | Episode Reward: 7  | Average Reward 6.08  | Actor loss: 0.44 | Critic loss: 2.72 | Entropy loss: -0.0044  | Total Loss: 3.16 | Total Steps: 33\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1003/129000  | Episode Reward: 7  | Average Reward 6.07  | Actor loss: -0.10 | Critic loss: 5.89 | Entropy loss: -0.0019  | Total Loss: 5.78 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1004/129000  | Episode Reward: 7  | Average Reward 6.05  | Actor loss: 0.07 | Critic loss: 6.66 | Entropy loss: -0.0003  | Total Loss: 6.73 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1005/129000  | Episode Reward: 7  | Average Reward 6.07  | Actor loss: -0.07 | Critic loss: 5.50 | Entropy loss: -0.0004  | Total Loss: 5.43 | Total Steps: 30\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1006/129000  | Episode Reward: 10  | Average Reward 6.10  | Actor loss: 1.31 | Critic loss: 8.05 | Entropy loss: -0.0023  | Total Loss: 9.36 | Total Steps: 15\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1007/129000  | Episode Reward: 4  | Average Reward 6.10  | Actor loss: -0.03 | Critic loss: 10.15 | Entropy loss: -0.0009  | Total Loss: 10.12 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1008/129000  | Episode Reward: -24  | Average Reward 5.93  | Actor loss: 0.05 | Critic loss: 2.80 | Entropy loss: -0.0002  | Total Loss: 2.85 | Total Steps: 257\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1009/129000  | Episode Reward: 10  | Average Reward 5.97  | Actor loss: -0.14 | Critic loss: 10.12 | Entropy loss: -0.0023  | Total Loss: 9.98 | Total Steps: 25\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1010/129000  | Episode Reward: 4  | Average Reward 5.95  | Actor loss: 0.16 | Critic loss: 7.49 | Entropy loss: -0.0033  | Total Loss: 7.65 | Total Steps: 45\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1011/129000  | Episode Reward: 10  | Average Reward 5.99  | Actor loss: 2.20 | Critic loss: 8.79 | Entropy loss: -0.0013  | Total Loss: 10.98 | Total Steps: 9\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1012/129000  | Episode Reward: 4  | Average Reward 5.96  | Actor loss: -0.07 | Critic loss: 7.41 | Entropy loss: -0.0029  | Total Loss: 7.33 | Total Steps: 45\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1013/129000  | Episode Reward: 4  | Average Reward 5.95  | Actor loss: 0.15 | Critic loss: 4.10 | Entropy loss: -0.0022  | Total Loss: 4.25 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1014/129000  | Episode Reward: -5  | Average Reward 5.90  | Actor loss: -0.83 | Critic loss: 23.97 | Entropy loss: -0.0038  | Total Loss: 23.13 | Total Steps: 87\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1015/129000  | Episode Reward: 4  | Average Reward 5.88  | Actor loss: -0.25 | Critic loss: 8.33 | Entropy loss: -0.0059  | Total Loss: 8.08 | Total Steps: 46\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1016/129000  | Episode Reward: 10  | Average Reward 5.88  | Actor loss: 0.01 | Critic loss: 7.26 | Entropy loss: -0.0000  | Total Loss: 7.28 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1017/129000  | Episode Reward: -2  | Average Reward 5.84  | Actor loss: -0.49 | Critic loss: 12.37 | Entropy loss: -0.0047  | Total Loss: 11.87 | Total Steps: 88\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1018/129000  | Episode Reward: 10  | Average Reward 5.88  | Actor loss: 0.26 | Critic loss: 5.69 | Entropy loss: -0.0003  | Total Loss: 5.95 | Total Steps: 8\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1019/129000  | Episode Reward: -1  | Average Reward 5.83  | Actor loss: -0.67 | Critic loss: 12.90 | Entropy loss: -0.0051  | Total Loss: 12.23 | Total Steps: 56\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1020/129000  | Episode Reward: 4  | Average Reward 5.80  | Actor loss: -0.06 | Critic loss: 3.77 | Entropy loss: -0.0004  | Total Loss: 3.71 | Total Steps: 42\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1021/129000  | Episode Reward: 10  | Average Reward 5.80  | Actor loss: 0.01 | Critic loss: 7.79 | Entropy loss: -0.0000  | Total Loss: 7.80 | Total Steps: 6\n",
      "---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1022/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: -0.21 | Critic loss: 9.60 | Entropy loss: -0.0075  | Total Loss: 9.38 | Total Steps: 55\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1023/129000  | Episode Reward: 7  | Average Reward 5.82  | Actor loss: -0.02 | Critic loss: 3.88 | Entropy loss: -0.0003  | Total Loss: 3.86 | Total Steps: 42\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1024/129000  | Episode Reward: 7  | Average Reward 5.82  | Actor loss: 0.21 | Critic loss: 6.98 | Entropy loss: -0.0008  | Total Loss: 7.18 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1025/129000  | Episode Reward: 4  | Average Reward 5.79  | Actor loss: -0.75 | Critic loss: 7.73 | Entropy loss: -0.0082  | Total Loss: 6.97 | Total Steps: 83\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1026/129000  | Episode Reward: 10  | Average Reward 5.79  | Actor loss: 0.01 | Critic loss: 2.51 | Entropy loss: -0.0000  | Total Loss: 2.52 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1027/129000  | Episode Reward: 10  | Average Reward 5.83  | Actor loss: 0.02 | Critic loss: 2.02 | Entropy loss: -0.0000  | Total Loss: 2.04 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1028/129000  | Episode Reward: 4  | Average Reward 5.80  | Actor loss: -0.16 | Critic loss: 8.51 | Entropy loss: -0.0041  | Total Loss: 8.34 | Total Steps: 48\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1029/129000  | Episode Reward: 4  | Average Reward 5.77  | Actor loss: -0.38 | Critic loss: 8.71 | Entropy loss: -0.0017  | Total Loss: 8.32 | Total Steps: 38\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1030/129000  | Episode Reward: 10  | Average Reward 5.77  | Actor loss: 1.66 | Critic loss: 8.90 | Entropy loss: -0.0010  | Total Loss: 10.55 | Total Steps: 7\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1031/129000  | Episode Reward: 7  | Average Reward 5.79  | Actor loss: 0.11 | Critic loss: 7.91 | Entropy loss: -0.0004  | Total Loss: 8.02 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1032/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: 0.03 | Critic loss: 6.57 | Entropy loss: -0.0002  | Total Loss: 6.60 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1033/129000  | Episode Reward: 4  | Average Reward 5.79  | Actor loss: -0.37 | Critic loss: 5.51 | Entropy loss: -0.0033  | Total Loss: 5.13 | Total Steps: 55\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1034/129000  | Episode Reward: 10  | Average Reward 5.82  | Actor loss: 0.02 | Critic loss: 5.28 | Entropy loss: -0.0017  | Total Loss: 5.30 | Total Steps: 25\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1035/129000  | Episode Reward: 7  | Average Reward 5.89  | Actor loss: 0.08 | Critic loss: 3.02 | Entropy loss: -0.0034  | Total Loss: 3.10 | Total Steps: 40\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1036/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.41 | Critic loss: 18.36 | Entropy loss: -0.0002  | Total Loss: 18.77 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1037/129000  | Episode Reward: 4  | Average Reward 5.92  | Actor loss: 0.06 | Critic loss: 9.41 | Entropy loss: -0.0031  | Total Loss: 9.47 | Total Steps: 56\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1038/129000  | Episode Reward: 1  | Average Reward 5.88  | Actor loss: -0.51 | Critic loss: 9.36 | Entropy loss: -0.0019  | Total Loss: 8.85 | Total Steps: 54\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1039/129000  | Episode Reward: 10  | Average Reward 5.89  | Actor loss: 0.01 | Critic loss: 2.83 | Entropy loss: -0.0000  | Total Loss: 2.84 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1040/129000  | Episode Reward: 7  | Average Reward 5.89  | Actor loss: 0.27 | Critic loss: 6.42 | Entropy loss: -0.0008  | Total Loss: 6.69 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1041/129000  | Episode Reward: 1  | Average Reward 5.89  | Actor loss: -0.73 | Critic loss: 13.39 | Entropy loss: -0.0021  | Total Loss: 12.65 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1042/129000  | Episode Reward: 4  | Average Reward 5.86  | Actor loss: -0.13 | Critic loss: 6.78 | Entropy loss: -0.0034  | Total Loss: 6.64 | Total Steps: 56\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1043/129000  | Episode Reward: 7  | Average Reward 5.86  | Actor loss: 0.03 | Critic loss: 7.78 | Entropy loss: -0.0011  | Total Loss: 7.82 | Total Steps: 30\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1044/129000  | Episode Reward: 10  | Average Reward 5.86  | Actor loss: -0.31 | Critic loss: 4.94 | Entropy loss: -0.0018  | Total Loss: 4.63 | Total Steps: 31\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1045/129000  | Episode Reward: 10  | Average Reward 5.88  | Actor loss: 1.00 | Critic loss: 7.32 | Entropy loss: -0.0023  | Total Loss: 8.31 | Total Steps: 17\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1046/129000  | Episode Reward: 7  | Average Reward 5.89  | Actor loss: 0.09 | Critic loss: 3.80 | Entropy loss: -0.0005  | Total Loss: 3.89 | Total Steps: 34\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1047/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.88 | Critic loss: 3.36 | Entropy loss: -0.0027  | Total Loss: 4.24 | Total Steps: 13\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1048/129000  | Episode Reward: 7  | Average Reward 5.93  | Actor loss: 0.19 | Critic loss: 4.60 | Entropy loss: -0.0010  | Total Loss: 4.79 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1049/129000  | Episode Reward: 1  | Average Reward 5.91  | Actor loss: -0.06 | Critic loss: 8.87 | Entropy loss: -0.0006  | Total Loss: 8.81 | Total Steps: 53\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1050/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 1.35 | Critic loss: 2.60 | Entropy loss: -0.0030  | Total Loss: 3.95 | Total Steps: 14\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1051/129000  | Episode Reward: 10  | Average Reward 5.93  | Actor loss: -0.17 | Critic loss: 3.15 | Entropy loss: -0.0032  | Total Loss: 2.98 | Total Steps: 46\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  | Episode: 1052/129000  | Episode Reward: 10  | Average Reward 5.95  | Actor loss: 0.02 | Critic loss: 1.62 | Entropy loss: -0.0000  | Total Loss: 1.64 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1053/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.02 | Critic loss: 1.90 | Entropy loss: -0.0000  | Total Loss: 1.91 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1054/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.01 | Critic loss: 1.30 | Entropy loss: -0.0000  | Total Loss: 1.31 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1055/129000  | Episode Reward: 10  | Average Reward 6.00  | Actor loss: 0.40 | Critic loss: 5.45 | Entropy loss: -0.0027  | Total Loss: 5.85 | Total Steps: 10\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1056/129000  | Episode Reward: 7  | Average Reward 6.01  | Actor loss: 0.28 | Critic loss: 6.63 | Entropy loss: -0.0017  | Total Loss: 6.90 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1057/129000  | Episode Reward: 7  | Average Reward 6.04  | Actor loss: -0.02 | Critic loss: 2.07 | Entropy loss: -0.0003  | Total Loss: 2.06 | Total Steps: 38\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1058/129000  | Episode Reward: 10  | Average Reward 6.07  | Actor loss: 0.06 | Critic loss: 2.70 | Entropy loss: -0.0062  | Total Loss: 2.76 | Total Steps: 52\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1059/129000  | Episode Reward: 7  | Average Reward 6.07  | Actor loss: 0.58 | Critic loss: 7.25 | Entropy loss: -0.0029  | Total Loss: 7.83 | Total Steps: 31\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1060/129000  | Episode Reward: 4  | Average Reward 6.05  | Actor loss: -0.12 | Critic loss: 8.72 | Entropy loss: -0.0009  | Total Loss: 8.60 | Total Steps: 47\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1061/129000  | Episode Reward: 4  | Average Reward 6.04  | Actor loss: -0.11 | Critic loss: 9.01 | Entropy loss: -0.0004  | Total Loss: 8.89 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1062/129000  | Episode Reward: 7  | Average Reward 6.04  | Actor loss: -0.31 | Critic loss: 2.31 | Entropy loss: -0.0023  | Total Loss: 2.00 | Total Steps: 68\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1063/129000  | Episode Reward: 4  | Average Reward 6.04  | Actor loss: 0.16 | Critic loss: 6.15 | Entropy loss: -0.0025  | Total Loss: 6.31 | Total Steps: 45\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1064/129000  | Episode Reward: 1  | Average Reward 6.04  | Actor loss: -0.65 | Critic loss: 8.12 | Entropy loss: -0.0113  | Total Loss: 7.46 | Total Steps: 71\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1065/129000  | Episode Reward: 4  | Average Reward 6.03  | Actor loss: -0.06 | Critic loss: 8.16 | Entropy loss: -0.0005  | Total Loss: 8.10 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1066/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.01 | Critic loss: 3.97 | Entropy loss: -0.0002  | Total Loss: 3.96 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1067/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.16 | Critic loss: 2.64 | Entropy loss: -0.0002  | Total Loss: 2.80 | Total Steps: 8\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1068/129000  | Episode Reward: 7  | Average Reward 6.10  | Actor loss: 0.06 | Critic loss: 3.21 | Entropy loss: -0.0005  | Total Loss: 3.27 | Total Steps: 30\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1069/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: -0.04 | Critic loss: 2.85 | Entropy loss: -0.0007  | Total Loss: 2.80 | Total Steps: 43\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1070/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.25 | Critic loss: 3.55 | Entropy loss: -0.0003  | Total Loss: 3.80 | Total Steps: 8\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1071/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.07 | Critic loss: 3.98 | Entropy loss: -0.0002  | Total Loss: 4.05 | Total Steps: 30\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1072/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: -0.10 | Critic loss: 1.48 | Entropy loss: -0.0022  | Total Loss: 1.37 | Total Steps: 44\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1073/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.01 | Critic loss: 6.33 | Entropy loss: -0.0000  | Total Loss: 6.34 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1074/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.27 | Critic loss: 3.16 | Entropy loss: -0.0005  | Total Loss: 3.43 | Total Steps: 8\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1075/129000  | Episode Reward: 3  | Average Reward 6.20  | Actor loss: -0.99 | Critic loss: 10.68 | Entropy loss: -0.0056  | Total Loss: 9.68 | Total Steps: 47\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1076/129000  | Episode Reward: 7  | Average Reward 6.18  | Actor loss: 0.03 | Critic loss: 6.71 | Entropy loss: -0.0004  | Total Loss: 6.74 | Total Steps: 30\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1077/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.01 | Critic loss: 2.01 | Entropy loss: -0.0000  | Total Loss: 2.02 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1078/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: 0.02 | Critic loss: 2.97 | Entropy loss: -0.0002  | Total Loss: 3.00 | Total Steps: 30\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1079/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.25 | Critic loss: 9.53 | Entropy loss: -0.0038  | Total Loss: 9.28 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1080/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: -0.00 | Critic loss: 2.54 | Entropy loss: -0.0037  | Total Loss: 2.53 | Total Steps: 45\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1081/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.17 | Critic loss: 2.79 | Entropy loss: -0.0015  | Total Loss: 2.95 | Total Steps: 32\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1082/129000  | Episode Reward: 7  | Average Reward 6.18  | Actor loss: -0.02 | Critic loss: 4.51 | Entropy loss: -0.0064  | Total Loss: 4.49 | Total Steps: 54\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1083/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.06 | Critic loss: 6.84 | Entropy loss: -0.0015  | Total Loss: 6.77 | Total Steps: 45\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1084/129000  | Episode Reward: 1  | Average Reward 6.16  | Actor loss: -0.16 | Critic loss: 12.92 | Entropy loss: -0.0006  | Total Loss: 12.76 | Total Steps: 52\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1085/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: 0.04 | Critic loss: 3.55 | Entropy loss: -0.0014  | Total Loss: 3.59 | Total Steps: 47\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1086/129000  | Episode Reward: 4  | Average Reward 6.14  | Actor loss: -0.52 | Critic loss: 7.98 | Entropy loss: -0.0075  | Total Loss: 7.45 | Total Steps: 54\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1087/129000  | Episode Reward: 1  | Average Reward 6.11  | Actor loss: -0.63 | Critic loss: 9.78 | Entropy loss: -0.0036  | Total Loss: 9.15 | Total Steps: 67\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1088/129000  | Episode Reward: 7  | Average Reward 6.11  | Actor loss: 0.22 | Critic loss: 6.32 | Entropy loss: -0.0047  | Total Loss: 6.54 | Total Steps: 46\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1089/129000  | Episode Reward: 1  | Average Reward 6.08  | Actor loss: -0.36 | Critic loss: 12.41 | Entropy loss: -0.0011  | Total Loss: 12.05 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1090/129000  | Episode Reward: 4  | Average Reward 6.08  | Actor loss: -0.71 | Critic loss: 7.77 | Entropy loss: -0.0043  | Total Loss: 7.06 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1091/129000  | Episode Reward: 1  | Average Reward 6.08  | Actor loss: -0.11 | Critic loss: 9.39 | Entropy loss: -0.0014  | Total Loss: 9.28 | Total Steps: 53\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1092/129000  | Episode Reward: 6  | Average Reward 6.15  | Actor loss: -0.30 | Critic loss: 3.35 | Entropy loss: -0.0071  | Total Loss: 3.04 | Total Steps: 56\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1093/129000  | Episode Reward: 7  | Average Reward 6.15  | Actor loss: -0.19 | Critic loss: 5.35 | Entropy loss: -0.0017  | Total Loss: 5.16 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1094/129000  | Episode Reward: 4  | Average Reward 6.15  | Actor loss: -0.35 | Critic loss: 10.03 | Entropy loss: -0.0047  | Total Loss: 9.67 | Total Steps: 54\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1095/129000  | Episode Reward: 10  | Average Reward 6.15  | Actor loss: 0.01 | Critic loss: 2.25 | Entropy loss: -0.0000  | Total Loss: 2.26 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1096/129000  | Episode Reward: 7  | Average Reward 6.15  | Actor loss: 0.25 | Critic loss: 5.82 | Entropy loss: -0.0011  | Total Loss: 6.07 | Total Steps: 32\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1097/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.02 | Critic loss: 15.80 | Entropy loss: -0.0000  | Total Loss: 15.83 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1098/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.08 | Critic loss: 7.42 | Entropy loss: -0.0003  | Total Loss: 7.49 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1099/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.06 | Critic loss: 3.01 | Entropy loss: -0.0005  | Total Loss: 3.07 | Total Steps: 31\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1100/129000  | Episode Reward: 7  | Average Reward 6.22  | Actor loss: 0.04 | Critic loss: 5.74 | Entropy loss: -0.0007  | Total Loss: 5.77 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1101/129000  | Episode Reward: 4  | Average Reward 6.20  | Actor loss: -0.23 | Critic loss: 6.63 | Entropy loss: -0.0024  | Total Loss: 6.40 | Total Steps: 44\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 7  | Average Reward 2.89  | Actor loss: 0.00 | Critic loss: 7.21 | Entropy loss: -0.0058  | Total Loss: 7.20 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 5  | Average Reward 2.88  | Actor loss: 0.01 | Critic loss: 1.08 | Entropy loss: -0.0224  | Total Loss: 1.07 | Total Steps: 66\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 3  | Average Reward 2.87  | Actor loss: 0.05 | Critic loss: 1.61 | Entropy loss: -0.0207  | Total Loss: 1.64 | Total Steps: 59\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 1  | Average Reward 2.85  | Actor loss: 0.00 | Critic loss: 8.60 | Entropy loss: -0.0197  | Total Loss: 8.59 | Total Steps: 66\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 1  | Average Reward 2.81  | Actor loss: 0.05 | Critic loss: 1.88 | Entropy loss: -0.0147  | Total Loss: 1.91 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 10  | Average Reward 2.83  | Actor loss: 0.00 | Critic loss: 3.51 | Entropy loss: -0.0011  | Total Loss: 3.51 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward 2.83  | Actor loss: 2.15 | Critic loss: 2.38 | Entropy loss: -0.0139  | Total Loss: 4.52 | Total Steps: 54\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 7  | Average Reward 2.87  | Actor loss: 0.02 | Critic loss: 2.90 | Entropy loss: -0.0086  | Total Loss: 2.92 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward 2.87  | Actor loss: 0.52 | Critic loss: 8.83 | Entropy loss: -0.0342  | Total Loss: 9.31 | Total Steps: 58\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward 2.88  | Actor loss: 0.00 | Critic loss: 8.53 | Entropy loss: -0.0061  | Total Loss: 8.52 | Total Steps: 6\n",
      "TEST: ---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward 2.88  | Actor loss: 0.00 | Critic loss: 4.14 | Entropy loss: -0.0010  | Total Loss: 4.14 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Average Reward 2.88  | Actor loss: 0.04 | Critic loss: 2.77 | Entropy loss: -0.0032  | Total Loss: 2.81 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 10  | Average Reward 2.92  | Actor loss: 0.10 | Critic loss: 10.28 | Entropy loss: -0.0058  | Total Loss: 10.37 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 1  | Average Reward 2.92  | Actor loss: 0.00 | Critic loss: 4.56 | Entropy loss: -0.0221  | Total Loss: 4.53 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 7  | Average Reward 2.94  | Actor loss: 0.03 | Critic loss: 2.37 | Entropy loss: -0.0035  | Total Loss: 2.39 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 2.98  | Actor loss: 0.32 | Critic loss: 9.67 | Entropy loss: -0.0035  | Total Loss: 9.99 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 7  | Average Reward 2.98  | Actor loss: 0.01 | Critic loss: 2.18 | Entropy loss: -0.0060  | Total Loss: 2.18 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 7  | Average Reward 3.44  | Actor loss: 0.00 | Critic loss: 6.65 | Entropy loss: -0.0007  | Total Loss: 6.65 | Total Steps: 34\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Average Reward 3.52  | Actor loss: 0.00 | Critic loss: 6.93 | Entropy loss: -0.0006  | Total Loss: 6.93 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward 3.52  | Actor loss: 0.01 | Critic loss: 2.74 | Entropy loss: -0.0033  | Total Loss: 2.74 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 4  | Average Reward 3.50  | Actor loss: 0.06 | Critic loss: 1.76 | Entropy loss: -0.0063  | Total Loss: 1.81 | Total Steps: 170\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 10  | Average Reward 3.50  | Actor loss: 0.77 | Critic loss: 11.26 | Entropy loss: -0.0071  | Total Loss: 12.02 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward 3.50  | Actor loss: 0.10 | Critic loss: 10.25 | Entropy loss: -0.0069  | Total Loss: 10.35 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Average Reward 3.50  | Actor loss: 2.35 | Critic loss: 14.18 | Entropy loss: -0.0297  | Total Loss: 16.50 | Total Steps: 45\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 10  | Average Reward 3.51  | Actor loss: 0.32 | Critic loss: 9.74 | Entropy loss: -0.0078  | Total Loss: 10.05 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 3.52  | Actor loss: 0.00 | Critic loss: 8.60 | Entropy loss: -0.0017  | Total Loss: 8.61 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward 3.52  | Actor loss: 0.00 | Critic loss: 3.18 | Entropy loss: -0.0083  | Total Loss: 3.17 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 10  | Average Reward 3.56  | Actor loss: 0.32 | Critic loss: 9.67 | Entropy loss: -0.0043  | Total Loss: 9.98 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward 3.65  | Actor loss: 0.00 | Critic loss: 10.58 | Entropy loss: -0.0012  | Total Loss: 10.58 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 10  | Average Reward 3.66  | Actor loss: 0.05 | Critic loss: 1.50 | Entropy loss: -0.0499  | Total Loss: 1.51 | Total Steps: 10\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 4  | Average Reward 3.63  | Actor loss: 1.40 | Critic loss: 17.98 | Entropy loss: -0.0330  | Total Loss: 19.35 | Total Steps: 124\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward 3.70  | Actor loss: 0.01 | Critic loss: 0.95 | Entropy loss: -0.0220  | Total Loss: 0.94 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward 3.73  | Actor loss: 0.02 | Critic loss: 1.54 | Entropy loss: -0.0335  | Total Loss: 1.52 | Total Steps: 11\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Average Reward 3.78  | Actor loss: 0.01 | Critic loss: 3.03 | Entropy loss: -0.0063  | Total Loss: 3.03 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 1  | Average Reward 3.73  | Actor loss: 0.34 | Critic loss: 2.75 | Entropy loss: -0.0042  | Total Loss: 3.09 | Total Steps: 51\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 4  | Average Reward 3.71  | Actor loss: 0.01 | Critic loss: 1.73 | Entropy loss: -0.0075  | Total Loss: 1.73 | Total Steps: 42\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 10  | Average Reward 3.71  | Actor loss: 0.01 | Critic loss: 2.89 | Entropy loss: -0.0032  | Total Loss: 2.89 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Average Reward 3.75  | Actor loss: 0.01 | Critic loss: 4.33 | Entropy loss: -0.0088  | Total Loss: 4.33 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 4  | Average Reward 4.09  | Actor loss: 0.00 | Critic loss: 2.16 | Entropy loss: -0.0030  | Total Loss: 2.16 | Total Steps: 42\n",
      "TEST: ---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 4.11  | Actor loss: 0.30 | Critic loss: 4.85 | Entropy loss: -0.0173  | Total Loss: 5.13 | Total Steps: 14\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 10  | Average Reward 4.13  | Actor loss: 0.02 | Critic loss: 29.16 | Entropy loss: -0.0349  | Total Loss: 29.14 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 6  | Average Reward 4.13  | Actor loss: 0.02 | Critic loss: 0.63 | Entropy loss: -0.0350  | Total Loss: 0.61 | Total Steps: 67\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 10  | Average Reward 4.17  | Actor loss: 0.01 | Critic loss: 2.91 | Entropy loss: -0.0100  | Total Loss: 2.91 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 10  | Average Reward 4.24  | Actor loss: 0.00 | Critic loss: 7.15 | Entropy loss: -0.0046  | Total Loss: 7.15 | Total Steps: 61\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 4.24  | Actor loss: 0.00 | Critic loss: 2.47 | Entropy loss: -0.0025  | Total Loss: 2.47 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 7  | Average Reward 4.22  | Actor loss: 0.00 | Critic loss: 3.59 | Entropy loss: -0.0004  | Total Loss: 3.59 | Total Steps: 38\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 7  | Average Reward 4.24  | Actor loss: 0.01 | Critic loss: 4.96 | Entropy loss: -0.0173  | Total Loss: 4.96 | Total Steps: 24\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward 4.24  | Actor loss: 0.00 | Critic loss: 6.87 | Entropy loss: -0.0011  | Total Loss: 6.87 | Total Steps: 34\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 4.25  | Actor loss: 1.62 | Critic loss: 2.72 | Entropy loss: -0.0134  | Total Loss: 4.33 | Total Steps: 35\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 1  | Average Reward 4.21  | Actor loss: 0.05 | Critic loss: 1.65 | Entropy loss: -0.0120  | Total Loss: 1.69 | Total Steps: 54\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Average Reward 4.24  | Actor loss: 0.00 | Critic loss: 3.17 | Entropy loss: -0.0453  | Total Loss: 3.13 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 4  | Average Reward 4.21  | Actor loss: 0.00 | Critic loss: 4.11 | Entropy loss: -0.0031  | Total Loss: 4.10 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 7  | Average Reward 4.25  | Actor loss: 0.00 | Critic loss: 2.67 | Entropy loss: -0.0303  | Total Loss: 2.64 | Total Steps: 49\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: -5  | Average Reward 4.22  | Actor loss: 0.15 | Critic loss: 1.75 | Entropy loss: -0.0223  | Total Loss: 1.88 | Total Steps: 63\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Average Reward 4.24  | Actor loss: 0.00 | Critic loss: 1.99 | Entropy loss: -0.0234  | Total Loss: 1.96 | Total Steps: 38\n",
      "TEST: ---black---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 56/100  | Episode Reward: -10  | Average Reward 4.13  | Actor loss: -0.00 | Critic loss: 82.43 | Entropy loss: -0.0002  | Total Loss: 82.42 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 4.15  | Actor loss: 0.00 | Critic loss: 4.83 | Entropy loss: -0.0004  | Total Loss: 4.84 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 4.24  | Actor loss: 0.02 | Critic loss: 2.96 | Entropy loss: -0.0020  | Total Loss: 2.97 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 1  | Average Reward 4.20  | Actor loss: 0.02 | Critic loss: 1.52 | Entropy loss: -0.0315  | Total Loss: 1.52 | Total Steps: 45\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 7  | Average Reward 4.18  | Actor loss: 0.01 | Critic loss: 2.18 | Entropy loss: -0.0099  | Total Loss: 2.19 | Total Steps: 30\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward 4.21  | Actor loss: 0.03 | Critic loss: 2.35 | Entropy loss: -0.0031  | Total Loss: 2.37 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 10  | Average Reward 4.21  | Actor loss: 0.01 | Critic loss: 0.88 | Entropy loss: -0.0073  | Total Loss: 0.89 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward 4.21  | Actor loss: 0.01 | Critic loss: 2.85 | Entropy loss: -0.0038  | Total Loss: 2.85 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 1  | Average Reward 4.17  | Actor loss: 0.30 | Critic loss: 14.41 | Entropy loss: -0.0175  | Total Loss: 14.69 | Total Steps: 63\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward 4.17  | Actor loss: 0.15 | Critic loss: 1.90 | Entropy loss: -0.0178  | Total Loss: 2.03 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 10  | Average Reward 4.21  | Actor loss: 0.04 | Critic loss: 3.42 | Entropy loss: -0.0112  | Total Loss: 3.45 | Total Steps: 30\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 10  | Average Reward 4.25  | Actor loss: 0.01 | Critic loss: 2.71 | Entropy loss: -0.0057  | Total Loss: 2.71 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 10  | Average Reward 4.30  | Actor loss: 0.00 | Critic loss: 5.14 | Entropy loss: -0.0013  | Total Loss: 5.15 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward 4.27  | Actor loss: 0.00 | Critic loss: 4.66 | Entropy loss: -0.0032  | Total Loss: 4.65 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 9  | Average Reward 4.31  | Actor loss: 0.00 | Critic loss: 0.45 | Entropy loss: -0.0301  | Total Loss: 0.43 | Total Steps: 66\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 10  | Average Reward 4.36  | Actor loss: 0.00 | Critic loss: 3.83 | Entropy loss: -0.0437  | Total Loss: 3.79 | Total Steps: 9\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 1  | Average Reward 4.36  | Actor loss: 0.02 | Critic loss: 5.84 | Entropy loss: -0.0276  | Total Loss: 5.83 | Total Steps: 61\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 4.37  | Actor loss: 0.00 | Critic loss: 3.90 | Entropy loss: -0.0058  | Total Loss: 3.90 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 7  | Average Reward 4.38  | Actor loss: 0.00 | Critic loss: 0.51 | Entropy loss: -0.0378  | Total Loss: 0.47 | Total Steps: 61\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward 4.43  | Actor loss: 0.01 | Critic loss: 9.59 | Entropy loss: -0.0158  | Total Loss: 9.59 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 7  | Average Reward 4.43  | Actor loss: 0.00 | Critic loss: 2.34 | Entropy loss: -0.0317  | Total Loss: 2.31 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: -2  | Average Reward 4.38  | Actor loss: 0.01 | Critic loss: 1.23 | Entropy loss: -0.0262  | Total Loss: 1.21 | Total Steps: 51\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 4.42  | Actor loss: 0.01 | Critic loss: 3.04 | Entropy loss: -0.0050  | Total Loss: 3.04 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 10  | Average Reward 4.45  | Actor loss: 0.02 | Critic loss: 29.72 | Entropy loss: -0.0327  | Total Loss: 29.70 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 10  | Average Reward 4.46  | Actor loss: 0.11 | Critic loss: 6.88 | Entropy loss: -0.0612  | Total Loss: 6.93 | Total Steps: 9\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 4  | Average Reward 4.43  | Actor loss: 0.04 | Critic loss: 1.57 | Entropy loss: -0.0225  | Total Loss: 1.58 | Total Steps: 58\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward 4.46  | Actor loss: 0.36 | Critic loss: 4.93 | Entropy loss: -0.0269  | Total Loss: 5.26 | Total Steps: 14\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 10  | Average Reward 4.49  | Actor loss: 0.01 | Critic loss: 4.43 | Entropy loss: -0.0102  | Total Loss: 4.43 | Total Steps: 8\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 7  | Average Reward 4.47  | Actor loss: 0.01 | Critic loss: 16.03 | Entropy loss: -0.0013  | Total Loss: 16.04 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 85/100  | Episode Reward: -10  | Average Reward 4.41  | Actor loss: -0.00 | Critic loss: 78.46 | Entropy loss: -0.0026  | Total Loss: 78.46 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Average Reward 4.41  | Actor loss: 0.00 | Critic loss: 9.06 | Entropy loss: -0.0008  | Total Loss: 9.07 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Average Reward 4.41  | Actor loss: 0.01 | Critic loss: 2.82 | Entropy loss: -0.0025  | Total Loss: 2.82 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward 4.43  | Actor loss: 0.00 | Critic loss: 3.60 | Entropy loss: -0.0011  | Total Loss: 3.60 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 1  | Average Reward 4.45  | Actor loss: 0.01 | Critic loss: 1.14 | Entropy loss: -0.0074  | Total Loss: 1.14 | Total Steps: 54\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: -3  | Average Reward 4.40  | Actor loss: 0.00 | Critic loss: 3.34 | Entropy loss: -0.0353  | Total Loss: 3.30 | Total Steps: 65\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 1  | Average Reward 4.38  | Actor loss: 0.01 | Critic loss: 4.20 | Entropy loss: -0.0027  | Total Loss: 4.20 | Total Steps: 50\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 7  | Average Reward 4.40  | Actor loss: 0.03 | Critic loss: 2.45 | Entropy loss: -0.0028  | Total Loss: 2.48 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 10  | Average Reward 4.40  | Actor loss: 0.00 | Critic loss: 2.61 | Entropy loss: -0.0335  | Total Loss: 2.58 | Total Steps: 9\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 4  | Average Reward 4.42  | Actor loss: 0.01 | Critic loss: 1.53 | Entropy loss: -0.0028  | Total Loss: 1.54 | Total Steps: 49\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 10  | Average Reward 4.45  | Actor loss: 0.02 | Critic loss: 7.22 | Entropy loss: -0.0216  | Total Loss: 7.22 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 7  | Average Reward 4.45  | Actor loss: 0.01 | Critic loss: 16.06 | Entropy loss: -0.0017  | Total Loss: 16.06 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 7  | Average Reward 4.49  | Actor loss: 0.01 | Critic loss: 2.25 | Entropy loss: -0.0167  | Total Loss: 2.24 | Total Steps: 31\n",
      "TEST: ---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 4.49  | Actor loss: 0.00 | Critic loss: 3.53 | Entropy loss: -0.0015  | Total Loss: 3.53 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 2  | Average Reward 4.48  | Actor loss: 0.00 | Critic loss: 0.94 | Entropy loss: -0.0311  | Total Loss: 0.91 | Total Steps: 67\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 1  | Average Reward 4.53  | Actor loss: 4.07 | Critic loss: 5.78 | Entropy loss: -0.0281  | Total Loss: 9.82 | Total Steps: 44\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1102/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.01 | Critic loss: 1.92 | Entropy loss: -0.0000  | Total Loss: 1.93 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1103/129000  | Episode Reward: 10  | Average Reward 6.26  | Actor loss: 0.48 | Critic loss: 2.46 | Entropy loss: -0.0009  | Total Loss: 2.94 | Total Steps: 10\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1104/129000  | Episode Reward: 10  | Average Reward 6.31  | Actor loss: 0.01 | Critic loss: 1.71 | Entropy loss: -0.0000  | Total Loss: 1.72 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1105/129000  | Episode Reward: 10  | Average Reward 6.31  | Actor loss: 0.06 | Critic loss: 2.65 | Entropy loss: -0.0001  | Total Loss: 2.71 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1106/129000  | Episode Reward: 3  | Average Reward 6.28  | Actor loss: -0.62 | Critic loss: 5.49 | Entropy loss: -0.0067  | Total Loss: 4.86 | Total Steps: 51\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1107/129000  | Episode Reward: -4  | Average Reward 6.22  | Actor loss: -1.34 | Critic loss: 20.29 | Entropy loss: -0.0053  | Total Loss: 18.95 | Total Steps: 63\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1108/129000  | Episode Reward: 4  | Average Reward 6.24  | Actor loss: -0.03 | Critic loss: 3.40 | Entropy loss: -0.0004  | Total Loss: 3.37 | Total Steps: 42\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1109/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.01 | Critic loss: 2.13 | Entropy loss: -0.0000  | Total Loss: 2.14 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1110/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.44 | Critic loss: 1.46 | Entropy loss: -0.0009  | Total Loss: 1.90 | Total Steps: 9\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1111/129000  | Episode Reward: 2  | Average Reward 6.33  | Actor loss: -0.96 | Critic loss: 9.93 | Entropy loss: -0.0066  | Total Loss: 8.96 | Total Steps: 60\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1112/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.02 | Critic loss: 2.86 | Entropy loss: -0.0001  | Total Loss: 2.88 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1113/129000  | Episode Reward: 7  | Average Reward 6.37  | Actor loss: 0.10 | Critic loss: 5.99 | Entropy loss: -0.0017  | Total Loss: 6.08 | Total Steps: 40\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1114/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.71 | Critic loss: 1.84 | Entropy loss: -0.0008  | Total Loss: 2.55 | Total Steps: 8\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1115/129000  | Episode Reward: 1  | Average Reward 6.38  | Actor loss: -0.62 | Critic loss: 13.04 | Entropy loss: -0.0037  | Total Loss: 12.41 | Total Steps: 54\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1116/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: 0.10 | Critic loss: 6.88 | Entropy loss: -0.0006  | Total Loss: 6.98 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1117/129000  | Episode Reward: 1  | Average Reward 6.39  | Actor loss: -0.04 | Critic loss: 9.62 | Entropy loss: -0.0017  | Total Loss: 9.58 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1118/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: 0.16 | Critic loss: 4.85 | Entropy loss: -0.0007  | Total Loss: 5.01 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1119/129000  | Episode Reward: 4  | Average Reward 6.39  | Actor loss: -0.02 | Critic loss: 5.52 | Entropy loss: -0.0003  | Total Loss: 5.49 | Total Steps: 52\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1120/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 0.01 | Critic loss: 4.57 | Entropy loss: -0.0000  | Total Loss: 4.58 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1121/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: -0.03 | Critic loss: 2.76 | Entropy loss: -0.0004  | Total Loss: 2.73 | Total Steps: 42\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1122/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: -0.13 | Critic loss: 4.96 | Entropy loss: -0.0041  | Total Loss: 4.83 | Total Steps: 54\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1123/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.05 | Critic loss: 2.14 | Entropy loss: -0.0030  | Total Loss: 2.19 | Total Steps: 44\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1124/129000  | Episode Reward: 4  | Average Reward 6.41  | Actor loss: -0.68 | Critic loss: 6.35 | Entropy loss: -0.0038  | Total Loss: 5.67 | Total Steps: 52\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1125/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: -0.07 | Critic loss: 2.67 | Entropy loss: -0.0053  | Total Loss: 2.59 | Total Steps: 47\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1126/129000  | Episode Reward: 7  | Average Reward 6.39  | Actor loss: 0.04 | Critic loss: 6.19 | Entropy loss: -0.0004  | Total Loss: 6.23 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1127/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: 0.04 | Critic loss: 3.30 | Entropy loss: -0.0004  | Total Loss: 3.34 | Total Steps: 30\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1128/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.04 | Critic loss: 2.55 | Entropy loss: -0.0000  | Total Loss: 2.59 | Total Steps: 6\n",
      "---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1129/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.03 | Critic loss: 1.90 | Entropy loss: -0.0000  | Total Loss: 1.93 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1130/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.01 | Critic loss: 2.59 | Entropy loss: -0.0000  | Total Loss: 2.59 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1131/129000  | Episode Reward: 3  | Average Reward 6.40  | Actor loss: -0.42 | Critic loss: 9.67 | Entropy loss: -0.0033  | Total Loss: 9.25 | Total Steps: 54\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1132/129000  | Episode Reward: 4  | Average Reward 6.40  | Actor loss: -0.07 | Critic loss: 7.38 | Entropy loss: -0.0008  | Total Loss: 7.31 | Total Steps: 43\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1133/129000  | Episode Reward: 9  | Average Reward 6.39  | Actor loss: -0.93 | Critic loss: 5.86 | Entropy loss: -0.0105  | Total Loss: 4.92 | Total Steps: 90\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1134/129000  | Episode Reward: 1  | Average Reward 6.37  | Actor loss: -0.43 | Critic loss: 11.88 | Entropy loss: -0.0027  | Total Loss: 11.44 | Total Steps: 56\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1135/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.01 | Critic loss: 2.03 | Entropy loss: -0.0000  | Total Loss: 2.04 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1136/129000  | Episode Reward: 7  | Average Reward 6.37  | Actor loss: 0.03 | Critic loss: 3.82 | Entropy loss: -0.0003  | Total Loss: 3.85 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1137/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 0.02 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.05 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1138/129000  | Episode Reward: 1  | Average Reward 6.41  | Actor loss: -0.25 | Critic loss: 8.62 | Entropy loss: -0.0029  | Total Loss: 8.37 | Total Steps: 56\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1139/129000  | Episode Reward: 4  | Average Reward 6.38  | Actor loss: -0.51 | Critic loss: 10.08 | Entropy loss: -0.0051  | Total Loss: 9.57 | Total Steps: 58\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1140/129000  | Episode Reward: 4  | Average Reward 6.35  | Actor loss: -0.11 | Critic loss: 4.02 | Entropy loss: -0.0022  | Total Loss: 3.91 | Total Steps: 51\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1141/129000  | Episode Reward: 7  | Average Reward 6.35  | Actor loss: -0.04 | Critic loss: 2.14 | Entropy loss: -0.0009  | Total Loss: 2.10 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1142/129000  | Episode Reward: 6  | Average Reward 6.38  | Actor loss: -0.81 | Critic loss: 4.12 | Entropy loss: -0.0054  | Total Loss: 3.30 | Total Steps: 44\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1143/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 0.21 | Critic loss: 4.32 | Entropy loss: -0.0008  | Total Loss: 4.53 | Total Steps: 30\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1144/129000  | Episode Reward: 10  | Average Reward 6.39  | Actor loss: 0.02 | Critic loss: 2.64 | Entropy loss: -0.0000  | Total Loss: 2.66 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1145/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.01 | Critic loss: 1.71 | Entropy loss: -0.0000  | Total Loss: 1.72 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1146/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.81 | Critic loss: 1.82 | Entropy loss: -0.0019  | Total Loss: 2.63 | Total Steps: 11\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1147/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: 0.58 | Critic loss: 5.05 | Entropy loss: -0.0027  | Total Loss: 5.63 | Total Steps: 32\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1148/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.58 | Critic loss: 3.21 | Entropy loss: -0.0006  | Total Loss: 3.78 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1149/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.03 | Critic loss: 0.64 | Entropy loss: -0.0001  | Total Loss: 0.67 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1150/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.06 | Critic loss: 0.64 | Entropy loss: -0.0001  | Total Loss: 0.71 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1151/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.15 | Critic loss: 8.20 | Entropy loss: -0.0007  | Total Loss: 8.35 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1152/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: -0.04 | Critic loss: 2.37 | Entropy loss: -0.0006  | Total Loss: 2.32 | Total Steps: 43\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1153/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: -0.00 | Critic loss: 6.14 | Entropy loss: -0.0008  | Total Loss: 6.14 | Total Steps: 29\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1154/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.01 | Critic loss: 0.89 | Entropy loss: -0.0000  | Total Loss: 0.89 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1155/129000  | Episode Reward: 1  | Average Reward 6.42  | Actor loss: -0.20 | Critic loss: 10.33 | Entropy loss: -0.0013  | Total Loss: 10.13 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1156/129000  | Episode Reward: 1  | Average Reward 6.41  | Actor loss: -0.60 | Critic loss: 5.91 | Entropy loss: -0.0029  | Total Loss: 5.30 | Total Steps: 52\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1157/129000  | Episode Reward: 10  | Average Reward 6.41  | Actor loss: 0.02 | Critic loss: 2.01 | Entropy loss: -0.0000  | Total Loss: 2.03 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1158/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.12 | Critic loss: 0.94 | Entropy loss: -0.0002  | Total Loss: 1.06 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1159/129000  | Episode Reward: 1  | Average Reward 6.39  | Actor loss: -0.58 | Critic loss: 6.57 | Entropy loss: -0.0050  | Total Loss: 5.99 | Total Steps: 50\n",
      "---green---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1160/129000  | Episode Reward: 7  | Average Reward 6.48  | Actor loss: -0.03 | Critic loss: 5.31 | Entropy loss: -0.0017  | Total Loss: 5.28 | Total Steps: 44\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1161/129000  | Episode Reward: 2  | Average Reward 6.44  | Actor loss: -1.04 | Critic loss: 6.52 | Entropy loss: -0.0066  | Total Loss: 5.48 | Total Steps: 50\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1162/129000  | Episode Reward: 4  | Average Reward 6.44  | Actor loss: -0.16 | Critic loss: 6.45 | Entropy loss: -0.0011  | Total Loss: 6.29 | Total Steps: 54\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1163/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: -0.01 | Critic loss: 3.27 | Entropy loss: -0.0012  | Total Loss: 3.26 | Total Steps: 31\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1164/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.01 | Critic loss: 1.85 | Entropy loss: -0.0000  | Total Loss: 1.86 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1165/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: 0.07 | Critic loss: 3.29 | Entropy loss: -0.0004  | Total Loss: 3.36 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1166/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.28 | Critic loss: 3.65 | Entropy loss: -0.0016  | Total Loss: 3.93 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1167/129000  | Episode Reward: 4  | Average Reward 6.47  | Actor loss: -0.03 | Critic loss: 3.49 | Entropy loss: -0.0004  | Total Loss: 3.46 | Total Steps: 42\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1168/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.17 | Critic loss: 2.64 | Entropy loss: -0.0002  | Total Loss: 2.80 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1169/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.14 | Critic loss: 3.97 | Entropy loss: -0.0026  | Total Loss: 3.83 | Total Steps: 154\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1170/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.57 | Critic loss: 2.84 | Entropy loss: -0.0029  | Total Loss: 2.27 | Total Steps: 41\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1171/129000  | Episode Reward: 7  | Average Reward 6.47  | Actor loss: -0.81 | Critic loss: 3.78 | Entropy loss: -0.0050  | Total Loss: 2.97 | Total Steps: 55\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1172/129000  | Episode Reward: 1  | Average Reward 6.46  | Actor loss: -0.11 | Critic loss: 11.52 | Entropy loss: -0.0008  | Total Loss: 11.41 | Total Steps: 51\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1173/129000  | Episode Reward: 1  | Average Reward 6.44  | Actor loss: -0.10 | Critic loss: 7.44 | Entropy loss: -0.0005  | Total Loss: 7.35 | Total Steps: 52\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1174/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.06 | Critic loss: 2.17 | Entropy loss: -0.0004  | Total Loss: 2.24 | Total Steps: 34\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1175/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.03 | Critic loss: 1.31 | Entropy loss: -0.0000  | Total Loss: 1.34 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1176/129000  | Episode Reward: 7  | Average Reward 6.47  | Actor loss: 0.60 | Critic loss: 8.60 | Entropy loss: -0.0039  | Total Loss: 9.20 | Total Steps: 34\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1177/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 1.09 | Entropy loss: -0.0000  | Total Loss: 1.10 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1178/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.06 | Critic loss: 12.16 | Entropy loss: -0.0021  | Total Loss: 12.10 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1179/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 1.67 | Entropy loss: -0.0000  | Total Loss: 1.67 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1180/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.03 | Critic loss: 1.30 | Entropy loss: -0.0000  | Total Loss: 1.33 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1181/129000  | Episode Reward: 4  | Average Reward 6.54  | Actor loss: -0.06 | Critic loss: 3.47 | Entropy loss: -0.0005  | Total Loss: 3.40 | Total Steps: 42\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1182/129000  | Episode Reward: 7  | Average Reward 6.53  | Actor loss: 0.09 | Critic loss: 8.82 | Entropy loss: -0.0004  | Total Loss: 8.92 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1183/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.17 | Critic loss: 6.77 | Entropy loss: -0.0054  | Total Loss: 6.59 | Total Steps: 51\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1184/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: 0.08 | Critic loss: 3.20 | Entropy loss: -0.0004  | Total Loss: 3.28 | Total Steps: 36\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1185/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.47 | Critic loss: 5.27 | Entropy loss: -0.0022  | Total Loss: 5.74 | Total Steps: 33\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1186/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.25 | Critic loss: 8.89 | Entropy loss: -0.0018  | Total Loss: 8.64 | Total Steps: 43\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1187/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 1.74 | Entropy loss: -0.0000  | Total Loss: 1.75 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1188/129000  | Episode Reward: 1  | Average Reward 6.47  | Actor loss: -0.52 | Critic loss: 11.07 | Entropy loss: -0.0068  | Total Loss: 10.55 | Total Steps: 62\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1189/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 3.16 | Entropy loss: -0.0000  | Total Loss: 3.18 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1190/129000  | Episode Reward: 1  | Average Reward 6.50  | Actor loss: -1.26 | Critic loss: 11.79 | Entropy loss: -0.0055  | Total Loss: 10.53 | Total Steps: 56\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1191/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.15 | Critic loss: 4.75 | Entropy loss: -0.0010  | Total Loss: 4.60 | Total Steps: 49\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1192/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.20 | Critic loss: 2.67 | Entropy loss: -0.0023  | Total Loss: 2.47 | Total Steps: 43\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1193/129000  | Episode Reward: 1  | Average Reward 6.45  | Actor loss: -0.12 | Critic loss: 6.88 | Entropy loss: -0.0008  | Total Loss: 6.77 | Total Steps: 54\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1194/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: -0.02 | Critic loss: 4.20 | Entropy loss: -0.0012  | Total Loss: 4.17 | Total Steps: 31\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1195/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.70 | Critic loss: 0.67 | Entropy loss: -0.0014  | Total Loss: 1.37 | Total Steps: 7\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1196/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.53 | Critic loss: 2.68 | Entropy loss: -0.0009  | Total Loss: 3.20 | Total Steps: 9\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1197/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.16 | Critic loss: 2.35 | Entropy loss: -0.0002  | Total Loss: 2.52 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1198/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.15 | Critic loss: 4.08 | Entropy loss: -0.0007  | Total Loss: 4.23 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1199/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.43 | Critic loss: 5.35 | Entropy loss: -0.0045  | Total Loss: 4.92 | Total Steps: 38\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1200/129000  | Episode Reward: 4  | Average Reward 6.47  | Actor loss: -0.07 | Critic loss: 4.25 | Entropy loss: -0.0004  | Total Loss: 4.18 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1201/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.02 | Critic loss: 2.47 | Entropy loss: -0.0000  | Total Loss: 2.49 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 1  | Average Reward 4.50  | Actor loss: 0.01 | Critic loss: 3.21 | Entropy loss: -0.0087  | Total Loss: 3.21 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 4  | Average Reward 4.51  | Actor loss: 0.14 | Critic loss: 0.93 | Entropy loss: -0.0051  | Total Loss: 1.06 | Total Steps: 46\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 4  | Average Reward 4.50  | Actor loss: 0.06 | Critic loss: 6.44 | Entropy loss: -0.0049  | Total Loss: 6.49 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 1  | Average Reward 4.48  | Actor loss: 0.03 | Critic loss: 0.75 | Entropy loss: -0.0022  | Total Loss: 0.78 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 10  | Average Reward 4.53  | Actor loss: 0.12 | Critic loss: 6.04 | Entropy loss: -0.0229  | Total Loss: 6.14 | Total Steps: 12\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 1  | Average Reward 4.53  | Actor loss: 0.00 | Critic loss: 5.96 | Entropy loss: -0.0019  | Total Loss: 5.96 | Total Steps: 52\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 10  | Average Reward 4.53  | Actor loss: 0.00 | Critic loss: 2.41 | Entropy loss: -0.0010  | Total Loss: 2.41 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward 4.54  | Actor loss: 0.01 | Critic loss: 1.14 | Entropy loss: -0.0156  | Total Loss: 1.13 | Total Steps: 44\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 0  | Average Reward 4.50  | Actor loss: 0.01 | Critic loss: 0.83 | Entropy loss: -0.0195  | Total Loss: 0.82 | Total Steps: 55\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 7  | Average Reward 4.88  | Actor loss: 0.19 | Critic loss: 4.67 | Entropy loss: -0.0054  | Total Loss: 4.86 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward 4.84  | Actor loss: 0.01 | Critic loss: 7.70 | Entropy loss: -0.0059  | Total Loss: 7.70 | Total Steps: 48\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 4  | Average Reward 4.88  | Actor loss: 0.19 | Critic loss: 3.55 | Entropy loss: -0.0074  | Total Loss: 3.73 | Total Steps: 47\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward 4.88  | Actor loss: 0.00 | Critic loss: 8.28 | Entropy loss: -0.0008  | Total Loss: 8.28 | Total Steps: 38\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 4  | Average Reward 4.84  | Actor loss: 0.34 | Critic loss: 13.72 | Entropy loss: -0.0387  | Total Loss: 14.02 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 7  | Average Reward 4.83  | Actor loss: 1.54 | Critic loss: 23.66 | Entropy loss: -0.0376  | Total Loss: 25.16 | Total Steps: 67\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 4.89  | Actor loss: 0.09 | Critic loss: 4.68 | Entropy loss: -0.0462  | Total Loss: 4.72 | Total Steps: 12\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 4  | Average Reward 4.86  | Actor loss: 0.09 | Critic loss: 8.07 | Entropy loss: -0.0076  | Total Loss: 8.15 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 7  | Average Reward 4.84  | Actor loss: 1.08 | Critic loss: 26.80 | Entropy loss: -0.0158  | Total Loss: 27.87 | Total Steps: 33\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Average Reward 4.89  | Actor loss: 0.39 | Critic loss: 18.13 | Entropy loss: -0.0388  | Total Loss: 18.47 | Total Steps: 35\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward 4.91  | Actor loss: 0.00 | Critic loss: 2.30 | Entropy loss: -0.0071  | Total Loss: 2.30 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 21/100  | Episode Reward: -100  | Average Reward 4.37  | Actor loss: -1.50 | Critic loss: 97.13 | Entropy loss: -0.0270  | Total Loss: 95.60 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 1  | Average Reward 4.36  | Actor loss: 0.40 | Critic loss: 2.35 | Entropy loss: -0.0057  | Total Loss: 2.75 | Total Steps: 51\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 4  | Average Reward 4.33  | Actor loss: 0.31 | Critic loss: 16.29 | Entropy loss: -0.0357  | Total Loss: 16.56 | Total Steps: 41\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 0  | Average Reward 4.28  | Actor loss: 0.01 | Critic loss: 2.01 | Entropy loss: -0.0334  | Total Loss: 1.99 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward 4.28  | Actor loss: 0.53 | Critic loss: 16.32 | Entropy loss: -0.0220  | Total Loss: 16.83 | Total Steps: 34\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 7  | Average Reward 4.28  | Actor loss: 0.89 | Critic loss: 17.73 | Entropy loss: -0.0129  | Total Loss: 18.61 | Total Steps: 24\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 10  | Average Reward 4.33  | Actor loss: 0.01 | Critic loss: 11.03 | Entropy loss: -0.0024  | Total Loss: 11.04 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 28/100  | Episode Reward: -94  | Average Reward 3.83  | Actor loss: -0.04 | Critic loss: 69.55 | Entropy loss: -0.0318  | Total Loss: 69.48 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward 3.83  | Actor loss: 0.01 | Critic loss: 8.64 | Entropy loss: -0.0014  | Total Loss: 8.65 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 10  | Average Reward 3.89  | Actor loss: 0.34 | Critic loss: 13.03 | Entropy loss: -0.0033  | Total Loss: 13.37 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward 3.92  | Actor loss: 0.13 | Critic loss: 7.62 | Entropy loss: -0.0047  | Total Loss: 7.75 | Total Steps: 40\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: -17  | Average Reward 4.21  | Actor loss: 14.65 | Critic loss: 10.62 | Entropy loss: -0.0393  | Total Loss: 25.23 | Total Steps: 196\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 7  | Average Reward 4.20  | Actor loss: 0.00 | Critic loss: 1.54 | Entropy loss: -0.0035  | Total Loss: 1.54 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 4  | Average Reward 4.23  | Actor loss: 0.80 | Critic loss: 24.71 | Entropy loss: -0.0308  | Total Loss: 25.48 | Total Steps: 105\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 7  | Average Reward 4.21  | Actor loss: 0.00 | Critic loss: 1.54 | Entropy loss: -0.0032  | Total Loss: 1.54 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: -2  | Average Reward 4.20  | Actor loss: 0.06 | Critic loss: 3.23 | Entropy loss: -0.0398  | Total Loss: 3.24 | Total Steps: 59\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward 4.18  | Actor loss: 1.12 | Critic loss: 24.00 | Entropy loss: -0.0201  | Total Loss: 25.10 | Total Steps: 62\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 1  | Average Reward 4.16  | Actor loss: 0.41 | Critic loss: 2.44 | Entropy loss: -0.0059  | Total Loss: 2.85 | Total Steps: 51\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward 4.20  | Actor loss: 0.00 | Critic loss: 1.88 | Entropy loss: -0.0019  | Total Loss: 1.88 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 4.23  | Actor loss: 0.02 | Critic loss: 18.39 | Entropy loss: -0.0859  | Total Loss: 18.33 | Total Steps: 7\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: -11  | Average Reward 4.16  | Actor loss: 0.66 | Critic loss: 15.34 | Entropy loss: -0.0228  | Total Loss: 15.98 | Total Steps: 148\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 10  | Average Reward 4.16  | Actor loss: 0.00 | Critic loss: 2.70 | Entropy loss: -0.0052  | Total Loss: 2.70 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: -5  | Average Reward 4.13  | Actor loss: 2.00 | Critic loss: 28.77 | Entropy loss: -0.0306  | Total Loss: 30.73 | Total Steps: 202\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 44/100  | Episode Reward: -103  | Average Reward 3.58  | Actor loss: -0.18 | Critic loss: 96.79 | Entropy loss: -0.0232  | Total Loss: 96.58 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 7  | Average Reward 3.58  | Actor loss: 1.23 | Critic loss: 19.42 | Entropy loss: -0.0388  | Total Loss: 20.61 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 3.58  | Actor loss: 0.02 | Critic loss: 19.00 | Entropy loss: -0.0068  | Total Loss: 19.01 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Average Reward 3.60  | Actor loss: 1.11 | Critic loss: 24.19 | Entropy loss: -0.0214  | Total Loss: 25.28 | Total Steps: 54\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 1  | Average Reward 3.56  | Actor loss: 0.01 | Critic loss: 8.44 | Entropy loss: -0.0101  | Total Loss: 8.44 | Total Steps: 62\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 3.60  | Actor loss: 0.01 | Critic loss: 8.85 | Entropy loss: -0.0151  | Total Loss: 8.84 | Total Steps: 36\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 50/100  | Episode Reward: -13  | Average Reward 3.50  | Actor loss: -0.00 | Critic loss: 82.90 | Entropy loss: -0.0012  | Total Loss: 82.90 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 1  | Average Reward 3.50  | Actor loss: 0.04 | Critic loss: 5.25 | Entropy loss: -0.0035  | Total Loss: 5.29 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 1  | Average Reward 3.45  | Actor loss: 0.03 | Critic loss: 2.85 | Entropy loss: -0.0051  | Total Loss: 2.87 | Total Steps: 50\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 1  | Average Reward 3.53  | Actor loss: 0.06 | Critic loss: 23.81 | Entropy loss: -0.0372  | Total Loss: 23.84 | Total Steps: 58\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 54/100  | Episode Reward: -82  | Average Reward 3.08  | Actor loss: -0.22 | Critic loss: 148.72 | Entropy loss: -0.0285  | Total Loss: 148.47 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 10  | Average Reward 3.13  | Actor loss: 0.00 | Critic loss: 1.77 | Entropy loss: -0.0052  | Total Loss: 1.77 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward 3.15  | Actor loss: 0.99 | Critic loss: 23.89 | Entropy loss: -0.0184  | Total Loss: 24.86 | Total Steps: 16\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -28  | Average Reward 3.02  | Actor loss: 1.39 | Critic loss: 21.72 | Entropy loss: -0.0370  | Total Loss: 23.08 | Total Steps: 493\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 3.03  | Actor loss: 0.01 | Critic loss: 11.93 | Entropy loss: -0.0374  | Total Loss: 11.90 | Total Steps: 10\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: -5  | Average Reward 2.97  | Actor loss: 0.16 | Critic loss: 3.48 | Entropy loss: -0.0174  | Total Loss: 3.62 | Total Steps: 97\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: -33  | Average Reward 2.77  | Actor loss: 1.00 | Critic loss: 14.66 | Entropy loss: -0.0253  | Total Loss: 15.64 | Total Steps: 292\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 4  | Average Reward 2.75  | Actor loss: 0.00 | Critic loss: 0.80 | Entropy loss: -0.0065  | Total Loss: 0.80 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward 2.75  | Actor loss: 0.42 | Critic loss: 10.84 | Entropy loss: -0.0059  | Total Loss: 11.25 | Total Steps: 35\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 4  | Average Reward 2.73  | Actor loss: 0.38 | Critic loss: 2.96 | Entropy loss: -0.0141  | Total Loss: 3.33 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 0.20 | Critic loss: 9.79 | Entropy loss: -0.0230  | Total Loss: 9.97 | Total Steps: 62\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 10  | Average Reward 2.74  | Actor loss: 0.00 | Critic loss: 2.36 | Entropy loss: -0.0109  | Total Loss: 2.35 | Total Steps: 8\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 66/100  | Episode Reward: -73  | Average Reward 2.34  | Actor loss: -10.82 | Critic loss: 81.59 | Entropy loss: -0.0298  | Total Loss: 70.75 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 7  | Average Reward 2.33  | Actor loss: 0.40 | Critic loss: 9.75 | Entropy loss: -0.0069  | Total Loss: 10.14 | Total Steps: 39\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: -14  | Average Reward 2.22  | Actor loss: 0.74 | Critic loss: 20.36 | Entropy loss: -0.0328  | Total Loss: 21.07 | Total Steps: 445\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 12.31 | Entropy loss: -0.0031  | Total Loss: 12.31 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 4  | Average Reward 2.23  | Actor loss: 2.01 | Critic loss: 22.97 | Entropy loss: -0.0077  | Total Loss: 24.97 | Total Steps: 54\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 7  | Average Reward 2.27  | Actor loss: 1.49 | Critic loss: 30.90 | Entropy loss: -0.0314  | Total Loss: 32.35 | Total Steps: 33\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 10  | Average Reward 2.31  | Actor loss: 0.13 | Critic loss: 6.02 | Entropy loss: -0.0128  | Total Loss: 6.13 | Total Steps: 8\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 2.35  | Actor loss: 0.00 | Critic loss: 1.83 | Entropy loss: -0.0026  | Total Loss: 1.83 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 7  | Average Reward 2.33  | Actor loss: 4.62 | Critic loss: 2.31 | Entropy loss: -0.0426  | Total Loss: 6.89 | Total Steps: 40\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward 2.36  | Actor loss: 0.29 | Critic loss: 16.37 | Entropy loss: -0.0314  | Total Loss: 16.63 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 10  | Average Reward 2.36  | Actor loss: 0.11 | Critic loss: 4.83 | Entropy loss: -0.0500  | Total Loss: 4.89 | Total Steps: 9\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Average Reward 2.36  | Actor loss: 0.00 | Critic loss: 2.30 | Entropy loss: -0.0090  | Total Loss: 2.29 | Total Steps: 6\n",
      "TEST: ---capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 2.39  | Actor loss: 0.01 | Critic loss: 12.71 | Entropy loss: -0.0014  | Total Loss: 12.72 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 10  | Average Reward 2.39  | Actor loss: 0.82 | Critic loss: 16.67 | Entropy loss: -0.0049  | Total Loss: 17.48 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 7  | Average Reward 2.39  | Actor loss: 0.95 | Critic loss: 17.99 | Entropy loss: -0.0123  | Total Loss: 18.93 | Total Steps: 24\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 81/100  | Episode Reward: -115  | Average Reward 1.80  | Actor loss: -0.00 | Critic loss: 75.42 | Entropy loss: -0.0248  | Total Loss: 75.39 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward 1.84  | Actor loss: 0.13 | Critic loss: 6.00 | Entropy loss: -0.0118  | Total Loss: 6.11 | Total Steps: 8\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 7  | Average Reward 1.83  | Actor loss: 0.02 | Critic loss: 14.70 | Entropy loss: -0.0201  | Total Loss: 14.71 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: -2  | Average Reward 1.84  | Actor loss: 1.75 | Critic loss: 28.94 | Entropy loss: -0.0233  | Total Loss: 30.67 | Total Steps: 82\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 4  | Average Reward 1.87  | Actor loss: 0.28 | Critic loss: 7.86 | Entropy loss: -0.0124  | Total Loss: 8.13 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 1  | Average Reward 1.93  | Actor loss: 0.04 | Critic loss: 3.23 | Entropy loss: -0.0165  | Total Loss: 3.25 | Total Steps: 54\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Average Reward 1.95  | Actor loss: 0.76 | Critic loss: 16.69 | Entropy loss: -0.0072  | Total Loss: 17.44 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 4  | Average Reward 1.92  | Actor loss: 7.99 | Critic loss: 18.94 | Entropy loss: -0.0284  | Total Loss: 26.90 | Total Steps: 39\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward 1.92  | Actor loss: 0.01 | Critic loss: 8.27 | Entropy loss: -0.1190  | Total Loss: 8.16 | Total Steps: 7\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 7  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 1.95 | Entropy loss: -0.0072  | Total Loss: 1.95 | Total Steps: 34\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 7  | Average Reward 1.96  | Actor loss: 1.18 | Critic loss: 18.77 | Entropy loss: -0.0165  | Total Loss: 19.93 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: -13  | Average Reward 1.84  | Actor loss: 2.17 | Critic loss: 18.46 | Entropy loss: -0.0393  | Total Loss: 20.59 | Total Steps: 179\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 4  | Average Reward 1.83  | Actor loss: 0.02 | Critic loss: 1.59 | Entropy loss: -0.0086  | Total Loss: 1.61 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 1  | Average Reward 1.84  | Actor loss: 0.61 | Critic loss: 15.10 | Entropy loss: -0.0154  | Total Loss: 15.70 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 0  | Average Reward 1.84  | Actor loss: 0.01 | Critic loss: 1.69 | Entropy loss: -0.0295  | Total Loss: 1.67 | Total Steps: 118\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Average Reward 2.29  | Actor loss: 1.43 | Critic loss: 14.65 | Entropy loss: -0.0278  | Total Loss: 16.05 | Total Steps: 52\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: -5  | Average Reward 2.24  | Actor loss: 2.12 | Critic loss: 22.00 | Entropy loss: -0.0230  | Total Loss: 24.10 | Total Steps: 127\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.00 | Critic loss: 2.18 | Entropy loss: -0.0046  | Total Loss: 2.18 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward 2.24  | Actor loss: 0.01 | Critic loss: 3.32 | Entropy loss: -0.0158  | Total Loss: 3.31 | Total Steps: 36\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.01 | Critic loss: 0.59 | Entropy loss: -0.0382  | Total Loss: 0.56 | Total Steps: 11\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1202/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.01 | Critic loss: 1.59 | Entropy loss: -0.0000  | Total Loss: 1.60 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1203/129000  | Episode Reward: 1  | Average Reward 6.46  | Actor loss: -1.72 | Critic loss: 14.64 | Entropy loss: -0.0113  | Total Loss: 12.90 | Total Steps: 94\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1204/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: 0.12 | Critic loss: 8.16 | Entropy loss: -0.0020  | Total Loss: 8.29 | Total Steps: 44\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1205/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: -0.44 | Critic loss: 3.66 | Entropy loss: -0.0039  | Total Loss: 3.22 | Total Steps: 55\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1206/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.03 | Critic loss: 11.62 | Entropy loss: -0.0000  | Total Loss: 11.65 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1207/129000  | Episode Reward: -18  | Average Reward 6.35  | Actor loss: -1.45 | Critic loss: 27.26 | Entropy loss: -0.0161  | Total Loss: 25.79 | Total Steps: 185\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1208/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: -0.41 | Critic loss: 4.25 | Entropy loss: -0.0174  | Total Loss: 3.82 | Total Steps: 103\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1209/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.05 | Critic loss: 6.00 | Entropy loss: -0.0014  | Total Loss: 5.94 | Total Steps: 48\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1210/129000  | Episode Reward: 10  | Average Reward 6.52  | Actor loss: 1.10 | Critic loss: 5.70 | Entropy loss: -0.0016  | Total Loss: 6.79 | Total Steps: 9\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1211/129000  | Episode Reward: 1  | Average Reward 6.47  | Actor loss: 0.05 | Critic loss: 8.76 | Entropy loss: -0.0011  | Total Loss: 8.81 | Total Steps: 54\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1212/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.29 | Critic loss: 3.75 | Entropy loss: -0.0003  | Total Loss: 4.04 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1213/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.01 | Critic loss: 5.53 | Entropy loss: -0.0000  | Total Loss: 5.54 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.61. Model has been saved-----\n",
      "Training  | Episode: 1214/129000  | Episode Reward: 10  | Average Reward 6.61  | Actor loss: 0.51 | Critic loss: 1.50 | Entropy loss: -0.0037  | Total Loss: 2.00 | Total Steps: 38\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1215/129000  | Episode Reward: -2  | Average Reward 6.58  | Actor loss: -0.55 | Critic loss: 13.20 | Entropy loss: -0.0084  | Total Loss: 12.65 | Total Steps: 108\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1216/129000  | Episode Reward: 4  | Average Reward 6.55  | Actor loss: 0.11 | Critic loss: 10.07 | Entropy loss: -0.0068  | Total Loss: 10.17 | Total Steps: 36\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1217/129000  | Episode Reward: 10  | Average Reward 6.61  | Actor loss: 0.06 | Critic loss: 5.01 | Entropy loss: -0.0003  | Total Loss: 5.07 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1218/129000  | Episode Reward: 7  | Average Reward 6.59  | Actor loss: 0.72 | Critic loss: 8.64 | Entropy loss: -0.0026  | Total Loss: 9.37 | Total Steps: 32\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.635. Model has been saved-----\n",
      "Training  | Episode: 1219/129000  | Episode Reward: 7  | Average Reward 6.63  | Actor loss: 0.09 | Critic loss: 3.19 | Entropy loss: -0.0030  | Total Loss: 3.27 | Total Steps: 54\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.65. Model has been saved-----\n",
      "Training  | Episode: 1220/129000  | Episode Reward: 7  | Average Reward 6.65  | Actor loss: 0.05 | Critic loss: 5.28 | Entropy loss: -0.0002  | Total Loss: 5.33 | Total Steps: 29\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1221/129000  | Episode Reward: 10  | Average Reward 6.65  | Actor loss: 0.13 | Critic loss: 15.79 | Entropy loss: -0.0001  | Total Loss: 15.91 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1222/129000  | Episode Reward: 7  | Average Reward 6.65  | Actor loss: 0.30 | Critic loss: 3.52 | Entropy loss: -0.0096  | Total Loss: 3.81 | Total Steps: 55\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.665. Model has been saved-----\n",
      "Training  | Episode: 1223/129000  | Episode Reward: 10  | Average Reward 6.67  | Actor loss: 0.27 | Critic loss: 6.38 | Entropy loss: -0.0051  | Total Loss: 6.65 | Total Steps: 35\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.68. Model has been saved-----\n",
      "Training  | Episode: 1224/129000  | Episode Reward: 10  | Average Reward 6.68  | Actor loss: -0.04 | Critic loss: 3.34 | Entropy loss: -0.0004  | Total Loss: 3.30 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.695. Model has been saved-----\n",
      "Training  | Episode: 1225/129000  | Episode Reward: 7  | Average Reward 6.70  | Actor loss: 0.08 | Critic loss: 3.74 | Entropy loss: -0.0005  | Total Loss: 3.81 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1226/129000  | Episode Reward: 4  | Average Reward 6.67  | Actor loss: -0.07 | Critic loss: 4.28 | Entropy loss: -0.0006  | Total Loss: 4.21 | Total Steps: 43\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1227/129000  | Episode Reward: 7  | Average Reward 6.65  | Actor loss: -0.01 | Critic loss: 2.31 | Entropy loss: -0.0003  | Total Loss: 2.30 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1228/129000  | Episode Reward: 10  | Average Reward 6.68  | Actor loss: -0.05 | Critic loss: 9.44 | Entropy loss: -0.0042  | Total Loss: 9.39 | Total Steps: 29\n",
      "---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1229/129000  | Episode Reward: 7  | Average Reward 6.70  | Actor loss: -0.11 | Critic loss: 3.38 | Entropy loss: -0.0006  | Total Loss: 3.27 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1230/129000  | Episode Reward: 10  | Average Reward 6.70  | Actor loss: 0.03 | Critic loss: 13.19 | Entropy loss: -0.0000  | Total Loss: 13.23 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.71. Model has been saved-----\n",
      "Training  | Episode: 1231/129000  | Episode Reward: 10  | Average Reward 6.71  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0000  | Total Loss: 1.39 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1232/129000  | Episode Reward: 7  | Average Reward 6.71  | Actor loss: -0.00 | Critic loss: 5.76 | Entropy loss: -0.0002  | Total Loss: 5.76 | Total Steps: 42\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1233/129000  | Episode Reward: -20  | Average Reward 6.59  | Actor loss: -0.95 | Critic loss: 29.80 | Entropy loss: -0.0101  | Total Loss: 28.84 | Total Steps: 156\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1234/129000  | Episode Reward: -6  | Average Reward 6.51  | Actor loss: -2.05 | Critic loss: 14.42 | Entropy loss: -0.0388  | Total Loss: 12.33 | Total Steps: 223\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1235/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.81 | Critic loss: 7.10 | Entropy loss: -0.0036  | Total Loss: 7.91 | Total Steps: 35\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1236/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: -0.01 | Critic loss: 6.40 | Entropy loss: -0.0028  | Total Loss: 6.39 | Total Steps: 43\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1237/129000  | Episode Reward: 10  | Average Reward 6.53  | Actor loss: 0.03 | Critic loss: 2.47 | Entropy loss: -0.0000  | Total Loss: 2.50 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1238/129000  | Episode Reward: 7  | Average Reward 6.55  | Actor loss: -0.37 | Critic loss: 4.72 | Entropy loss: -0.0027  | Total Loss: 4.34 | Total Steps: 38\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1239/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: -0.50 | Critic loss: 10.04 | Entropy loss: -0.0042  | Total Loss: 9.54 | Total Steps: 35\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1240/129000  | Episode Reward: 4  | Average Reward 6.53  | Actor loss: -0.21 | Critic loss: 8.89 | Entropy loss: -0.0010  | Total Loss: 8.69 | Total Steps: 43\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1241/129000  | Episode Reward: 10  | Average Reward 6.57  | Actor loss: 0.01 | Critic loss: 1.51 | Entropy loss: -0.0000  | Total Loss: 1.51 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1242/129000  | Episode Reward: 4  | Average Reward 6.57  | Actor loss: -0.17 | Critic loss: 9.21 | Entropy loss: -0.0011  | Total Loss: 9.04 | Total Steps: 52\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1243/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.83 | Critic loss: 8.25 | Entropy loss: -0.0057  | Total Loss: 9.07 | Total Steps: 32\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1244/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.02 | Critic loss: 1.52 | Entropy loss: -0.0000  | Total Loss: 1.54 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1245/129000  | Episode Reward: 7  | Average Reward 6.57  | Actor loss: 0.24 | Critic loss: 7.40 | Entropy loss: -0.0024  | Total Loss: 7.63 | Total Steps: 28\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1246/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.01 | Critic loss: 1.99 | Entropy loss: -0.0000  | Total Loss: 2.00 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1247/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.80 | Critic loss: 0.88 | Entropy loss: -0.0017  | Total Loss: 1.68 | Total Steps: 7\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1248/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: -0.08 | Critic loss: 4.73 | Entropy loss: -0.0009  | Total Loss: 4.65 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1249/129000  | Episode Reward: 10  | Average Reward 6.63  | Actor loss: 0.02 | Critic loss: 1.00 | Entropy loss: -0.0000  | Total Loss: 1.02 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1250/129000  | Episode Reward: 4  | Average Reward 6.60  | Actor loss: 0.05 | Critic loss: 8.65 | Entropy loss: -0.0016  | Total Loss: 8.70 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1251/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.00 | Critic loss: 4.88 | Entropy loss: -0.0016  | Total Loss: 4.88 | Total Steps: 56\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1252/129000  | Episode Reward: 1  | Average Reward 6.54  | Actor loss: -0.20 | Critic loss: 12.95 | Entropy loss: -0.0019  | Total Loss: 12.75 | Total Steps: 54\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1253/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1254/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.07 | Critic loss: 3.89 | Entropy loss: -0.0005  | Total Loss: 3.83 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1255/129000  | Episode Reward: 4  | Average Reward 6.48  | Actor loss: -0.65 | Critic loss: 9.29 | Entropy loss: -0.0082  | Total Loss: 8.63 | Total Steps: 61\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1256/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.03 | Critic loss: 10.12 | Entropy loss: -0.0000  | Total Loss: 10.15 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1257/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.09 | Critic loss: 3.00 | Entropy loss: -0.0045  | Total Loss: 3.09 | Total Steps: 42\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1258/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.03 | Critic loss: 1.94 | Entropy loss: -0.0001  | Total Loss: 1.97 | Total Steps: 6\n",
      "---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1259/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: -0.38 | Critic loss: 9.64 | Entropy loss: -0.0022  | Total Loss: 9.26 | Total Steps: 31\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1260/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.47 | Critic loss: 9.32 | Entropy loss: -0.0040  | Total Loss: 8.84 | Total Steps: 54\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1261/129000  | Episode Reward: 10  | Average Reward 6.53  | Actor loss: 0.00 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1262/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -1.37 | Critic loss: 10.40 | Entropy loss: -0.0057  | Total Loss: 9.02 | Total Steps: 61\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1263/129000  | Episode Reward: 7  | Average Reward 6.53  | Actor loss: -0.56 | Critic loss: 5.26 | Entropy loss: -0.0080  | Total Loss: 4.69 | Total Steps: 61\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1264/129000  | Episode Reward: 10  | Average Reward 6.57  | Actor loss: 0.21 | Critic loss: 2.49 | Entropy loss: -0.0003  | Total Loss: 2.70 | Total Steps: 8\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1265/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: -0.04 | Critic loss: 6.59 | Entropy loss: -0.0004  | Total Loss: 6.55 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1266/129000  | Episode Reward: 7  | Average Reward 6.60  | Actor loss: -0.07 | Critic loss: 2.49 | Entropy loss: -0.0008  | Total Loss: 2.42 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1267/129000  | Episode Reward: -2  | Average Reward 6.54  | Actor loss: -1.56 | Critic loss: 15.00 | Entropy loss: -0.0086  | Total Loss: 13.43 | Total Steps: 79\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1268/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: 0.71 | Critic loss: 4.69 | Entropy loss: -0.0049  | Total Loss: 5.39 | Total Steps: 41\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1269/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 1.17 | Entropy loss: -0.0000  | Total Loss: 1.19 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1270/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.40 | Critic loss: 6.66 | Entropy loss: -0.0021  | Total Loss: 6.25 | Total Steps: 52\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1271/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: -0.01 | Critic loss: 3.15 | Entropy loss: -0.0004  | Total Loss: 3.14 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1272/129000  | Episode Reward: 7  | Average Reward 6.48  | Actor loss: 0.06 | Critic loss: 4.20 | Entropy loss: -0.0003  | Total Loss: 4.26 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1273/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: -0.10 | Critic loss: 7.46 | Entropy loss: -0.0008  | Total Loss: 7.36 | Total Steps: 42\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1274/129000  | Episode Reward: -2  | Average Reward 6.39  | Actor loss: -0.53 | Critic loss: 12.42 | Entropy loss: -0.0046  | Total Loss: 11.88 | Total Steps: 71\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1275/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: -0.42 | Critic loss: 6.77 | Entropy loss: -0.0090  | Total Loss: 6.34 | Total Steps: 58\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1276/129000  | Episode Reward: 7  | Average Reward 6.41  | Actor loss: 0.20 | Critic loss: 6.79 | Entropy loss: -0.0006  | Total Loss: 6.99 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1277/129000  | Episode Reward: 7  | Average Reward 6.39  | Actor loss: -0.36 | Critic loss: 9.34 | Entropy loss: -0.0011  | Total Loss: 8.98 | Total Steps: 24\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1278/129000  | Episode Reward: 1  | Average Reward 6.37  | Actor loss: 0.13 | Critic loss: 11.21 | Entropy loss: -0.0029  | Total Loss: 11.34 | Total Steps: 73\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1279/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: -0.07 | Critic loss: 4.97 | Entropy loss: -0.0009  | Total Loss: 4.90 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1280/129000  | Episode Reward: 4  | Average Reward 6.37  | Actor loss: -0.14 | Critic loss: 9.49 | Entropy loss: -0.0011  | Total Loss: 9.34 | Total Steps: 53\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1281/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 3.93 | Critic loss: 12.32 | Entropy loss: -0.0021  | Total Loss: 16.26 | Total Steps: 7\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1282/129000  | Episode Reward: 7  | Average Reward 6.37  | Actor loss: 0.09 | Critic loss: 7.19 | Entropy loss: -0.0003  | Total Loss: 7.28 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1283/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: -0.83 | Critic loss: 5.28 | Entropy loss: -0.0024  | Total Loss: 4.46 | Total Steps: 30\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1284/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 0.98 | Critic loss: 2.48 | Entropy loss: -0.0017  | Total Loss: 3.46 | Total Steps: 10\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1285/129000  | Episode Reward: 10  | Average Reward 6.44  | Actor loss: 0.04 | Critic loss: 11.56 | Entropy loss: -0.0000  | Total Loss: 11.59 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1286/129000  | Episode Reward: 1  | Average Reward 6.42  | Actor loss: -0.21 | Critic loss: 10.26 | Entropy loss: -0.0014  | Total Loss: 10.05 | Total Steps: 71\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1287/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.12 | Critic loss: 8.03 | Entropy loss: -0.0006  | Total Loss: 8.14 | Total Steps: 29\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1288/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.51 | Critic loss: 4.45 | Entropy loss: -0.0008  | Total Loss: 4.96 | Total Steps: 11\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1289/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.11 | Critic loss: 7.65 | Entropy loss: -0.0005  | Total Loss: 7.76 | Total Steps: 29\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1290/129000  | Episode Reward: 10  | Average Reward 6.53  | Actor loss: 1.43 | Critic loss: 4.25 | Entropy loss: -0.0023  | Total Loss: 5.67 | Total Steps: 10\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1291/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.03 | Critic loss: 1.61 | Entropy loss: -0.0000  | Total Loss: 1.64 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1292/129000  | Episode Reward: -2  | Average Reward 6.54  | Actor loss: -1.90 | Critic loss: 14.20 | Entropy loss: -0.0103  | Total Loss: 12.29 | Total Steps: 56\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1293/129000  | Episode Reward: 1  | Average Reward 6.50  | Actor loss: -1.24 | Critic loss: 8.74 | Entropy loss: -0.0200  | Total Loss: 7.47 | Total Steps: 90\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1294/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.85 | Critic loss: 10.78 | Entropy loss: -0.0100  | Total Loss: 9.91 | Total Steps: 84\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1295/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: -0.33 | Critic loss: 6.70 | Entropy loss: -0.0041  | Total Loss: 6.37 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1296/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.01 | Critic loss: 8.82 | Entropy loss: -0.0000  | Total Loss: 8.83 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1297/129000  | Episode Reward: 1  | Average Reward 6.43  | Actor loss: -1.31 | Critic loss: 9.38 | Entropy loss: -0.0086  | Total Loss: 8.06 | Total Steps: 58\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1298/129000  | Episode Reward: 7  | Average Reward 6.43  | Actor loss: 0.08 | Critic loss: 6.02 | Entropy loss: -0.0003  | Total Loss: 6.10 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1299/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -0.07 | Critic loss: 9.10 | Entropy loss: -0.0005  | Total Loss: 9.03 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1300/129000  | Episode Reward: 4  | Average Reward 6.40  | Actor loss: -0.26 | Critic loss: 6.11 | Entropy loss: -0.0021  | Total Loss: 5.84 | Total Steps: 48\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1301/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.03 | Critic loss: 1.18 | Entropy loss: -0.0000  | Total Loss: 1.21 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 1  | Average Reward 2.23  | Actor loss: 0.03 | Critic loss: 1.93 | Entropy loss: -0.0123  | Total Loss: 1.94 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 4  | Average Reward 2.22  | Actor loss: 0.01 | Critic loss: 1.22 | Entropy loss: -0.0012  | Total Loss: 1.22 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 1  | Average Reward 2.21  | Actor loss: 0.06 | Critic loss: 1.65 | Entropy loss: -0.0215  | Total Loss: 1.69 | Total Steps: 56\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.10 | Critic loss: 1.78 | Entropy loss: -0.0533  | Total Loss: 1.83 | Total Steps: 9\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 1  | Average Reward 2.25  | Actor loss: 0.13 | Critic loss: 1.65 | Entropy loss: -0.0070  | Total Loss: 1.77 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.01 | Critic loss: 2.21 | Entropy loss: -0.0021  | Total Loss: 2.21 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 10  | Average Reward 2.30  | Actor loss: 0.30 | Critic loss: 5.46 | Entropy loss: -0.0634  | Total Loss: 5.70 | Total Steps: 14\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: -2  | Average Reward 2.25  | Actor loss: 0.06 | Critic loss: 3.58 | Entropy loss: -0.0290  | Total Loss: 3.61 | Total Steps: 61\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 7  | Average Reward 2.24  | Actor loss: 0.01 | Critic loss: 4.71 | Entropy loss: -0.0024  | Total Loss: 4.73 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 4  | Average Reward 2.21  | Actor loss: 0.18 | Critic loss: 4.52 | Entropy loss: -0.0208  | Total Loss: 4.67 | Total Steps: 51\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.17 | Entropy loss: -0.0015  | Total Loss: 1.18 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.39 | Entropy loss: -0.0136  | Total Loss: 1.39 | Total Steps: 40\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.16 | Entropy loss: -0.0034  | Total Loss: 1.17 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 4  | Average Reward 2.23  | Actor loss: 0.01 | Critic loss: 8.30 | Entropy loss: -0.0005  | Total Loss: 8.31 | Total Steps: 49\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward 2.24  | Actor loss: 0.02 | Critic loss: 1.39 | Entropy loss: -0.0009  | Total Loss: 1.41 | Total Steps: 6\n",
      "TEST: ---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 4  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.18 | Entropy loss: -0.0029  | Total Loss: 1.18 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward 2.23  | Actor loss: 0.02 | Critic loss: 5.74 | Entropy loss: -0.0013  | Total Loss: 5.76 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward 2.24  | Actor loss: 0.01 | Critic loss: 1.25 | Entropy loss: -0.0012  | Total Loss: 1.26 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.01 | Critic loss: 2.23 | Entropy loss: -0.0010  | Total Loss: 2.23 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 20/100  | Episode Reward: -58  | Average Reward 1.92  | Actor loss: -1.28 | Critic loss: 120.20 | Entropy loss: -0.0414  | Total Loss: 118.88 | Total Steps: 500\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 4  | Average Reward 1.92  | Actor loss: 0.39 | Critic loss: 1.32 | Entropy loss: -0.0082  | Total Loss: 1.70 | Total Steps: 47\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 10  | Average Reward 1.92  | Actor loss: 0.01 | Critic loss: 1.25 | Entropy loss: -0.0025  | Total Loss: 1.27 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 3  | Average Reward 1.88  | Actor loss: 0.03 | Critic loss: 1.27 | Entropy loss: -0.0481  | Total Loss: 1.25 | Total Steps: 110\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Average Reward 1.91  | Actor loss: 0.02 | Critic loss: 14.49 | Entropy loss: -0.0088  | Total Loss: 14.50 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 10  | Average Reward 1.91  | Actor loss: 0.01 | Critic loss: 1.24 | Entropy loss: -0.0024  | Total Loss: 1.26 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 1.91  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0009  | Total Loss: 1.39 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 27/100  | Episode Reward: -43  | Average Reward 1.66  | Actor loss: -2.50 | Critic loss: 92.08 | Entropy loss: -0.0417  | Total Loss: 89.54 | Total Steps: 500\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.02 | Critic loss: 6.66 | Entropy loss: -0.0039  | Total Loss: 6.67 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.01 | Critic loss: 2.57 | Entropy loss: -0.0005  | Total Loss: 2.57 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward 1.65  | Actor loss: 0.01 | Critic loss: 1.30 | Entropy loss: -0.0006  | Total Loss: 1.30 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 1  | Average Reward 1.63  | Actor loss: 0.37 | Critic loss: 10.45 | Entropy loss: -0.0365  | Total Loss: 10.79 | Total Steps: 103\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 4  | Average Reward 1.60  | Actor loss: 0.00 | Critic loss: 1.12 | Entropy loss: -0.0149  | Total Loss: 1.11 | Total Steps: 44\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 7  | Average Reward 1.58  | Actor loss: 0.00 | Critic loss: 6.86 | Entropy loss: -0.0010  | Total Loss: 6.86 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 7  | Average Reward 1.57  | Actor loss: 0.03 | Critic loss: 0.74 | Entropy loss: -0.0159  | Total Loss: 0.75 | Total Steps: 44\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 1  | Average Reward 1.57  | Actor loss: 0.08 | Critic loss: 0.89 | Entropy loss: -0.0293  | Total Loss: 0.94 | Total Steps: 110\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: -32  | Average Reward 1.39  | Actor loss: 0.00 | Critic loss: 4.30 | Entropy loss: -0.0091  | Total Loss: 4.30 | Total Steps: 278\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward 1.36  | Actor loss: 0.01 | Critic loss: 0.94 | Entropy loss: -0.0073  | Total Loss: 0.94 | Total Steps: 47\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Average Reward 1.36  | Actor loss: 0.00 | Critic loss: 1.98 | Entropy loss: -0.0023  | Total Loss: 1.99 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward 1.39  | Actor loss: 0.01 | Critic loss: 1.34 | Entropy loss: -0.0012  | Total Loss: 1.35 | Total Steps: 6\n",
      "TEST: ---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 4  | Average Reward 1.36  | Actor loss: 0.03 | Critic loss: 3.71 | Entropy loss: -0.0005  | Total Loss: 3.74 | Total Steps: 46\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 10  | Average Reward 1.36  | Actor loss: 0.00 | Critic loss: 2.45 | Entropy loss: -0.0020  | Total Loss: 2.45 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 1  | Average Reward 1.33  | Actor loss: 0.02 | Critic loss: 3.30 | Entropy loss: -0.0021  | Total Loss: 3.31 | Total Steps: 50\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 43/100  | Episode Reward: -49  | Average Reward 1.04  | Actor loss: -4.72 | Critic loss: 89.41 | Entropy loss: -0.0371  | Total Loss: 84.65 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.03 | Critic loss: 13.36 | Entropy loss: -0.0070  | Total Loss: 13.37 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.01 | Critic loss: 1.53 | Entropy loss: -0.0019  | Total Loss: 1.53 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 4  | Average Reward 1.02  | Actor loss: 0.00 | Critic loss: 3.67 | Entropy loss: -0.0012  | Total Loss: 3.67 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.68 | Critic loss: 9.52 | Entropy loss: -0.0052  | Total Loss: 10.20 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0015  | Total Loss: 1.39 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.01 | Critic loss: 1.85 | Entropy loss: -0.0142  | Total Loss: 1.85 | Total Steps: 40\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 1  | Average Reward 1.04  | Actor loss: 0.04 | Critic loss: 0.76 | Entropy loss: -0.0077  | Total Loss: 0.80 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 1  | Average Reward 0.99  | Actor loss: 0.02 | Critic loss: 0.76 | Entropy loss: -0.0013  | Total Loss: 0.78 | Total Steps: 50\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 7  | Average Reward 1.01  | Actor loss: 0.01 | Critic loss: 15.04 | Entropy loss: -0.0014  | Total Loss: 15.05 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: -8  | Average Reward 0.94  | Actor loss: 2.50 | Critic loss: 16.24 | Entropy loss: -0.0449  | Total Loss: 18.69 | Total Steps: 166\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward 1.01  | Actor loss: 0.01 | Critic loss: 2.48 | Entropy loss: -0.0081  | Total Loss: 2.48 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 1  | Average Reward 0.98  | Actor loss: 0.00 | Critic loss: 2.98 | Entropy loss: -0.0129  | Total Loss: 2.97 | Total Steps: 56\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 4  | Average Reward 1.05  | Actor loss: 0.00 | Critic loss: 3.63 | Entropy loss: -0.0013  | Total Loss: 3.63 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.01 | Critic loss: 2.97 | Entropy loss: -0.0081  | Total Loss: 2.97 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 4  | Average Reward 1.02  | Actor loss: 0.00 | Critic loss: 1.15 | Entropy loss: -0.0126  | Total Loss: 1.15 | Total Steps: 46\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 7  | Average Reward 1.05  | Actor loss: 0.00 | Critic loss: 4.22 | Entropy loss: -0.0007  | Total Loss: 4.22 | Total Steps: 38\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 4  | Average Reward 1.03  | Actor loss: 0.03 | Critic loss: 1.14 | Entropy loss: -0.0098  | Total Loss: 1.16 | Total Steps: 45\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward 1.03  | Actor loss: 0.01 | Critic loss: 2.60 | Entropy loss: -0.0199  | Total Loss: 2.59 | Total Steps: 35\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: -2  | Average Reward 0.97  | Actor loss: 0.01 | Critic loss: 0.38 | Entropy loss: -0.0448  | Total Loss: 0.35 | Total Steps: 66\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 7  | Average Reward 0.96  | Actor loss: 0.01 | Critic loss: 14.76 | Entropy loss: -0.0027  | Total Loss: 14.77 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward 1.00  | Actor loss: 0.01 | Critic loss: 3.10 | Entropy loss: -0.0076  | Total Loss: 3.10 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 7  | Average Reward 1.02  | Actor loss: 0.46 | Critic loss: 10.44 | Entropy loss: -0.0376  | Total Loss: 10.86 | Total Steps: 47\n",
      "TEST: ---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward 1.00  | Actor loss: 0.01 | Critic loss: 1.44 | Entropy loss: -0.0014  | Total Loss: 1.44 | Total Steps: 38\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 10  | Average Reward 1.00  | Actor loss: 0.02 | Critic loss: 15.34 | Entropy loss: -0.0010  | Total Loss: 15.36 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 7  | Average Reward 0.99  | Actor loss: 0.04 | Critic loss: 5.36 | Entropy loss: -0.0142  | Total Loss: 5.39 | Total Steps: 52\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward 0.99  | Actor loss: 0.01 | Critic loss: 2.46 | Entropy loss: -0.0024  | Total Loss: 2.47 | Total Steps: 199\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 0.99  | Actor loss: 0.02 | Critic loss: 12.62 | Entropy loss: -0.0012  | Total Loss: 12.65 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 10  | Average Reward 0.99  | Actor loss: 0.06 | Critic loss: 1.95 | Entropy loss: -0.0172  | Total Loss: 1.99 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.01 | Critic loss: 2.45 | Entropy loss: -0.0034  | Total Loss: 2.45 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 4  | Average Reward 1.01  | Actor loss: 0.01 | Critic loss: 3.04 | Entropy loss: -0.0033  | Total Loss: 3.05 | Total Steps: 43\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 1.02  | Actor loss: 0.01 | Critic loss: 1.16 | Entropy loss: -0.0015  | Total Loss: 1.17 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.01 | Critic loss: 0.56 | Entropy loss: -0.0382  | Total Loss: 0.53 | Total Steps: 13\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.00 | Critic loss: 2.08 | Entropy loss: -0.0073  | Total Loss: 2.08 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 7  | Average Reward 1.10  | Actor loss: 0.01 | Critic loss: 8.96 | Entropy loss: -0.0066  | Total Loss: 8.96 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 1.10  | Actor loss: 0.69 | Critic loss: 9.79 | Entropy loss: -0.0060  | Total Loss: 10.48 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 10  | Average Reward 1.10  | Actor loss: 0.01 | Critic loss: 1.23 | Entropy loss: -0.0019  | Total Loss: 1.24 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 80/100  | Episode Reward: -64  | Average Reward 0.73  | Actor loss: -1.81 | Critic loss: 96.85 | Entropy loss: -0.0359  | Total Loss: 95.00 | Total Steps: 500\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 0.76  | Actor loss: 0.99 | Critic loss: 7.28 | Entropy loss: -0.0619  | Total Loss: 8.21 | Total Steps: 9\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 7  | Average Reward 0.74  | Actor loss: 0.06 | Critic loss: 1.14 | Entropy loss: -0.0103  | Total Loss: 1.18 | Total Steps: 39\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 4  | Average Reward 0.71  | Actor loss: 0.01 | Critic loss: 4.33 | Entropy loss: -0.0156  | Total Loss: 4.32 | Total Steps: 48\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 4  | Average Reward 0.70  | Actor loss: 0.01 | Critic loss: 1.75 | Entropy loss: -0.0056  | Total Loss: 1.75 | Total Steps: 43\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 1  | Average Reward 0.76  | Actor loss: 0.02 | Critic loss: 1.79 | Entropy loss: -0.0070  | Total Loss: 1.80 | Total Steps: 53\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Average Reward 0.76  | Actor loss: 0.68 | Critic loss: 9.54 | Entropy loss: -0.0061  | Total Loss: 10.21 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward 0.74  | Actor loss: 0.01 | Critic loss: 15.13 | Entropy loss: -0.0015  | Total Loss: 15.14 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward 0.74  | Actor loss: 0.01 | Critic loss: 1.29 | Entropy loss: -0.0011  | Total Loss: 1.29 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 7  | Average Reward 0.77  | Actor loss: 0.00 | Critic loss: 2.84 | Entropy loss: -0.0089  | Total Loss: 2.83 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward 0.83  | Actor loss: 0.01 | Critic loss: 1.17 | Entropy loss: -0.0024  | Total Loss: 1.18 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 1  | Average Reward 0.83  | Actor loss: 0.00 | Critic loss: 2.99 | Entropy loss: -0.0044  | Total Loss: 2.99 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 4  | Average Reward 0.82  | Actor loss: 1.23 | Critic loss: 2.27 | Entropy loss: -0.0082  | Total Loss: 3.49 | Total Steps: 37\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 1  | Average Reward 0.78  | Actor loss: 0.01 | Critic loss: 14.41 | Entropy loss: -0.0103  | Total Loss: 14.41 | Total Steps: 66\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 10  | Average Reward 0.81  | Actor loss: 0.05 | Critic loss: 1.79 | Entropy loss: -0.0089  | Total Loss: 1.83 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward 0.79  | Actor loss: 0.01 | Critic loss: 1.38 | Entropy loss: -0.0026  | Total Loss: 1.39 | Total Steps: 36\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 10  | Average Reward 0.81  | Actor loss: 0.02 | Critic loss: 10.38 | Entropy loss: -0.0012  | Total Loss: 10.40 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 10  | Average Reward 0.82  | Actor loss: 0.01 | Critic loss: 2.18 | Entropy loss: -0.0012  | Total Loss: 2.18 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward 0.81  | Actor loss: 0.01 | Critic loss: 1.33 | Entropy loss: -0.0012  | Total Loss: 1.33 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward 0.83  | Actor loss: 0.45 | Critic loss: 5.07 | Entropy loss: -0.0052  | Total Loss: 5.52 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward 0.86  | Actor loss: 0.05 | Critic loss: 6.00 | Entropy loss: -0.0138  | Total Loss: 6.03 | Total Steps: 32\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1302/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.01 | Critic loss: 2.01 | Entropy loss: -0.0000  | Total Loss: 2.02 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1303/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.01 | Critic loss: 3.09 | Entropy loss: -0.0000  | Total Loss: 3.10 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1304/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.02 | Critic loss: 1.49 | Entropy loss: -0.0000  | Total Loss: 1.51 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1305/129000  | Episode Reward: 8  | Average Reward 6.42  | Actor loss: -0.69 | Critic loss: 3.34 | Entropy loss: -0.0085  | Total Loss: 2.64 | Total Steps: 67\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1306/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -0.07 | Critic loss: 7.18 | Entropy loss: -0.0032  | Total Loss: 7.11 | Total Steps: 54\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1307/129000  | Episode Reward: 1  | Average Reward 6.45  | Actor loss: -0.41 | Critic loss: 7.57 | Entropy loss: -0.0035  | Total Loss: 7.16 | Total Steps: 52\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1308/129000  | Episode Reward: 10  | Average Reward 6.48  | Actor loss: 0.01 | Critic loss: 1.16 | Entropy loss: -0.0000  | Total Loss: 1.17 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1309/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: -0.43 | Critic loss: 4.27 | Entropy loss: -0.0085  | Total Loss: 3.83 | Total Steps: 71\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1310/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.05 | Critic loss: 1.36 | Entropy loss: -0.0001  | Total Loss: 1.41 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1311/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.20 | Critic loss: 3.73 | Entropy loss: -0.0002  | Total Loss: 3.93 | Total Steps: 8\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1312/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.11 | Critic loss: 6.60 | Entropy loss: -0.0008  | Total Loss: 6.49 | Total Steps: 47\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1313/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: -0.34 | Critic loss: 2.58 | Entropy loss: -0.0121  | Total Loss: 2.22 | Total Steps: 61\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1314/129000  | Episode Reward: 4  | Average Reward 6.47  | Actor loss: -0.02 | Critic loss: 7.02 | Entropy loss: -0.0033  | Total Loss: 7.00 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1315/129000  | Episode Reward: 10  | Average Reward 6.52  | Actor loss: 0.51 | Critic loss: 6.70 | Entropy loss: -0.0007  | Total Loss: 7.20 | Total Steps: 12\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1316/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.33 | Critic loss: 3.26 | Entropy loss: -0.0004  | Total Loss: 3.58 | Total Steps: 8\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1317/129000  | Episode Reward: 7  | Average Reward 6.57  | Actor loss: 0.11 | Critic loss: 8.69 | Entropy loss: -0.0008  | Total Loss: 8.80 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1318/129000  | Episode Reward: 1  | Average Reward 6.54  | Actor loss: -1.49 | Critic loss: 13.49 | Entropy loss: -0.0058  | Total Loss: 11.99 | Total Steps: 56\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1319/129000  | Episode Reward: 10  | Average Reward 6.57  | Actor loss: 0.01 | Critic loss: 1.27 | Entropy loss: -0.0000  | Total Loss: 1.28 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1320/129000  | Episode Reward: 4  | Average Reward 6.54  | Actor loss: -1.14 | Critic loss: 6.27 | Entropy loss: -0.0026  | Total Loss: 5.13 | Total Steps: 42\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1321/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.01 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.14 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1322/129000  | Episode Reward: 10  | Average Reward 6.57  | Actor loss: 0.01 | Critic loss: 0.92 | Entropy loss: -0.0000  | Total Loss: 0.93 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1323/129000  | Episode Reward: 10  | Average Reward 6.57  | Actor loss: 0.03 | Critic loss: 0.45 | Entropy loss: -0.0001  | Total Loss: 0.47 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1324/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 0.01 | Critic loss: 1.54 | Entropy loss: -0.0000  | Total Loss: 1.55 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1325/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.04 | Critic loss: 6.24 | Entropy loss: -0.0002  | Total Loss: 6.28 | Total Steps: 30\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1326/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 0.01 | Critic loss: 0.62 | Entropy loss: -0.0000  | Total Loss: 0.62 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1327/129000  | Episode Reward: 7  | Average Reward 6.59  | Actor loss: -0.16 | Critic loss: 4.08 | Entropy loss: -0.0021  | Total Loss: 3.91 | Total Steps: 109\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1328/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 0.01 | Critic loss: 0.77 | Entropy loss: -0.0000  | Total Loss: 0.78 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1329/129000  | Episode Reward: 5  | Average Reward 6.57  | Actor loss: -0.84 | Critic loss: 6.39 | Entropy loss: -0.0087  | Total Loss: 5.55 | Total Steps: 61\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1330/129000  | Episode Reward: 1  | Average Reward 6.53  | Actor loss: -0.49 | Critic loss: 8.80 | Entropy loss: -0.0033  | Total Loss: 8.31 | Total Steps: 51\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1331/129000  | Episode Reward: 10  | Average Reward 6.56  | Actor loss: 0.02 | Critic loss: 1.68 | Entropy loss: -0.0000  | Total Loss: 1.70 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1332/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 0.54 | Critic loss: 1.63 | Entropy loss: -0.0020  | Total Loss: 2.17 | Total Steps: 12\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1333/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.03 | Critic loss: 6.57 | Entropy loss: -0.0006  | Total Loss: 6.60 | Total Steps: 30\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1334/129000  | Episode Reward: 4  | Average Reward 6.59  | Actor loss: -0.11 | Critic loss: 7.23 | Entropy loss: -0.0051  | Total Loss: 7.12 | Total Steps: 80\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1335/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: -0.20 | Critic loss: 5.27 | Entropy loss: -0.0038  | Total Loss: 5.07 | Total Steps: 36\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1336/129000  | Episode Reward: 10  | Average Reward 6.61  | Actor loss: 0.58 | Critic loss: 0.86 | Entropy loss: -0.0012  | Total Loss: 1.44 | Total Steps: 9\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1337/129000  | Episode Reward: 10  | Average Reward 6.61  | Actor loss: -0.11 | Critic loss: 2.75 | Entropy loss: -0.0067  | Total Loss: 2.63 | Total Steps: 48\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1338/129000  | Episode Reward: 4  | Average Reward 6.62  | Actor loss: -0.15 | Critic loss: 7.48 | Entropy loss: -0.0019  | Total Loss: 7.33 | Total Steps: 36\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1339/129000  | Episode Reward: 7  | Average Reward 6.64  | Actor loss: -0.44 | Critic loss: 3.04 | Entropy loss: -0.0034  | Total Loss: 2.60 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1340/129000  | Episode Reward: 7  | Average Reward 6.66  | Actor loss: 0.16 | Critic loss: 2.98 | Entropy loss: -0.0012  | Total Loss: 3.14 | Total Steps: 35\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1341/129000  | Episode Reward: 10  | Average Reward 6.67  | Actor loss: 0.33 | Critic loss: 2.63 | Entropy loss: -0.0022  | Total Loss: 2.95 | Total Steps: 34\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1342/129000  | Episode Reward: 10  | Average Reward 6.69  | Actor loss: 0.20 | Critic loss: 1.75 | Entropy loss: -0.0003  | Total Loss: 1.96 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1343/129000  | Episode Reward: 10  | Average Reward 6.69  | Actor loss: -0.16 | Critic loss: 4.65 | Entropy loss: -0.0035  | Total Loss: 4.48 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1344/129000  | Episode Reward: 7  | Average Reward 6.67  | Actor loss: 0.10 | Critic loss: 6.03 | Entropy loss: -0.0012  | Total Loss: 6.13 | Total Steps: 37\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1345/129000  | Episode Reward: 1  | Average Reward 6.63  | Actor loss: -1.89 | Critic loss: 12.37 | Entropy loss: -0.0093  | Total Loss: 10.47 | Total Steps: 68\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1346/129000  | Episode Reward: 4  | Average Reward 6.60  | Actor loss: -0.54 | Critic loss: 4.98 | Entropy loss: -0.0023  | Total Loss: 4.44 | Total Steps: 45\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1347/129000  | Episode Reward: 10  | Average Reward 6.62  | Actor loss: 1.17 | Critic loss: 0.76 | Entropy loss: -0.0035  | Total Loss: 1.93 | Total Steps: 7\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1348/129000  | Episode Reward: 10  | Average Reward 6.62  | Actor loss: -0.65 | Critic loss: 3.05 | Entropy loss: -0.0065  | Total Loss: 2.39 | Total Steps: 53\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1349/129000  | Episode Reward: 10  | Average Reward 6.62  | Actor loss: 0.01 | Critic loss: 0.92 | Entropy loss: -0.0001  | Total Loss: 0.94 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1350/129000  | Episode Reward: 10  | Average Reward 6.62  | Actor loss: 0.95 | Critic loss: 6.32 | Entropy loss: -0.0018  | Total Loss: 7.26 | Total Steps: 17\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1351/129000  | Episode Reward: 10  | Average Reward 6.63  | Actor loss: 0.01 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1352/129000  | Episode Reward: 10  | Average Reward 6.64  | Actor loss: 0.02 | Critic loss: 0.88 | Entropy loss: -0.0000  | Total Loss: 0.90 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1353/129000  | Episode Reward: 7  | Average Reward 6.64  | Actor loss: 0.07 | Critic loss: 6.54 | Entropy loss: -0.0038  | Total Loss: 6.60 | Total Steps: 46\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1354/129000  | Episode Reward: 10  | Average Reward 6.64  | Actor loss: 0.02 | Critic loss: 0.37 | Entropy loss: -0.0001  | Total Loss: 0.39 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1355/129000  | Episode Reward: 7  | Average Reward 6.67  | Actor loss: -0.11 | Critic loss: 2.69 | Entropy loss: -0.0004  | Total Loss: 2.58 | Total Steps: 34\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.72. Model has been saved-----\n",
      "Training  | Episode: 1356/129000  | Episode Reward: 10  | Average Reward 6.72  | Actor loss: 0.09 | Critic loss: 3.73 | Entropy loss: -0.0007  | Total Loss: 3.83 | Total Steps: 30\n",
      "---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1357/129000  | Episode Reward: 10  | Average Reward 6.72  | Actor loss: -0.21 | Critic loss: 2.67 | Entropy loss: -0.0033  | Total Loss: 2.46 | Total Steps: 36\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1358/129000  | Episode Reward: 10  | Average Reward 6.72  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0000  | Total Loss: 0.73 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.765. Model has been saved-----\n",
      "Training  | Episode: 1359/129000  | Episode Reward: 10  | Average Reward 6.76  | Actor loss: -0.49 | Critic loss: 3.02 | Entropy loss: -0.0053  | Total Loss: 2.53 | Total Steps: 52\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1360/129000  | Episode Reward: 1  | Average Reward 6.74  | Actor loss: -0.56 | Critic loss: 10.12 | Entropy loss: -0.0018  | Total Loss: 9.56 | Total Steps: 48\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.775. Model has been saved-----\n",
      "Training  | Episode: 1361/129000  | Episode Reward: 10  | Average Reward 6.78  | Actor loss: 0.01 | Critic loss: 0.81 | Entropy loss: -0.0000  | Total Loss: 0.82 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.805. Model has been saved-----\n",
      "Training  | Episode: 1362/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.00 | Critic loss: 0.45 | Entropy loss: -0.0000  | Total Loss: 0.46 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1363/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.01 | Critic loss: 0.57 | Entropy loss: -0.0000  | Total Loss: 0.58 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1364/129000  | Episode Reward: 7  | Average Reward 6.79  | Actor loss: -0.01 | Critic loss: 7.25 | Entropy loss: -0.0011  | Total Loss: 7.24 | Total Steps: 32\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1365/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.03 | Critic loss: 0.50 | Entropy loss: -0.0001  | Total Loss: 0.53 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1366/129000  | Episode Reward: 7  | Average Reward 6.80  | Actor loss: -0.36 | Critic loss: 3.90 | Entropy loss: -0.0020  | Total Loss: 3.53 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.835. Model has been saved-----\n",
      "Training  | Episode: 1367/129000  | Episode Reward: 10  | Average Reward 6.83  | Actor loss: 0.18 | Critic loss: 1.76 | Entropy loss: -0.0003  | Total Loss: 1.94 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1368/129000  | Episode Reward: 4  | Average Reward 6.80  | Actor loss: -0.54 | Critic loss: 6.77 | Entropy loss: -0.0035  | Total Loss: 6.23 | Total Steps: 44\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1369/129000  | Episode Reward: 1  | Average Reward 6.78  | Actor loss: -0.59 | Critic loss: 11.67 | Entropy loss: -0.0040  | Total Loss: 11.07 | Total Steps: 51\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1370/129000  | Episode Reward: 10  | Average Reward 6.79  | Actor loss: 0.28 | Critic loss: 1.38 | Entropy loss: -0.0003  | Total Loss: 1.66 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1371/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.00 | Critic loss: 0.55 | Entropy loss: -0.0000  | Total Loss: 0.55 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.85. Model has been saved-----\n",
      "Training  | Episode: 1372/129000  | Episode Reward: 10  | Average Reward 6.85  | Actor loss: 0.60 | Critic loss: 0.38 | Entropy loss: -0.0023  | Total Loss: 0.97 | Total Steps: 10\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.865. Model has been saved-----\n",
      "Training  | Episode: 1373/129000  | Episode Reward: 4  | Average Reward 6.87  | Actor loss: -0.01 | Critic loss: 6.39 | Entropy loss: -0.0001  | Total Loss: 6.37 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1374/129000  | Episode Reward: 10  | Average Reward 6.87  | Actor loss: 0.01 | Critic loss: 0.28 | Entropy loss: -0.0000  | Total Loss: 0.29 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1375/129000  | Episode Reward: 4  | Average Reward 6.83  | Actor loss: -0.35 | Critic loss: 6.12 | Entropy loss: -0.0036  | Total Loss: 5.77 | Total Steps: 51\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1376/129000  | Episode Reward: 10  | Average Reward 6.85  | Actor loss: 0.02 | Critic loss: 0.98 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1377/129000  | Episode Reward: 10  | Average Reward 6.85  | Actor loss: 0.01 | Critic loss: 2.55 | Entropy loss: -0.0000  | Total Loss: 2.56 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.895. Model has been saved-----\n",
      "Training  | Episode: 1378/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 0.02 | Critic loss: 0.67 | Entropy loss: -0.0000  | Total Loss: 0.69 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1379/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: 0.04 | Critic loss: 4.99 | Entropy loss: -0.0018  | Total Loss: 5.02 | Total Steps: 32\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1380/129000  | Episode Reward: 7  | Average Reward 6.87  | Actor loss: -0.84 | Critic loss: 5.38 | Entropy loss: -0.0064  | Total Loss: 4.53 | Total Steps: 45\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1381/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 0.03 | Critic loss: 0.92 | Entropy loss: -0.0001  | Total Loss: 0.94 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.91. Model has been saved-----\n",
      "Training  | Episode: 1382/129000  | Episode Reward: 10  | Average Reward 6.91  | Actor loss: 0.01 | Critic loss: 0.38 | Entropy loss: -0.0000  | Total Loss: 0.39 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.94. Model has been saved-----\n",
      "Training  | Episode: 1383/129000  | Episode Reward: 10  | Average Reward 6.94  | Actor loss: 0.01 | Critic loss: 0.83 | Entropy loss: -0.0000  | Total Loss: 0.83 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.955. Model has been saved-----\n",
      "Training  | Episode: 1384/129000  | Episode Reward: 10  | Average Reward 6.96  | Actor loss: 0.00 | Critic loss: 0.27 | Entropy loss: -0.0000  | Total Loss: 0.28 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1385/129000  | Episode Reward: 10  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 0.44 | Entropy loss: -0.0000  | Total Loss: 0.44 | Total Steps: 6\n",
      "---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 6.985. Model has been saved-----\n",
      "Training  | Episode: 1386/129000  | Episode Reward: 10  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 0.13 | Entropy loss: -0.0000  | Total Loss: 0.13 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1387/129000  | Episode Reward: 10  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 0.31 | Entropy loss: -0.0000  | Total Loss: 0.32 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.03. Model has been saved-----\n",
      "Training  | Episode: 1388/129000  | Episode Reward: 10  | Average Reward 7.03  | Actor loss: -0.32 | Critic loss: 2.43 | Entropy loss: -0.0063  | Total Loss: 2.10 | Total Steps: 64\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1389/129000  | Episode Reward: 7  | Average Reward 7.01  | Actor loss: 0.01 | Critic loss: 7.95 | Entropy loss: -0.0001  | Total Loss: 7.96 | Total Steps: 29\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.06. Model has been saved-----\n",
      "Training  | Episode: 1390/129000  | Episode Reward: 10  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 0.39 | Entropy loss: -0.0000  | Total Loss: 0.40 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.09. Model has been saved-----\n",
      "Training  | Episode: 1391/129000  | Episode Reward: 10  | Average Reward 7.09  | Actor loss: 0.01 | Critic loss: 0.13 | Entropy loss: -0.0000  | Total Loss: 0.14 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1392/129000  | Episode Reward: 1  | Average Reward 7.06  | Actor loss: -0.91 | Critic loss: 9.73 | Entropy loss: -0.0109  | Total Loss: 8.81 | Total Steps: 82\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.105. Model has been saved-----\n",
      "Training  | Episode: 1393/129000  | Episode Reward: 10  | Average Reward 7.11  | Actor loss: 0.00 | Critic loss: 0.21 | Entropy loss: -0.0001  | Total Loss: 0.21 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1394/129000  | Episode Reward: 10  | Average Reward 7.11  | Actor loss: 0.47 | Critic loss: 5.86 | Entropy loss: -0.0008  | Total Loss: 6.33 | Total Steps: 12\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1395/129000  | Episode Reward: -5  | Average Reward 7.03  | Actor loss: -1.53 | Critic loss: 20.01 | Entropy loss: -0.0050  | Total Loss: 18.48 | Total Steps: 48\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1396/129000  | Episode Reward: 4  | Average Reward 7.00  | Actor loss: -1.09 | Critic loss: 7.58 | Entropy loss: -0.0112  | Total Loss: 6.49 | Total Steps: 71\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1397/129000  | Episode Reward: 7  | Average Reward 6.99  | Actor loss: -0.08 | Critic loss: 2.90 | Entropy loss: -0.0012  | Total Loss: 2.81 | Total Steps: 98\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1398/129000  | Episode Reward: 10  | Average Reward 7.00  | Actor loss: 0.52 | Critic loss: 3.72 | Entropy loss: -0.0034  | Total Loss: 4.24 | Total Steps: 24\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1399/129000  | Episode Reward: 7  | Average Reward 7.01  | Actor loss: 0.19 | Critic loss: 8.23 | Entropy loss: -0.0009  | Total Loss: 8.42 | Total Steps: 29\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1400/129000  | Episode Reward: 10  | Average Reward 7.04  | Actor loss: 0.00 | Critic loss: 0.29 | Entropy loss: -0.0000  | Total Loss: 0.29 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1401/129000  | Episode Reward: 7  | Average Reward 7.03  | Actor loss: -0.13 | Critic loss: 4.05 | Entropy loss: -0.0006  | Total Loss: 3.92 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward 0.91  | Actor loss: 0.20 | Critic loss: 8.86 | Entropy loss: -0.0301  | Total Loss: 9.02 | Total Steps: 7\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Average Reward 0.94  | Actor loss: 0.13 | Critic loss: 3.96 | Entropy loss: -0.0102  | Total Loss: 4.08 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 4  | Average Reward 0.94  | Actor loss: 0.00 | Critic loss: 0.41 | Entropy loss: -0.0085  | Total Loss: 0.40 | Total Steps: 44\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward 0.96  | Actor loss: 0.02 | Critic loss: 11.29 | Entropy loss: -0.0009  | Total Loss: 11.30 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 10  | Average Reward 0.96  | Actor loss: 0.02 | Critic loss: 12.16 | Entropy loss: -0.0006  | Total Loss: 12.17 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 1  | Average Reward 0.96  | Actor loss: 0.45 | Critic loss: 12.66 | Entropy loss: -0.0131  | Total Loss: 13.10 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward 0.92  | Actor loss: 0.08 | Critic loss: 0.26 | Entropy loss: -0.0160  | Total Loss: 0.32 | Total Steps: 52\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 7  | Average Reward 0.94  | Actor loss: 0.01 | Critic loss: 3.99 | Entropy loss: -0.0044  | Total Loss: 3.99 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 4  | Average Reward 0.95  | Actor loss: 0.00 | Critic loss: 0.45 | Entropy loss: -0.0007  | Total Loss: 0.45 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 4  | Average Reward 0.94  | Actor loss: 0.00 | Critic loss: 0.41 | Entropy loss: -0.0038  | Total Loss: 0.41 | Total Steps: 42\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward 0.97  | Actor loss: 2.66 | Critic loss: 10.09 | Entropy loss: -0.0192  | Total Loss: 12.73 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 1  | Average Reward 0.95  | Actor loss: 0.51 | Critic loss: 3.74 | Entropy loss: -0.0180  | Total Loss: 4.23 | Total Steps: 53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 4  | Average Reward 0.94  | Actor loss: 2.74 | Critic loss: 28.15 | Entropy loss: -0.0204  | Total Loss: 30.88 | Total Steps: 34\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 4  | Average Reward 0.94  | Actor loss: 0.27 | Critic loss: 8.80 | Entropy loss: -0.0067  | Total Loss: 9.06 | Total Steps: 50\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 1  | Average Reward 0.91  | Actor loss: 0.07 | Critic loss: 5.55 | Entropy loss: -0.0082  | Total Loss: 5.61 | Total Steps: 62\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 0.91  | Actor loss: 0.02 | Critic loss: 11.50 | Entropy loss: -0.0007  | Total Loss: 11.52 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward 0.94  | Actor loss: 0.04 | Critic loss: 14.05 | Entropy loss: -0.0007  | Total Loss: 14.09 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward 0.95  | Actor loss: 0.00 | Critic loss: 0.51 | Entropy loss: -0.0006  | Total Loss: 0.52 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 4  | Average Reward 0.94  | Actor loss: 0.02 | Critic loss: 5.26 | Entropy loss: -0.0003  | Total Loss: 5.28 | Total Steps: 46\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward 0.94  | Actor loss: 0.00 | Critic loss: 0.57 | Entropy loss: -0.0005  | Total Loss: 0.58 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 7  | Average Reward 1.48  | Actor loss: 3.20 | Critic loss: 24.35 | Entropy loss: -0.0303  | Total Loss: 27.52 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 1  | Average Reward 1.48  | Actor loss: 0.06 | Critic loss: 1.36 | Entropy loss: -0.0022  | Total Loss: 1.42 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 7  | Average Reward 1.49  | Actor loss: 0.01 | Critic loss: 3.90 | Entropy loss: -0.0039  | Total Loss: 3.90 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Average Reward 1.51  | Actor loss: 9.17 | Critic loss: 19.81 | Entropy loss: -0.0102  | Total Loss: 28.97 | Total Steps: 45\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward 1.51  | Actor loss: 0.01 | Critic loss: 10.71 | Entropy loss: -0.0006  | Total Loss: 10.72 | Total Steps: 34\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 1.52  | Actor loss: 3.67 | Critic loss: 15.18 | Entropy loss: -0.0279  | Total Loss: 18.83 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 4  | Average Reward 1.50  | Actor loss: 0.01 | Critic loss: 1.81 | Entropy loss: -0.0010  | Total Loss: 1.82 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward 1.99  | Actor loss: 3.14 | Critic loss: 29.30 | Entropy loss: -0.0325  | Total Loss: 32.40 | Total Steps: 80\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 4  | Average Reward 1.96  | Actor loss: 0.75 | Critic loss: 9.47 | Entropy loss: -0.0113  | Total Loss: 10.21 | Total Steps: 48\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: -8  | Average Reward 1.86  | Actor loss: 1.98 | Critic loss: 20.57 | Entropy loss: -0.0290  | Total Loss: 22.52 | Total Steps: 105\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward 1.86  | Actor loss: 0.01 | Critic loss: 1.97 | Entropy loss: -0.0043  | Total Loss: 1.98 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 1  | Average Reward 1.96  | Actor loss: 0.19 | Critic loss: 2.91 | Entropy loss: -0.0029  | Total Loss: 3.10 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward 1.97  | Actor loss: 48.27 | Critic loss: 23.85 | Entropy loss: -0.0960  | Total Loss: 72.02 | Total Steps: 9\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 1  | Average Reward 1.96  | Actor loss: 1.01 | Critic loss: 14.88 | Entropy loss: -0.0185  | Total Loss: 15.87 | Total Steps: 54\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 4  | Average Reward 1.94  | Actor loss: 1.37 | Critic loss: 18.24 | Entropy loss: -0.0380  | Total Loss: 19.58 | Total Steps: 72\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 4  | Average Reward 1.97  | Actor loss: 0.01 | Critic loss: 6.42 | Entropy loss: -0.0014  | Total Loss: 6.43 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 7  | Average Reward 1.99  | Actor loss: 0.02 | Critic loss: 9.71 | Entropy loss: -0.0061  | Total Loss: 9.72 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 38/100  | Episode Reward: -97  | Average Reward 1.50  | Actor loss: -19.71 | Critic loss: 80.62 | Entropy loss: -0.0229  | Total Loss: 60.88 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward 1.50  | Actor loss: 0.15 | Critic loss: 2.13 | Entropy loss: -0.0987  | Total Loss: 2.18 | Total Steps: 12\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 1.50  | Actor loss: 14.74 | Critic loss: 18.93 | Entropy loss: -0.0732  | Total Loss: 33.60 | Total Steps: 7\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 10  | Average Reward 1.60  | Actor loss: 0.03 | Critic loss: 23.04 | Entropy loss: -0.0707  | Total Loss: 23.00 | Total Steps: 8\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 4  | Average Reward 1.57  | Actor loss: 0.02 | Critic loss: 4.85 | Entropy loss: -0.0066  | Total Loss: 4.86 | Total Steps: 46\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 10  | Average Reward 1.65  | Actor loss: 0.03 | Critic loss: 8.82 | Entropy loss: -0.0016  | Total Loss: 8.85 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 44/100  | Episode Reward: -121  | Average Reward 1.55  | Actor loss: -0.03 | Critic loss: 96.85 | Entropy loss: -0.0163  | Total Loss: 96.81 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 45/100  | Episode Reward: -118  | Average Reward 0.93  | Actor loss: -2.17 | Critic loss: 87.04 | Entropy loss: -0.0281  | Total Loss: 84.84 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 46/100  | Episode Reward: -10  | Average Reward 0.83  | Actor loss: -0.00 | Critic loss: 77.98 | Entropy loss: -0.0012  | Total Loss: 77.98 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Average Reward 0.83  | Actor loss: 0.02 | Critic loss: 1.38 | Entropy loss: -0.0009  | Total Loss: 1.40 | Total Steps: 49\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 0.88  | Actor loss: 12.76 | Critic loss: 17.55 | Entropy loss: -0.0436  | Total Loss: 30.27 | Total Steps: 7\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 1  | Average Reward 0.83  | Actor loss: 0.22 | Critic loss: 3.93 | Entropy loss: -0.0312  | Total Loss: 4.12 | Total Steps: 58\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 1  | Average Reward 0.90  | Actor loss: 0.19 | Critic loss: 6.50 | Entropy loss: -0.0074  | Total Loss: 6.68 | Total Steps: 63\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 7  | Average Reward 0.93  | Actor loss: 0.01 | Critic loss: 7.02 | Entropy loss: -0.0004  | Total Loss: 7.03 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 7  | Average Reward 0.96  | Actor loss: 0.02 | Critic loss: 11.80 | Entropy loss: -0.0006  | Total Loss: 11.81 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 10  | Average Reward 1.00  | Actor loss: 2.48 | Critic loss: 17.58 | Entropy loss: -0.0139  | Total Loss: 20.04 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Average Reward 1.45  | Actor loss: 5.28 | Critic loss: 22.78 | Entropy loss: -0.0182  | Total Loss: 28.04 | Total Steps: 33\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 55/100  | Episode Reward: -10  | Average Reward 1.35  | Actor loss: -0.00 | Critic loss: 78.04 | Entropy loss: -0.0001  | Total Loss: 78.04 | Total Steps: 500\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 4  | Average Reward 1.32  | Actor loss: 19.70 | Critic loss: 18.25 | Entropy loss: -0.0401  | Total Loss: 37.91 | Total Steps: 116\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -26  | Average Reward 1.33  | Actor loss: 0.12 | Critic loss: 7.68 | Entropy loss: -0.0066  | Total Loss: 7.79 | Total Steps: 184\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 1.33  | Actor loss: 0.02 | Critic loss: 13.65 | Entropy loss: -0.0016  | Total Loss: 13.66 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 59/100  | Episode Reward: -103  | Average Reward 0.84  | Actor loss: -10.45 | Critic loss: 79.89 | Entropy loss: -0.0357  | Total Loss: 69.40 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.05 | Critic loss: 0.76 | Entropy loss: -0.0244  | Total Loss: 0.79 | Total Steps: 40\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Average Reward 1.07  | Actor loss: 2.17 | Critic loss: 18.79 | Entropy loss: -0.0613  | Total Loss: 20.90 | Total Steps: 14\n",
      "TEST: ---cylinder---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward 1.07  | Actor loss: 0.01 | Critic loss: 1.67 | Entropy loss: -0.0025  | Total Loss: 1.68 | Total Steps: 458\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 7  | Average Reward 1.08  | Actor loss: 0.01 | Critic loss: 10.71 | Entropy loss: -0.0004  | Total Loss: 10.72 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 4  | Average Reward 1.07  | Actor loss: 6.61 | Critic loss: 19.77 | Entropy loss: -0.0210  | Total Loss: 26.36 | Total Steps: 53\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 10  | Average Reward 1.07  | Actor loss: 0.02 | Critic loss: 13.52 | Entropy loss: -0.0040  | Total Loss: 13.53 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 1  | Average Reward 1.44  | Actor loss: 0.03 | Critic loss: 0.83 | Entropy loss: -0.0042  | Total Loss: 0.86 | Total Steps: 50\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 10  | Average Reward 1.46  | Actor loss: 0.00 | Critic loss: 0.54 | Entropy loss: -0.0012  | Total Loss: 0.55 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 7  | Average Reward 1.56  | Actor loss: 0.02 | Critic loss: 2.25 | Entropy loss: -0.0005  | Total Loss: 2.27 | Total Steps: 38\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward 1.56  | Actor loss: 0.02 | Critic loss: 1.61 | Entropy loss: -0.0008  | Total Loss: 1.63 | Total Steps: 42\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 1.59  | Actor loss: 0.04 | Critic loss: 5.37 | Entropy loss: -0.0004  | Total Loss: 5.41 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 10  | Average Reward 1.60  | Actor loss: 0.02 | Critic loss: 13.65 | Entropy loss: -0.0005  | Total Loss: 13.67 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 7  | Average Reward 1.59  | Actor loss: 0.02 | Critic loss: 2.36 | Entropy loss: -0.0035  | Total Loss: 2.37 | Total Steps: 30\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 4  | Average Reward 1.56  | Actor loss: 1.03 | Critic loss: 4.63 | Entropy loss: -0.0294  | Total Loss: 5.63 | Total Steps: 46\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 1.57  | Actor loss: 0.02 | Critic loss: 3.74 | Entropy loss: -0.0013  | Total Loss: 3.76 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward 1.57  | Actor loss: 0.01 | Critic loss: 4.76 | Entropy loss: -0.0110  | Total Loss: 4.75 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 10  | Average Reward 1.57  | Actor loss: 0.02 | Critic loss: 12.16 | Entropy loss: -0.0023  | Total Loss: 12.17 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 4  | Average Reward 1.54  | Actor loss: 1.54 | Critic loss: 2.31 | Entropy loss: -0.0094  | Total Loss: 3.84 | Total Steps: 45\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 1.54  | Actor loss: 0.03 | Critic loss: 11.29 | Entropy loss: -0.0021  | Total Loss: 11.31 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 7  | Average Reward 1.53  | Actor loss: 0.01 | Critic loss: 1.84 | Entropy loss: -0.0088  | Total Loss: 1.84 | Total Steps: 190\n",
      "TEST: ---capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 4  | Average Reward 1.51  | Actor loss: 0.00 | Critic loss: 1.99 | Entropy loss: -0.0020  | Total Loss: 1.99 | Total Steps: 49\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 2.14  | Actor loss: 0.03 | Critic loss: 4.83 | Entropy loss: -0.0004  | Total Loss: 4.86 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward 2.14  | Actor loss: 1.48 | Critic loss: 5.36 | Entropy loss: -0.0527  | Total Loss: 6.79 | Total Steps: 11\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: -11  | Average Reward 2.05  | Actor loss: 1.22 | Critic loss: 7.13 | Entropy loss: -0.0096  | Total Loss: 8.34 | Total Steps: 126\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 1  | Average Reward 2.06  | Actor loss: 0.06 | Critic loss: 1.28 | Entropy loss: -0.0019  | Total Loss: 1.34 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 1  | Average Reward 2.05  | Actor loss: 0.42 | Critic loss: 5.27 | Entropy loss: -0.0159  | Total Loss: 5.68 | Total Steps: 61\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Average Reward 2.10  | Actor loss: 2.02 | Critic loss: 19.47 | Entropy loss: -0.0341  | Total Loss: 21.46 | Total Steps: 21\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Average Reward 2.10  | Actor loss: 0.02 | Critic loss: 11.25 | Entropy loss: -0.0015  | Total Loss: 11.27 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: -5  | Average Reward 2.05  | Actor loss: 0.04 | Critic loss: 19.33 | Entropy loss: -0.0239  | Total Loss: 19.34 | Total Steps: 113\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 89/100  | Episode Reward: -115  | Average Reward 1.43  | Actor loss: -7.82 | Critic loss: 140.20 | Entropy loss: -0.0199  | Total Loss: 132.37 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward 1.44  | Actor loss: 0.00 | Critic loss: 0.44 | Entropy loss: -0.0037  | Total Loss: 0.44 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward 1.43  | Actor loss: 0.01 | Critic loss: 1.86 | Entropy loss: -0.0007  | Total Loss: 1.87 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 7  | Average Reward 1.52  | Actor loss: 0.75 | Critic loss: 22.74 | Entropy loss: -0.0109  | Total Loss: 23.48 | Total Steps: 24\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 1  | Average Reward 1.51  | Actor loss: 0.01 | Critic loss: 0.76 | Entropy loss: -0.0025  | Total Loss: 0.76 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 94/100  | Episode Reward: -118  | Average Reward 0.92  | Actor loss: -0.03 | Critic loss: 71.64 | Entropy loss: -0.0218  | Total Loss: 71.59 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 10  | Average Reward 0.96  | Actor loss: 3.86 | Critic loss: 22.23 | Entropy loss: -0.0367  | Total Loss: 26.05 | Total Steps: 14\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Average Reward 0.96  | Actor loss: 0.01 | Critic loss: 6.42 | Entropy loss: -0.0013  | Total Loss: 6.43 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 3.68 | Critic loss: 15.46 | Entropy loss: -0.0261  | Total Loss: 19.12 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.00 | Critic loss: 0.58 | Entropy loss: -0.0048  | Total Loss: 0.58 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.02 | Critic loss: 2.35 | Entropy loss: -0.0037  | Total Loss: 2.36 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 1  | Average Reward 0.99  | Actor loss: 0.10 | Critic loss: 3.14 | Entropy loss: -0.0289  | Total Loss: 3.21 | Total Steps: 68\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1402/129000  | Episode Reward: 7  | Average Reward 7.01  | Actor loss: 0.00 | Critic loss: 5.41 | Entropy loss: -0.0008  | Total Loss: 5.41 | Total Steps: 30\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1403/129000  | Episode Reward: 10  | Average Reward 7.06  | Actor loss: 0.73 | Critic loss: 2.08 | Entropy loss: -0.0016  | Total Loss: 2.81 | Total Steps: 9\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1404/129000  | Episode Reward: 7  | Average Reward 7.08  | Actor loss: 0.02 | Critic loss: 9.11 | Entropy loss: -0.0003  | Total Loss: 9.13 | Total Steps: 30\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1405/129000  | Episode Reward: 10  | Average Reward 7.08  | Actor loss: 1.29 | Critic loss: 8.11 | Entropy loss: -0.0020  | Total Loss: 9.39 | Total Steps: 10\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1406/129000  | Episode Reward: 7  | Average Reward 7.06  | Actor loss: 0.02 | Critic loss: 6.51 | Entropy loss: -0.0005  | Total Loss: 6.53 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.17. Model has been saved-----\n",
      "Training  | Episode: 1407/129000  | Episode Reward: 4  | Average Reward 7.17  | Actor loss: -0.04 | Critic loss: 8.34 | Entropy loss: -0.0015  | Total Loss: 8.30 | Total Steps: 45\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1408/129000  | Episode Reward: 7  | Average Reward 7.17  | Actor loss: -0.07 | Critic loss: 8.39 | Entropy loss: -0.0002  | Total Loss: 8.32 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1409/129000  | Episode Reward: 7  | Average Reward 7.17  | Actor loss: -0.15 | Critic loss: 6.70 | Entropy loss: -0.0011  | Total Loss: 6.55 | Total Steps: 42\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1410/129000  | Episode Reward: 10  | Average Reward 7.17  | Actor loss: 0.18 | Critic loss: 1.85 | Entropy loss: -0.0003  | Total Loss: 2.03 | Total Steps: 8\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.215. Model has been saved-----\n",
      "Training  | Episode: 1411/129000  | Episode Reward: 10  | Average Reward 7.21  | Actor loss: 0.17 | Critic loss: 3.59 | Entropy loss: -0.0005  | Total Loss: 3.76 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1412/129000  | Episode Reward: -5  | Average Reward 7.14  | Actor loss: -1.59 | Critic loss: 16.37 | Entropy loss: -0.0073  | Total Loss: 14.77 | Total Steps: 69\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1413/129000  | Episode Reward: 4  | Average Reward 7.11  | Actor loss: -0.05 | Critic loss: 4.31 | Entropy loss: -0.0003  | Total Loss: 4.26 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1414/129000  | Episode Reward: 7  | Average Reward 7.09  | Actor loss: 0.07 | Critic loss: 6.50 | Entropy loss: -0.0009  | Total Loss: 6.57 | Total Steps: 34\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1415/129000  | Episode Reward: 7  | Average Reward 7.14  | Actor loss: 0.25 | Critic loss: 8.97 | Entropy loss: -0.0007  | Total Loss: 9.22 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1416/129000  | Episode Reward: -2  | Average Reward 7.11  | Actor loss: -0.05 | Critic loss: 16.28 | Entropy loss: -0.0013  | Total Loss: 16.24 | Total Steps: 68\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1417/129000  | Episode Reward: 4  | Average Reward 7.08  | Actor loss: -0.08 | Critic loss: 9.83 | Entropy loss: -0.0005  | Total Loss: 9.75 | Total Steps: 47\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1418/129000  | Episode Reward: 7  | Average Reward 7.08  | Actor loss: 0.28 | Critic loss: 7.86 | Entropy loss: -0.0015  | Total Loss: 8.13 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1419/129000  | Episode Reward: 4  | Average Reward 7.07  | Actor loss: 0.12 | Critic loss: 6.60 | Entropy loss: -0.0012  | Total Loss: 6.73 | Total Steps: 48\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1420/129000  | Episode Reward: 1  | Average Reward 7.04  | Actor loss: -0.57 | Critic loss: 10.83 | Entropy loss: -0.0036  | Total Loss: 10.27 | Total Steps: 91\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1421/129000  | Episode Reward: 10  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 6.40 | Entropy loss: -0.0000  | Total Loss: 6.41 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1422/129000  | Episode Reward: 1  | Average Reward 7.00  | Actor loss: -0.12 | Critic loss: 13.54 | Entropy loss: -0.0007  | Total Loss: 13.42 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1423/129000  | Episode Reward: 10  | Average Reward 7.00  | Actor loss: 0.01 | Critic loss: 0.57 | Entropy loss: -0.0000  | Total Loss: 0.58 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1424/129000  | Episode Reward: 4  | Average Reward 6.97  | Actor loss: -0.82 | Critic loss: 12.32 | Entropy loss: -0.0080  | Total Loss: 11.49 | Total Steps: 62\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1425/129000  | Episode Reward: 7  | Average Reward 6.97  | Actor loss: -0.01 | Critic loss: 6.71 | Entropy loss: -0.0004  | Total Loss: 6.70 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1426/129000  | Episode Reward: 7  | Average Reward 6.99  | Actor loss: 0.08 | Critic loss: 8.08 | Entropy loss: -0.0006  | Total Loss: 8.16 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1427/129000  | Episode Reward: 4  | Average Reward 6.97  | Actor loss: -0.05 | Critic loss: 9.47 | Entropy loss: -0.0025  | Total Loss: 9.42 | Total Steps: 46\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1428/129000  | Episode Reward: 1  | Average Reward 6.93  | Actor loss: -0.21 | Critic loss: 12.15 | Entropy loss: -0.0016  | Total Loss: 11.94 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1429/129000  | Episode Reward: -2  | Average Reward 6.88  | Actor loss: -0.46 | Critic loss: 16.73 | Entropy loss: -0.0039  | Total Loss: 16.27 | Total Steps: 73\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1430/129000  | Episode Reward: 3  | Average Reward 6.85  | Actor loss: 0.20 | Critic loss: 8.79 | Entropy loss: -0.0036  | Total Loss: 8.99 | Total Steps: 46\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1431/129000  | Episode Reward: 10  | Average Reward 6.85  | Actor loss: 0.01 | Critic loss: 5.88 | Entropy loss: -0.0000  | Total Loss: 5.88 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1432/129000  | Episode Reward: 4  | Average Reward 6.83  | Actor loss: -0.00 | Critic loss: 8.27 | Entropy loss: -0.0070  | Total Loss: 8.26 | Total Steps: 61\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1433/129000  | Episode Reward: 7  | Average Reward 6.97  | Actor loss: 0.14 | Critic loss: 7.82 | Entropy loss: -0.0005  | Total Loss: 7.96 | Total Steps: 29\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1434/129000  | Episode Reward: 10  | Average Reward 7.05  | Actor loss: 0.10 | Critic loss: 5.28 | Entropy loss: -0.0004  | Total Loss: 5.38 | Total Steps: 30\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1435/129000  | Episode Reward: 1  | Average Reward 7.02  | Actor loss: -0.18 | Critic loss: 9.53 | Entropy loss: -0.0008  | Total Loss: 9.34 | Total Steps: 52\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1436/129000  | Episode Reward: 10  | Average Reward 7.04  | Actor loss: 0.04 | Critic loss: 7.27 | Entropy loss: -0.0000  | Total Loss: 7.30 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1437/129000  | Episode Reward: 7  | Average Reward 7.02  | Actor loss: 0.10 | Critic loss: 8.78 | Entropy loss: -0.0002  | Total Loss: 8.88 | Total Steps: 29\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1438/129000  | Episode Reward: 10  | Average Reward 7.04  | Actor loss: 0.09 | Critic loss: 4.60 | Entropy loss: -0.0005  | Total Loss: 4.69 | Total Steps: 32\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1439/129000  | Episode Reward: 10  | Average Reward 7.05  | Actor loss: 0.00 | Critic loss: 0.88 | Entropy loss: -0.0000  | Total Loss: 0.89 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1440/129000  | Episode Reward: 7  | Average Reward 7.07  | Actor loss: -0.10 | Critic loss: 4.68 | Entropy loss: -0.0008  | Total Loss: 4.58 | Total Steps: 43\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1441/129000  | Episode Reward: -8  | Average Reward 6.97  | Actor loss: -1.06 | Critic loss: 25.51 | Entropy loss: -0.0061  | Total Loss: 24.45 | Total Steps: 90\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1442/129000  | Episode Reward: 7  | Average Reward 6.99  | Actor loss: 0.29 | Critic loss: 2.80 | Entropy loss: -0.0018  | Total Loss: 3.09 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1443/129000  | Episode Reward: 6  | Average Reward 6.97  | Actor loss: -1.36 | Critic loss: 5.49 | Entropy loss: -0.0075  | Total Loss: 4.12 | Total Steps: 67\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1444/129000  | Episode Reward: 10  | Average Reward 6.97  | Actor loss: 0.24 | Critic loss: 18.47 | Entropy loss: -0.0001  | Total Loss: 18.71 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1445/129000  | Episode Reward: 1  | Average Reward 6.94  | Actor loss: 0.08 | Critic loss: 11.90 | Entropy loss: -0.0021  | Total Loss: 11.98 | Total Steps: 54\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1446/129000  | Episode Reward: 1  | Average Reward 6.89  | Actor loss: -0.01 | Critic loss: 10.43 | Entropy loss: -0.0017  | Total Loss: 10.42 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1447/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: -0.03 | Critic loss: 7.73 | Entropy loss: -0.0005  | Total Loss: 7.70 | Total Steps: 29\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1448/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: 0.06 | Critic loss: 3.87 | Entropy loss: -0.0003  | Total Loss: 3.93 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1449/129000  | Episode Reward: 4  | Average Reward 6.85  | Actor loss: -0.02 | Critic loss: 9.51 | Entropy loss: -0.0009  | Total Loss: 9.49 | Total Steps: 53\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1450/129000  | Episode Reward: 10  | Average Reward 6.88  | Actor loss: 0.09 | Critic loss: 3.99 | Entropy loss: -0.0003  | Total Loss: 4.08 | Total Steps: 29\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1451/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 0.96 | Critic loss: 5.14 | Entropy loss: -0.0019  | Total Loss: 6.10 | Total Steps: 13\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1452/129000  | Episode Reward: 1  | Average Reward 6.89  | Actor loss: -0.39 | Critic loss: 13.96 | Entropy loss: -0.0048  | Total Loss: 13.56 | Total Steps: 57\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1453/129000  | Episode Reward: 1  | Average Reward 6.85  | Actor loss: -0.73 | Critic loss: 9.41 | Entropy loss: -0.0024  | Total Loss: 8.69 | Total Steps: 53\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1454/129000  | Episode Reward: 10  | Average Reward 6.88  | Actor loss: 0.56 | Critic loss: 3.98 | Entropy loss: -0.0034  | Total Loss: 4.53 | Total Steps: 40\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1455/129000  | Episode Reward: 7  | Average Reward 6.89  | Actor loss: -0.25 | Critic loss: 5.66 | Entropy loss: -0.0059  | Total Loss: 5.40 | Total Steps: 56\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1456/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 0.01 | Critic loss: 1.52 | Entropy loss: -0.0000  | Total Loss: 1.53 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1457/129000  | Episode Reward: 7  | Average Reward 6.89  | Actor loss: 0.18 | Critic loss: 6.28 | Entropy loss: -0.0036  | Total Loss: 6.45 | Total Steps: 47\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1458/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: 0.07 | Critic loss: 4.80 | Entropy loss: -0.0003  | Total Loss: 4.87 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1459/129000  | Episode Reward: 1  | Average Reward 6.85  | Actor loss: -1.42 | Critic loss: 13.27 | Entropy loss: -0.0037  | Total Loss: 11.85 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1460/129000  | Episode Reward: 7  | Average Reward 6.87  | Actor loss: 0.19 | Critic loss: 6.29 | Entropy loss: -0.0009  | Total Loss: 6.47 | Total Steps: 30\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1461/129000  | Episode Reward: 10  | Average Reward 6.87  | Actor loss: 0.04 | Critic loss: 2.08 | Entropy loss: -0.0001  | Total Loss: 2.12 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1462/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 1.26 | Critic loss: 7.69 | Entropy loss: -0.0008  | Total Loss: 8.96 | Total Steps: 8\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1463/129000  | Episode Reward: 4  | Average Reward 6.88  | Actor loss: -0.08 | Critic loss: 7.24 | Entropy loss: -0.0015  | Total Loss: 7.16 | Total Steps: 39\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1464/129000  | Episode Reward: 4  | Average Reward 6.85  | Actor loss: -0.09 | Critic loss: 8.99 | Entropy loss: -0.0009  | Total Loss: 8.90 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1465/129000  | Episode Reward: 7  | Average Reward 6.85  | Actor loss: 0.04 | Critic loss: 2.67 | Entropy loss: -0.0024  | Total Loss: 2.71 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1466/129000  | Episode Reward: 7  | Average Reward 6.85  | Actor loss: 0.05 | Critic loss: 7.59 | Entropy loss: -0.0004  | Total Loss: 7.63 | Total Steps: 32\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1467/129000  | Episode Reward: 10  | Average Reward 6.91  | Actor loss: 0.02 | Critic loss: 3.35 | Entropy loss: -0.0005  | Total Loss: 3.37 | Total Steps: 53\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1468/129000  | Episode Reward: 10  | Average Reward 6.92  | Actor loss: -0.11 | Critic loss: 6.17 | Entropy loss: -0.0054  | Total Loss: 6.05 | Total Steps: 36\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1469/129000  | Episode Reward: 10  | Average Reward 6.92  | Actor loss: 0.16 | Critic loss: 2.52 | Entropy loss: -0.0002  | Total Loss: 2.68 | Total Steps: 8\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1470/129000  | Episode Reward: 7  | Average Reward 6.94  | Actor loss: 0.20 | Critic loss: 5.30 | Entropy loss: -0.0017  | Total Loss: 5.50 | Total Steps: 31\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1471/129000  | Episode Reward: 1  | Average Reward 6.91  | Actor loss: 0.02 | Critic loss: 11.16 | Entropy loss: -0.0007  | Total Loss: 11.18 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1472/129000  | Episode Reward: 1  | Average Reward 6.88  | Actor loss: -0.54 | Critic loss: 8.52 | Entropy loss: -0.0018  | Total Loss: 7.99 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1473/129000  | Episode Reward: 7  | Average Reward 6.89  | Actor loss: 0.05 | Critic loss: 6.64 | Entropy loss: -0.0022  | Total Loss: 6.68 | Total Steps: 44\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1474/129000  | Episode Reward: 7  | Average Reward 6.94  | Actor loss: -0.17 | Critic loss: 5.19 | Entropy loss: -0.0024  | Total Loss: 5.02 | Total Steps: 54\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1475/129000  | Episode Reward: 4  | Average Reward 6.92  | Actor loss: -0.16 | Critic loss: 8.65 | Entropy loss: -0.0011  | Total Loss: 8.49 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1476/129000  | Episode Reward: 7  | Average Reward 6.92  | Actor loss: 0.09 | Critic loss: 8.60 | Entropy loss: -0.0003  | Total Loss: 8.69 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1477/129000  | Episode Reward: 4  | Average Reward 6.91  | Actor loss: -0.27 | Critic loss: 8.99 | Entropy loss: -0.0032  | Total Loss: 8.71 | Total Steps: 55\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1478/129000  | Episode Reward: 4  | Average Reward 6.92  | Actor loss: -0.23 | Critic loss: 6.18 | Entropy loss: -0.0022  | Total Loss: 5.95 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1479/129000  | Episode Reward: 4  | Average Reward 6.91  | Actor loss: -0.29 | Critic loss: 6.37 | Entropy loss: -0.0070  | Total Loss: 6.07 | Total Steps: 67\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1480/129000  | Episode Reward: 10  | Average Reward 6.94  | Actor loss: 0.03 | Critic loss: 11.65 | Entropy loss: -0.0000  | Total Loss: 11.68 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1481/129000  | Episode Reward: 10  | Average Reward 6.94  | Actor loss: 0.01 | Critic loss: 8.05 | Entropy loss: -0.0000  | Total Loss: 8.06 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1482/129000  | Episode Reward: 7  | Average Reward 6.94  | Actor loss: 0.57 | Critic loss: 8.22 | Entropy loss: -0.0031  | Total Loss: 8.78 | Total Steps: 33\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1483/129000  | Episode Reward: 10  | Average Reward 6.96  | Actor loss: 0.10 | Critic loss: 3.93 | Entropy loss: -0.0001  | Total Loss: 4.03 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1484/129000  | Episode Reward: 7  | Average Reward 6.94  | Actor loss: -1.37 | Critic loss: 8.66 | Entropy loss: -0.0060  | Total Loss: 7.29 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1485/129000  | Episode Reward: 10  | Average Reward 6.94  | Actor loss: 0.04 | Critic loss: 1.21 | Entropy loss: -0.0001  | Total Loss: 1.25 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1486/129000  | Episode Reward: 3  | Average Reward 6.95  | Actor loss: -0.33 | Critic loss: 8.17 | Entropy loss: -0.0033  | Total Loss: 7.84 | Total Steps: 45\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1487/129000  | Episode Reward: 4  | Average Reward 6.93  | Actor loss: -0.29 | Critic loss: 5.92 | Entropy loss: -0.0085  | Total Loss: 5.62 | Total Steps: 56\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1488/129000  | Episode Reward: 7  | Average Reward 6.92  | Actor loss: -0.06 | Critic loss: 7.98 | Entropy loss: -0.0012  | Total Loss: 7.92 | Total Steps: 24\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1489/129000  | Episode Reward: 7  | Average Reward 6.92  | Actor loss: 0.12 | Critic loss: 5.50 | Entropy loss: -0.0009  | Total Loss: 5.62 | Total Steps: 32\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1490/129000  | Episode Reward: 4  | Average Reward 6.89  | Actor loss: -0.51 | Critic loss: 7.51 | Entropy loss: -0.0035  | Total Loss: 7.00 | Total Steps: 45\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1491/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 0.01 | Critic loss: 3.79 | Entropy loss: -0.0000  | Total Loss: 3.80 | Total Steps: 6\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1492/129000  | Episode Reward: 10  | Average Reward 6.95  | Actor loss: 0.02 | Critic loss: 10.32 | Entropy loss: -0.0000  | Total Loss: 10.33 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1493/129000  | Episode Reward: 4  | Average Reward 6.96  | Actor loss: -0.03 | Critic loss: 4.82 | Entropy loss: -0.0004  | Total Loss: 4.79 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1494/129000  | Episode Reward: 0  | Average Reward 6.96  | Actor loss: -0.62 | Critic loss: 15.80 | Entropy loss: -0.0031  | Total Loss: 15.18 | Total Steps: 53\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1495/129000  | Episode Reward: 10  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 1.35 | Entropy loss: -0.0000  | Total Loss: 1.36 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1496/129000  | Episode Reward: 10  | Average Reward 6.99  | Actor loss: 1.76 | Critic loss: 4.53 | Entropy loss: -0.0034  | Total Loss: 6.29 | Total Steps: 13\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1497/129000  | Episode Reward: 4  | Average Reward 7.00  | Actor loss: -0.38 | Critic loss: 5.88 | Entropy loss: -0.0051  | Total Loss: 5.50 | Total Steps: 58\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1498/129000  | Episode Reward: 7  | Average Reward 7.00  | Actor loss: -0.34 | Critic loss: 3.96 | Entropy loss: -0.0027  | Total Loss: 3.61 | Total Steps: 48\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1499/129000  | Episode Reward: 4  | Average Reward 7.00  | Actor loss: -0.32 | Critic loss: 7.68 | Entropy loss: -0.0022  | Total Loss: 7.35 | Total Steps: 51\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1500/129000  | Episode Reward: 4  | Average Reward 7.00  | Actor loss: 0.17 | Critic loss: 7.19 | Entropy loss: -0.0012  | Total Loss: 7.36 | Total Steps: 44\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1501/129000  | Episode Reward: 10  | Average Reward 7.00  | Actor loss: 0.31 | Critic loss: 2.37 | Entropy loss: -0.0005  | Total Loss: 2.67 | Total Steps: 8\n",
      "Model has been saved\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward 1.04  | Actor loss: 0.17 | Critic loss: 17.11 | Entropy loss: -0.0586  | Total Loss: 17.22 | Total Steps: 13\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 4  | Average Reward 1.04  | Actor loss: 0.01 | Critic loss: 0.70 | Entropy loss: -0.0078  | Total Loss: 0.71 | Total Steps: 43\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 4  | Average Reward 1.05  | Actor loss: 0.21 | Critic loss: 3.46 | Entropy loss: -0.0345  | Total Loss: 3.63 | Total Steps: 54\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.24 | Critic loss: 14.64 | Entropy loss: -0.0378  | Total Loss: 14.84 | Total Steps: 38\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 4  | Average Reward 1.05  | Actor loss: 0.02 | Critic loss: 2.30 | Entropy loss: -0.0014  | Total Loss: 2.31 | Total Steps: 46\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: -2  | Average Reward 0.99  | Actor loss: 0.09 | Critic loss: 7.01 | Entropy loss: -0.0422  | Total Loss: 7.07 | Total Steps: 67\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 7  | Average Reward 0.98  | Actor loss: 0.01 | Critic loss: 3.12 | Entropy loss: -0.0038  | Total Loss: 3.13 | Total Steps: 30\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward 1.01  | Actor loss: 0.01 | Critic loss: 1.30 | Entropy loss: -0.0167  | Total Loss: 1.29 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 4  | Average Reward 0.99  | Actor loss: 0.01 | Critic loss: 1.43 | Entropy loss: -0.0058  | Total Loss: 1.43 | Total Steps: 44\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward 1.02  | Actor loss: 0.04 | Critic loss: 3.13 | Entropy loss: -0.0023  | Total Loss: 3.17 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.01 | Critic loss: 3.80 | Entropy loss: -0.0336  | Total Loss: 3.78 | Total Steps: 9\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 10  | Average Reward 1.07  | Actor loss: 0.01 | Critic loss: 5.61 | Entropy loss: -0.0060  | Total Loss: 5.61 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 1  | Average Reward 1.02  | Actor loss: 0.00 | Critic loss: 1.80 | Entropy loss: -0.0225  | Total Loss: 1.78 | Total Steps: 52\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.51 | Critic loss: 2.47 | Entropy loss: -0.0299  | Total Loss: 2.94 | Total Steps: 67\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 7  | Average Reward 1.02  | Actor loss: 0.01 | Critic loss: 1.79 | Entropy loss: -0.0088  | Total Loss: 1.79 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.00 | Critic loss: 1.12 | Entropy loss: -0.0025  | Total Loss: 1.13 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.01 | Critic loss: 2.48 | Entropy loss: -0.0075  | Total Loss: 2.48 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 4  | Average Reward 1.02  | Actor loss: 1.96 | Critic loss: 7.89 | Entropy loss: -0.0102  | Total Loss: 9.84 | Total Steps: 51\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 1.02  | Actor loss: 0.00 | Critic loss: 2.25 | Entropy loss: -0.0040  | Total Loss: 2.25 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward 1.36  | Actor loss: 0.01 | Critic loss: 4.40 | Entropy loss: -0.0010  | Total Loss: 4.40 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 1  | Average Reward 1.35  | Actor loss: 0.22 | Critic loss: 2.62 | Entropy loss: -0.0046  | Total Loss: 2.84 | Total Steps: 54\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 7  | Average Reward 1.33  | Actor loss: 0.00 | Critic loss: 3.18 | Entropy loss: -0.0288  | Total Loss: 3.16 | Total Steps: 32\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 4  | Average Reward 1.34  | Actor loss: 0.00 | Critic loss: 3.83 | Entropy loss: -0.0047  | Total Loss: 3.82 | Total Steps: 43\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 1  | Average Reward 1.29  | Actor loss: 0.64 | Critic loss: 2.67 | Entropy loss: -0.0402  | Total Loss: 3.27 | Total Steps: 57\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 10  | Average Reward 1.29  | Actor loss: 0.47 | Critic loss: 11.07 | Entropy loss: -0.0115  | Total Loss: 11.52 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 1  | Average Reward 1.25  | Actor loss: 0.32 | Critic loss: 6.85 | Entropy loss: -0.0118  | Total Loss: 7.15 | Total Steps: 50\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: -5  | Average Reward 1.44  | Actor loss: 0.11 | Critic loss: 1.79 | Entropy loss: -0.0198  | Total Loss: 1.88 | Total Steps: 89\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward 1.41  | Actor loss: 0.26 | Critic loss: 2.33 | Entropy loss: -0.0161  | Total Loss: 2.57 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 7  | Average Reward 1.40  | Actor loss: 0.00 | Critic loss: 4.36 | Entropy loss: -0.0203  | Total Loss: 4.34 | Total Steps: 39\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward 1.40  | Actor loss: 0.00 | Critic loss: 3.05 | Entropy loss: -0.0111  | Total Loss: 3.04 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Average Reward 1.44  | Actor loss: 4.15 | Critic loss: 13.99 | Entropy loss: -0.0280  | Total Loss: 18.12 | Total Steps: 11\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward 1.47  | Actor loss: 0.46 | Critic loss: 11.14 | Entropy loss: -0.0092  | Total Loss: 11.59 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 4  | Average Reward 1.46  | Actor loss: 0.01 | Critic loss: 1.06 | Entropy loss: -0.0015  | Total Loss: 1.07 | Total Steps: 49\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 4  | Average Reward 1.44  | Actor loss: 0.00 | Critic loss: 3.26 | Entropy loss: -0.0018  | Total Loss: 3.26 | Total Steps: 43\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: -2  | Average Reward 1.43  | Actor loss: 0.07 | Critic loss: 7.63 | Entropy loss: -0.0289  | Total Loss: 7.67 | Total Steps: 63\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 7  | Average Reward 1.62  | Actor loss: 0.04 | Critic loss: 7.15 | Entropy loss: -0.0189  | Total Loss: 7.17 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 10  | Average Reward 1.65  | Actor loss: 0.04 | Critic loss: 1.22 | Entropy loss: -0.0122  | Total Loss: 1.25 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Average Reward 1.65  | Actor loss: 0.03 | Critic loss: 2.42 | Entropy loss: -0.0087  | Total Loss: 2.44 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward 1.65  | Actor loss: 0.00 | Critic loss: 5.21 | Entropy loss: -0.0018  | Total Loss: 5.21 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 1.68  | Actor loss: 0.05 | Critic loss: 3.70 | Entropy loss: -0.0021  | Total Loss: 3.75 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward 1.65  | Actor loss: 0.65 | Critic loss: 1.83 | Entropy loss: -0.0196  | Total Loss: 2.46 | Total Steps: 51\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 10  | Average Reward 1.70  | Actor loss: 0.01 | Critic loss: 2.33 | Entropy loss: -0.0038  | Total Loss: 2.33 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: -11  | Average Reward 1.89  | Actor loss: 0.61 | Critic loss: 14.62 | Entropy loss: -0.0336  | Total Loss: 15.20 | Total Steps: 247\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 1.87  | Actor loss: 7.14 | Critic loss: 8.85 | Entropy loss: -0.0311  | Total Loss: 15.96 | Total Steps: 99\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 4  | Average Reward 1.84  | Actor loss: 0.03 | Critic loss: 3.46 | Entropy loss: -0.0114  | Total Loss: 3.48 | Total Steps: 50\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 4  | Average Reward 1.84  | Actor loss: 0.04 | Critic loss: 1.05 | Entropy loss: -0.0181  | Total Loss: 1.07 | Total Steps: 56\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward 1.84  | Actor loss: 0.01 | Critic loss: 6.90 | Entropy loss: -0.0034  | Total Loss: 6.90 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 1.84  | Actor loss: 0.05 | Critic loss: 1.94 | Entropy loss: -0.0365  | Total Loss: 1.96 | Total Steps: 14\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 1.85  | Actor loss: 1.74 | Critic loss: 11.21 | Entropy loss: -0.0139  | Total Loss: 12.93 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 10  | Average Reward 1.90  | Actor loss: 0.87 | Critic loss: 9.48 | Entropy loss: -0.0074  | Total Loss: 10.34 | Total Steps: 39\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 7  | Average Reward 1.93  | Actor loss: 0.03 | Critic loss: 1.91 | Entropy loss: -0.0077  | Total Loss: 1.94 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward 1.95  | Actor loss: 0.00 | Critic loss: 4.83 | Entropy loss: -0.0007  | Total Loss: 4.84 | Total Steps: 31\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: -2  | Average Reward 1.98  | Actor loss: 0.00 | Critic loss: 12.60 | Entropy loss: -0.0042  | Total Loss: 12.60 | Total Steps: 68\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Average Reward 1.96  | Actor loss: 0.00 | Critic loss: 2.55 | Entropy loss: -0.0108  | Total Loss: 2.54 | Total Steps: 64\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Average Reward 1.99  | Actor loss: 0.01 | Critic loss: 14.44 | Entropy loss: -0.0013  | Total Loss: 14.44 | Total Steps: 29\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 3  | Average Reward 1.99  | Actor loss: 0.04 | Critic loss: 2.74 | Entropy loss: -0.0251  | Total Loss: 2.76 | Total Steps: 66\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 1.99  | Actor loss: 0.00 | Critic loss: 1.55 | Entropy loss: -0.0063  | Total Loss: 1.54 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 2.02  | Actor loss: 0.66 | Critic loss: 11.21 | Entropy loss: -0.0341  | Total Loss: 11.84 | Total Steps: 60\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 4  | Average Reward 2.00  | Actor loss: 0.03 | Critic loss: 1.12 | Entropy loss: -0.0196  | Total Loss: 1.13 | Total Steps: 48\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 7  | Average Reward 2.02  | Actor loss: 0.82 | Critic loss: 12.54 | Entropy loss: -0.0321  | Total Loss: 13.33 | Total Steps: 27\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Average Reward 2.03  | Actor loss: 0.13 | Critic loss: 2.71 | Entropy loss: -0.0059  | Total Loss: 2.82 | Total Steps: 186\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 1  | Average Reward 2.04  | Actor loss: 0.00 | Critic loss: 1.59 | Entropy loss: -0.0349  | Total Loss: 1.56 | Total Steps: 64\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 4  | Average Reward 2.03  | Actor loss: 0.00 | Critic loss: 3.62 | Entropy loss: -0.0045  | Total Loss: 3.62 | Total Steps: 47\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward 2.03  | Actor loss: 0.00 | Critic loss: 1.53 | Entropy loss: -0.0033  | Total Loss: 1.53 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward 2.02  | Actor loss: 0.01 | Critic loss: 1.63 | Entropy loss: -0.0072  | Total Loss: 1.63 | Total Steps: 47\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward 2.02  | Actor loss: 0.26 | Critic loss: 5.18 | Entropy loss: -0.0147  | Total Loss: 5.42 | Total Steps: 25\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 7  | Average Reward 2.00  | Actor loss: 0.01 | Critic loss: 2.70 | Entropy loss: -0.0010  | Total Loss: 2.71 | Total Steps: 38\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 4  | Average Reward 1.99  | Actor loss: 0.01 | Critic loss: 2.29 | Entropy loss: -0.0014  | Total Loss: 2.29 | Total Steps: 42\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Average Reward 2.02  | Actor loss: 0.17 | Critic loss: 6.93 | Entropy loss: -0.0105  | Total Loss: 7.09 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 2.02  | Actor loss: 0.00 | Critic loss: 1.55 | Entropy loss: -0.0038  | Total Loss: 1.54 | Total Steps: 6\n",
      "TEST: ---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 7  | Average Reward 2.00  | Actor loss: 0.21 | Critic loss: 4.32 | Entropy loss: -0.0036  | Total Loss: 4.53 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 0  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 0.51 | Entropy loss: -0.0227  | Total Loss: 0.49 | Total Steps: 107\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 1.98  | Actor loss: 0.02 | Critic loss: 3.88 | Entropy loss: -0.0206  | Total Loss: 3.88 | Total Steps: 32\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 1.98  | Actor loss: 0.00 | Critic loss: 4.83 | Entropy loss: -0.0011  | Total Loss: 4.84 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 1.98  | Actor loss: 0.00 | Critic loss: 1.60 | Entropy loss: -0.0057  | Total Loss: 1.60 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 0.51 | Entropy loss: -0.0301  | Total Loss: 0.49 | Total Steps: 58\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Average Reward 1.97  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0063  | Total Loss: 1.15 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 7  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 1.33 | Entropy loss: -0.0079  | Total Loss: 1.33 | Total Steps: 38\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: -2  | Average Reward 1.89  | Actor loss: 0.09 | Critic loss: 7.28 | Entropy loss: -0.0183  | Total Loss: 7.36 | Total Steps: 103\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 10  | Average Reward 2.26  | Actor loss: 0.01 | Critic loss: 3.54 | Entropy loss: -0.0011  | Total Loss: 3.54 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 2.26  | Actor loss: 0.03 | Critic loss: 1.41 | Entropy loss: -0.0138  | Total Loss: 1.43 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward 2.27  | Actor loss: 0.02 | Critic loss: 4.41 | Entropy loss: -0.0088  | Total Loss: 4.42 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 1  | Average Reward 2.26  | Actor loss: 0.03 | Critic loss: 1.70 | Entropy loss: -0.0020  | Total Loss: 1.72 | Total Steps: 53\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 6  | Average Reward 2.27  | Actor loss: 0.02 | Critic loss: 1.51 | Entropy loss: -0.0259  | Total Loss: 1.50 | Total Steps: 70\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 10  | Average Reward 2.31  | Actor loss: 0.07 | Critic loss: 4.18 | Entropy loss: -0.0065  | Total Loss: 4.24 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward 2.30  | Actor loss: 0.01 | Critic loss: 1.79 | Entropy loss: -0.0080  | Total Loss: 1.79 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 4  | Average Reward 2.29  | Actor loss: 0.00 | Critic loss: 2.66 | Entropy loss: -0.0016  | Total Loss: 2.66 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 1  | Average Reward 2.24  | Actor loss: 2.46 | Critic loss: 15.86 | Entropy loss: -0.0416  | Total Loss: 18.27 | Total Steps: 148\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.02 | Critic loss: 2.37 | Entropy loss: -0.0686  | Total Loss: 2.32 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 4  | Average Reward 2.23  | Actor loss: 0.29 | Critic loss: 8.29 | Entropy loss: -0.0102  | Total Loss: 8.56 | Total Steps: 50\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Average Reward 2.27  | Actor loss: 0.01 | Critic loss: 2.17 | Entropy loss: -0.0021  | Total Loss: 2.17 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 1  | Average Reward 2.25  | Actor loss: 0.01 | Critic loss: 2.59 | Entropy loss: -0.0094  | Total Loss: 2.59 | Total Steps: 55\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: -38  | Average Reward 2.06  | Actor loss: 0.10 | Critic loss: 10.27 | Entropy loss: -0.0317  | Total Loss: 10.34 | Total Steps: 438\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 10  | Average Reward 2.06  | Actor loss: 0.02 | Critic loss: 1.72 | Entropy loss: -0.0073  | Total Loss: 1.73 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: -5  | Average Reward 2.00  | Actor loss: 0.79 | Critic loss: 6.97 | Entropy loss: -0.0215  | Total Loss: 7.74 | Total Steps: 64\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 10  | Average Reward 2.00  | Actor loss: 0.46 | Critic loss: 11.14 | Entropy loss: -0.0081  | Total Loss: 11.59 | Total Steps: 6\n",
      "TEST: ---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 10  | Average Reward 2.00  | Actor loss: 0.00 | Critic loss: 2.22 | Entropy loss: -0.0030  | Total Loss: 2.22 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward 2.00  | Actor loss: 0.21 | Critic loss: 4.37 | Entropy loss: -0.0036  | Total Loss: 4.58 | Total Steps: 31\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 1  | Average Reward 1.97  | Actor loss: 5.68 | Critic loss: 2.03 | Entropy loss: -0.0160  | Total Loss: 7.69 | Total Steps: 54\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 4  | Average Reward 1.96  | Actor loss: 1.17 | Critic loss: 14.00 | Entropy loss: -0.0222  | Total Loss: 15.15 | Total Steps: 59\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1502/129000  | Episode Reward: 10  | Average Reward 7.00  | Actor loss: 0.32 | Critic loss: 2.43 | Entropy loss: -0.0004  | Total Loss: 2.75 | Total Steps: 8\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1503/129000  | Episode Reward: 1  | Average Reward 6.96  | Actor loss: -0.43 | Critic loss: 11.11 | Entropy loss: -0.0018  | Total Loss: 10.68 | Total Steps: 52\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1504/129000  | Episode Reward: 7  | Average Reward 6.95  | Actor loss: 0.17 | Critic loss: 5.46 | Entropy loss: -0.0037  | Total Loss: 5.62 | Total Steps: 43\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1505/129000  | Episode Reward: 6  | Average Reward 6.93  | Actor loss: -0.22 | Critic loss: 4.94 | Entropy loss: -0.0050  | Total Loss: 4.72 | Total Steps: 58\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1506/129000  | Episode Reward: 4  | Average Reward 6.93  | Actor loss: -0.02 | Critic loss: 5.53 | Entropy loss: -0.0005  | Total Loss: 5.51 | Total Steps: 53\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1507/129000  | Episode Reward: 10  | Average Reward 6.98  | Actor loss: 0.01 | Critic loss: 2.88 | Entropy loss: -0.0000  | Total Loss: 2.89 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1508/129000  | Episode Reward: 4  | Average Reward 6.95  | Actor loss: -1.29 | Critic loss: 9.17 | Entropy loss: -0.0064  | Total Loss: 7.88 | Total Steps: 44\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1509/129000  | Episode Reward: 10  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 1.55 | Entropy loss: -0.0000  | Total Loss: 1.56 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1510/129000  | Episode Reward: 10  | Average Reward 6.96  | Actor loss: 0.02 | Critic loss: 1.95 | Entropy loss: -0.0000  | Total Loss: 1.97 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1511/129000  | Episode Reward: 7  | Average Reward 6.95  | Actor loss: 0.27 | Critic loss: 7.73 | Entropy loss: -0.0015  | Total Loss: 8.00 | Total Steps: 30\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1512/129000  | Episode Reward: 1  | Average Reward 6.92  | Actor loss: -0.13 | Critic loss: 9.45 | Entropy loss: -0.0006  | Total Loss: 9.32 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1513/129000  | Episode Reward: 4  | Average Reward 6.89  | Actor loss: -0.05 | Critic loss: 8.80 | Entropy loss: -0.0011  | Total Loss: 8.76 | Total Steps: 51\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1514/129000  | Episode Reward: 10  | Average Reward 6.92  | Actor loss: 0.07 | Critic loss: 0.99 | Entropy loss: -0.0001  | Total Loss: 1.06 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1515/129000  | Episode Reward: 4  | Average Reward 6.89  | Actor loss: -0.57 | Critic loss: 5.59 | Entropy loss: -0.0078  | Total Loss: 5.02 | Total Steps: 62\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1516/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: -0.01 | Critic loss: 3.06 | Entropy loss: -0.0004  | Total Loss: 3.05 | Total Steps: 43\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1517/129000  | Episode Reward: 4  | Average Reward 6.86  | Actor loss: -0.54 | Critic loss: 9.11 | Entropy loss: -0.0032  | Total Loss: 8.56 | Total Steps: 40\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1518/129000  | Episode Reward: 10  | Average Reward 6.91  | Actor loss: 0.01 | Critic loss: 2.22 | Entropy loss: -0.0000  | Total Loss: 2.23 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1519/129000  | Episode Reward: 10  | Average Reward 6.91  | Actor loss: -0.06 | Critic loss: 2.65 | Entropy loss: -0.0048  | Total Loss: 2.59 | Total Steps: 49\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1520/129000  | Episode Reward: 4  | Average Reward 6.91  | Actor loss: -0.33 | Critic loss: 6.06 | Entropy loss: -0.0044  | Total Loss: 5.73 | Total Steps: 48\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1521/129000  | Episode Reward: 7  | Average Reward 6.89  | Actor loss: -0.26 | Critic loss: 3.06 | Entropy loss: -0.0109  | Total Loss: 2.79 | Total Steps: 82\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1522/129000  | Episode Reward: 10  | Average Reward 6.89  | Actor loss: 0.00 | Critic loss: 1.39 | Entropy loss: -0.0000  | Total Loss: 1.39 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1523/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: 0.17 | Critic loss: 4.04 | Entropy loss: -0.0009  | Total Loss: 4.20 | Total Steps: 34\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1524/129000  | Episode Reward: 10  | Average Reward 6.88  | Actor loss: 0.00 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1525/129000  | Episode Reward: 1  | Average Reward 6.84  | Actor loss: -0.10 | Critic loss: 8.59 | Entropy loss: -0.0010  | Total Loss: 8.49 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1526/129000  | Episode Reward: 1  | Average Reward 6.80  | Actor loss: -0.08 | Critic loss: 10.87 | Entropy loss: -0.0009  | Total Loss: 10.79 | Total Steps: 54\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1527/129000  | Episode Reward: 10  | Average Reward 6.82  | Actor loss: 0.00 | Critic loss: 0.89 | Entropy loss: -0.0000  | Total Loss: 0.89 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1528/129000  | Episode Reward: 10  | Average Reward 6.82  | Actor loss: 0.01 | Critic loss: 6.71 | Entropy loss: -0.0000  | Total Loss: 6.72 | Total Steps: 6\n",
      "---blue---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1529/129000  | Episode Reward: 10  | Average Reward 6.84  | Actor loss: 0.01 | Critic loss: 2.20 | Entropy loss: -0.0000  | Total Loss: 2.21 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1530/129000  | Episode Reward: 10  | Average Reward 6.88  | Actor loss: 1.27 | Critic loss: 5.03 | Entropy loss: -0.0022  | Total Loss: 6.30 | Total Steps: 10\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1531/129000  | Episode Reward: 3  | Average Reward 6.85  | Actor loss: -0.48 | Critic loss: 8.09 | Entropy loss: -0.0035  | Total Loss: 7.60 | Total Steps: 55\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1532/129000  | Episode Reward: 10  | Average Reward 6.85  | Actor loss: 0.02 | Critic loss: 1.41 | Entropy loss: -0.0000  | Total Loss: 1.43 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1533/129000  | Episode Reward: 10  | Average Reward 6.87  | Actor loss: 0.61 | Critic loss: 0.82 | Entropy loss: -0.0013  | Total Loss: 1.43 | Total Steps: 9\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1534/129000  | Episode Reward: 7  | Average Reward 6.88  | Actor loss: 0.14 | Critic loss: 8.72 | Entropy loss: -0.0006  | Total Loss: 8.86 | Total Steps: 29\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1535/129000  | Episode Reward: 4  | Average Reward 6.85  | Actor loss: -0.10 | Critic loss: 3.90 | Entropy loss: -0.0004  | Total Loss: 3.81 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1536/129000  | Episode Reward: 1  | Average Reward 6.80  | Actor loss: -0.16 | Critic loss: 8.84 | Entropy loss: -0.0006  | Total Loss: 8.67 | Total Steps: 51\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1537/129000  | Episode Reward: 7  | Average Reward 6.79  | Actor loss: 0.02 | Critic loss: 9.10 | Entropy loss: -0.0002  | Total Loss: 9.13 | Total Steps: 29\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1538/129000  | Episode Reward: 7  | Average Reward 6.80  | Actor loss: 0.06 | Critic loss: 7.09 | Entropy loss: -0.0002  | Total Loss: 7.15 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1539/129000  | Episode Reward: 7  | Average Reward 6.80  | Actor loss: 0.30 | Critic loss: 2.12 | Entropy loss: -0.0025  | Total Loss: 2.41 | Total Steps: 36\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1540/129000  | Episode Reward: 7  | Average Reward 6.80  | Actor loss: 0.04 | Critic loss: 6.47 | Entropy loss: -0.0001  | Total Loss: 6.51 | Total Steps: 29\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1541/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.02 | Critic loss: 0.44 | Entropy loss: -0.0001  | Total Loss: 0.47 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1542/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.00 | Critic loss: 0.86 | Entropy loss: -0.0000  | Total Loss: 0.86 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1543/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.01 | Critic loss: 0.42 | Entropy loss: -0.0000  | Total Loss: 0.43 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1544/129000  | Episode Reward: 10  | Average Reward 6.82  | Actor loss: 0.64 | Critic loss: 1.07 | Entropy loss: -0.0020  | Total Loss: 1.70 | Total Steps: 10\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1545/129000  | Episode Reward: 4  | Average Reward 6.83  | Actor loss: -0.22 | Critic loss: 8.58 | Entropy loss: -0.0018  | Total Loss: 8.36 | Total Steps: 54\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1546/129000  | Episode Reward: 10  | Average Reward 6.87  | Actor loss: 0.02 | Critic loss: 0.49 | Entropy loss: -0.0000  | Total Loss: 0.50 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1547/129000  | Episode Reward: 10  | Average Reward 6.87  | Actor loss: -0.27 | Critic loss: 2.49 | Entropy loss: -0.0103  | Total Loss: 2.21 | Total Steps: 63\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1548/129000  | Episode Reward: 4  | Average Reward 6.83  | Actor loss: 0.04 | Critic loss: 3.70 | Entropy loss: -0.0025  | Total Loss: 3.74 | Total Steps: 43\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1549/129000  | Episode Reward: 7  | Average Reward 6.82  | Actor loss: 0.05 | Critic loss: 5.95 | Entropy loss: -0.0028  | Total Loss: 6.00 | Total Steps: 32\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1550/129000  | Episode Reward: 10  | Average Reward 6.82  | Actor loss: -0.31 | Critic loss: 2.78 | Entropy loss: -0.0055  | Total Loss: 2.47 | Total Steps: 58\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1551/129000  | Episode Reward: 10  | Average Reward 6.82  | Actor loss: -0.17 | Critic loss: 2.51 | Entropy loss: -0.0059  | Total Loss: 2.33 | Total Steps: 47\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1552/129000  | Episode Reward: 10  | Average Reward 6.82  | Actor loss: 0.01 | Critic loss: 1.16 | Entropy loss: -0.0000  | Total Loss: 1.17 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1553/129000  | Episode Reward: 7  | Average Reward 6.82  | Actor loss: 0.02 | Critic loss: 6.92 | Entropy loss: -0.0002  | Total Loss: 6.95 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1554/129000  | Episode Reward: 3  | Average Reward 6.79  | Actor loss: -0.59 | Critic loss: 9.95 | Entropy loss: -0.0061  | Total Loss: 9.36 | Total Steps: 60\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1555/129000  | Episode Reward: 10  | Average Reward 6.80  | Actor loss: 0.21 | Critic loss: 2.20 | Entropy loss: -0.0002  | Total Loss: 2.40 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1556/129000  | Episode Reward: 1  | Average Reward 6.75  | Actor loss: -1.29 | Critic loss: 13.00 | Entropy loss: -0.0044  | Total Loss: 11.70 | Total Steps: 59\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1557/129000  | Episode Reward: 7  | Average Reward 6.74  | Actor loss: 0.09 | Critic loss: 6.59 | Entropy loss: -0.0018  | Total Loss: 6.68 | Total Steps: 39\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1558/129000  | Episode Reward: 10  | Average Reward 6.74  | Actor loss: 0.08 | Critic loss: 2.18 | Entropy loss: -0.0033  | Total Loss: 2.26 | Total Steps: 42\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1559/129000  | Episode Reward: 7  | Average Reward 6.72  | Actor loss: 0.05 | Critic loss: 5.21 | Entropy loss: -0.0003  | Total Loss: 5.26 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1560/129000  | Episode Reward: 10  | Average Reward 6.77  | Actor loss: 0.07 | Critic loss: 0.65 | Entropy loss: -0.0001  | Total Loss: 0.73 | Total Steps: 6\n",
      "---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1561/129000  | Episode Reward: 4  | Average Reward 6.74  | Actor loss: -0.72 | Critic loss: 5.45 | Entropy loss: -0.0058  | Total Loss: 4.72 | Total Steps: 59\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1562/129000  | Episode Reward: 10  | Average Reward 6.74  | Actor loss: 0.48 | Critic loss: 2.43 | Entropy loss: -0.0015  | Total Loss: 2.90 | Total Steps: 13\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1563/129000  | Episode Reward: 7  | Average Reward 6.72  | Actor loss: 0.08 | Critic loss: 7.39 | Entropy loss: -0.0006  | Total Loss: 7.47 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1564/129000  | Episode Reward: 4  | Average Reward 6.71  | Actor loss: -0.07 | Critic loss: 8.67 | Entropy loss: -0.0004  | Total Loss: 8.60 | Total Steps: 42\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1565/129000  | Episode Reward: 10  | Average Reward 6.71  | Actor loss: 0.37 | Critic loss: 2.04 | Entropy loss: -0.0014  | Total Loss: 2.41 | Total Steps: 14\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1566/129000  | Episode Reward: 10  | Average Reward 6.72  | Actor loss: -0.04 | Critic loss: 1.32 | Entropy loss: -0.0006  | Total Loss: 1.28 | Total Steps: 38\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1567/129000  | Episode Reward: 10  | Average Reward 6.72  | Actor loss: 0.01 | Critic loss: 3.65 | Entropy loss: -0.0000  | Total Loss: 3.66 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1568/129000  | Episode Reward: 10  | Average Reward 6.75  | Actor loss: 0.14 | Critic loss: 1.49 | Entropy loss: -0.0002  | Total Loss: 1.62 | Total Steps: 8\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1569/129000  | Episode Reward: 4  | Average Reward 6.77  | Actor loss: -0.09 | Critic loss: 7.76 | Entropy loss: -0.0005  | Total Loss: 7.67 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1570/129000  | Episode Reward: 10  | Average Reward 6.77  | Actor loss: 0.01 | Critic loss: 0.38 | Entropy loss: -0.0000  | Total Loss: 0.39 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1571/129000  | Episode Reward: 4  | Average Reward 6.74  | Actor loss: 0.01 | Critic loss: 8.43 | Entropy loss: -0.0007  | Total Loss: 8.44 | Total Steps: 53\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1572/129000  | Episode Reward: 10  | Average Reward 6.74  | Actor loss: 0.57 | Critic loss: 1.81 | Entropy loss: -0.0017  | Total Loss: 2.37 | Total Steps: 10\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1573/129000  | Episode Reward: 4  | Average Reward 6.74  | Actor loss: -0.06 | Critic loss: 4.98 | Entropy loss: -0.0004  | Total Loss: 4.91 | Total Steps: 47\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1574/129000  | Episode Reward: 4  | Average Reward 6.71  | Actor loss: -0.30 | Critic loss: 7.04 | Entropy loss: -0.0019  | Total Loss: 6.74 | Total Steps: 51\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1575/129000  | Episode Reward: 10  | Average Reward 6.74  | Actor loss: 0.05 | Critic loss: 5.30 | Entropy loss: -0.0002  | Total Loss: 5.35 | Total Steps: 29\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1576/129000  | Episode Reward: 10  | Average Reward 6.74  | Actor loss: 0.01 | Critic loss: 0.69 | Entropy loss: -0.0000  | Total Loss: 0.69 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1577/129000  | Episode Reward: -2  | Average Reward 6.68  | Actor loss: -1.27 | Critic loss: 14.37 | Entropy loss: -0.0049  | Total Loss: 13.10 | Total Steps: 57\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1578/129000  | Episode Reward: 7  | Average Reward 6.67  | Actor loss: 0.08 | Critic loss: 8.55 | Entropy loss: -0.0004  | Total Loss: 8.63 | Total Steps: 30\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1579/129000  | Episode Reward: 10  | Average Reward 6.68  | Actor loss: 0.01 | Critic loss: 1.49 | Entropy loss: -0.0000  | Total Loss: 1.49 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1580/129000  | Episode Reward: 7  | Average Reward 6.68  | Actor loss: 0.02 | Critic loss: 2.64 | Entropy loss: -0.0004  | Total Loss: 2.66 | Total Steps: 34\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1581/129000  | Episode Reward: 4  | Average Reward 6.65  | Actor loss: -0.23 | Critic loss: 6.02 | Entropy loss: -0.0015  | Total Loss: 5.79 | Total Steps: 49\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1582/129000  | Episode Reward: 7  | Average Reward 6.63  | Actor loss: -0.97 | Critic loss: 5.36 | Entropy loss: -0.0023  | Total Loss: 4.39 | Total Steps: 30\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1583/129000  | Episode Reward: 4  | Average Reward 6.61  | Actor loss: -0.69 | Critic loss: 8.96 | Entropy loss: -0.0078  | Total Loss: 8.27 | Total Steps: 54\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1584/129000  | Episode Reward: 1  | Average Reward 6.56  | Actor loss: -0.41 | Critic loss: 8.16 | Entropy loss: -0.0039  | Total Loss: 7.75 | Total Steps: 52\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1585/129000  | Episode Reward: 10  | Average Reward 6.56  | Actor loss: -0.01 | Critic loss: 3.33 | Entropy loss: -0.0004  | Total Loss: 3.31 | Total Steps: 31\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1586/129000  | Episode Reward: 4  | Average Reward 6.53  | Actor loss: -0.12 | Critic loss: 8.81 | Entropy loss: -0.0011  | Total Loss: 8.69 | Total Steps: 44\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1587/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.23 | Critic loss: 4.77 | Entropy loss: -0.0009  | Total Loss: 4.99 | Total Steps: 34\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1588/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.24 | Critic loss: 10.14 | Entropy loss: -0.0017  | Total Loss: 9.89 | Total Steps: 45\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1589/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.43 | Critic loss: 4.98 | Entropy loss: -0.0048  | Total Loss: 4.55 | Total Steps: 58\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1590/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0000  | Total Loss: 1.67 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1591/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: -0.16 | Critic loss: 5.46 | Entropy loss: -0.0008  | Total Loss: 5.30 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1592/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.20 | Critic loss: 2.19 | Entropy loss: -0.0011  | Total Loss: 1.99 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1593/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.01 | Critic loss: 0.69 | Entropy loss: -0.0000  | Total Loss: 0.69 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1594/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: 0.09 | Critic loss: 6.37 | Entropy loss: -0.0014  | Total Loss: 6.45 | Total Steps: 39\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1595/129000  | Episode Reward: 10  | Average Reward 6.53  | Actor loss: 0.01 | Critic loss: 1.62 | Entropy loss: -0.0000  | Total Loss: 1.64 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1596/129000  | Episode Reward: 1  | Average Reward 6.51  | Actor loss: -0.06 | Critic loss: 8.72 | Entropy loss: -0.0006  | Total Loss: 8.66 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1597/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: -0.52 | Critic loss: 5.28 | Entropy loss: -0.0012  | Total Loss: 4.75 | Total Steps: 34\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1598/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.43 | Critic loss: 4.04 | Entropy loss: -0.0033  | Total Loss: 4.47 | Total Steps: 36\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1599/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.69 | Critic loss: 8.87 | Entropy loss: -0.0040  | Total Loss: 8.17 | Total Steps: 48\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1600/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 0.48 | Entropy loss: -0.0000  | Total Loss: 0.49 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1601/129000  | Episode Reward: -2  | Average Reward 6.46  | Actor loss: -0.85 | Critic loss: 14.04 | Entropy loss: -0.0044  | Total Loss: 13.18 | Total Steps: 48\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: -2  | Average Reward 1.90  | Actor loss: 0.00 | Critic loss: 4.03 | Entropy loss: -0.0094  | Total Loss: 4.02 | Total Steps: 54\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: -8  | Average Reward 1.80  | Actor loss: 0.10 | Critic loss: 17.04 | Entropy loss: -0.0128  | Total Loss: 17.13 | Total Steps: 73\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 4  | Average Reward 1.80  | Actor loss: 0.12 | Critic loss: 8.28 | Entropy loss: -0.0090  | Total Loss: 8.39 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 10  | Average Reward 1.82  | Actor loss: 0.00 | Critic loss: 3.07 | Entropy loss: -0.0023  | Total Loss: 3.07 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 10  | Average Reward 1.82  | Actor loss: 8.00 | Critic loss: 19.75 | Entropy loss: -0.0287  | Total Loss: 27.72 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 7  | Average Reward 1.85  | Actor loss: 0.00 | Critic loss: 1.88 | Entropy loss: -0.0010  | Total Loss: 1.88 | Total Steps: 38\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward 1.85  | Actor loss: 0.30 | Critic loss: 4.30 | Entropy loss: -0.0024  | Total Loss: 4.60 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward 1.83  | Actor loss: 0.01 | Critic loss: 10.58 | Entropy loss: -0.0015  | Total Loss: 10.58 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward 1.86  | Actor loss: 0.34 | Critic loss: 8.39 | Entropy loss: -0.0309  | Total Loss: 8.70 | Total Steps: 9\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 1  | Average Reward 1.85  | Actor loss: 0.01 | Critic loss: 4.53 | Entropy loss: -0.0159  | Total Loss: 4.52 | Total Steps: 54\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 7  | Average Reward 1.83  | Actor loss: 0.24 | Critic loss: 10.15 | Entropy loss: -0.0021  | Total Loss: 10.38 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 10  | Average Reward 1.88  | Actor loss: 0.02 | Critic loss: 17.75 | Entropy loss: -0.0007  | Total Loss: 17.78 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 7  | Average Reward 1.90  | Actor loss: 0.90 | Critic loss: 15.42 | Entropy loss: -0.0120  | Total Loss: 16.30 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 7  | Average Reward 1.91  | Actor loss: 0.00 | Critic loss: 2.34 | Entropy loss: -0.0055  | Total Loss: 2.34 | Total Steps: 65\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward 1.96  | Actor loss: 1.58 | Critic loss: 21.41 | Entropy loss: -0.0086  | Total Loss: 22.98 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: -21  | Average Reward 1.80  | Actor loss: 1.10 | Critic loss: 5.96 | Entropy loss: -0.0209  | Total Loss: 7.04 | Total Steps: 197\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward 1.80  | Actor loss: 17.78 | Critic loss: 22.70 | Entropy loss: -0.0656  | Total Loss: 40.41 | Total Steps: 6\n",
      "TEST: ---prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 1  | Average Reward 1.75  | Actor loss: 13.38 | Critic loss: 7.50 | Entropy loss: -0.0170  | Total Loss: 20.86 | Total Steps: 56\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 1.78  | Actor loss: 0.02 | Critic loss: 19.78 | Entropy loss: -0.0032  | Total Loss: 19.80 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Average Reward 1.77  | Actor loss: 1.65 | Critic loss: 29.03 | Entropy loss: -0.0244  | Total Loss: 30.66 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 1  | Average Reward 1.74  | Actor loss: 0.66 | Critic loss: 18.56 | Entropy loss: -0.0248  | Total Loss: 19.19 | Total Steps: 55\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 1  | Average Reward 1.74  | Actor loss: 0.02 | Critic loss: 7.92 | Entropy loss: -0.0145  | Total Loss: 7.92 | Total Steps: 62\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 4  | Average Reward 1.73  | Actor loss: 0.01 | Critic loss: 11.31 | Entropy loss: -0.0046  | Total Loss: 11.31 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Average Reward 1.75  | Actor loss: 2.84 | Critic loss: 33.44 | Entropy loss: -0.0260  | Total Loss: 36.25 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 10  | Average Reward 1.77  | Actor loss: 0.56 | Critic loss: 16.45 | Entropy loss: -0.0298  | Total Loss: 16.98 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 4  | Average Reward 1.74  | Actor loss: 0.01 | Critic loss: 11.80 | Entropy loss: -0.0022  | Total Loss: 11.81 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 10  | Average Reward 1.77  | Actor loss: 1.11 | Critic loss: 25.71 | Entropy loss: -0.0149  | Total Loss: 26.81 | Total Steps: 16\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 7  | Average Reward 1.78  | Actor loss: 0.41 | Critic loss: 5.24 | Entropy loss: -0.0080  | Total Loss: 5.64 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward 1.81  | Actor loss: 0.02 | Critic loss: 12.16 | Entropy loss: -0.0017  | Total Loss: 12.17 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 10  | Average Reward 1.91  | Actor loss: 0.02 | Critic loss: 15.24 | Entropy loss: -0.0006  | Total Loss: 15.25 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 4  | Average Reward 1.89  | Actor loss: 0.00 | Critic loss: 7.06 | Entropy loss: -0.0028  | Total Loss: 7.06 | Total Steps: 44\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward 1.94  | Actor loss: 0.00 | Critic loss: 2.82 | Entropy loss: -0.0013  | Total Loss: 2.82 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 4  | Average Reward 1.91  | Actor loss: 0.55 | Critic loss: 15.78 | Entropy loss: -0.0218  | Total Loss: 16.30 | Total Steps: 36\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 4  | Average Reward 1.92  | Actor loss: 0.13 | Critic loss: 8.34 | Entropy loss: -0.0034  | Total Loss: 8.46 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 7  | Average Reward 1.94  | Actor loss: 0.01 | Critic loss: 15.08 | Entropy loss: -0.0019  | Total Loss: 15.09 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 4  | Average Reward 1.94  | Actor loss: 0.02 | Critic loss: 2.59 | Entropy loss: -0.0397  | Total Loss: 2.56 | Total Steps: 45\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 1  | Average Reward 1.91  | Actor loss: 0.01 | Critic loss: 1.24 | Entropy loss: -0.0028  | Total Loss: 1.24 | Total Steps: 53\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 7  | Average Reward 2.42  | Actor loss: 0.91 | Critic loss: 16.92 | Entropy loss: -0.0071  | Total Loss: 17.82 | Total Steps: 30\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 4  | Average Reward 2.40  | Actor loss: 0.11 | Critic loss: 8.84 | Entropy loss: -0.0228  | Total Loss: 8.92 | Total Steps: 32\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 1  | Average Reward 2.35  | Actor loss: 0.01 | Critic loss: 2.18 | Entropy loss: -0.0029  | Total Loss: 2.19 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward 2.32  | Actor loss: 0.01 | Critic loss: 1.11 | Entropy loss: -0.0192  | Total Loss: 1.09 | Total Steps: 48\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: -9  | Average Reward 2.25  | Actor loss: 0.71 | Critic loss: 17.23 | Entropy loss: -0.0294  | Total Loss: 17.91 | Total Steps: 162\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward 2.24  | Actor loss: 1.05 | Critic loss: 15.72 | Entropy loss: -0.0034  | Total Loss: 16.76 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 2.88  | Actor loss: 0.01 | Critic loss: 11.99 | Entropy loss: -0.0004  | Total Loss: 12.00 | Total Steps: 38\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: -8  | Average Reward 3.43  | Actor loss: 0.05 | Critic loss: 23.59 | Entropy loss: -0.0222  | Total Loss: 23.62 | Total Steps: 108\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: -35  | Average Reward 3.31  | Actor loss: 1.55 | Critic loss: 7.86 | Entropy loss: -0.0238  | Total Loss: 9.39 | Total Steps: 206\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 7  | Average Reward 3.32  | Actor loss: 0.00 | Critic loss: 4.08 | Entropy loss: -0.0069  | Total Loss: 4.07 | Total Steps: 34\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 48/100  | Episode Reward: -124  | Average Reward 2.65  | Actor loss: -0.04 | Critic loss: 74.71 | Entropy loss: -0.0266  | Total Loss: 74.65 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 4  | Average Reward 2.67  | Actor loss: 0.01 | Critic loss: 12.52 | Entropy loss: -0.0050  | Total Loss: 12.53 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward 2.69  | Actor loss: 6.08 | Critic loss: 20.25 | Entropy loss: -0.0271  | Total Loss: 26.30 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 4  | Average Reward 2.68  | Actor loss: 0.01 | Critic loss: 15.19 | Entropy loss: -0.0129  | Total Loss: 15.19 | Total Steps: 46\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 1  | Average Reward 2.65  | Actor loss: 0.07 | Critic loss: 10.58 | Entropy loss: -0.0040  | Total Loss: 10.64 | Total Steps: 63\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 10  | Average Reward 2.65  | Actor loss: 0.02 | Critic loss: 16.77 | Entropy loss: -0.0010  | Total Loss: 16.79 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 4  | Average Reward 2.63  | Actor loss: 0.01 | Critic loss: 16.70 | Entropy loss: -0.0079  | Total Loss: 16.71 | Total Steps: 45\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: -2  | Average Reward 2.67  | Actor loss: 0.80 | Critic loss: 25.46 | Entropy loss: -0.0320  | Total Loss: 26.23 | Total Steps: 102\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward 2.71  | Actor loss: 0.00 | Critic loss: 3.66 | Entropy loss: -0.0013  | Total Loss: 3.67 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 2.88  | Actor loss: 0.02 | Critic loss: 17.27 | Entropy loss: -0.0005  | Total Loss: 17.29 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: -8  | Average Reward 2.79  | Actor loss: 6.75 | Critic loss: 17.35 | Entropy loss: -0.0285  | Total Loss: 24.07 | Total Steps: 103\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 4  | Average Reward 3.33  | Actor loss: 0.01 | Critic loss: 11.25 | Entropy loss: -0.0020  | Total Loss: 11.26 | Total Steps: 42\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 4  | Average Reward 3.31  | Actor loss: 0.26 | Critic loss: 10.54 | Entropy loss: -0.0051  | Total Loss: 10.79 | Total Steps: 138\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 61/100  | Episode Reward: -106  | Average Reward 2.73  | Actor loss: -0.00 | Critic loss: 80.02 | Entropy loss: -0.0225  | Total Loss: 80.00 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 2.11 | Critic loss: 16.36 | Entropy loss: -0.0188  | Total Loss: 18.45 | Total Steps: 25\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 4  | Average Reward 2.72  | Actor loss: 0.01 | Critic loss: 11.26 | Entropy loss: -0.0017  | Total Loss: 11.26 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 15.06 | Entropy loss: -0.0017  | Total Loss: 15.07 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 65/100  | Episode Reward: -115  | Average Reward 2.11  | Actor loss: -8.96 | Critic loss: 88.20 | Entropy loss: -0.0245  | Total Loss: 79.21 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: -2  | Average Reward 2.10  | Actor loss: 0.02 | Critic loss: 8.95 | Entropy loss: -0.0268  | Total Loss: 8.94 | Total Steps: 64\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 1  | Average Reward 2.05  | Actor loss: 0.30 | Critic loss: 4.40 | Entropy loss: -0.0028  | Total Loss: 4.70 | Total Steps: 52\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 1  | Average Reward 2.02  | Actor loss: 0.01 | Critic loss: 6.20 | Entropy loss: -0.0016  | Total Loss: 6.21 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: -5  | Average Reward 1.98  | Actor loss: 0.04 | Critic loss: 16.68 | Entropy loss: -0.0216  | Total Loss: 16.70 | Total Steps: 109\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 7  | Average Reward 1.96  | Actor loss: 0.01 | Critic loss: 15.08 | Entropy loss: -0.0025  | Total Loss: 15.09 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 1  | Average Reward 1.92  | Actor loss: 0.04 | Critic loss: 1.04 | Entropy loss: -0.0019  | Total Loss: 1.08 | Total Steps: 52\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 4  | Average Reward 1.90  | Actor loss: 0.00 | Critic loss: 0.85 | Entropy loss: -0.0141  | Total Loss: 0.84 | Total Steps: 48\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 1  | Average Reward 1.89  | Actor loss: 0.02 | Critic loss: 33.19 | Entropy loss: -0.0221  | Total Loss: 33.19 | Total Steps: 99\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 7  | Average Reward 1.87  | Actor loss: 0.61 | Critic loss: 18.85 | Entropy loss: -0.0085  | Total Loss: 19.45 | Total Steps: 24\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward 1.87  | Actor loss: 0.40 | Critic loss: 5.09 | Entropy loss: -0.0077  | Total Loss: 5.48 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Average Reward 1.84  | Actor loss: 0.34 | Critic loss: 19.16 | Entropy loss: -0.0103  | Total Loss: 19.49 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: -6  | Average Reward 1.79  | Actor loss: 0.82 | Critic loss: 16.81 | Entropy loss: -0.0302  | Total Loss: 17.60 | Total Steps: 118\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 1.79  | Actor loss: 0.00 | Critic loss: 1.78 | Entropy loss: -0.0053  | Total Loss: 1.78 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 4  | Average Reward 1.77  | Actor loss: 0.05 | Critic loss: 6.70 | Entropy loss: -0.0018  | Total Loss: 6.74 | Total Steps: 43\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: -14  | Average Reward 1.69  | Actor loss: 5.63 | Critic loss: 23.13 | Entropy loss: -0.0368  | Total Loss: 28.72 | Total Steps: 236\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 1.69  | Actor loss: 0.00 | Critic loss: 3.66 | Entropy loss: -0.0014  | Total Loss: 3.67 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 1  | Average Reward 1.64  | Actor loss: 0.00 | Critic loss: 1.55 | Entropy loss: -0.0332  | Total Loss: 1.52 | Total Steps: 55\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: -5  | Average Reward 1.67  | Actor loss: 0.91 | Critic loss: 21.53 | Entropy loss: -0.0288  | Total Loss: 22.42 | Total Steps: 79\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Average Reward 1.72  | Actor loss: 0.02 | Critic loss: 19.56 | Entropy loss: -0.0051  | Total Loss: 19.57 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 85/100  | Episode Reward: -109  | Average Reward 1.17  | Actor loss: -27.76 | Critic loss: 116.15 | Entropy loss: -0.0233  | Total Loss: 88.37 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: -5  | Average Reward 1.09  | Actor loss: 1.29 | Critic loss: 25.06 | Entropy loss: -0.0369  | Total Loss: 26.32 | Total Steps: 116\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 10  | Average Reward 1.09  | Actor loss: 0.21 | Critic loss: 12.39 | Entropy loss: -0.0022  | Total Loss: 12.60 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: -5  | Average Reward 1.09  | Actor loss: 0.01 | Critic loss: 2.55 | Entropy loss: -0.0085  | Total Loss: 2.56 | Total Steps: 245\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 7  | Average Reward 1.70  | Actor loss: 0.00 | Critic loss: 1.88 | Entropy loss: -0.0005  | Total Loss: 1.88 | Total Steps: 38\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "TEST: Step: 100\n",
      "Testing  | Episode: 90/100  | Episode Reward: -11  | Average Reward 1.59  | Actor loss: 0.06 | Critic loss: 1.97 | Entropy loss: -0.0094  | Total Loss: 2.02 | Total Steps: 100\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 7  | Average Reward 1.61  | Actor loss: 0.12 | Critic loss: 31.11 | Entropy loss: -0.0258  | Total Loss: 31.20 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward 1.62  | Actor loss: 0.00 | Critic loss: 1.73 | Entropy loss: -0.0046  | Total Loss: 1.73 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 4  | Average Reward 1.64  | Actor loss: 0.04 | Critic loss: 11.45 | Entropy loss: -0.0030  | Total Loss: 11.49 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 1  | Average Reward 2.23  | Actor loss: 0.12 | Critic loss: 8.00 | Entropy loss: -0.0064  | Total Loss: 8.12 | Total Steps: 53\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 1  | Average Reward 2.19  | Actor loss: 0.54 | Critic loss: 6.56 | Entropy loss: -0.0077  | Total Loss: 7.10 | Total Steps: 51\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Average Reward 2.19  | Actor loss: 0.38 | Critic loss: 13.71 | Entropy loss: -0.0025  | Total Loss: 14.09 | Total Steps: 47\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 4  | Average Reward 2.16  | Actor loss: 0.00 | Critic loss: 0.92 | Entropy loss: -0.0051  | Total Loss: 0.92 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 2.16  | Actor loss: 0.57 | Critic loss: 16.47 | Entropy loss: -0.0084  | Total Loss: 17.03 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 4  | Average Reward 2.15  | Actor loss: 0.00 | Critic loss: 2.71 | Entropy loss: -0.0114  | Total Loss: 2.71 | Total Steps: 46\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 1  | Average Reward 2.15  | Actor loss: 0.01 | Critic loss: 1.80 | Entropy loss: -0.0199  | Total Loss: 1.79 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1602/129000  | Episode Reward: 4  | Average Reward 6.44  | Actor loss: -1.01 | Critic loss: 5.61 | Entropy loss: -0.0069  | Total Loss: 4.59 | Total Steps: 64\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1603/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.23 | Critic loss: 7.25 | Entropy loss: -0.0034  | Total Loss: 7.48 | Total Steps: 49\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1604/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.07 | Critic loss: 5.53 | Entropy loss: -0.0003  | Total Loss: 5.61 | Total Steps: 29\n",
      "---cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1605/129000  | Episode Reward: 10  | Average Reward 6.42  | Actor loss: 1.24 | Critic loss: 5.30 | Entropy loss: -0.0024  | Total Loss: 6.54 | Total Steps: 11\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1606/129000  | Episode Reward: 10  | Average Reward 6.44  | Actor loss: 0.21 | Critic loss: 4.05 | Entropy loss: -0.0006  | Total Loss: 4.25 | Total Steps: 29\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1607/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: -1.25 | Critic loss: 6.54 | Entropy loss: -0.0094  | Total Loss: 5.28 | Total Steps: 40\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1608/129000  | Episode Reward: 1  | Average Reward 6.44  | Actor loss: -0.28 | Critic loss: 9.40 | Entropy loss: -0.0027  | Total Loss: 9.12 | Total Steps: 55\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1609/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.16 | Critic loss: 4.66 | Entropy loss: -0.0008  | Total Loss: 4.82 | Total Steps: 29\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1610/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.17 | Critic loss: 8.51 | Entropy loss: -0.0031  | Total Loss: 8.67 | Total Steps: 26\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1611/129000  | Episode Reward: -8  | Average Reward 6.37  | Actor loss: -0.35 | Critic loss: 17.96 | Entropy loss: -0.0050  | Total Loss: 17.60 | Total Steps: 113\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1612/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.06 | Critic loss: 6.25 | Entropy loss: -0.0005  | Total Loss: 6.30 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1613/129000  | Episode Reward: 7  | Average Reward 6.44  | Actor loss: 0.37 | Critic loss: 5.26 | Entropy loss: -0.0015  | Total Loss: 5.62 | Total Steps: 34\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1614/129000  | Episode Reward: 7  | Average Reward 6.44  | Actor loss: 0.24 | Critic loss: 4.54 | Entropy loss: -0.0019  | Total Loss: 4.78 | Total Steps: 37\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1615/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -0.81 | Critic loss: 4.91 | Entropy loss: -0.0032  | Total Loss: 4.10 | Total Steps: 47\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1616/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.37 | Critic loss: 3.84 | Entropy loss: -0.0006  | Total Loss: 4.21 | Total Steps: 8\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1617/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.00 | Critic loss: 1.75 | Entropy loss: -0.0000  | Total Loss: 1.75 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1618/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.22 | Critic loss: 5.21 | Entropy loss: -0.0008  | Total Loss: 4.98 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1619/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.45 | Critic loss: 8.72 | Entropy loss: -0.0015  | Total Loss: 8.28 | Total Steps: 52\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1620/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: 0.02 | Critic loss: 7.21 | Entropy loss: -0.0025  | Total Loss: 7.23 | Total Steps: 44\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1621/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.04 | Critic loss: 2.92 | Entropy loss: -0.0001  | Total Loss: 2.96 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1622/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: 0.15 | Critic loss: 5.74 | Entropy loss: -0.0012  | Total Loss: 5.89 | Total Steps: 43\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1623/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 0.01 | Critic loss: 2.17 | Entropy loss: -0.0000  | Total Loss: 2.18 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1624/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.13 | Critic loss: 4.96 | Entropy loss: -0.0018  | Total Loss: 4.83 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1625/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: -0.01 | Critic loss: 4.00 | Entropy loss: -0.0003  | Total Loss: 3.99 | Total Steps: 46\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1626/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: 0.02 | Critic loss: 10.99 | Entropy loss: -0.0022  | Total Loss: 11.01 | Total Steps: 55\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1627/129000  | Episode Reward: 1  | Average Reward 6.47  | Actor loss: -0.34 | Critic loss: 10.70 | Entropy loss: -0.0051  | Total Loss: 10.36 | Total Steps: 75\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1628/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: -0.28 | Critic loss: 4.30 | Entropy loss: -0.0027  | Total Loss: 4.02 | Total Steps: 53\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1629/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.11 | Critic loss: 3.93 | Entropy loss: -0.0001  | Total Loss: 4.04 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1630/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.01 | Critic loss: 2.26 | Entropy loss: -0.0000  | Total Loss: 2.27 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1631/129000  | Episode Reward: 4  | Average Reward 6.55  | Actor loss: -0.08 | Critic loss: 7.68 | Entropy loss: -0.0005  | Total Loss: 7.60 | Total Steps: 52\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1632/129000  | Episode Reward: 7  | Average Reward 6.57  | Actor loss: 0.34 | Critic loss: 7.60 | Entropy loss: -0.0012  | Total Loss: 7.95 | Total Steps: 30\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1633/129000  | Episode Reward: 4  | Average Reward 6.55  | Actor loss: -0.29 | Critic loss: 4.69 | Entropy loss: -0.0041  | Total Loss: 4.39 | Total Steps: 42\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1634/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.51 | Critic loss: 19.41 | Entropy loss: -0.0002  | Total Loss: 19.91 | Total Steps: 6\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1635/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 1.83 | Critic loss: 6.83 | Entropy loss: -0.0010  | Total Loss: 8.66 | Total Steps: 7\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1636/129000  | Episode Reward: 10  | Average Reward 6.59  | Actor loss: 0.01 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1637/129000  | Episode Reward: 7  | Average Reward 6.59  | Actor loss: -0.04 | Critic loss: 5.26 | Entropy loss: -0.0003  | Total Loss: 5.22 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1638/129000  | Episode Reward: 1  | Average Reward 6.55  | Actor loss: -0.79 | Critic loss: 9.20 | Entropy loss: -0.0050  | Total Loss: 8.40 | Total Steps: 61\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1639/129000  | Episode Reward: 4  | Average Reward 6.52  | Actor loss: 0.18 | Critic loss: 5.27 | Entropy loss: -0.0023  | Total Loss: 5.45 | Total Steps: 50\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1640/129000  | Episode Reward: 7  | Average Reward 6.52  | Actor loss: -0.09 | Critic loss: 3.18 | Entropy loss: -0.0006  | Total Loss: 3.09 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1641/129000  | Episode Reward: 7  | Average Reward 6.59  | Actor loss: 0.32 | Critic loss: 9.39 | Entropy loss: -0.0009  | Total Loss: 9.70 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1642/129000  | Episode Reward: 4  | Average Reward 6.58  | Actor loss: -0.09 | Critic loss: 9.48 | Entropy loss: -0.0025  | Total Loss: 9.39 | Total Steps: 54\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1643/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.22 | Critic loss: 4.34 | Entropy loss: -0.0012  | Total Loss: 4.55 | Total Steps: 31\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1644/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.20 | Critic loss: 5.14 | Entropy loss: -0.0006  | Total Loss: 5.34 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1645/129000  | Episode Reward: 7  | Average Reward 6.62  | Actor loss: 0.11 | Critic loss: 7.92 | Entropy loss: -0.0004  | Total Loss: 8.03 | Total Steps: 30\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1646/129000  | Episode Reward: 10  | Average Reward 6.66  | Actor loss: 0.01 | Critic loss: 1.61 | Entropy loss: -0.0000  | Total Loss: 1.62 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1647/129000  | Episode Reward: 1  | Average Reward 6.63  | Actor loss: -0.24 | Critic loss: 7.67 | Entropy loss: -0.0019  | Total Loss: 7.43 | Total Steps: 56\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1648/129000  | Episode Reward: 4  | Average Reward 6.62  | Actor loss: 0.02 | Critic loss: 4.36 | Entropy loss: -0.0008  | Total Loss: 4.38 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1649/129000  | Episode Reward: 7  | Average Reward 6.63  | Actor loss: 0.15 | Critic loss: 3.87 | Entropy loss: -0.0015  | Total Loss: 4.01 | Total Steps: 32\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1650/129000  | Episode Reward: 4  | Average Reward 6.60  | Actor loss: -0.03 | Critic loss: 4.56 | Entropy loss: -0.0044  | Total Loss: 4.53 | Total Steps: 47\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1651/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: -1.34 | Critic loss: 6.35 | Entropy loss: -0.0042  | Total Loss: 5.01 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1652/129000  | Episode Reward: 4  | Average Reward 6.60  | Actor loss: 0.05 | Critic loss: 6.92 | Entropy loss: -0.0027  | Total Loss: 6.97 | Total Steps: 48\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1653/129000  | Episode Reward: 4  | Average Reward 6.62  | Actor loss: -0.01 | Critic loss: 3.87 | Entropy loss: -0.0019  | Total Loss: 3.87 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1654/129000  | Episode Reward: 4  | Average Reward 6.58  | Actor loss: -0.19 | Critic loss: 3.99 | Entropy loss: -0.0013  | Total Loss: 3.80 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1655/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.11 | Critic loss: 6.85 | Entropy loss: -0.0005  | Total Loss: 6.96 | Total Steps: 32\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1656/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.02 | Critic loss: 2.09 | Entropy loss: -0.0000  | Total Loss: 2.11 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1657/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.19 | Critic loss: 5.77 | Entropy loss: -0.0005  | Total Loss: 5.96 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1658/129000  | Episode Reward: 3  | Average Reward 6.57  | Actor loss: -1.31 | Critic loss: 10.51 | Entropy loss: -0.0068  | Total Loss: 9.20 | Total Steps: 57\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1659/129000  | Episode Reward: 7  | Average Reward 6.59  | Actor loss: -0.04 | Critic loss: 1.14 | Entropy loss: -0.0006  | Total Loss: 1.09 | Total Steps: 38\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1660/129000  | Episode Reward: 4  | Average Reward 6.58  | Actor loss: -0.19 | Critic loss: 5.08 | Entropy loss: -0.0011  | Total Loss: 4.89 | Total Steps: 43\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1661/129000  | Episode Reward: 10  | Average Reward 6.58  | Actor loss: 0.02 | Critic loss: 9.53 | Entropy loss: -0.0000  | Total Loss: 9.55 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1662/129000  | Episode Reward: 7  | Average Reward 6.57  | Actor loss: -0.03 | Critic loss: 2.25 | Entropy loss: -0.0007  | Total Loss: 2.21 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1663/129000  | Episode Reward: 7  | Average Reward 6.58  | Actor loss: 0.04 | Critic loss: 5.36 | Entropy loss: -0.0003  | Total Loss: 5.40 | Total Steps: 30\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1664/129000  | Episode Reward: 10  | Average Reward 6.61  | Actor loss: 0.23 | Critic loss: 3.03 | Entropy loss: -0.0003  | Total Loss: 3.26 | Total Steps: 8\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1665/129000  | Episode Reward: 7  | Average Reward 6.61  | Actor loss: 0.12 | Critic loss: 4.29 | Entropy loss: -0.0006  | Total Loss: 4.40 | Total Steps: 29\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1666/129000  | Episode Reward: 10  | Average Reward 6.62  | Actor loss: 0.25 | Critic loss: 15.74 | Entropy loss: -0.0001  | Total Loss: 15.99 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1667/129000  | Episode Reward: 4  | Average Reward 6.59  | Actor loss: -0.26 | Critic loss: 8.14 | Entropy loss: -0.0032  | Total Loss: 7.88 | Total Steps: 55\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1668/129000  | Episode Reward: 4  | Average Reward 6.57  | Actor loss: -0.04 | Critic loss: 4.29 | Entropy loss: -0.0004  | Total Loss: 4.25 | Total Steps: 42\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1669/129000  | Episode Reward: 1  | Average Reward 6.52  | Actor loss: -0.11 | Critic loss: 11.19 | Entropy loss: -0.0008  | Total Loss: 11.08 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1670/129000  | Episode Reward: 7  | Average Reward 6.52  | Actor loss: 0.18 | Critic loss: 4.18 | Entropy loss: -0.0007  | Total Loss: 4.36 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1671/129000  | Episode Reward: 4  | Average Reward 6.54  | Actor loss: -0.05 | Critic loss: 4.59 | Entropy loss: -0.0004  | Total Loss: 4.54 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1672/129000  | Episode Reward: 7  | Average Reward 6.57  | Actor loss: 0.24 | Critic loss: 6.69 | Entropy loss: -0.0011  | Total Loss: 6.93 | Total Steps: 31\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1673/129000  | Episode Reward: 1  | Average Reward 6.54  | Actor loss: -0.18 | Critic loss: 11.25 | Entropy loss: -0.0006  | Total Loss: 11.07 | Total Steps: 52\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1674/129000  | Episode Reward: 4  | Average Reward 6.52  | Actor loss: -0.25 | Critic loss: 6.70 | Entropy loss: -0.0018  | Total Loss: 6.44 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1675/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: 0.10 | Critic loss: 5.38 | Entropy loss: -0.0007  | Total Loss: 5.49 | Total Steps: 32\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1676/129000  | Episode Reward: 1  | Average Reward 6.50  | Actor loss: -0.96 | Critic loss: 9.51 | Entropy loss: -0.0057  | Total Loss: 8.54 | Total Steps: 49\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1677/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.23 | Critic loss: 9.40 | Entropy loss: -0.0013  | Total Loss: 9.16 | Total Steps: 44\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1678/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 14.51 | Entropy loss: -0.0000  | Total Loss: 14.53 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1679/129000  | Episode Reward: 7  | Average Reward 6.55  | Actor loss: 0.11 | Critic loss: 6.08 | Entropy loss: -0.0004  | Total Loss: 6.19 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1680/129000  | Episode Reward: 4  | Average Reward 6.52  | Actor loss: -0.19 | Critic loss: 4.85 | Entropy loss: -0.0011  | Total Loss: 4.67 | Total Steps: 43\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1681/129000  | Episode Reward: 10  | Average Reward 6.52  | Actor loss: 0.27 | Critic loss: 3.04 | Entropy loss: -0.0024  | Total Loss: 3.31 | Total Steps: 45\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1682/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.01 | Critic loss: 2.59 | Entropy loss: -0.0000  | Total Loss: 2.61 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1683/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 8.88 | Entropy loss: -0.0000  | Total Loss: 8.90 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1684/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: -0.06 | Critic loss: 2.79 | Entropy loss: -0.0004  | Total Loss: 2.73 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1685/129000  | Episode Reward: -5  | Average Reward 6.46  | Actor loss: -0.65 | Critic loss: 18.26 | Entropy loss: -0.0055  | Total Loss: 17.61 | Total Steps: 57\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1686/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: -0.15 | Critic loss: 8.71 | Entropy loss: -0.0009  | Total Loss: 8.56 | Total Steps: 42\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1687/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 3.45 | Entropy loss: -0.0000  | Total Loss: 3.47 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1688/129000  | Episode Reward: 4  | Average Reward 6.48  | Actor loss: -0.25 | Critic loss: 7.54 | Entropy loss: -0.0021  | Total Loss: 7.28 | Total Steps: 42\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1689/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.22 | Critic loss: 3.58 | Entropy loss: -0.0002  | Total Loss: 3.81 | Total Steps: 8\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1690/129000  | Episode Reward: 1  | Average Reward 6.48  | Actor loss: -0.14 | Critic loss: 9.58 | Entropy loss: -0.0005  | Total Loss: 9.43 | Total Steps: 51\n",
      "---capsule---\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1691/129000  | Episode Reward: 10  | Average Reward 6.48  | Actor loss: 0.02 | Critic loss: 3.20 | Entropy loss: -0.0014  | Total Loss: 3.23 | Total Steps: 421\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1692/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.00 | Critic loss: 2.76 | Entropy loss: -0.0004  | Total Loss: 2.77 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1693/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.02 | Critic loss: 10.52 | Entropy loss: -0.0000  | Total Loss: 10.54 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1694/129000  | Episode Reward: 7  | Average Reward 6.53  | Actor loss: 0.11 | Critic loss: 5.78 | Entropy loss: -0.0017  | Total Loss: 5.88 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1695/129000  | Episode Reward: 7  | Average Reward 6.51  | Actor loss: 0.27 | Critic loss: 5.17 | Entropy loss: -0.0012  | Total Loss: 5.44 | Total Steps: 31\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1696/129000  | Episode Reward: 4  | Average Reward 6.49  | Actor loss: 0.03 | Critic loss: 7.91 | Entropy loss: -0.0019  | Total Loss: 7.93 | Total Steps: 44\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1697/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: -0.04 | Critic loss: 2.96 | Entropy loss: -0.0003  | Total Loss: 2.92 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1698/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.18 | Critic loss: 5.52 | Entropy loss: -0.0008  | Total Loss: 5.70 | Total Steps: 31\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1699/129000  | Episode Reward: 4  | Average Reward 6.50  | Actor loss: -0.02 | Critic loss: 7.28 | Entropy loss: -0.0005  | Total Loss: 7.26 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1700/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.36 | Critic loss: 12.49 | Entropy loss: -0.0026  | Total Loss: 12.13 | Total Steps: 54\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1701/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: -0.25 | Critic loss: 4.24 | Entropy loss: -0.0019  | Total Loss: 3.99 | Total Steps: 46\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 7  | Average Reward 2.13  | Actor loss: 0.01 | Critic loss: 6.12 | Entropy loss: -0.0139  | Total Loss: 6.12 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 4  | Average Reward 2.13  | Actor loss: 0.01 | Critic loss: 2.51 | Entropy loss: -0.0044  | Total Loss: 2.51 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 10  | Average Reward 2.16  | Actor loss: 0.00 | Critic loss: 3.36 | Entropy loss: -0.0031  | Total Loss: 3.36 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 10  | Average Reward 2.17  | Actor loss: 1.85 | Critic loss: 12.58 | Entropy loss: -0.0083  | Total Loss: 14.42 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 7  | Average Reward 2.19  | Actor loss: 0.00 | Critic loss: 2.71 | Entropy loss: -0.0010  | Total Loss: 2.72 | Total Steps: 38\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 1  | Average Reward 2.21  | Actor loss: 1.74 | Critic loss: 2.02 | Entropy loss: -0.0115  | Total Loss: 3.75 | Total Steps: 53\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward 2.17  | Actor loss: 0.01 | Critic loss: 1.31 | Entropy loss: -0.0216  | Total Loss: 1.30 | Total Steps: 51\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.09 | Critic loss: 3.13 | Entropy loss: -0.0194  | Total Loss: 3.20 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 1  | Average Reward 2.19  | Actor loss: 0.30 | Critic loss: 10.91 | Entropy loss: -0.0150  | Total Loss: 11.20 | Total Steps: 64\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 7  | Average Reward 2.17  | Actor loss: 0.03 | Critic loss: 4.20 | Entropy loss: -0.0123  | Total Loss: 4.22 | Total Steps: 70\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward 2.17  | Actor loss: 0.01 | Critic loss: 2.09 | Entropy loss: -0.0028  | Total Loss: 2.09 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 10  | Average Reward 2.17  | Actor loss: 0.01 | Critic loss: 10.81 | Entropy loss: -0.0012  | Total Loss: 10.81 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 10  | Average Reward 2.22  | Actor loss: 0.00 | Critic loss: 2.35 | Entropy loss: -0.0059  | Total Loss: 2.35 | Total Steps: 65\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 10  | Average Reward 2.23  | Actor loss: 0.43 | Critic loss: 12.62 | Entropy loss: -0.0044  | Total Loss: 13.04 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.14 | Critic loss: 4.23 | Entropy loss: -0.0130  | Total Loss: 4.36 | Total Steps: 12\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.01 | Critic loss: 10.24 | Entropy loss: -0.0029  | Total Loss: 10.25 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 10  | Average Reward 2.25  | Actor loss: 0.11 | Critic loss: 3.24 | Entropy loss: -0.0225  | Total Loss: 3.33 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward 2.28  | Actor loss: 0.01 | Critic loss: 2.85 | Entropy loss: -0.0044  | Total Loss: 2.85 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 2.28  | Actor loss: 0.02 | Critic loss: 15.54 | Entropy loss: -0.0015  | Total Loss: 15.57 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 1  | Average Reward 2.23  | Actor loss: 0.01 | Critic loss: 10.52 | Entropy loss: -0.0012  | Total Loss: 10.52 | Total Steps: 53\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward 2.28  | Actor loss: 4.56 | Critic loss: 14.47 | Entropy loss: -0.0166  | Total Loss: 19.02 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: -2  | Average Reward 2.23  | Actor loss: 0.01 | Critic loss: 2.97 | Entropy loss: -0.0316  | Total Loss: 2.95 | Total Steps: 46\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 7  | Average Reward 2.25  | Actor loss: 0.00 | Critic loss: 8.80 | Entropy loss: -0.0014  | Total Loss: 8.80 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 7  | Average Reward 2.28  | Actor loss: 0.10 | Critic loss: 3.69 | Entropy loss: -0.0033  | Total Loss: 3.79 | Total Steps: 30\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: -2  | Average Reward 2.22  | Actor loss: 0.00 | Critic loss: 3.23 | Entropy loss: -0.0099  | Total Loss: 3.22 | Total Steps: 55\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 1  | Average Reward 2.22  | Actor loss: 0.01 | Critic loss: 2.74 | Entropy loss: -0.0020  | Total Loss: 2.75 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward 2.28  | Actor loss: 0.37 | Critic loss: 5.49 | Entropy loss: -0.0042  | Total Loss: 5.87 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 10  | Average Reward 2.31  | Actor loss: 0.01 | Critic loss: 1.62 | Entropy loss: -0.0011  | Total Loss: 1.63 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 10  | Average Reward 2.33  | Actor loss: 0.01 | Critic loss: 1.96 | Entropy loss: -0.0005  | Total Loss: 1.96 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 1  | Average Reward 2.29  | Actor loss: 0.27 | Critic loss: 1.82 | Entropy loss: -0.0231  | Total Loss: 2.06 | Total Steps: 57\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Average Reward 2.29  | Actor loss: 0.00 | Critic loss: 3.13 | Entropy loss: -0.0062  | Total Loss: 3.13 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward 2.29  | Actor loss: 0.01 | Critic loss: 1.68 | Entropy loss: -0.0023  | Total Loss: 1.69 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 4  | Average Reward 2.29  | Actor loss: 0.01 | Critic loss: 1.89 | Entropy loss: -0.0013  | Total Loss: 1.90 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 4  | Average Reward 2.29  | Actor loss: 0.04 | Critic loss: 3.01 | Entropy loss: -0.0402  | Total Loss: 3.00 | Total Steps: 57\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 2  | Average Reward 2.31  | Actor loss: 0.01 | Critic loss: 0.97 | Entropy loss: -0.0346  | Total Loss: 0.95 | Total Steps: 120\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 1  | Average Reward 2.29  | Actor loss: 0.10 | Critic loss: 2.94 | Entropy loss: -0.0115  | Total Loss: 3.02 | Total Steps: 55\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: -5  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 13.80 | Entropy loss: -0.0044  | Total Loss: 13.80 | Total Steps: 73\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.83 | Entropy loss: -0.0018  | Total Loss: 1.83 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 1  | Average Reward 2.17  | Actor loss: 0.01 | Critic loss: 3.05 | Entropy loss: -0.0175  | Total Loss: 3.04 | Total Steps: 56\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 1  | Average Reward 2.12  | Actor loss: 0.11 | Critic loss: 1.80 | Entropy loss: -0.0132  | Total Loss: 1.89 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 1  | Average Reward 2.10  | Actor loss: 0.00 | Critic loss: 5.74 | Entropy loss: -0.0042  | Total Loss: 5.74 | Total Steps: 53\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 7  | Average Reward 2.09  | Actor loss: 0.00 | Critic loss: 4.49 | Entropy loss: -0.0097  | Total Loss: 4.48 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward 2.18  | Actor loss: 0.01 | Critic loss: 3.06 | Entropy loss: -0.0086  | Total Loss: 3.06 | Total Steps: 40\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 4  | Average Reward 2.17  | Actor loss: 0.00 | Critic loss: 5.51 | Entropy loss: -0.0013  | Total Loss: 5.51 | Total Steps: 42\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 2.19  | Actor loss: 0.57 | Critic loss: 8.89 | Entropy loss: -0.0493  | Total Loss: 9.41 | Total Steps: 88\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 7  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0044  | Total Loss: 1.66 | Total Steps: 34\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 47/100  | Episode Reward: -34  | Average Reward 1.99  | Actor loss: -1.28 | Critic loss: 93.44 | Entropy loss: -0.0427  | Total Loss: 92.11 | Total Steps: 500\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: -3  | Average Reward 1.93  | Actor loss: 0.02 | Critic loss: 4.01 | Entropy loss: -0.0380  | Total Loss: 3.99 | Total Steps: 63\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 1.93  | Actor loss: 0.01 | Critic loss: 1.69 | Entropy loss: -0.0075  | Total Loss: 1.69 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward 1.91  | Actor loss: 0.05 | Critic loss: 2.10 | Entropy loss: -0.0065  | Total Loss: 2.14 | Total Steps: 39\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 3  | Average Reward 1.89  | Actor loss: 0.14 | Critic loss: 6.79 | Entropy loss: -0.0306  | Total Loss: 6.90 | Total Steps: 68\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward 1.89  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0057  | Total Loss: 1.66 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 4  | Average Reward 1.92  | Actor loss: 0.01 | Critic loss: 2.02 | Entropy loss: -0.0030  | Total Loss: 2.03 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward 1.94  | Actor loss: 0.03 | Critic loss: 17.27 | Entropy loss: -0.0013  | Total Loss: 17.30 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 10  | Average Reward 1.95  | Actor loss: 0.01 | Critic loss: 6.03 | Entropy loss: -0.0010  | Total Loss: 6.04 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward 1.99  | Actor loss: 0.01 | Critic loss: 1.74 | Entropy loss: -0.0008  | Total Loss: 1.75 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 4  | Average Reward 1.96  | Actor loss: 0.00 | Critic loss: 4.96 | Entropy loss: -0.0009  | Total Loss: 4.96 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: -2  | Average Reward 1.90  | Actor loss: 0.13 | Critic loss: 2.26 | Entropy loss: -0.0199  | Total Loss: 2.37 | Total Steps: 63\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward 1.93  | Actor loss: 0.00 | Critic loss: 5.31 | Entropy loss: -0.0069  | Total Loss: 5.31 | Total Steps: 128\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: -29  | Average Reward 1.75  | Actor loss: 9.19 | Critic loss: 11.68 | Entropy loss: -0.0377  | Total Loss: 20.83 | Total Steps: 407\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Average Reward 1.75  | Actor loss: 0.01 | Critic loss: 1.61 | Entropy loss: -0.0077  | Total Loss: 1.61 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 1  | Average Reward 1.75  | Actor loss: 0.01 | Critic loss: 3.01 | Entropy loss: -0.0183  | Total Loss: 3.00 | Total Steps: 47\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 4  | Average Reward 1.75  | Actor loss: 0.01 | Critic loss: 2.47 | Entropy loss: -0.0118  | Total Loss: 2.47 | Total Steps: 39\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: -8  | Average Reward 1.66  | Actor loss: 0.97 | Critic loss: 10.42 | Entropy loss: -0.0334  | Total Loss: 11.35 | Total Steps: 164\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 10  | Average Reward 1.69  | Actor loss: 1.76 | Critic loss: 12.44 | Entropy loss: -0.0131  | Total Loss: 14.19 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 1  | Average Reward 1.66  | Actor loss: 0.00 | Critic loss: 29.39 | Entropy loss: -0.0077  | Total Loss: 29.38 | Total Steps: 51\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 10  | Average Reward 1.67  | Actor loss: 0.01 | Critic loss: 3.18 | Entropy loss: -0.0078  | Total Loss: 3.18 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 10  | Average Reward 1.70  | Actor loss: 0.08 | Critic loss: 2.63 | Entropy loss: -0.0106  | Total Loss: 2.70 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward 1.67  | Actor loss: 0.01 | Critic loss: 1.70 | Entropy loss: -0.0311  | Total Loss: 1.69 | Total Steps: 45\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 1  | Average Reward 1.62  | Actor loss: 0.02 | Critic loss: 2.35 | Entropy loss: -0.0121  | Total Loss: 2.36 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 4  | Average Reward 1.61  | Actor loss: 0.01 | Critic loss: 1.97 | Entropy loss: -0.0056  | Total Loss: 1.98 | Total Steps: 49\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.02 | Critic loss: 17.62 | Entropy loss: -0.0017  | Total Loss: 17.64 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.01 | Critic loss: 1.74 | Entropy loss: -0.0022  | Total Loss: 1.74 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 7  | Average Reward 1.65  | Actor loss: 0.01 | Critic loss: 4.59 | Entropy loss: -0.0022  | Total Loss: 4.60 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward 1.63  | Actor loss: 0.01 | Critic loss: 16.00 | Entropy loss: -0.0011  | Total Loss: 16.01 | Total Steps: 29\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 7  | Average Reward 1.65  | Actor loss: 0.00 | Critic loss: 4.36 | Entropy loss: -0.0053  | Total Loss: 4.36 | Total Steps: 36\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Average Reward 1.65  | Actor loss: 0.01 | Critic loss: 2.73 | Entropy loss: -0.0013  | Total Loss: 2.73 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward 1.63  | Actor loss: 0.12 | Critic loss: 2.41 | Entropy loss: -0.0233  | Total Loss: 2.51 | Total Steps: 46\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 4  | Average Reward 1.66  | Actor loss: 0.00 | Critic loss: 7.91 | Entropy loss: -0.0028  | Total Loss: 7.91 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.01 | Critic loss: 6.04 | Entropy loss: -0.0011  | Total Loss: 6.04 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.00 | Critic loss: 3.11 | Entropy loss: -0.0408  | Total Loss: 3.07 | Total Steps: 10\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.01 | Critic loss: 10.10 | Entropy loss: -0.0020  | Total Loss: 10.11 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 10  | Average Reward 1.71  | Actor loss: 0.00 | Critic loss: 2.46 | Entropy loss: -0.0012  | Total Loss: 2.47 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 84/100  | Episode Reward: -64  | Average Reward 1.35  | Actor loss: -11.51 | Critic loss: 110.99 | Entropy loss: -0.0326  | Total Loss: 99.45 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 4  | Average Reward 1.32  | Actor loss: 0.00 | Critic loss: 4.99 | Entropy loss: -0.0018  | Total Loss: 4.98 | Total Steps: 42\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Average Reward 1.34  | Actor loss: 0.01 | Critic loss: 4.36 | Entropy loss: -0.0003  | Total Loss: 4.37 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward 1.35  | Actor loss: 0.05 | Critic loss: 4.45 | Entropy loss: -0.0149  | Total Loss: 4.48 | Total Steps: 51\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 7  | Average Reward 1.39  | Actor loss: 0.00 | Critic loss: 2.71 | Entropy loss: -0.0011  | Total Loss: 2.72 | Total Steps: 38\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 1  | Average Reward 1.34  | Actor loss: 0.05 | Critic loss: 2.27 | Entropy loss: -0.0215  | Total Loss: 2.30 | Total Steps: 54\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward 1.37  | Actor loss: 0.02 | Critic loss: 17.93 | Entropy loss: -0.0044  | Total Loss: 17.95 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 10  | Average Reward 1.37  | Actor loss: 0.01 | Critic loss: 2.70 | Entropy loss: -0.0012  | Total Loss: 2.71 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 1  | Average Reward 1.37  | Actor loss: 0.01 | Critic loss: 2.58 | Entropy loss: -0.0117  | Total Loss: 2.58 | Total Steps: 53\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 4  | Average Reward 1.58  | Actor loss: 0.00 | Critic loss: 2.93 | Entropy loss: -0.0080  | Total Loss: 2.93 | Total Steps: 47\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 1  | Average Reward 1.53  | Actor loss: 0.02 | Critic loss: 2.83 | Entropy loss: -0.0087  | Total Loss: 2.84 | Total Steps: 54\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 4  | Average Reward 1.58  | Actor loss: 0.43 | Critic loss: 3.78 | Entropy loss: -0.0118  | Total Loss: 4.20 | Total Steps: 48\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Average Reward 1.55  | Actor loss: 0.00 | Critic loss: 2.36 | Entropy loss: -0.0133  | Total Loss: 2.35 | Total Steps: 42\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 10  | Average Reward 1.55  | Actor loss: 0.00 | Critic loss: 4.30 | Entropy loss: -0.0008  | Total Loss: 4.30 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 4  | Average Reward 1.53  | Actor loss: 0.01 | Critic loss: 2.02 | Entropy loss: -0.0173  | Total Loss: 2.01 | Total Steps: 51\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 10  | Average Reward 1.58  | Actor loss: 0.01 | Critic loss: 1.91 | Entropy loss: -0.0034  | Total Loss: 1.92 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Average Reward 1.61  | Actor loss: 0.02 | Critic loss: 20.14 | Entropy loss: -0.0009  | Total Loss: 20.17 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1702/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.02 | Critic loss: 1.49 | Entropy loss: -0.0000  | Total Loss: 1.51 | Total Steps: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1703/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: 0.48 | Critic loss: 1.82 | Entropy loss: -0.0009  | Total Loss: 2.30 | Total Steps: 9\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1704/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.00 | Critic loss: 5.92 | Entropy loss: -0.0016  | Total Loss: 5.92 | Total Steps: 45\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1705/129000  | Episode Reward: 10  | Average Reward 6.52  | Actor loss: -0.10 | Critic loss: 3.59 | Entropy loss: -0.0054  | Total Loss: 3.48 | Total Steps: 45\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1706/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.74 | Critic loss: 4.08 | Entropy loss: -0.0031  | Total Loss: 4.83 | Total Steps: 15\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1707/129000  | Episode Reward: 1  | Average Reward 6.50  | Actor loss: -0.48 | Critic loss: 8.84 | Entropy loss: -0.0021  | Total Loss: 8.36 | Total Steps: 51\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1708/129000  | Episode Reward: 7  | Average Reward 6.52  | Actor loss: -0.53 | Critic loss: 4.23 | Entropy loss: -0.0018  | Total Loss: 3.69 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1709/129000  | Episode Reward: 3  | Average Reward 6.49  | Actor loss: -1.59 | Critic loss: 7.70 | Entropy loss: -0.0102  | Total Loss: 6.10 | Total Steps: 66\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1710/129000  | Episode Reward: 1  | Average Reward 6.44  | Actor loss: -1.03 | Critic loss: 9.58 | Entropy loss: -0.0061  | Total Loss: 8.55 | Total Steps: 55\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1711/129000  | Episode Reward: 7  | Average Reward 6.44  | Actor loss: -0.37 | Critic loss: 3.22 | Entropy loss: -0.0085  | Total Loss: 2.84 | Total Steps: 53\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1712/129000  | Episode Reward: 10  | Average Reward 6.49  | Actor loss: 0.01 | Critic loss: 2.25 | Entropy loss: -0.0000  | Total Loss: 2.26 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1713/129000  | Episode Reward: 10  | Average Reward 6.51  | Actor loss: 1.18 | Critic loss: 5.64 | Entropy loss: -0.0026  | Total Loss: 6.82 | Total Steps: 18\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1714/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.03 | Critic loss: 7.80 | Entropy loss: -0.0003  | Total Loss: 7.83 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1715/129000  | Episode Reward: 1  | Average Reward 6.49  | Actor loss: -0.41 | Critic loss: 8.93 | Entropy loss: -0.0021  | Total Loss: 8.51 | Total Steps: 52\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1716/129000  | Episode Reward: 4  | Average Reward 6.47  | Actor loss: -0.10 | Critic loss: 4.97 | Entropy loss: -0.0010  | Total Loss: 4.87 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1717/129000  | Episode Reward: 7  | Average Reward 6.49  | Actor loss: -0.12 | Critic loss: 3.58 | Entropy loss: -0.0019  | Total Loss: 3.46 | Total Steps: 94\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1718/129000  | Episode Reward: -5  | Average Reward 6.41  | Actor loss: -0.67 | Critic loss: 16.71 | Entropy loss: -0.0071  | Total Loss: 16.03 | Total Steps: 92\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1719/129000  | Episode Reward: -2  | Average Reward 6.35  | Actor loss: -0.78 | Critic loss: 13.01 | Entropy loss: -0.0068  | Total Loss: 12.22 | Total Steps: 62\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1720/129000  | Episode Reward: 4  | Average Reward 6.35  | Actor loss: -1.05 | Critic loss: 6.17 | Entropy loss: -0.0131  | Total Loss: 5.10 | Total Steps: 71\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1721/129000  | Episode Reward: 7  | Average Reward 6.35  | Actor loss: 0.05 | Critic loss: 4.56 | Entropy loss: -0.0003  | Total Loss: 4.60 | Total Steps: 30\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1722/129000  | Episode Reward: 10  | Average Reward 6.35  | Actor loss: 0.02 | Critic loss: 6.09 | Entropy loss: -0.0000  | Total Loss: 6.11 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1723/129000  | Episode Reward: 4  | Average Reward 6.33  | Actor loss: -0.74 | Critic loss: 6.32 | Entropy loss: -0.0122  | Total Loss: 5.57 | Total Steps: 60\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1724/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -1.03 | Critic loss: 9.04 | Entropy loss: -0.0018  | Total Loss: 8.00 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1725/129000  | Episode Reward: 10  | Average Reward 6.35  | Actor loss: 0.28 | Critic loss: 2.37 | Entropy loss: -0.0009  | Total Loss: 2.65 | Total Steps: 12\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1726/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: 0.18 | Critic loss: 2.74 | Entropy loss: -0.0012  | Total Loss: 2.92 | Total Steps: 34\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1727/129000  | Episode Reward: 10  | Average Reward 6.38  | Actor loss: -0.00 | Critic loss: 3.02 | Entropy loss: -0.0047  | Total Loss: 3.01 | Total Steps: 55\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1728/129000  | Episode Reward: 10  | Average Reward 6.38  | Actor loss: 0.01 | Critic loss: 1.30 | Entropy loss: -0.0000  | Total Loss: 1.30 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1729/129000  | Episode Reward: 7  | Average Reward 6.37  | Actor loss: -0.05 | Critic loss: 6.36 | Entropy loss: -0.0066  | Total Loss: 6.30 | Total Steps: 39\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1730/129000  | Episode Reward: 10  | Average Reward 6.37  | Actor loss: 0.03 | Critic loss: 1.88 | Entropy loss: -0.0001  | Total Loss: 1.92 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1731/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: -0.26 | Critic loss: 10.09 | Entropy loss: -0.0012  | Total Loss: 9.83 | Total Steps: 24\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1732/129000  | Episode Reward: 1  | Average Reward 6.34  | Actor loss: -0.63 | Critic loss: 7.07 | Entropy loss: -0.0039  | Total Loss: 6.43 | Total Steps: 53\n",
      "---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1733/129000  | Episode Reward: 10  | Average Reward 6.34  | Actor loss: 0.35 | Critic loss: 1.67 | Entropy loss: -0.0006  | Total Loss: 2.02 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1734/129000  | Episode Reward: 7  | Average Reward 6.34  | Actor loss: 0.04 | Critic loss: 4.22 | Entropy loss: -0.0003  | Total Loss: 4.25 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1735/129000  | Episode Reward: 6  | Average Reward 6.35  | Actor loss: -0.58 | Critic loss: 5.52 | Entropy loss: -0.0061  | Total Loss: 4.94 | Total Steps: 60\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1736/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: 0.04 | Critic loss: 3.47 | Entropy loss: -0.0003  | Total Loss: 3.50 | Total Steps: 30\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1737/129000  | Episode Reward: 1  | Average Reward 6.35  | Actor loss: -0.06 | Critic loss: 6.40 | Entropy loss: -0.0021  | Total Loss: 6.34 | Total Steps: 50\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1738/129000  | Episode Reward: 4  | Average Reward 6.33  | Actor loss: -0.06 | Critic loss: 7.03 | Entropy loss: -0.0003  | Total Loss: 6.97 | Total Steps: 50\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1739/129000  | Episode Reward: 6  | Average Reward 6.33  | Actor loss: -1.04 | Critic loss: 9.45 | Entropy loss: -0.0186  | Total Loss: 8.39 | Total Steps: 160\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1740/129000  | Episode Reward: 4  | Average Reward 6.32  | Actor loss: -0.01 | Critic loss: 5.00 | Entropy loss: -0.0006  | Total Loss: 4.99 | Total Steps: 53\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1741/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.22 | Critic loss: 6.20 | Entropy loss: -0.0002  | Total Loss: 6.42 | Total Steps: 8\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1742/129000  | Episode Reward: 1  | Average Reward 6.27  | Actor loss: -1.53 | Critic loss: 10.27 | Entropy loss: -0.0082  | Total Loss: 8.74 | Total Steps: 54\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1743/129000  | Episode Reward: 4  | Average Reward 6.24  | Actor loss: -0.02 | Critic loss: 3.93 | Entropy loss: -0.0003  | Total Loss: 3.92 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1744/129000  | Episode Reward: 7  | Average Reward 6.22  | Actor loss: 0.01 | Critic loss: 4.65 | Entropy loss: -0.0002  | Total Loss: 4.67 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1745/129000  | Episode Reward: 10  | Average Reward 6.25  | Actor loss: 1.51 | Critic loss: 6.13 | Entropy loss: -0.0018  | Total Loss: 7.64 | Total Steps: 11\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1746/129000  | Episode Reward: 4  | Average Reward 6.22  | Actor loss: -0.03 | Critic loss: 3.54 | Entropy loss: -0.0010  | Total Loss: 3.50 | Total Steps: 53\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1747/129000  | Episode Reward: 4  | Average Reward 6.20  | Actor loss: -0.55 | Critic loss: 6.51 | Entropy loss: -0.0059  | Total Loss: 5.96 | Total Steps: 47\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1748/129000  | Episode Reward: 4  | Average Reward 6.20  | Actor loss: -0.04 | Critic loss: 3.32 | Entropy loss: -0.0013  | Total Loss: 3.28 | Total Steps: 43\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1749/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.63 | Critic loss: 7.24 | Entropy loss: -0.0033  | Total Loss: 6.61 | Total Steps: 55\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1750/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.19 | Critic loss: 5.05 | Entropy loss: -0.0008  | Total Loss: 5.25 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1751/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: -0.08 | Critic loss: 3.82 | Entropy loss: -0.0019  | Total Loss: 3.74 | Total Steps: 44\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1752/129000  | Episode Reward: 7  | Average Reward 6.12  | Actor loss: 0.11 | Critic loss: 6.16 | Entropy loss: -0.0003  | Total Loss: 6.27 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1753/129000  | Episode Reward: 1  | Average Reward 6.09  | Actor loss: -0.22 | Critic loss: 9.09 | Entropy loss: -0.0012  | Total Loss: 8.87 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1754/129000  | Episode Reward: 7  | Average Reward 6.11  | Actor loss: 0.05 | Critic loss: 5.29 | Entropy loss: -0.0002  | Total Loss: 5.34 | Total Steps: 30\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1755/129000  | Episode Reward: 10  | Average Reward 6.11  | Actor loss: 0.02 | Critic loss: 9.30 | Entropy loss: -0.0000  | Total Loss: 9.32 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1756/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: -0.32 | Critic loss: 11.20 | Entropy loss: -0.0026  | Total Loss: 10.88 | Total Steps: 27\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1757/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.18 | Critic loss: 4.22 | Entropy loss: -0.0002  | Total Loss: 4.40 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1758/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.13 | Critic loss: 3.71 | Entropy loss: -0.0021  | Total Loss: 3.84 | Total Steps: 30\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1759/129000  | Episode Reward: 7  | Average Reward 6.16  | Actor loss: 0.34 | Critic loss: 6.37 | Entropy loss: -0.0012  | Total Loss: 6.71 | Total Steps: 30\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1760/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.18 | Critic loss: 7.61 | Entropy loss: -0.0016  | Total Loss: 7.43 | Total Steps: 49\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1761/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.01 | Critic loss: 1.60 | Entropy loss: -0.0000  | Total Loss: 1.61 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  | Episode: 1762/129000  | Episode Reward: 9  | Average Reward 6.15  | Actor loss: -0.35 | Critic loss: 2.58 | Entropy loss: -0.0061  | Total Loss: 2.22 | Total Steps: 61\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1763/129000  | Episode Reward: 7  | Average Reward 6.15  | Actor loss: 0.09 | Critic loss: 6.87 | Entropy loss: -0.0007  | Total Loss: 6.96 | Total Steps: 34\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1764/129000  | Episode Reward: 4  | Average Reward 6.15  | Actor loss: -0.02 | Critic loss: 4.65 | Entropy loss: -0.0002  | Total Loss: 4.63 | Total Steps: 47\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1765/129000  | Episode Reward: 10  | Average Reward 6.15  | Actor loss: 0.19 | Critic loss: 3.00 | Entropy loss: -0.0002  | Total Loss: 3.19 | Total Steps: 8\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1766/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.14 | Critic loss: 5.73 | Entropy loss: -0.0014  | Total Loss: 5.59 | Total Steps: 52\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1767/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.01 | Critic loss: 3.50 | Entropy loss: -0.0000  | Total Loss: 3.50 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1768/129000  | Episode Reward: 4  | Average Reward 6.09  | Actor loss: -0.07 | Critic loss: 3.38 | Entropy loss: -0.0006  | Total Loss: 3.31 | Total Steps: 43\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1769/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.01 | Critic loss: 1.70 | Entropy loss: -0.0000  | Total Loss: 1.71 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1770/129000  | Episode Reward: 0  | Average Reward 6.07  | Actor loss: -0.85 | Critic loss: 9.65 | Entropy loss: -0.0053  | Total Loss: 8.80 | Total Steps: 66\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1771/129000  | Episode Reward: 1  | Average Reward 6.05  | Actor loss: -0.78 | Critic loss: 9.73 | Entropy loss: -0.0044  | Total Loss: 8.95 | Total Steps: 64\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1772/129000  | Episode Reward: 10  | Average Reward 6.05  | Actor loss: 0.71 | Critic loss: 1.77 | Entropy loss: -0.0014  | Total Loss: 2.49 | Total Steps: 10\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1773/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 0.04 | Critic loss: 0.76 | Entropy loss: -0.0001  | Total Loss: 0.80 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1774/129000  | Episode Reward: 7  | Average Reward 6.10  | Actor loss: 0.04 | Critic loss: 5.67 | Entropy loss: -0.0009  | Total Loss: 5.71 | Total Steps: 44\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1775/129000  | Episode Reward: 10  | Average Reward 6.10  | Actor loss: -0.27 | Critic loss: 6.08 | Entropy loss: -0.0013  | Total Loss: 5.81 | Total Steps: 24\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1776/129000  | Episode Reward: 10  | Average Reward 6.10  | Actor loss: 0.02 | Critic loss: 0.72 | Entropy loss: -0.0000  | Total Loss: 0.74 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1777/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: -0.02 | Critic loss: 7.14 | Entropy loss: -0.0066  | Total Loss: 7.12 | Total Steps: 56\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1778/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.03 | Critic loss: 1.48 | Entropy loss: -0.0001  | Total Loss: 1.51 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1779/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.05 | Critic loss: 5.79 | Entropy loss: -0.0002  | Total Loss: 5.83 | Total Steps: 29\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1780/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.22 | Critic loss: 3.77 | Entropy loss: -0.0013  | Total Loss: 3.99 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1781/129000  | Episode Reward: 1  | Average Reward 6.12  | Actor loss: -0.17 | Critic loss: 7.99 | Entropy loss: -0.0021  | Total Loss: 7.82 | Total Steps: 46\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1782/129000  | Episode Reward: 7  | Average Reward 6.12  | Actor loss: 0.14 | Critic loss: 7.38 | Entropy loss: -0.0007  | Total Loss: 7.52 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1783/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.12 | Critic loss: 8.30 | Entropy loss: -0.0005  | Total Loss: 8.42 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1784/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.58 | Critic loss: 5.39 | Entropy loss: -0.0009  | Total Loss: 5.97 | Total Steps: 12\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1785/129000  | Episode Reward: 7  | Average Reward 6.16  | Actor loss: 0.05 | Critic loss: 6.93 | Entropy loss: -0.0003  | Total Loss: 6.98 | Total Steps: 34\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1786/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.10 | Critic loss: 4.39 | Entropy loss: -0.0004  | Total Loss: 4.48 | Total Steps: 29\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1787/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.01 | Critic loss: 2.92 | Entropy loss: -0.0003  | Total Loss: 2.93 | Total Steps: 36\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1788/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.01 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1789/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: 0.06 | Critic loss: 5.14 | Entropy loss: -0.0002  | Total Loss: 5.20 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1790/129000  | Episode Reward: -2  | Average Reward 6.14  | Actor loss: -0.39 | Critic loss: 14.93 | Entropy loss: -0.0054  | Total Loss: 14.53 | Total Steps: 90\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1791/129000  | Episode Reward: 3  | Average Reward 6.14  | Actor loss: -0.23 | Critic loss: 11.24 | Entropy loss: -0.0019  | Total Loss: 11.01 | Total Steps: 45\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1792/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -1.21 | Critic loss: 10.83 | Entropy loss: -0.0052  | Total Loss: 9.61 | Total Steps: 42\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1793/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.90 | Critic loss: 2.95 | Entropy loss: -0.0015  | Total Loss: 3.85 | Total Steps: 9\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1794/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: -0.57 | Critic loss: 7.86 | Entropy loss: -0.0043  | Total Loss: 7.29 | Total Steps: 47\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1795/129000  | Episode Reward: 1  | Average Reward 6.09  | Actor loss: -0.28 | Critic loss: 9.06 | Entropy loss: -0.0019  | Total Loss: 8.78 | Total Steps: 66\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1796/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.01 | Critic loss: 1.56 | Entropy loss: -0.0000  | Total Loss: 1.57 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1797/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.00 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.15 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1798/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.37 | Critic loss: 0.51 | Entropy loss: -0.0014  | Total Loss: 0.87 | Total Steps: 8\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1799/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.01 | Critic loss: 1.00 | Entropy loss: -0.0000  | Total Loss: 1.01 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1800/129000  | Episode Reward: 4  | Average Reward 6.16  | Actor loss: -0.02 | Critic loss: 8.31 | Entropy loss: -0.0004  | Total Loss: 8.29 | Total Steps: 53\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1801/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.19 | Critic loss: 3.36 | Entropy loss: -0.0012  | Total Loss: 3.55 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward 1.67  | Actor loss: 0.03 | Critic loss: 22.65 | Entropy loss: -0.0005  | Total Loss: 22.67 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 1  | Average Reward 1.72  | Actor loss: 0.10 | Critic loss: 11.67 | Entropy loss: -0.0036  | Total Loss: 11.76 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 3/100  | Episode Reward: -125  | Average Reward 1.07  | Actor loss: -22.11 | Critic loss: 129.18 | Entropy loss: -0.0155  | Total Loss: 107.06 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 4/100  | Episode Reward: -109  | Average Reward 0.47  | Actor loss: -0.02 | Critic loss: 86.96 | Entropy loss: -0.0203  | Total Loss: 86.92 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 7  | Average Reward 0.46  | Actor loss: 0.01 | Critic loss: 13.42 | Entropy loss: -0.0065  | Total Loss: 13.42 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 10  | Average Reward 0.47  | Actor loss: 0.00 | Critic loss: 2.84 | Entropy loss: -0.0013  | Total Loss: 2.84 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward 0.47  | Actor loss: 0.00 | Critic loss: 3.27 | Entropy loss: -0.0095  | Total Loss: 3.26 | Total Steps: 50\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 7  | Average Reward 0.49  | Actor loss: 0.02 | Critic loss: 17.19 | Entropy loss: -0.0172  | Total Loss: 17.19 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 4  | Average Reward 0.46  | Actor loss: 0.01 | Critic loss: 13.66 | Entropy loss: -0.0027  | Total Loss: 13.67 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward 0.51  | Actor loss: 4.31 | Critic loss: 15.84 | Entropy loss: -0.0211  | Total Loss: 20.13 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward 0.49  | Actor loss: 0.01 | Critic loss: 2.49 | Entropy loss: -0.0021  | Total Loss: 2.49 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 12/100  | Episode Reward: -118  | Average Reward -0.15  | Actor loss: -0.02 | Critic loss: 88.93 | Entropy loss: -0.0179  | Total Loss: 88.90 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 10  | Average Reward -0.14  | Actor loss: 2.23 | Critic loss: 25.54 | Entropy loss: -0.0158  | Total Loss: 27.75 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: -2  | Average Reward -0.18  | Actor loss: 2.67 | Critic loss: 22.03 | Entropy loss: -0.0247  | Total Loss: 24.68 | Total Steps: 95\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 7  | Average Reward -0.20  | Actor loss: 0.01 | Critic loss: 25.24 | Entropy loss: -0.0320  | Total Loss: 25.22 | Total Steps: 51\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 10  | Average Reward -0.04  | Actor loss: 0.00 | Critic loss: 5.13 | Entropy loss: -0.0056  | Total Loss: 5.13 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 7  | Average Reward -0.06  | Actor loss: 0.02 | Critic loss: 15.80 | Entropy loss: -0.0011  | Total Loss: 15.82 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward -0.01  | Actor loss: 0.02 | Critic loss: 25.22 | Entropy loss: -0.0008  | Total Loss: 25.24 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Average Reward -0.03  | Actor loss: 0.31 | Critic loss: 15.01 | Entropy loss: -0.0144  | Total Loss: 15.30 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward -0.01  | Actor loss: 0.02 | Critic loss: 16.03 | Entropy loss: -0.0181  | Total Loss: 16.03 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 4  | Average Reward 0.01  | Actor loss: 0.00 | Critic loss: 1.57 | Entropy loss: -0.0088  | Total Loss: 1.57 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 1  | Average Reward 0.01  | Actor loss: 0.03 | Critic loss: 2.66 | Entropy loss: -0.0224  | Total Loss: 2.67 | Total Steps: 55\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward 0.04  | Actor loss: 11.62 | Critic loss: 17.54 | Entropy loss: -0.0388  | Total Loss: 29.12 | Total Steps: 7\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 7  | Average Reward 0.02  | Actor loss: 0.02 | Critic loss: 5.23 | Entropy loss: -0.0050  | Total Loss: 5.25 | Total Steps: 36\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 4  | Average Reward -0.01  | Actor loss: 1.82 | Critic loss: 13.67 | Entropy loss: -0.0222  | Total Loss: 15.47 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: -5  | Average Reward -0.06  | Actor loss: 3.74 | Critic loss: 9.67 | Entropy loss: -0.0191  | Total Loss: 13.39 | Total Steps: 87\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 10  | Average Reward -0.06  | Actor loss: 0.01 | Critic loss: 29.93 | Entropy loss: -0.0374  | Total Loss: 29.90 | Total Steps: 8\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 7  | Average Reward -0.06  | Actor loss: 0.02 | Critic loss: 16.62 | Entropy loss: -0.0021  | Total Loss: 16.64 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 7  | Average Reward -0.07  | Actor loss: 0.01 | Critic loss: 13.12 | Entropy loss: -0.0069  | Total Loss: 13.12 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward -0.09  | Actor loss: 0.01 | Critic loss: 6.17 | Entropy loss: -0.0024  | Total Loss: 6.18 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward -0.07  | Actor loss: 0.56 | Critic loss: 13.10 | Entropy loss: -0.0107  | Total Loss: 13.65 | Total Steps: 54\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 4  | Average Reward -0.10  | Actor loss: 0.01 | Critic loss: 24.13 | Entropy loss: -0.0067  | Total Loss: 24.14 | Total Steps: 54\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 4  | Average Reward -0.10  | Actor loss: 0.00 | Critic loss: 2.66 | Entropy loss: -0.0014  | Total Loss: 2.66 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Average Reward -0.07  | Actor loss: 0.00 | Critic loss: 4.81 | Entropy loss: -0.0004  | Total Loss: 4.82 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 7  | Average Reward -0.07  | Actor loss: 0.02 | Critic loss: 2.43 | Entropy loss: -0.0039  | Total Loss: 2.44 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 10  | Average Reward -0.04  | Actor loss: 0.03 | Critic loss: 24.38 | Entropy loss: -0.0012  | Total Loss: 24.42 | Total Steps: 6\n",
      "TEST: ---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward -0.03  | Actor loss: 0.01 | Critic loss: 2.95 | Entropy loss: -0.0019  | Total Loss: 2.96 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 1  | Average Reward -0.06  | Actor loss: 0.07 | Critic loss: 9.69 | Entropy loss: -0.0035  | Total Loss: 9.76 | Total Steps: 59\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward -0.03  | Actor loss: 0.13 | Critic loss: 5.45 | Entropy loss: -0.0035  | Total Loss: 5.58 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 4  | Average Reward -0.01  | Actor loss: 0.21 | Critic loss: 8.43 | Entropy loss: -0.0207  | Total Loss: 8.62 | Total Steps: 54\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward -0.01  | Actor loss: 0.03 | Critic loss: 2.36 | Entropy loss: -0.0024  | Total Loss: 2.39 | Total Steps: 49\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 4  | Average Reward 0.06  | Actor loss: 0.02 | Critic loss: 3.64 | Entropy loss: -0.0070  | Total Loss: 3.65 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward 0.06  | Actor loss: 1.33 | Critic loss: 7.93 | Entropy loss: -0.0040  | Total Loss: 9.26 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 0.06  | Actor loss: 0.00 | Critic loss: 4.02 | Entropy loss: -0.0064  | Total Loss: 4.02 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 7  | Average Reward 0.13  | Actor loss: 0.04 | Critic loss: 5.04 | Entropy loss: -0.0114  | Total Loss: 5.07 | Total Steps: 35\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 0.35  | Actor loss: 4.31 | Critic loss: 15.84 | Entropy loss: -0.0226  | Total Loss: 20.13 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 7  | Average Reward 0.35  | Actor loss: 0.02 | Critic loss: 15.76 | Entropy loss: -0.0016  | Total Loss: 15.77 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 1.02  | Actor loss: 0.00 | Critic loss: 4.89 | Entropy loss: -0.0006  | Total Loss: 4.89 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.02 | Critic loss: 4.07 | Entropy loss: -0.0009  | Total Loss: 4.08 | Total Steps: 38\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward 1.04  | Actor loss: 0.01 | Critic loss: 9.82 | Entropy loss: -0.0074  | Total Loss: 9.82 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 0  | Average Reward 1.02  | Actor loss: 0.95 | Critic loss: 25.95 | Entropy loss: -0.0288  | Total Loss: 26.87 | Total Steps: 68\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 7  | Average Reward 1.05  | Actor loss: 1.64 | Critic loss: 28.87 | Entropy loss: -0.0119  | Total Loss: 30.50 | Total Steps: 39\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 4  | Average Reward 1.02  | Actor loss: 0.01 | Critic loss: 14.77 | Entropy loss: -0.0054  | Total Loss: 14.77 | Total Steps: 139\n",
      "TEST: ---cylinder---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 10  | Average Reward 1.05  | Actor loss: 0.00 | Critic loss: 3.09 | Entropy loss: -0.0028  | Total Loss: 3.09 | Total Steps: 305\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Average Reward 1.09  | Actor loss: 0.01 | Critic loss: 9.88 | Entropy loss: -0.0075  | Total Loss: 9.88 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "TEST: Step: 100\n",
      "Testing  | Episode: 56/100  | Episode Reward: -8  | Average Reward 1.00  | Actor loss: 0.50 | Critic loss: 3.57 | Entropy loss: -0.0229  | Total Loss: 4.05 | Total Steps: 100\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 1.00  | Actor loss: 0.02 | Critic loss: 2.31 | Entropy loss: -0.0093  | Total Loss: 2.32 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 7  | Average Reward 1.08  | Actor loss: 0.01 | Critic loss: 16.53 | Entropy loss: -0.0007  | Total Loss: 16.54 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward 1.11  | Actor loss: 0.03 | Critic loss: 23.89 | Entropy loss: -0.0007  | Total Loss: 23.91 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: -2  | Average Reward 1.08  | Actor loss: 0.00 | Critic loss: 1.53 | Entropy loss: -0.0126  | Total Loss: 1.52 | Total Steps: 55\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: -35  | Average Reward 1.44  | Actor loss: 17.11 | Critic loss: 11.00 | Entropy loss: -0.0309  | Total Loss: 28.07 | Total Steps: 199\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 7  | Average Reward 1.44  | Actor loss: 0.01 | Critic loss: 6.41 | Entropy loss: -0.0029  | Total Loss: 6.41 | Total Steps: 30\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 4  | Average Reward 1.44  | Actor loss: 0.02 | Critic loss: 1.33 | Entropy loss: -0.0017  | Total Loss: 1.35 | Total Steps: 46\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 7  | Average Reward 1.44  | Actor loss: 0.01 | Critic loss: 15.29 | Entropy loss: -0.0004  | Total Loss: 15.30 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 1  | Average Reward 2.02  | Actor loss: 0.14 | Critic loss: 6.66 | Entropy loss: -0.0123  | Total Loss: 6.79 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 66/100  | Episode Reward: -115  | Average Reward 1.45  | Actor loss: -0.01 | Critic loss: 136.13 | Entropy loss: -0.0132  | Total Loss: 136.11 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 7  | Average Reward 1.48  | Actor loss: 0.00 | Critic loss: 4.89 | Entropy loss: -0.0025  | Total Loss: 4.89 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 10  | Average Reward 1.52  | Actor loss: 2.56 | Critic loss: 34.47 | Entropy loss: -0.0127  | Total Loss: 37.02 | Total Steps: 23\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Average Reward 1.60  | Actor loss: 0.17 | Critic loss: 5.84 | Entropy loss: -0.0054  | Total Loss: 6.01 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 1.61  | Actor loss: 0.03 | Critic loss: 21.18 | Entropy loss: -0.0014  | Total Loss: 21.21 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 10  | Average Reward 1.66  | Actor loss: 0.00 | Critic loss: 4.56 | Entropy loss: -0.0005  | Total Loss: 4.56 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 7  | Average Reward 1.68  | Actor loss: 0.52 | Critic loss: 2.82 | Entropy loss: -0.0089  | Total Loss: 3.33 | Total Steps: 95\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 7  | Average Reward 1.71  | Actor loss: 0.05 | Critic loss: 4.84 | Entropy loss: -0.0102  | Total Loss: 4.88 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward 1.72  | Actor loss: 9.26 | Critic loss: 17.90 | Entropy loss: -0.0313  | Total Loss: 27.12 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 4  | Average Reward 1.71  | Actor loss: 0.01 | Critic loss: 15.50 | Entropy loss: -0.0289  | Total Loss: 15.48 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Average Reward 1.71  | Actor loss: 0.57 | Critic loss: 12.62 | Entropy loss: -0.0140  | Total Loss: 13.18 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 77/100  | Episode Reward: -118  | Average Reward 1.15  | Actor loss: -0.02 | Critic loss: 107.51 | Entropy loss: -0.0222  | Total Loss: 107.47 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward 1.11  | Actor loss: 1.41 | Critic loss: 8.65 | Entropy loss: -0.0107  | Total Loss: 10.05 | Total Steps: 48\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: -71  | Average Reward 0.74  | Actor loss: 10.61 | Critic loss: 7.01 | Entropy loss: -0.0169  | Total Loss: 17.60 | Total Steps: 376\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 4  | Average Reward 0.83  | Actor loss: 0.00 | Critic loss: 8.89 | Entropy loss: -0.0032  | Total Loss: 8.89 | Total Steps: 44\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 0  | Average Reward 0.78  | Actor loss: 0.05 | Critic loss: 10.77 | Entropy loss: -0.0167  | Total Loss: 10.80 | Total Steps: 77\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 4  | Average Reward 0.80  | Actor loss: 0.58 | Critic loss: 13.54 | Entropy loss: -0.0297  | Total Loss: 14.09 | Total Steps: 55\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 4  | Average Reward 0.84  | Actor loss: 0.00 | Critic loss: 2.21 | Entropy loss: -0.0089  | Total Loss: 2.21 | Total Steps: 45\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 84/100  | Episode Reward: -121  | Average Reward 0.18  | Actor loss: -1.75 | Critic loss: 88.14 | Entropy loss: -0.0178  | Total Loss: 86.37 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 10  | Average Reward 0.78  | Actor loss: 0.01 | Critic loss: 6.01 | Entropy loss: -0.0309  | Total Loss: 5.99 | Total Steps: 8\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 1  | Average Reward 0.81  | Actor loss: 0.01 | Critic loss: 12.85 | Entropy loss: -0.0076  | Total Loss: 12.86 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 4  | Average Reward 0.78  | Actor loss: 0.10 | Critic loss: 9.85 | Entropy loss: -0.0096  | Total Loss: 9.94 | Total Steps: 67\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 88/100  | Episode Reward: -121  | Average Reward 0.20  | Actor loss: -7.12 | Critic loss: 82.97 | Entropy loss: -0.0197  | Total Loss: 75.83 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward 0.21  | Actor loss: 0.04 | Critic loss: 3.41 | Entropy loss: -0.0147  | Total Loss: 3.44 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 4  | Average Reward 0.29  | Actor loss: 0.02 | Critic loss: 2.24 | Entropy loss: -0.0065  | Total Loss: 2.26 | Total Steps: 49\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 1  | Average Reward 0.26  | Actor loss: 0.04 | Critic loss: 2.25 | Entropy loss: -0.0166  | Total Loss: 2.27 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward 0.26  | Actor loss: 0.09 | Critic loss: 12.77 | Entropy loss: -0.0055  | Total Loss: 12.85 | Total Steps: 35\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: -11  | Average Reward 0.18  | Actor loss: 0.03 | Critic loss: 6.48 | Entropy loss: -0.0322  | Total Loss: 6.49 | Total Steps: 114\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: -5  | Average Reward 0.15  | Actor loss: 8.92 | Critic loss: 18.87 | Entropy loss: -0.0159  | Total Loss: 27.78 | Total Steps: 92\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 1  | Average Reward 0.15  | Actor loss: 0.11 | Critic loss: 10.08 | Entropy loss: -0.0143  | Total Loss: 10.17 | Total Steps: 61\n",
      "TEST: ---cube---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 96/100  | Episode Reward: -10  | Average Reward 0.09  | Actor loss: -0.00 | Critic loss: 77.35 | Entropy loss: -0.0002  | Total Loss: 77.34 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 7  | Average Reward 0.10  | Actor loss: 0.14 | Critic loss: 11.12 | Entropy loss: -0.0047  | Total Loss: 11.26 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 0.10  | Actor loss: 7.46 | Critic loss: 20.46 | Entropy loss: -0.0229  | Total Loss: 27.90 | Total Steps: 8\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward 0.12  | Actor loss: 1.40 | Critic loss: 28.80 | Entropy loss: -0.0192  | Total Loss: 30.18 | Total Steps: 34\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Average Reward 0.16  | Actor loss: 0.00 | Critic loss: 1.52 | Entropy loss: -0.0036  | Total Loss: 1.52 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1802/129000  | Episode Reward: 1  | Average Reward 6.20  | Actor loss: -0.54 | Critic loss: 11.89 | Entropy loss: -0.0035  | Total Loss: 11.35 | Total Steps: 84\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1803/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.41 | Critic loss: 9.38 | Entropy loss: -0.0025  | Total Loss: 8.96 | Total Steps: 88\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1804/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.04 | Critic loss: 6.42 | Entropy loss: -0.0004  | Total Loss: 6.38 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1805/129000  | Episode Reward: 4  | Average Reward 6.14  | Actor loss: 0.42 | Critic loss: 6.79 | Entropy loss: -0.0042  | Total Loss: 7.21 | Total Steps: 47\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1806/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.63 | Critic loss: 3.23 | Entropy loss: -0.0017  | Total Loss: 3.86 | Total Steps: 13\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1807/129000  | Episode Reward: 7  | Average Reward 6.12  | Actor loss: 0.03 | Critic loss: 5.09 | Entropy loss: -0.0001  | Total Loss: 5.12 | Total Steps: 34\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1808/129000  | Episode Reward: 4  | Average Reward 6.14  | Actor loss: -0.11 | Critic loss: 4.22 | Entropy loss: -0.0015  | Total Loss: 4.11 | Total Steps: 47\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1809/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.00 | Critic loss: 3.02 | Entropy loss: -0.0000  | Total Loss: 3.02 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1810/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.01 | Critic loss: 2.05 | Entropy loss: -0.0000  | Total Loss: 2.05 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1811/129000  | Episode Reward: 10  | Average Reward 6.23  | Actor loss: 0.00 | Critic loss: 1.34 | Entropy loss: -0.0000  | Total Loss: 1.34 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1812/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -0.03 | Critic loss: 7.16 | Entropy loss: -0.0002  | Total Loss: 7.13 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1813/129000  | Episode Reward: 4  | Average Reward 6.20  | Actor loss: 0.11 | Critic loss: 5.31 | Entropy loss: -0.0017  | Total Loss: 5.43 | Total Steps: 49\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1814/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.43 | Critic loss: 3.21 | Entropy loss: -0.0041  | Total Loss: 3.64 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1815/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -0.02 | Critic loss: 4.79 | Entropy loss: -0.0003  | Total Loss: 4.77 | Total Steps: 47\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1816/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.26 | Critic loss: 6.70 | Entropy loss: -0.0016  | Total Loss: 6.95 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1817/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: 0.10 | Critic loss: 4.24 | Entropy loss: -0.0015  | Total Loss: 4.34 | Total Steps: 48\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1818/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.01 | Critic loss: 1.69 | Entropy loss: -0.0000  | Total Loss: 1.70 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1819/129000  | Episode Reward: 10  | Average Reward 6.25  | Actor loss: 0.02 | Critic loss: 9.54 | Entropy loss: -0.0000  | Total Loss: 9.56 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1820/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.01 | Critic loss: 1.11 | Entropy loss: -0.0000  | Total Loss: 1.12 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1821/129000  | Episode Reward: 1  | Average Reward 6.23  | Actor loss: -0.18 | Critic loss: 8.39 | Entropy loss: -0.0011  | Total Loss: 8.21 | Total Steps: 51\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1822/129000  | Episode Reward: 4  | Average Reward 6.23  | Actor loss: -0.05 | Critic loss: 5.28 | Entropy loss: -0.0003  | Total Loss: 5.23 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1823/129000  | Episode Reward: 4  | Average Reward 6.20  | Actor loss: -0.36 | Critic loss: 4.84 | Entropy loss: -0.0059  | Total Loss: 4.47 | Total Steps: 55\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1824/129000  | Episode Reward: -20  | Average Reward 6.08  | Actor loss: -0.55 | Critic loss: 18.49 | Entropy loss: -0.0052  | Total Loss: 17.94 | Total Steps: 154\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1825/129000  | Episode Reward: 10  | Average Reward 6.09  | Actor loss: 0.23 | Critic loss: 4.02 | Entropy loss: -0.0013  | Total Loss: 4.25 | Total Steps: 32\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1826/129000  | Episode Reward: 4  | Average Reward 6.11  | Actor loss: -0.06 | Critic loss: 4.69 | Entropy loss: -0.0004  | Total Loss: 4.63 | Total Steps: 43\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1827/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: 0.32 | Critic loss: 3.65 | Entropy loss: -0.0037  | Total Loss: 3.97 | Total Steps: 47\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1828/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.14 | Critic loss: 3.73 | Entropy loss: -0.0008  | Total Loss: 3.87 | Total Steps: 31\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1829/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.08 | Critic loss: 3.95 | Entropy loss: -0.0006  | Total Loss: 4.03 | Total Steps: 31\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1830/129000  | Episode Reward: -14  | Average Reward 6.05  | Actor loss: -0.63 | Critic loss: 26.80 | Entropy loss: -0.0043  | Total Loss: 26.17 | Total Steps: 117\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1831/129000  | Episode Reward: 7  | Average Reward 6.07  | Actor loss: -0.12 | Critic loss: 2.69 | Entropy loss: -0.0032  | Total Loss: 2.57 | Total Steps: 52\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1832/129000  | Episode Reward: -2  | Average Reward 6.02  | Actor loss: -1.31 | Critic loss: 13.40 | Entropy loss: -0.0095  | Total Loss: 12.08 | Total Steps: 78\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1833/129000  | Episode Reward: 1  | Average Reward 6.00  | Actor loss: -0.20 | Critic loss: 10.48 | Entropy loss: -0.0041  | Total Loss: 10.28 | Total Steps: 86\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1834/129000  | Episode Reward: -5  | Average Reward 5.93  | Actor loss: -1.27 | Critic loss: 22.78 | Entropy loss: -0.0051  | Total Loss: 21.50 | Total Steps: 72\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1835/129000  | Episode Reward: 7  | Average Reward 5.92  | Actor loss: -0.00 | Critic loss: 7.10 | Entropy loss: -0.0003  | Total Loss: 7.09 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1836/129000  | Episode Reward: 4  | Average Reward 5.88  | Actor loss: -0.10 | Critic loss: 4.17 | Entropy loss: -0.0006  | Total Loss: 4.07 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1837/129000  | Episode Reward: -2  | Average Reward 5.84  | Actor loss: -0.45 | Critic loss: 14.33 | Entropy loss: -0.0051  | Total Loss: 13.87 | Total Steps: 76\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1838/129000  | Episode Reward: 7  | Average Reward 5.87  | Actor loss: 0.06 | Critic loss: 5.48 | Entropy loss: -0.0003  | Total Loss: 5.54 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1839/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: 0.13 | Critic loss: 3.68 | Entropy loss: -0.0007  | Total Loss: 3.82 | Total Steps: 34\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1840/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: 0.23 | Critic loss: 7.75 | Entropy loss: -0.0006  | Total Loss: 7.97 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1841/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: -0.30 | Critic loss: 4.45 | Entropy loss: -0.0017  | Total Loss: 4.15 | Total Steps: 30\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1842/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.07 | Critic loss: 3.83 | Entropy loss: -0.0003  | Total Loss: 3.91 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1843/129000  | Episode Reward: 7  | Average Reward 5.92  | Actor loss: 0.31 | Critic loss: 8.12 | Entropy loss: -0.0007  | Total Loss: 8.43 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1844/129000  | Episode Reward: 1  | Average Reward 5.87  | Actor loss: -0.70 | Critic loss: 14.90 | Entropy loss: -0.0102  | Total Loss: 14.19 | Total Steps: 97\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1845/129000  | Episode Reward: 7  | Average Reward 5.87  | Actor loss: 0.08 | Critic loss: 8.28 | Entropy loss: -0.0005  | Total Loss: 8.36 | Total Steps: 32\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1846/129000  | Episode Reward: 7  | Average Reward 5.86  | Actor loss: 0.14 | Critic loss: 3.28 | Entropy loss: -0.0009  | Total Loss: 3.42 | Total Steps: 34\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1847/129000  | Episode Reward: -20  | Average Reward 5.75  | Actor loss: -0.17 | Critic loss: 14.04 | Entropy loss: -0.0048  | Total Loss: 13.86 | Total Steps: 196\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1848/129000  | Episode Reward: 4  | Average Reward 5.75  | Actor loss: -0.96 | Critic loss: 7.43 | Entropy loss: -0.0048  | Total Loss: 6.47 | Total Steps: 41\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1849/129000  | Episode Reward: 4  | Average Reward 5.74  | Actor loss: -0.11 | Critic loss: 7.96 | Entropy loss: -0.0012  | Total Loss: 7.85 | Total Steps: 52\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1850/129000  | Episode Reward: 1  | Average Reward 5.72  | Actor loss: -0.07 | Critic loss: 5.11 | Entropy loss: -0.0035  | Total Loss: 5.03 | Total Steps: 193\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1851/129000  | Episode Reward: 10  | Average Reward 5.74  | Actor loss: 0.02 | Critic loss: 14.51 | Entropy loss: -0.0000  | Total Loss: 14.52 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1852/129000  | Episode Reward: 4  | Average Reward 5.74  | Actor loss: -0.17 | Critic loss: 4.21 | Entropy loss: -0.0010  | Total Loss: 4.05 | Total Steps: 42\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1853/129000  | Episode Reward: 7  | Average Reward 5.75  | Actor loss: 0.19 | Critic loss: 7.82 | Entropy loss: -0.0020  | Total Loss: 8.01 | Total Steps: 42\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1854/129000  | Episode Reward: 1  | Average Reward 5.74  | Actor loss: -0.25 | Critic loss: 11.87 | Entropy loss: -0.0047  | Total Loss: 11.62 | Total Steps: 58\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1855/129000  | Episode Reward: 10  | Average Reward 5.75  | Actor loss: 0.01 | Critic loss: 6.78 | Entropy loss: -0.0000  | Total Loss: 6.79 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1856/129000  | Episode Reward: 7  | Average Reward 5.74  | Actor loss: 0.09 | Critic loss: 6.94 | Entropy loss: -0.0004  | Total Loss: 7.03 | Total Steps: 32\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1857/129000  | Episode Reward: 7  | Average Reward 5.74  | Actor loss: 0.14 | Critic loss: 5.45 | Entropy loss: -0.0005  | Total Loss: 5.59 | Total Steps: 30\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1858/129000  | Episode Reward: 10  | Average Reward 5.77  | Actor loss: 0.01 | Critic loss: 4.28 | Entropy loss: -0.0000  | Total Loss: 4.29 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1859/129000  | Episode Reward: 4  | Average Reward 5.75  | Actor loss: -0.01 | Critic loss: 4.80 | Entropy loss: -0.0005  | Total Loss: 4.79 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1860/129000  | Episode Reward: 7  | Average Reward 5.77  | Actor loss: -0.19 | Critic loss: 5.61 | Entropy loss: -0.0019  | Total Loss: 5.42 | Total Steps: 77\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1861/129000  | Episode Reward: 10  | Average Reward 5.77  | Actor loss: 0.05 | Critic loss: 2.63 | Entropy loss: -0.0001  | Total Loss: 2.68 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1862/129000  | Episode Reward: -5  | Average Reward 5.71  | Actor loss: -0.50 | Critic loss: 19.96 | Entropy loss: -0.0027  | Total Loss: 19.45 | Total Steps: 87\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1863/129000  | Episode Reward: 7  | Average Reward 5.71  | Actor loss: -0.00 | Critic loss: 1.56 | Entropy loss: -0.0002  | Total Loss: 1.56 | Total Steps: 38\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1864/129000  | Episode Reward: 7  | Average Reward 5.70  | Actor loss: 0.12 | Critic loss: 6.36 | Entropy loss: -0.0050  | Total Loss: 6.48 | Total Steps: 59\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1865/129000  | Episode Reward: 10  | Average Reward 5.71  | Actor loss: 0.01 | Critic loss: 3.05 | Entropy loss: -0.0000  | Total Loss: 3.06 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1866/129000  | Episode Reward: 10  | Average Reward 5.71  | Actor loss: 0.07 | Critic loss: 4.03 | Entropy loss: -0.0001  | Total Loss: 4.10 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1867/129000  | Episode Reward: 7  | Average Reward 5.72  | Actor loss: 0.07 | Critic loss: 2.81 | Entropy loss: -0.0006  | Total Loss: 2.88 | Total Steps: 34\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1868/129000  | Episode Reward: 7  | Average Reward 5.74  | Actor loss: -0.01 | Critic loss: 2.39 | Entropy loss: -0.0004  | Total Loss: 2.39 | Total Steps: 44\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1869/129000  | Episode Reward: 1  | Average Reward 5.74  | Actor loss: -0.15 | Critic loss: 8.44 | Entropy loss: -0.0006  | Total Loss: 8.29 | Total Steps: 52\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1870/129000  | Episode Reward: 7  | Average Reward 5.74  | Actor loss: 0.08 | Critic loss: 4.39 | Entropy loss: -0.0003  | Total Loss: 4.46 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1871/129000  | Episode Reward: 4  | Average Reward 5.74  | Actor loss: -0.16 | Critic loss: 5.94 | Entropy loss: -0.0012  | Total Loss: 5.79 | Total Steps: 53\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1872/129000  | Episode Reward: 4  | Average Reward 5.72  | Actor loss: -0.01 | Critic loss: 8.44 | Entropy loss: -0.0008  | Total Loss: 8.42 | Total Steps: 39\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1873/129000  | Episode Reward: 10  | Average Reward 5.77  | Actor loss: 0.17 | Critic loss: 5.04 | Entropy loss: -0.0004  | Total Loss: 5.22 | Total Steps: 29\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1874/129000  | Episode Reward: 10  | Average Reward 5.80  | Actor loss: 0.01 | Critic loss: 2.60 | Entropy loss: -0.0000  | Total Loss: 2.61 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1875/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: -0.38 | Critic loss: 4.75 | Entropy loss: -0.0046  | Total Loss: 4.37 | Total Steps: 44\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1876/129000  | Episode Reward: 7  | Average Reward 5.83  | Actor loss: 0.07 | Critic loss: 5.27 | Entropy loss: -0.0003  | Total Loss: 5.35 | Total Steps: 30\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1877/129000  | Episode Reward: 10  | Average Reward 5.86  | Actor loss: 0.01 | Critic loss: 6.83 | Entropy loss: -0.0000  | Total Loss: 6.84 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1878/129000  | Episode Reward: 0  | Average Reward 5.81  | Actor loss: -0.53 | Critic loss: 14.26 | Entropy loss: -0.0032  | Total Loss: 13.73 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1879/129000  | Episode Reward: 7  | Average Reward 5.81  | Actor loss: 0.14 | Critic loss: 6.22 | Entropy loss: -0.0009  | Total Loss: 6.36 | Total Steps: 32\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1880/129000  | Episode Reward: 10  | Average Reward 5.84  | Actor loss: 0.01 | Critic loss: 1.46 | Entropy loss: -0.0000  | Total Loss: 1.48 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1881/129000  | Episode Reward: 1  | Average Reward 5.79  | Actor loss: -1.08 | Critic loss: 13.10 | Entropy loss: -0.0064  | Total Loss: 12.01 | Total Steps: 54\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1882/129000  | Episode Reward: 3  | Average Reward 5.76  | Actor loss: -2.43 | Critic loss: 6.48 | Entropy loss: -0.0152  | Total Loss: 4.03 | Total Steps: 63\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1883/129000  | Episode Reward: 7  | Average Reward 5.75  | Actor loss: 0.52 | Critic loss: 6.67 | Entropy loss: -0.0029  | Total Loss: 7.19 | Total Steps: 31\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1884/129000  | Episode Reward: 1  | Average Reward 5.71  | Actor loss: -0.03 | Critic loss: 8.85 | Entropy loss: -0.0004  | Total Loss: 8.82 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1885/129000  | Episode Reward: 4  | Average Reward 5.76  | Actor loss: -0.01 | Critic loss: 5.66 | Entropy loss: -0.0004  | Total Loss: 5.65 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1886/129000  | Episode Reward: 4  | Average Reward 5.76  | Actor loss: -0.05 | Critic loss: 3.26 | Entropy loss: -0.0004  | Total Loss: 3.20 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1887/129000  | Episode Reward: 10  | Average Reward 5.76  | Actor loss: 0.03 | Critic loss: 2.37 | Entropy loss: -0.0000  | Total Loss: 2.39 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1888/129000  | Episode Reward: 4  | Average Reward 5.76  | Actor loss: -0.75 | Critic loss: 7.48 | Entropy loss: -0.0097  | Total Loss: 6.72 | Total Steps: 74\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1889/129000  | Episode Reward: 4  | Average Reward 5.73  | Actor loss: -0.21 | Critic loss: 4.49 | Entropy loss: -0.0022  | Total Loss: 4.28 | Total Steps: 51\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1890/129000  | Episode Reward: 10  | Average Reward 5.78  | Actor loss: 0.07 | Critic loss: 5.27 | Entropy loss: -0.0002  | Total Loss: 5.34 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1891/129000  | Episode Reward: 4  | Average Reward 5.75  | Actor loss: -0.02 | Critic loss: 3.09 | Entropy loss: -0.0003  | Total Loss: 3.07 | Total Steps: 42\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1892/129000  | Episode Reward: 4  | Average Reward 5.73  | Actor loss: -0.09 | Critic loss: 3.16 | Entropy loss: -0.0011  | Total Loss: 3.07 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1893/129000  | Episode Reward: 10  | Average Reward 5.73  | Actor loss: 0.17 | Critic loss: 17.96 | Entropy loss: -0.0001  | Total Loss: 18.14 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1894/129000  | Episode Reward: 10  | Average Reward 5.75  | Actor loss: -0.86 | Critic loss: 5.25 | Entropy loss: -0.0166  | Total Loss: 4.37 | Total Steps: 110\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1895/129000  | Episode Reward: 10  | Average Reward 5.76  | Actor loss: 0.01 | Critic loss: 3.43 | Entropy loss: -0.0001  | Total Loss: 3.45 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1896/129000  | Episode Reward: 1  | Average Reward 5.75  | Actor loss: -0.16 | Critic loss: 13.87 | Entropy loss: -0.0005  | Total Loss: 13.71 | Total Steps: 52\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1897/129000  | Episode Reward: 7  | Average Reward 5.75  | Actor loss: 0.09 | Critic loss: 9.34 | Entropy loss: -0.0003  | Total Loss: 9.42 | Total Steps: 29\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1898/129000  | Episode Reward: -2  | Average Reward 5.70  | Actor loss: -0.72 | Critic loss: 12.93 | Entropy loss: -0.0068  | Total Loss: 12.21 | Total Steps: 93\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1899/129000  | Episode Reward: 1  | Average Reward 5.68  | Actor loss: -0.15 | Critic loss: 12.65 | Entropy loss: -0.0006  | Total Loss: 12.50 | Total Steps: 52\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1900/129000  | Episode Reward: 4  | Average Reward 5.70  | Actor loss: 0.03 | Critic loss: 5.30 | Entropy loss: -0.0028  | Total Loss: 5.33 | Total Steps: 55\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1901/129000  | Episode Reward: 7  | Average Reward 5.71  | Actor loss: 0.12 | Critic loss: 5.35 | Entropy loss: -0.0004  | Total Loss: 5.46 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward 0.17  | Actor loss: 0.04 | Critic loss: 6.50 | Entropy loss: -0.0098  | Total Loss: 6.53 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Average Reward 0.20  | Actor loss: 0.01 | Critic loss: 0.40 | Entropy loss: -0.0420  | Total Loss: 0.36 | Total Steps: 10\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 4  | Average Reward 0.17  | Actor loss: 3.37 | Critic loss: 4.68 | Entropy loss: -0.0327  | Total Loss: 8.02 | Total Steps: 97\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward 0.16  | Actor loss: 0.14 | Critic loss: 4.20 | Entropy loss: -0.0046  | Total Loss: 4.33 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 10  | Average Reward 0.17  | Actor loss: 0.06 | Critic loss: 5.90 | Entropy loss: -0.0032  | Total Loss: 5.96 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 4  | Average Reward 0.19  | Actor loss: 1.64 | Critic loss: 16.38 | Entropy loss: -0.0185  | Total Loss: 18.00 | Total Steps: 52\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 10  | Average Reward 0.23  | Actor loss: 0.01 | Critic loss: 5.59 | Entropy loss: -0.0065  | Total Loss: 5.59 | Total Steps: 29\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 1  | Average Reward 0.19  | Actor loss: 0.01 | Critic loss: 2.92 | Entropy loss: -0.0200  | Total Loss: 2.91 | Total Steps: 52\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward 0.23  | Actor loss: 0.01 | Critic loss: 10.69 | Entropy loss: -0.0041  | Total Loss: 10.70 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 4  | Average Reward 0.22  | Actor loss: 0.01 | Critic loss: 3.14 | Entropy loss: -0.0081  | Total Loss: 3.14 | Total Steps: 43\n",
      "TEST: ---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 1  | Average Reward 0.17  | Actor loss: 0.04 | Critic loss: 6.55 | Entropy loss: -0.0363  | Total Loss: 6.55 | Total Steps: 106\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 10  | Average Reward 0.17  | Actor loss: 0.26 | Critic loss: 8.43 | Entropy loss: -0.0116  | Total Loss: 8.68 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 4  | Average Reward 0.14  | Actor loss: 0.00 | Critic loss: 3.16 | Entropy loss: -0.0033  | Total Loss: 3.16 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 7  | Average Reward 0.13  | Actor loss: 0.00 | Critic loss: 5.41 | Entropy loss: -0.0006  | Total Loss: 5.41 | Total Steps: 34\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 10  | Average Reward 0.13  | Actor loss: 0.01 | Critic loss: 2.96 | Entropy loss: -0.0028  | Total Loss: 2.96 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 7  | Average Reward 0.12  | Actor loss: 1.13 | Critic loss: 26.22 | Entropy loss: -0.0180  | Total Loss: 27.34 | Total Steps: 51\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 4  | Average Reward 0.09  | Actor loss: 0.01 | Critic loss: 7.34 | Entropy loss: -0.0287  | Total Loss: 7.32 | Total Steps: 86\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 4  | Average Reward 0.06  | Actor loss: 0.41 | Critic loss: 7.26 | Entropy loss: -0.0232  | Total Loss: 7.65 | Total Steps: 42\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Average Reward 0.04  | Actor loss: 0.00 | Critic loss: 3.41 | Entropy loss: -0.0195  | Total Loss: 3.39 | Total Steps: 29\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 1  | Average Reward 0.04  | Actor loss: 0.02 | Critic loss: 26.23 | Entropy loss: -0.0476  | Total Loss: 26.20 | Total Steps: 75\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 1  | Average Reward -0.01  | Actor loss: 0.02 | Critic loss: 2.98 | Entropy loss: -0.0030  | Total Loss: 3.00 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 10  | Average Reward 0.06  | Actor loss: 0.01 | Critic loss: 5.12 | Entropy loss: -0.0011  | Total Loss: 5.14 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 7  | Average Reward 0.06  | Actor loss: 0.23 | Critic loss: 7.96 | Entropy loss: -0.0067  | Total Loss: 8.19 | Total Steps: 36\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 4  | Average Reward 0.04  | Actor loss: 0.00 | Critic loss: 1.44 | Entropy loss: -0.0114  | Total Loss: 1.43 | Total Steps: 44\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 4  | Average Reward 0.07  | Actor loss: 0.02 | Critic loss: 2.55 | Entropy loss: -0.0528  | Total Loss: 2.52 | Total Steps: 119\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 0.12  | Actor loss: 0.09 | Critic loss: 3.02 | Entropy loss: -0.0755  | Total Loss: 3.04 | Total Steps: 7\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 1  | Average Reward 0.09  | Actor loss: 1.68 | Critic loss: 26.37 | Entropy loss: -0.0271  | Total Loss: 28.02 | Total Steps: 59\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward 0.06  | Actor loss: 0.02 | Critic loss: 2.61 | Entropy loss: -0.0198  | Total Loss: 2.60 | Total Steps: 45\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 1  | Average Reward 0.01  | Actor loss: 0.00 | Critic loss: 1.71 | Entropy loss: -0.0256  | Total Loss: 1.69 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 4  | Average Reward 0.03  | Actor loss: 0.01 | Critic loss: 2.74 | Entropy loss: -0.0008  | Total Loss: 2.76 | Total Steps: 46\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: -2  | Average Reward -0.04  | Actor loss: 0.00 | Critic loss: 3.59 | Entropy loss: -0.0107  | Total Loss: 3.58 | Total Steps: 85\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 1  | Average Reward -0.08  | Actor loss: 0.03 | Critic loss: 3.41 | Entropy loss: -0.0183  | Total Loss: 3.42 | Total Steps: 95\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward -0.05  | Actor loss: 0.01 | Critic loss: 6.72 | Entropy loss: -0.0046  | Total Loss: 6.73 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 7  | Average Reward -0.04  | Actor loss: 0.08 | Critic loss: 11.79 | Entropy loss: -0.0065  | Total Loss: 11.87 | Total Steps: 35\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Average Reward 0.01  | Actor loss: 0.00 | Critic loss: 1.42 | Entropy loss: -0.0023  | Total Loss: 1.42 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 4  | Average Reward 0.02  | Actor loss: 0.01 | Critic loss: 2.64 | Entropy loss: -0.0038  | Total Loss: 2.64 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 2  | Average Reward 0.06  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0487  | Total Loss: 0.69 | Total Steps: 79\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Average Reward 0.06  | Actor loss: 0.03 | Critic loss: 3.01 | Entropy loss: -0.0095  | Total Loss: 3.03 | Total Steps: 6\n",
      "TEST: ---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: -2  | Average Reward 0.04  | Actor loss: 0.17 | Critic loss: 10.73 | Entropy loss: -0.0329  | Total Loss: 10.87 | Total Steps: 177\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 4  | Average Reward 0.06  | Actor loss: 0.02 | Critic loss: 2.46 | Entropy loss: -0.0024  | Total Loss: 2.48 | Total Steps: 49\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward 0.07  | Actor loss: 0.01 | Critic loss: 2.74 | Entropy loss: -0.0017  | Total Loss: 2.75 | Total Steps: 43\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 7  | Average Reward 0.07  | Actor loss: 1.66 | Critic loss: 9.08 | Entropy loss: -0.0053  | Total Loss: 10.74 | Total Steps: 31\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 10  | Average Reward 0.09  | Actor loss: 0.00 | Critic loss: 5.53 | Entropy loss: -0.0009  | Total Loss: 5.53 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 0.10  | Actor loss: 0.00 | Critic loss: 2.38 | Entropy loss: -0.0092  | Total Loss: 2.38 | Total Steps: 64\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 0.10  | Actor loss: 0.01 | Critic loss: 10.69 | Entropy loss: -0.0023  | Total Loss: 10.70 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 7  | Average Reward 0.10  | Actor loss: 0.55 | Critic loss: 16.57 | Entropy loss: -0.0329  | Total Loss: 17.09 | Total Steps: 61\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 10  | Average Reward 0.32  | Actor loss: 0.01 | Critic loss: 2.93 | Entropy loss: -0.0098  | Total Loss: 2.92 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward 0.37  | Actor loss: 0.01 | Critic loss: 3.82 | Entropy loss: -0.0011  | Total Loss: 3.83 | Total Steps: 38\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 4  | Average Reward 0.34  | Actor loss: 0.01 | Critic loss: 2.28 | Entropy loss: -0.0237  | Total Loss: 2.26 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 4  | Average Reward 0.33  | Actor loss: 0.00 | Critic loss: 3.70 | Entropy loss: -0.0016  | Total Loss: 3.70 | Total Steps: 42\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Average Reward 0.36  | Actor loss: 0.01 | Critic loss: 4.17 | Entropy loss: -0.0008  | Total Loss: 4.18 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 4  | Average Reward 0.33  | Actor loss: 0.22 | Critic loss: 11.10 | Entropy loss: -0.0168  | Total Loss: 11.30 | Total Steps: 50\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 4  | Average Reward 0.33  | Actor loss: 2.08 | Critic loss: 3.88 | Entropy loss: -0.0129  | Total Loss: 5.95 | Total Steps: 161\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 4  | Average Reward 0.30  | Actor loss: 0.03 | Critic loss: 1.89 | Entropy loss: -0.0158  | Total Loss: 1.91 | Total Steps: 39\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 5  | Average Reward 0.28  | Actor loss: 0.03 | Critic loss: 4.02 | Entropy loss: -0.0416  | Total Loss: 4.00 | Total Steps: 116\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 7  | Average Reward 0.26  | Actor loss: 0.01 | Critic loss: 8.41 | Entropy loss: -0.0159  | Total Loss: 8.40 | Total Steps: 24\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 4  | Average Reward 0.26  | Actor loss: 0.00 | Critic loss: 3.04 | Entropy loss: -0.0231  | Total Loss: 3.02 | Total Steps: 50\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 0.32  | Actor loss: 0.00 | Critic loss: 1.69 | Entropy loss: -0.0046  | Total Loss: 1.69 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 10  | Average Reward 0.32  | Actor loss: 0.89 | Critic loss: 8.09 | Entropy loss: -0.0378  | Total Loss: 8.94 | Total Steps: 11\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 3  | Average Reward 0.48  | Actor loss: 0.20 | Critic loss: 7.41 | Entropy loss: -0.0314  | Total Loss: 7.58 | Total Steps: 62\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward 0.47  | Actor loss: 0.01 | Critic loss: 3.42 | Entropy loss: -0.0044  | Total Loss: 3.42 | Total Steps: 30\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 4  | Average Reward 0.48  | Actor loss: 0.01 | Critic loss: 1.41 | Entropy loss: -0.0014  | Total Loss: 1.42 | Total Steps: 46\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 7  | Average Reward 0.49  | Actor loss: 0.14 | Critic loss: 12.09 | Entropy loss: -0.0206  | Total Loss: 12.21 | Total Steps: 43\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward 0.58  | Actor loss: 0.01 | Critic loss: 0.77 | Entropy loss: -0.0781  | Total Loss: 0.70 | Total Steps: 13\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 4  | Average Reward 0.56  | Actor loss: 0.11 | Critic loss: 8.06 | Entropy loss: -0.0295  | Total Loss: 8.14 | Total Steps: 47\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward 0.58  | Actor loss: 0.00 | Critic loss: 2.78 | Entropy loss: -0.0200  | Total Loss: 2.76 | Total Steps: 36\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: -2  | Average Reward 0.53  | Actor loss: 0.00 | Critic loss: 7.65 | Entropy loss: -0.0051  | Total Loss: 7.65 | Total Steps: 85\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 10  | Average Reward 0.53  | Actor loss: 0.03 | Critic loss: 3.08 | Entropy loss: -0.0023  | Total Loss: 3.11 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 10  | Average Reward 0.56  | Actor loss: 0.20 | Critic loss: 7.18 | Entropy loss: -0.0020  | Total Loss: 7.38 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 0.60  | Actor loss: 0.01 | Critic loss: 2.88 | Entropy loss: -0.0038  | Total Loss: 2.89 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 7  | Average Reward 0.61  | Actor loss: 0.04 | Critic loss: 2.47 | Entropy loss: -0.0056  | Total Loss: 2.50 | Total Steps: 69\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 7  | Average Reward 0.60  | Actor loss: 0.09 | Critic loss: 6.14 | Entropy loss: -0.0078  | Total Loss: 6.22 | Total Steps: 32\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 0.60  | Actor loss: 0.01 | Critic loss: 5.99 | Entropy loss: -0.0020  | Total Loss: 6.00 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 4  | Average Reward 0.58  | Actor loss: 4.06 | Critic loss: 8.54 | Entropy loss: -0.0151  | Total Loss: 12.58 | Total Steps: 41\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward 0.58  | Actor loss: 0.00 | Critic loss: 3.12 | Entropy loss: -0.0006  | Total Loss: 3.12 | Total Steps: 38\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 10  | Average Reward 0.60  | Actor loss: 0.01 | Critic loss: 0.48 | Entropy loss: -0.0188  | Total Loss: 0.47 | Total Steps: 8\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 4  | Average Reward 0.57  | Actor loss: 0.21 | Critic loss: 7.80 | Entropy loss: -0.0023  | Total Loss: 8.01 | Total Steps: 259\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 0.60  | Actor loss: 0.38 | Critic loss: 8.98 | Entropy loss: -0.0404  | Total Loss: 9.32 | Total Steps: 11\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 0  | Average Reward 0.58  | Actor loss: 3.40 | Critic loss: 20.21 | Entropy loss: -0.0335  | Total Loss: 23.57 | Total Steps: 64\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 10  | Average Reward 0.58  | Actor loss: 0.01 | Critic loss: 6.13 | Entropy loss: -0.0063  | Total Loss: 6.14 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 4  | Average Reward 0.55  | Actor loss: 0.00 | Critic loss: 3.05 | Entropy loss: -0.0034  | Total Loss: 3.05 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: -2  | Average Reward 0.49  | Actor loss: 0.00 | Critic loss: 2.21 | Entropy loss: -0.0211  | Total Loss: 2.19 | Total Steps: 89\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: -5  | Average Reward 0.41  | Actor loss: 0.05 | Critic loss: 15.41 | Entropy loss: -0.0373  | Total Loss: 15.42 | Total Steps: 173\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Average Reward 0.79  | Actor loss: 0.01 | Critic loss: 3.77 | Entropy loss: -0.0103  | Total Loss: 3.77 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 10  | Average Reward 0.81  | Actor loss: 0.05 | Critic loss: 7.15 | Entropy loss: -0.0176  | Total Loss: 7.18 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 10  | Average Reward 0.81  | Actor loss: 1.06 | Critic loss: 9.89 | Entropy loss: -0.0086  | Total Loss: 10.95 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward 0.81  | Actor loss: 0.05 | Critic loss: 3.37 | Entropy loss: -0.0320  | Total Loss: 3.38 | Total Steps: 26\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward 0.83  | Actor loss: 0.02 | Critic loss: 2.10 | Entropy loss: -0.0120  | Total Loss: 2.10 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 4  | Average Reward 0.84  | Actor loss: 0.00 | Critic loss: 2.18 | Entropy loss: -0.0071  | Total Loss: 2.18 | Total Steps: 46\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 1  | Average Reward 0.80  | Actor loss: 0.03 | Critic loss: 1.92 | Entropy loss: -0.0114  | Total Loss: 1.94 | Total Steps: 54\n",
      "TEST: ---black---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 91/100  | Episode Reward: -10  | Average Reward 0.70  | Actor loss: -0.00 | Critic loss: 79.40 | Entropy loss: -0.0010  | Total Loss: 79.40 | Total Steps: 500\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 9  | Average Reward 0.74  | Actor loss: 0.07 | Critic loss: 6.22 | Entropy loss: -0.0345  | Total Loss: 6.26 | Total Steps: 66\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 7  | Average Reward 0.76  | Actor loss: 0.01 | Critic loss: 6.46 | Entropy loss: -0.0150  | Total Loss: 6.46 | Total Steps: 34\n",
      "TEST: ---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 6  | Average Reward 0.78  | Actor loss: 0.01 | Critic loss: 0.55 | Entropy loss: -0.0288  | Total Loss: 0.53 | Total Steps: 70\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 1  | Average Reward 0.77  | Actor loss: 0.01 | Critic loss: 7.27 | Entropy loss: -0.0093  | Total Loss: 7.27 | Total Steps: 120\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: -11  | Average Reward 0.69  | Actor loss: 0.00 | Critic loss: 10.69 | Entropy loss: -0.0114  | Total Loss: 10.68 | Total Steps: 111\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 4  | Average Reward 0.66  | Actor loss: 0.00 | Critic loss: 4.29 | Entropy loss: -0.0118  | Total Loss: 4.28 | Total Steps: 47\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 0.69  | Actor loss: 0.01 | Critic loss: 4.26 | Entropy loss: -0.0025  | Total Loss: 4.27 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 10  | Average Reward 0.69  | Actor loss: 0.05 | Critic loss: 6.73 | Entropy loss: -0.0083  | Total Loss: 6.77 | Total Steps: 8\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 1  | Average Reward 0.65  | Actor loss: 0.01 | Critic loss: 4.89 | Entropy loss: -0.0014  | Total Loss: 4.90 | Total Steps: 53\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1902/129000  | Episode Reward: 1  | Average Reward 5.67  | Actor loss: -1.28 | Critic loss: 11.66 | Entropy loss: -0.0084  | Total Loss: 10.37 | Total Steps: 48\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1903/129000  | Episode Reward: 10  | Average Reward 5.67  | Actor loss: 0.00 | Critic loss: 1.59 | Entropy loss: -0.0000  | Total Loss: 1.60 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1904/129000  | Episode Reward: 4  | Average Reward 5.66  | Actor loss: 0.06 | Critic loss: 7.67 | Entropy loss: -0.0028  | Total Loss: 7.73 | Total Steps: 53\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1905/129000  | Episode Reward: 10  | Average Reward 5.66  | Actor loss: 0.04 | Critic loss: 2.23 | Entropy loss: -0.0000  | Total Loss: 2.27 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1906/129000  | Episode Reward: -3  | Average Reward 5.59  | Actor loss: -2.03 | Critic loss: 17.02 | Entropy loss: -0.0125  | Total Loss: 14.98 | Total Steps: 68\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1907/129000  | Episode Reward: 10  | Average Reward 5.63  | Actor loss: 0.18 | Critic loss: 2.80 | Entropy loss: -0.0003  | Total Loss: 2.98 | Total Steps: 8\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1908/129000  | Episode Reward: 10  | Average Reward 5.65  | Actor loss: 0.09 | Critic loss: 4.51 | Entropy loss: -0.0001  | Total Loss: 4.60 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1909/129000  | Episode Reward: 10  | Average Reward 5.68  | Actor loss: 0.01 | Critic loss: 2.19 | Entropy loss: -0.0000  | Total Loss: 2.20 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1910/129000  | Episode Reward: 7  | Average Reward 5.71  | Actor loss: -0.52 | Critic loss: 2.08 | Entropy loss: -0.0028  | Total Loss: 1.56 | Total Steps: 42\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1911/129000  | Episode Reward: 4  | Average Reward 5.70  | Actor loss: -1.19 | Critic loss: 7.90 | Entropy loss: -0.0079  | Total Loss: 6.71 | Total Steps: 63\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1912/129000  | Episode Reward: 10  | Average Reward 5.70  | Actor loss: 0.03 | Critic loss: 1.53 | Entropy loss: -0.0001  | Total Loss: 1.56 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1913/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: 0.06 | Critic loss: 3.52 | Entropy loss: -0.0003  | Total Loss: 3.58 | Total Steps: 36\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1914/129000  | Episode Reward: 7  | Average Reward 5.68  | Actor loss: -0.39 | Critic loss: 5.05 | Entropy loss: -0.0020  | Total Loss: 4.66 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1915/129000  | Episode Reward: 10  | Average Reward 5.73  | Actor loss: 0.01 | Critic loss: 0.65 | Entropy loss: -0.0000  | Total Loss: 0.66 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1916/129000  | Episode Reward: 10  | Average Reward 5.76  | Actor loss: 0.18 | Critic loss: 3.76 | Entropy loss: -0.0017  | Total Loss: 3.93 | Total Steps: 40\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1917/129000  | Episode Reward: 4  | Average Reward 5.75  | Actor loss: -0.14 | Critic loss: 7.37 | Entropy loss: -0.0006  | Total Loss: 7.24 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1918/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: 0.04 | Critic loss: 3.16 | Entropy loss: -0.0004  | Total Loss: 3.20 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1919/129000  | Episode Reward: 10  | Average Reward 5.87  | Actor loss: 0.33 | Critic loss: 2.96 | Entropy loss: -0.0004  | Total Loss: 3.29 | Total Steps: 8\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1920/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: -0.07 | Critic loss: 1.13 | Entropy loss: -0.0007  | Total Loss: 1.06 | Total Steps: 38\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1921/129000  | Episode Reward: 10  | Average Reward 5.89  | Actor loss: 0.01 | Critic loss: 2.07 | Entropy loss: -0.0000  | Total Loss: 2.07 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1922/129000  | Episode Reward: 10  | Average Reward 5.89  | Actor loss: 0.06 | Critic loss: 4.71 | Entropy loss: -0.0003  | Total Loss: 4.77 | Total Steps: 30\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1923/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.49 | Critic loss: 0.71 | Entropy loss: -0.0038  | Total Loss: 1.19 | Total Steps: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1924/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.01 | Critic loss: 1.02 | Entropy loss: -0.0000  | Total Loss: 1.03 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1925/129000  | Episode Reward: 7  | Average Reward 5.94  | Actor loss: 0.12 | Critic loss: 5.32 | Entropy loss: -0.0006  | Total Loss: 5.44 | Total Steps: 30\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1926/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.01 | Critic loss: 0.29 | Entropy loss: -0.0000  | Total Loss: 0.30 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1927/129000  | Episode Reward: 4  | Average Reward 5.92  | Actor loss: -0.05 | Critic loss: 6.29 | Entropy loss: -0.0004  | Total Loss: 6.24 | Total Steps: 47\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1928/129000  | Episode Reward: 7  | Average Reward 5.91  | Actor loss: 0.09 | Critic loss: 3.27 | Entropy loss: -0.0005  | Total Loss: 3.36 | Total Steps: 34\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1929/129000  | Episode Reward: 7  | Average Reward 5.91  | Actor loss: -0.05 | Critic loss: 4.64 | Entropy loss: -0.0003  | Total Loss: 4.59 | Total Steps: 52\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1930/129000  | Episode Reward: 7  | Average Reward 5.89  | Actor loss: 0.08 | Critic loss: 3.95 | Entropy loss: -0.0005  | Total Loss: 4.03 | Total Steps: 30\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1931/129000  | Episode Reward: 10  | Average Reward 5.91  | Actor loss: 0.01 | Critic loss: 0.81 | Entropy loss: -0.0000  | Total Loss: 0.82 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1932/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.01 | Critic loss: 0.52 | Entropy loss: -0.0000  | Total Loss: 0.52 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1933/129000  | Episode Reward: 7  | Average Reward 5.94  | Actor loss: 0.03 | Critic loss: 4.27 | Entropy loss: -0.0002  | Total Loss: 4.30 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1934/129000  | Episode Reward: -5  | Average Reward 5.88  | Actor loss: -0.92 | Critic loss: 20.80 | Entropy loss: -0.0050  | Total Loss: 19.88 | Total Steps: 75\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1935/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: 0.04 | Critic loss: 4.21 | Entropy loss: -0.0004  | Total Loss: 4.25 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1936/129000  | Episode Reward: 7  | Average Reward 5.88  | Actor loss: 0.09 | Critic loss: 3.49 | Entropy loss: -0.0007  | Total Loss: 3.59 | Total Steps: 29\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1937/129000  | Episode Reward: 1  | Average Reward 5.88  | Actor loss: -0.31 | Critic loss: 11.34 | Entropy loss: -0.0010  | Total Loss: 11.03 | Total Steps: 52\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1938/129000  | Episode Reward: 4  | Average Reward 5.88  | Actor loss: -0.31 | Critic loss: 5.90 | Entropy loss: -0.0036  | Total Loss: 5.58 | Total Steps: 47\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1939/129000  | Episode Reward: 7  | Average Reward 5.89  | Actor loss: 0.03 | Critic loss: 2.88 | Entropy loss: -0.0004  | Total Loss: 2.90 | Total Steps: 30\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1940/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.15 | Critic loss: 2.15 | Entropy loss: -0.0002  | Total Loss: 2.30 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1941/129000  | Episode Reward: 4  | Average Reward 5.89  | Actor loss: -0.19 | Critic loss: 6.05 | Entropy loss: -0.0014  | Total Loss: 5.86 | Total Steps: 49\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1942/129000  | Episode Reward: 10  | Average Reward 5.93  | Actor loss: 0.02 | Critic loss: 2.91 | Entropy loss: -0.0033  | Total Loss: 2.93 | Total Steps: 156\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1943/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.41 | Critic loss: 4.04 | Entropy loss: -0.0006  | Total Loss: 4.45 | Total Steps: 11\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1944/129000  | Episode Reward: 10  | Average Reward 5.98  | Actor loss: 0.77 | Critic loss: 4.31 | Entropy loss: -0.0011  | Total Loss: 5.08 | Total Steps: 12\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1945/129000  | Episode Reward: 1  | Average Reward 5.93  | Actor loss: -0.77 | Critic loss: 8.64 | Entropy loss: -0.0056  | Total Loss: 7.86 | Total Steps: 50\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1946/129000  | Episode Reward: 7  | Average Reward 5.95  | Actor loss: 0.07 | Critic loss: 8.72 | Entropy loss: -0.0003  | Total Loss: 8.79 | Total Steps: 30\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1947/129000  | Episode Reward: 1  | Average Reward 5.93  | Actor loss: -0.15 | Critic loss: 11.02 | Entropy loss: -0.0016  | Total Loss: 10.87 | Total Steps: 51\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1948/129000  | Episode Reward: 7  | Average Reward 5.95  | Actor loss: -0.10 | Critic loss: 3.47 | Entropy loss: -0.0007  | Total Loss: 3.37 | Total Steps: 29\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1949/129000  | Episode Reward: 4  | Average Reward 5.95  | Actor loss: -0.67 | Critic loss: 9.23 | Entropy loss: -0.0033  | Total Loss: 8.56 | Total Steps: 55\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1950/129000  | Episode Reward: 10  | Average Reward 5.96  | Actor loss: 0.00 | Critic loss: 1.25 | Entropy loss: -0.0000  | Total Loss: 1.25 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1951/129000  | Episode Reward: 10  | Average Reward 6.00  | Actor loss: 0.30 | Critic loss: 4.00 | Entropy loss: -0.0073  | Total Loss: 4.30 | Total Steps: 56\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1952/129000  | Episode Reward: 7  | Average Reward 6.00  | Actor loss: 0.46 | Critic loss: 6.10 | Entropy loss: -0.0022  | Total Loss: 6.56 | Total Steps: 44\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1953/129000  | Episode Reward: 7  | Average Reward 6.03  | Actor loss: 0.03 | Critic loss: 5.43 | Entropy loss: -0.0002  | Total Loss: 5.47 | Total Steps: 30\n",
      "---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1954/129000  | Episode Reward: 7  | Average Reward 6.03  | Actor loss: 0.10 | Critic loss: 2.38 | Entropy loss: -0.0009  | Total Loss: 2.48 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1955/129000  | Episode Reward: 10  | Average Reward 6.03  | Actor loss: 0.17 | Critic loss: 1.55 | Entropy loss: -0.0003  | Total Loss: 1.71 | Total Steps: 8\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1956/129000  | Episode Reward: 10  | Average Reward 6.04  | Actor loss: 0.34 | Critic loss: 2.53 | Entropy loss: -0.0005  | Total Loss: 2.88 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1957/129000  | Episode Reward: 4  | Average Reward 6.01  | Actor loss: -0.05 | Critic loss: 6.92 | Entropy loss: -0.0018  | Total Loss: 6.86 | Total Steps: 44\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1958/129000  | Episode Reward: 10  | Average Reward 6.01  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1959/129000  | Episode Reward: 10  | Average Reward 6.03  | Actor loss: -0.01 | Critic loss: 0.53 | Entropy loss: -0.0027  | Total Loss: 0.51 | Total Steps: 7\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1960/129000  | Episode Reward: 6  | Average Reward 6.04  | Actor loss: -0.81 | Critic loss: 5.55 | Entropy loss: -0.0079  | Total Loss: 4.73 | Total Steps: 59\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1961/129000  | Episode Reward: 4  | Average Reward 6.00  | Actor loss: -0.14 | Critic loss: 5.85 | Entropy loss: -0.0013  | Total Loss: 5.71 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1962/129000  | Episode Reward: 1  | Average Reward 5.96  | Actor loss: -0.79 | Critic loss: 9.10 | Entropy loss: -0.0072  | Total Loss: 8.30 | Total Steps: 63\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1963/129000  | Episode Reward: 10  | Average Reward 5.98  | Actor loss: 0.45 | Critic loss: 2.04 | Entropy loss: -0.0011  | Total Loss: 2.49 | Total Steps: 11\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1964/129000  | Episode Reward: 10  | Average Reward 6.01  | Actor loss: 0.01 | Critic loss: 2.53 | Entropy loss: -0.0000  | Total Loss: 2.54 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1965/129000  | Episode Reward: 4  | Average Reward 5.98  | Actor loss: -0.19 | Critic loss: 3.99 | Entropy loss: -0.0036  | Total Loss: 3.80 | Total Steps: 47\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1966/129000  | Episode Reward: 1  | Average Reward 5.96  | Actor loss: -0.06 | Critic loss: 6.55 | Entropy loss: -0.0005  | Total Loss: 6.49 | Total Steps: 53\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1967/129000  | Episode Reward: 7  | Average Reward 5.95  | Actor loss: -0.01 | Critic loss: 5.75 | Entropy loss: -0.0002  | Total Loss: 5.74 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1968/129000  | Episode Reward: 10  | Average Reward 5.98  | Actor loss: 0.48 | Critic loss: 0.57 | Entropy loss: -0.0018  | Total Loss: 1.05 | Total Steps: 10\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1969/129000  | Episode Reward: 4  | Average Reward 5.95  | Actor loss: -0.13 | Critic loss: 7.75 | Entropy loss: -0.0005  | Total Loss: 7.62 | Total Steps: 42\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1970/129000  | Episode Reward: 10  | Average Reward 6.00  | Actor loss: 0.49 | Critic loss: 7.54 | Entropy loss: -0.0014  | Total Loss: 8.03 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1971/129000  | Episode Reward: 4  | Average Reward 6.01  | Actor loss: -0.21 | Critic loss: 5.23 | Entropy loss: -0.0041  | Total Loss: 5.01 | Total Steps: 140\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1972/129000  | Episode Reward: 7  | Average Reward 6.00  | Actor loss: -0.02 | Critic loss: 0.85 | Entropy loss: -0.0004  | Total Loss: 0.82 | Total Steps: 38\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1973/129000  | Episode Reward: 10  | Average Reward 6.00  | Actor loss: 0.02 | Critic loss: 4.39 | Entropy loss: -0.0000  | Total Loss: 4.41 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1974/129000  | Episode Reward: 10  | Average Reward 6.01  | Actor loss: 0.02 | Critic loss: 2.15 | Entropy loss: -0.0002  | Total Loss: 2.17 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1975/129000  | Episode Reward: 7  | Average Reward 6.00  | Actor loss: 0.15 | Critic loss: 7.09 | Entropy loss: -0.0006  | Total Loss: 7.24 | Total Steps: 29\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1976/129000  | Episode Reward: 10  | Average Reward 6.00  | Actor loss: 0.02 | Critic loss: 1.83 | Entropy loss: -0.0000  | Total Loss: 1.85 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1977/129000  | Episode Reward: 10  | Average Reward 6.03  | Actor loss: 0.01 | Critic loss: 0.91 | Entropy loss: -0.0000  | Total Loss: 0.91 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1978/129000  | Episode Reward: 10  | Average Reward 6.03  | Actor loss: 0.00 | Critic loss: 0.68 | Entropy loss: -0.0000  | Total Loss: 0.68 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1979/129000  | Episode Reward: 7  | Average Reward 6.03  | Actor loss: 0.02 | Critic loss: 7.63 | Entropy loss: -0.0004  | Total Loss: 7.66 | Total Steps: 30\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1980/129000  | Episode Reward: 10  | Average Reward 6.04  | Actor loss: 0.01 | Critic loss: 0.59 | Entropy loss: -0.0000  | Total Loss: 0.60 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1981/129000  | Episode Reward: 4  | Average Reward 6.06  | Actor loss: 0.02 | Critic loss: 7.47 | Entropy loss: -0.0008  | Total Loss: 7.49 | Total Steps: 47\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1982/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 0.01 | Critic loss: 0.63 | Entropy loss: -0.0000  | Total Loss: 0.64 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1983/129000  | Episode Reward: 10  | Average Reward 6.09  | Actor loss: 0.01 | Critic loss: 1.33 | Entropy loss: -0.0000  | Total Loss: 1.34 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1984/129000  | Episode Reward: 7  | Average Reward 6.08  | Actor loss: -0.06 | Critic loss: 4.68 | Entropy loss: -0.0004  | Total Loss: 4.63 | Total Steps: 42\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1985/129000  | Episode Reward: 4  | Average Reward 6.06  | Actor loss: -0.04 | Critic loss: 4.73 | Entropy loss: -0.0003  | Total Loss: 4.70 | Total Steps: 42\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1986/129000  | Episode Reward: 10  | Average Reward 6.08  | Actor loss: 0.05 | Critic loss: 0.75 | Entropy loss: -0.0001  | Total Loss: 0.80 | Total Steps: 6\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1987/129000  | Episode Reward: 4  | Average Reward 6.06  | Actor loss: -0.09 | Critic loss: 6.18 | Entropy loss: -0.0014  | Total Loss: 6.10 | Total Steps: 45\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1988/129000  | Episode Reward: 7  | Average Reward 6.04  | Actor loss: -0.50 | Critic loss: 3.59 | Entropy loss: -0.0019  | Total Loss: 3.08 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1989/129000  | Episode Reward: 7  | Average Reward 6.04  | Actor loss: 0.10 | Critic loss: 2.41 | Entropy loss: -0.0006  | Total Loss: 2.51 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1990/129000  | Episode Reward: 4  | Average Reward 6.08  | Actor loss: -0.91 | Critic loss: 6.47 | Entropy loss: -0.0055  | Total Loss: 5.56 | Total Steps: 63\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1991/129000  | Episode Reward: 7  | Average Reward 6.09  | Actor loss: 0.01 | Critic loss: 8.09 | Entropy loss: -0.0011  | Total Loss: 8.10 | Total Steps: 30\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1992/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.00 | Critic loss: 0.75 | Entropy loss: -0.0000  | Total Loss: 0.76 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1993/129000  | Episode Reward: 6  | Average Reward 6.11  | Actor loss: -0.40 | Critic loss: 6.14 | Entropy loss: -0.0068  | Total Loss: 5.73 | Total Steps: 66\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1994/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.03 | Critic loss: 3.88 | Entropy loss: -0.0004  | Total Loss: 3.91 | Total Steps: 30\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1995/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.07 | Critic loss: 1.10 | Entropy loss: -0.0001  | Total Loss: 1.17 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1996/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.02 | Critic loss: 0.63 | Entropy loss: -0.0000  | Total Loss: 0.65 | Total Steps: 6\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1997/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.03 | Critic loss: 0.40 | Entropy loss: -0.0001  | Total Loss: 0.42 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1998/129000  | Episode Reward: 1  | Average Reward 6.12  | Actor loss: -0.16 | Critic loss: 12.09 | Entropy loss: -0.0023  | Total Loss: 11.93 | Total Steps: 52\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1999/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.04 | Critic loss: 1.91 | Entropy loss: -0.0004  | Total Loss: 1.95 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2000/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.09 | Critic loss: 7.21 | Entropy loss: -0.0011  | Total Loss: 7.12 | Total Steps: 45\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2001/129000  | Episode Reward: 10  | Average Reward 6.12  | Actor loss: 0.04 | Critic loss: 13.44 | Entropy loss: -0.0000  | Total Loss: 13.48 | Total Steps: 6\n",
      "Model has been saved\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 4  | Average Reward 0.61  | Actor loss: 0.01 | Critic loss: 1.76 | Entropy loss: -0.0032  | Total Loss: 1.76 | Total Steps: 49\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Average Reward 0.66  | Actor loss: 0.43 | Critic loss: 14.70 | Entropy loss: -0.0035  | Total Loss: 15.13 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 7  | Average Reward 1.32  | Actor loss: 0.31 | Critic loss: 3.90 | Entropy loss: -0.0061  | Total Loss: 4.21 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward 1.90  | Actor loss: 0.57 | Critic loss: 11.98 | Entropy loss: -0.0037  | Total Loss: 12.55 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 5/100  | Episode Reward: -88  | Average Reward 1.43  | Actor loss: -1.31 | Critic loss: 120.45 | Entropy loss: -0.0276  | Total Loss: 119.12 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 3  | Average Reward 1.39  | Actor loss: 1.68 | Critic loss: 14.77 | Entropy loss: -0.0203  | Total Loss: 16.43 | Total Steps: 57\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 4  | Average Reward 1.41  | Actor loss: 0.15 | Critic loss: 5.26 | Entropy loss: -0.0069  | Total Loss: 5.40 | Total Steps: 46\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 4  | Average Reward 1.39  | Actor loss: 3.48 | Critic loss: 15.52 | Entropy loss: -0.0067  | Total Loss: 18.99 | Total Steps: 49\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 10  | Average Reward 1.42  | Actor loss: 0.09 | Critic loss: 2.47 | Entropy loss: -0.0194  | Total Loss: 2.54 | Total Steps: 8\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 4  | Average Reward 1.39  | Actor loss: 0.00 | Critic loss: 2.47 | Entropy loss: -0.0071  | Total Loss: 2.46 | Total Steps: 47\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 10  | Average Reward 1.42  | Actor loss: 0.00 | Critic loss: 0.89 | Entropy loss: -0.0017  | Total Loss: 0.90 | Total Steps: 6\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: -22  | Average Reward 1.90  | Actor loss: 1.05 | Critic loss: 4.69 | Entropy loss: -0.0314  | Total Loss: 5.71 | Total Steps: 469\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 13/100  | Episode Reward: -10  | Average Reward 1.80  | Actor loss: -0.00 | Critic loss: 77.08 | Entropy loss: -0.0015  | Total Loss: 77.07 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 1  | Average Reward 1.81  | Actor loss: 3.59 | Critic loss: 17.26 | Entropy loss: -0.0087  | Total Loss: 20.84 | Total Steps: 54\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 7  | Average Reward 1.81  | Actor loss: 0.70 | Critic loss: 19.13 | Entropy loss: -0.0212  | Total Loss: 19.81 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 4  | Average Reward 1.78  | Actor loss: 0.03 | Critic loss: 4.44 | Entropy loss: -0.0034  | Total Loss: 4.47 | Total Steps: 43\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 4  | Average Reward 1.77  | Actor loss: 0.20 | Critic loss: 0.54 | Entropy loss: -0.0070  | Total Loss: 0.74 | Total Steps: 49\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 7  | Average Reward 1.75  | Actor loss: 0.24 | Critic loss: 3.25 | Entropy loss: -0.0059  | Total Loss: 3.48 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 7  | Average Reward 1.75  | Actor loss: 1.42 | Critic loss: 19.95 | Entropy loss: -0.0104  | Total Loss: 21.36 | Total Steps: 24\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 7  | Average Reward 1.74  | Actor loss: 0.15 | Critic loss: 19.87 | Entropy loss: -0.0136  | Total Loss: 20.01 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward 1.77  | Actor loss: 0.02 | Critic loss: 15.34 | Entropy loss: -0.0026  | Total Loss: 15.35 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 7  | Average Reward 1.80  | Actor loss: 0.01 | Critic loss: 13.57 | Entropy loss: -0.0021  | Total Loss: 13.57 | Total Steps: 29\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 4  | Average Reward 1.77  | Actor loss: 1.65 | Critic loss: 29.61 | Entropy loss: -0.0361  | Total Loss: 31.22 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 7  | Average Reward 1.77  | Actor loss: 0.60 | Critic loss: 15.24 | Entropy loss: -0.0283  | Total Loss: 15.81 | Total Steps: 35\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward 1.78  | Actor loss: 0.57 | Critic loss: 14.76 | Entropy loss: -0.0245  | Total Loss: 15.30 | Total Steps: 33\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 1.86  | Actor loss: 1.50 | Critic loss: 33.01 | Entropy loss: -0.0180  | Total Loss: 34.49 | Total Steps: 27\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 10  | Average Reward 1.86  | Actor loss: 0.00 | Critic loss: 1.20 | Entropy loss: -0.0044  | Total Loss: 1.20 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 4  | Average Reward 1.84  | Actor loss: 0.00 | Critic loss: 3.46 | Entropy loss: -0.0034  | Total Loss: 3.46 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 1  | Average Reward 1.81  | Actor loss: 1.87 | Critic loss: 9.67 | Entropy loss: -0.0067  | Total Loss: 11.53 | Total Steps: 51\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: -3  | Average Reward 1.76  | Actor loss: 1.18 | Critic loss: 13.08 | Entropy loss: -0.0264  | Total Loss: 14.23 | Total Steps: 107\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 7  | Average Reward 1.76  | Actor loss: 0.29 | Critic loss: 8.32 | Entropy loss: -0.0084  | Total Loss: 8.60 | Total Steps: 39\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 4  | Average Reward 1.76  | Actor loss: 0.11 | Critic loss: 7.37 | Entropy loss: -0.0042  | Total Loss: 7.48 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 7  | Average Reward 1.78  | Actor loss: 0.01 | Critic loss: 2.56 | Entropy loss: -0.0017  | Total Loss: 2.56 | Total Steps: 30\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 7  | Average Reward 1.76  | Actor loss: 0.02 | Critic loss: 16.80 | Entropy loss: -0.0171  | Total Loss: 16.80 | Total Steps: 32\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: -6  | Average Reward 1.70  | Actor loss: 0.90 | Critic loss: 4.19 | Entropy loss: -0.0340  | Total Loss: 5.05 | Total Steps: 118\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 7  | Average Reward 1.69  | Actor loss: 0.20 | Critic loss: 9.26 | Entropy loss: -0.0143  | Total Loss: 9.45 | Total Steps: 49\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 7  | Average Reward 1.70  | Actor loss: 0.94 | Critic loss: 24.93 | Entropy loss: -0.0130  | Total Loss: 25.86 | Total Steps: 43\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 38/100  | Episode Reward: -121  | Average Reward 1.09  | Actor loss: -0.88 | Critic loss: 103.34 | Entropy loss: -0.0226  | Total Loss: 102.43 | Total Steps: 500\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward 1.09  | Actor loss: 0.00 | Critic loss: 0.87 | Entropy loss: -0.0043  | Total Loss: 0.87 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 1.12  | Actor loss: 0.98 | Critic loss: 19.00 | Entropy loss: -0.0303  | Total Loss: 19.94 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 4  | Average Reward 1.12  | Actor loss: 0.00 | Critic loss: 2.06 | Entropy loss: -0.0046  | Total Loss: 2.06 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 7  | Average Reward 1.14  | Actor loss: 0.04 | Critic loss: 5.28 | Entropy loss: -0.0066  | Total Loss: 5.32 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward 1.14  | Actor loss: 0.86 | Critic loss: 25.87 | Entropy loss: -0.0193  | Total Loss: 26.71 | Total Steps: 27\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 7  | Average Reward 1.14  | Actor loss: 0.00 | Critic loss: 2.01 | Entropy loss: -0.0135  | Total Loss: 2.00 | Total Steps: 36\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 1.15  | Actor loss: 0.01 | Critic loss: 0.39 | Entropy loss: -0.0250  | Total Loss: 0.37 | Total Steps: 13\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 1.15  | Actor loss: 0.14 | Critic loss: 3.28 | Entropy loss: -0.0452  | Total Loss: 3.37 | Total Steps: 10\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Average Reward 1.14  | Actor loss: 9.38 | Critic loss: 26.97 | Entropy loss: -0.0482  | Total Loss: 36.30 | Total Steps: 27\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 7  | Average Reward 1.12  | Actor loss: 0.61 | Critic loss: 12.97 | Entropy loss: -0.0047  | Total Loss: 13.58 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 10  | Average Reward 1.14  | Actor loss: 0.02 | Critic loss: 14.90 | Entropy loss: -0.0011  | Total Loss: 14.92 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 7  | Average Reward 1.14  | Actor loss: 0.00 | Critic loss: 0.89 | Entropy loss: -0.0021  | Total Loss: 0.90 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: -6  | Average Reward 1.10  | Actor loss: 0.95 | Critic loss: 24.76 | Entropy loss: -0.0366  | Total Loss: 25.68 | Total Steps: 125\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 10  | Average Reward 1.12  | Actor loss: 0.10 | Critic loss: 3.16 | Entropy loss: -0.0129  | Total Loss: 3.25 | Total Steps: 8\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 7  | Average Reward 1.14  | Actor loss: 0.48 | Critic loss: 8.22 | Entropy loss: -0.0018  | Total Loss: 8.70 | Total Steps: 275\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 4  | Average Reward 1.10  | Actor loss: 0.03 | Critic loss: 3.70 | Entropy loss: -0.0157  | Total Loss: 3.72 | Total Steps: 62\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Average Reward 1.10  | Actor loss: 0.00 | Critic loss: 0.66 | Entropy loss: -0.0031  | Total Loss: 0.65 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward 1.20  | Actor loss: 0.01 | Critic loss: 9.82 | Entropy loss: -0.0013  | Total Loss: 9.83 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 1.20  | Actor loss: 0.00 | Critic loss: 0.99 | Entropy loss: -0.0011  | Total Loss: 0.99 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 10  | Average Reward 1.21  | Actor loss: 0.00 | Critic loss: 2.22 | Entropy loss: -0.0024  | Total Loss: 2.22 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: 1  | Average Reward 1.17  | Actor loss: 0.53 | Critic loss: 16.41 | Entropy loss: -0.0108  | Total Loss: 16.92 | Total Steps: 54\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 7  | Average Reward 1.21  | Actor loss: 0.01 | Critic loss: 14.48 | Entropy loss: -0.0018  | Total Loss: 14.49 | Total Steps: 29\n",
      "TEST: ---capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Average Reward 1.44  | Actor loss: 0.01 | Critic loss: 11.40 | Entropy loss: -0.0004  | Total Loss: 11.41 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: -13  | Average Reward 1.33  | Actor loss: 0.72 | Critic loss: 14.50 | Entropy loss: -0.0339  | Total Loss: 15.19 | Total Steps: 158\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 1  | Average Reward 1.32  | Actor loss: 0.14 | Critic loss: 5.72 | Entropy loss: -0.0136  | Total Loss: 5.84 | Total Steps: 55\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 10  | Average Reward 1.33  | Actor loss: 0.00 | Critic loss: 1.00 | Entropy loss: -0.0031  | Total Loss: 1.00 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 65/100  | Episode Reward: -103  | Average Reward 0.81  | Actor loss: -13.26 | Critic loss: 95.32 | Entropy loss: -0.0229  | Total Loss: 82.03 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 1  | Average Reward 1.40  | Actor loss: 0.04 | Critic loss: 3.49 | Entropy loss: -0.0096  | Total Loss: 3.51 | Total Steps: 52\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 7  | Average Reward 1.40  | Actor loss: 0.00 | Critic loss: 3.25 | Entropy loss: -0.0131  | Total Loss: 3.24 | Total Steps: 30\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 10  | Average Reward 1.40  | Actor loss: 2.09 | Critic loss: 36.53 | Entropy loss: -0.0308  | Total Loss: 38.59 | Total Steps: 32\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 4  | Average Reward 1.36  | Actor loss: 2.49 | Critic loss: 22.87 | Entropy loss: -0.0051  | Total Loss: 25.35 | Total Steps: 49\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 1  | Average Reward 1.32  | Actor loss: 0.03 | Critic loss: 0.41 | Entropy loss: -0.0208  | Total Loss: 0.42 | Total Steps: 54\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 1  | Average Reward 1.27  | Actor loss: 0.04 | Critic loss: 2.60 | Entropy loss: -0.0039  | Total Loss: 2.64 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 1  | Average Reward 1.25  | Actor loss: 0.08 | Critic loss: 0.56 | Entropy loss: -0.0240  | Total Loss: 0.61 | Total Steps: 55\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 7  | Average Reward 1.25  | Actor loss: 0.01 | Critic loss: 2.22 | Entropy loss: -0.0164  | Total Loss: 2.21 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: -41  | Average Reward 0.99  | Actor loss: 0.07 | Critic loss: 3.24 | Entropy loss: -0.0186  | Total Loss: 3.29 | Total Steps: 237\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 1.02  | Actor loss: 1.27 | Critic loss: 32.51 | Entropy loss: -0.0275  | Total Loss: 33.75 | Total Steps: 25\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 4  | Average Reward 1.02  | Actor loss: 0.00 | Critic loss: 7.35 | Entropy loss: -0.0032  | Total Loss: 7.35 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 7  | Average Reward 1.65  | Actor loss: 0.61 | Critic loss: 12.85 | Entropy loss: -0.0033  | Total Loss: 13.46 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 1.68  | Actor loss: 0.28 | Critic loss: 7.74 | Entropy loss: -0.0109  | Total Loss: 8.01 | Total Steps: 36\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 1  | Average Reward 2.04  | Actor loss: 0.00 | Critic loss: 2.17 | Entropy loss: -0.0196  | Total Loss: 2.15 | Total Steps: 50\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 4  | Average Reward 2.04  | Actor loss: 0.06 | Critic loss: 7.38 | Entropy loss: -0.0025  | Total Loss: 7.43 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 7  | Average Reward 2.07  | Actor loss: 0.15 | Critic loss: 19.92 | Entropy loss: -0.0049  | Total Loss: 20.07 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward 2.10  | Actor loss: 0.02 | Critic loss: 15.35 | Entropy loss: -0.0012  | Total Loss: 15.36 | Total Steps: 6\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: -2  | Average Reward 2.07  | Actor loss: 0.30 | Critic loss: 11.00 | Entropy loss: -0.0246  | Total Loss: 11.28 | Total Steps: 141\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: -6  | Average Reward 2.65  | Actor loss: 0.67 | Critic loss: 13.95 | Entropy loss: -0.0327  | Total Loss: 14.58 | Total Steps: 175\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 10  | Average Reward 2.65  | Actor loss: 0.00 | Critic loss: 1.11 | Entropy loss: -0.0070  | Total Loss: 1.10 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 4  | Average Reward 2.66  | Actor loss: 0.31 | Critic loss: 8.22 | Entropy loss: -0.0138  | Total Loss: 8.52 | Total Steps: 45\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: -3  | Average Reward 2.62  | Actor loss: 0.74 | Critic loss: 20.77 | Entropy loss: -0.0300  | Total Loss: 21.48 | Total Steps: 140\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 7  | Average Reward 3.27  | Actor loss: 1.11 | Critic loss: 19.94 | Entropy loss: -0.0251  | Total Loss: 21.03 | Total Steps: 33\n",
      "TEST: ---prism---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward 3.27  | Actor loss: 0.10 | Critic loss: 3.12 | Entropy loss: -0.0107  | Total Loss: 3.21 | Total Steps: 8\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: -2  | Average Reward 3.23  | Actor loss: 0.30 | Critic loss: 6.40 | Entropy loss: -0.0347  | Total Loss: 6.67 | Total Steps: 50\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 1  | Average Reward 3.23  | Actor loss: 0.09 | Critic loss: 3.63 | Entropy loss: -0.0097  | Total Loss: 3.71 | Total Steps: 51\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 92/100  | Episode Reward: -82  | Average Reward 2.77  | Actor loss: -33.49 | Critic loss: 108.53 | Entropy loss: -0.0245  | Total Loss: 75.02 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 10  | Average Reward 2.88  | Actor loss: 0.01 | Critic loss: 13.47 | Entropy loss: -0.0044  | Total Loss: 13.48 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 4  | Average Reward 2.92  | Actor loss: 0.00 | Critic loss: 1.51 | Entropy loss: -0.0203  | Total Loss: 1.49 | Total Steps: 44\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward 2.96  | Actor loss: 0.48 | Critic loss: 4.22 | Entropy loss: -0.0073  | Total Loss: 4.69 | Total Steps: 41\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 7  | Average Reward 3.04  | Actor loss: 0.00 | Critic loss: 1.23 | Entropy loss: -0.0055  | Total Loss: 1.23 | Total Steps: 65\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 7  | Average Reward 3.04  | Actor loss: 1.72 | Critic loss: 18.05 | Entropy loss: -0.0215  | Total Loss: 19.75 | Total Steps: 91\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 7  | Average Reward 3.02  | Actor loss: 1.11 | Critic loss: 14.69 | Entropy loss: -0.0218  | Total Loss: 15.78 | Total Steps: 27\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 7  | Average Reward 3.02  | Actor loss: 0.67 | Critic loss: 11.34 | Entropy loss: -0.0094  | Total Loss: 12.00 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 1  | Average Reward 2.98  | Actor loss: 0.01 | Critic loss: 0.84 | Entropy loss: -0.0110  | Total Loss: 0.84 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2002/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: -0.24 | Critic loss: 9.89 | Entropy loss: -0.0009  | Total Loss: 9.65 | Total Steps: 42\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2003/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.02 | Critic loss: 7.04 | Entropy loss: -0.0009  | Total Loss: 7.06 | Total Steps: 27\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2004/129000  | Episode Reward: 1  | Average Reward 6.15  | Actor loss: -0.82 | Critic loss: 12.38 | Entropy loss: -0.0038  | Total Loss: 11.55 | Total Steps: 52\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2005/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.11 | Critic loss: 1.61 | Entropy loss: -0.0002  | Total Loss: 1.72 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2006/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.05 | Critic loss: 8.77 | Entropy loss: -0.0004  | Total Loss: 8.83 | Total Steps: 32\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2007/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.02 | Critic loss: 12.63 | Entropy loss: -0.0000  | Total Loss: 12.66 | Total Steps: 6\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2008/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.84 | Critic loss: 11.29 | Entropy loss: -0.0132  | Total Loss: 10.44 | Total Steps: 111\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2009/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.15 | Critic loss: 1.90 | Entropy loss: -0.0002  | Total Loss: 2.04 | Total Steps: 8\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2010/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.07 | Critic loss: 13.44 | Entropy loss: -0.0000  | Total Loss: 13.52 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2011/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.04 | Critic loss: 8.59 | Entropy loss: -0.0002  | Total Loss: 8.63 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2012/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.24 | Critic loss: 11.48 | Entropy loss: -0.0008  | Total Loss: 11.24 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2013/129000  | Episode Reward: 1  | Average Reward 6.15  | Actor loss: -0.02 | Critic loss: 11.26 | Entropy loss: -0.0009  | Total Loss: 11.24 | Total Steps: 53\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2014/129000  | Episode Reward: 4  | Average Reward 6.12  | Actor loss: -0.52 | Critic loss: 5.21 | Entropy loss: -0.0039  | Total Loss: 4.69 | Total Steps: 48\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2015/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.05 | Critic loss: 6.29 | Entropy loss: -0.0004  | Total Loss: 6.33 | Total Steps: 30\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2016/129000  | Episode Reward: 10  | Average Reward 6.15  | Actor loss: 0.25 | Critic loss: 3.19 | Entropy loss: -0.0024  | Total Loss: 3.43 | Total Steps: 45\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2017/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.13 | Critic loss: 3.08 | Entropy loss: -0.0013  | Total Loss: 3.21 | Total Steps: 32\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2018/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: -0.24 | Critic loss: 6.77 | Entropy loss: -0.0024  | Total Loss: 6.53 | Total Steps: 53\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2019/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.01 | Critic loss: 2.91 | Entropy loss: -0.0000  | Total Loss: 2.91 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2020/129000  | Episode Reward: -5  | Average Reward 6.06  | Actor loss: -1.40 | Critic loss: 19.73 | Entropy loss: -0.0083  | Total Loss: 18.32 | Total Steps: 95\n",
      "---sphere---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2021/129000  | Episode Reward: 9  | Average Reward 6.10  | Actor loss: -0.74 | Critic loss: 5.41 | Entropy loss: -0.0186  | Total Loss: 4.65 | Total Steps: 125\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2022/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.13 | Critic loss: 5.35 | Entropy loss: -0.0011  | Total Loss: 5.48 | Total Steps: 24\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2023/129000  | Episode Reward: 1  | Average Reward 6.12  | Actor loss: -0.08 | Critic loss: 12.80 | Entropy loss: -0.0005  | Total Loss: 12.72 | Total Steps: 53\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2024/129000  | Episode Reward: 10  | Average Reward 6.26  | Actor loss: 0.01 | Critic loss: 0.75 | Entropy loss: -0.0000  | Total Loss: 0.75 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2025/129000  | Episode Reward: 4  | Average Reward 6.24  | Actor loss: -0.16 | Critic loss: 7.82 | Entropy loss: -0.0053  | Total Loss: 7.66 | Total Steps: 48\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2026/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.22 | Critic loss: 9.50 | Entropy loss: -0.0010  | Total Loss: 9.72 | Total Steps: 34\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2027/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.17 | Critic loss: 3.08 | Entropy loss: -0.0011  | Total Loss: 3.25 | Total Steps: 32\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2028/129000  | Episode Reward: 7  | Average Reward 6.24  | Actor loss: 0.29 | Critic loss: 5.62 | Entropy loss: -0.0037  | Total Loss: 5.91 | Total Steps: 41\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2029/129000  | Episode Reward: 7  | Average Reward 6.22  | Actor loss: 0.07 | Critic loss: 7.81 | Entropy loss: -0.0005  | Total Loss: 7.88 | Total Steps: 32\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2030/129000  | Episode Reward: 7  | Average Reward 6.33  | Actor loss: 0.21 | Critic loss: 5.62 | Entropy loss: -0.0005  | Total Loss: 5.82 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2031/129000  | Episode Reward: 7  | Average Reward 6.33  | Actor loss: 0.08 | Critic loss: 8.09 | Entropy loss: -0.0014  | Total Loss: 8.17 | Total Steps: 32\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2032/129000  | Episode Reward: 10  | Average Reward 6.38  | Actor loss: 0.15 | Critic loss: 14.97 | Entropy loss: -0.0001  | Total Loss: 15.11 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2033/129000  | Episode Reward: 4  | Average Reward 6.40  | Actor loss: -0.02 | Critic loss: 7.19 | Entropy loss: -0.0015  | Total Loss: 7.18 | Total Steps: 40\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2034/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.17 | Critic loss: 2.92 | Entropy loss: -0.0075  | Total Loss: 3.08 | Total Steps: 52\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2035/129000  | Episode Reward: -2  | Average Reward 6.43  | Actor loss: -0.77 | Critic loss: 18.97 | Entropy loss: -0.0034  | Total Loss: 18.20 | Total Steps: 71\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2036/129000  | Episode Reward: 1  | Average Reward 6.42  | Actor loss: -0.44 | Critic loss: 10.54 | Entropy loss: -0.0027  | Total Loss: 10.10 | Total Steps: 53\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2037/129000  | Episode Reward: 4  | Average Reward 6.45  | Actor loss: -0.01 | Critic loss: 5.82 | Entropy loss: -0.0049  | Total Loss: 5.80 | Total Steps: 53\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2038/129000  | Episode Reward: 7  | Average Reward 6.45  | Actor loss: 0.03 | Critic loss: 7.35 | Entropy loss: -0.0006  | Total Loss: 7.38 | Total Steps: 29\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2039/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.06 | Critic loss: 13.27 | Entropy loss: -0.0000  | Total Loss: 13.32 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2040/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: -0.06 | Critic loss: 2.72 | Entropy loss: -0.0053  | Total Loss: 2.65 | Total Steps: 51\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2041/129000  | Episode Reward: 6  | Average Reward 6.47  | Actor loss: 0.14 | Critic loss: 6.94 | Entropy loss: -0.0065  | Total Loss: 7.07 | Total Steps: 53\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2042/129000  | Episode Reward: 10  | Average Reward 6.47  | Actor loss: 0.01 | Critic loss: 1.12 | Entropy loss: -0.0000  | Total Loss: 1.13 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2043/129000  | Episode Reward: -8  | Average Reward 6.39  | Actor loss: -1.01 | Critic loss: 19.63 | Entropy loss: -0.0181  | Total Loss: 18.61 | Total Steps: 166\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2044/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: -0.49 | Critic loss: 4.74 | Entropy loss: -0.0033  | Total Loss: 4.24 | Total Steps: 55\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2045/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: 0.02 | Critic loss: 5.63 | Entropy loss: -0.0007  | Total Loss: 5.65 | Total Steps: 30\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2046/129000  | Episode Reward: 1  | Average Reward 6.39  | Actor loss: -0.80 | Critic loss: 7.86 | Entropy loss: -0.0066  | Total Loss: 7.06 | Total Steps: 57\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2047/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.04 | Critic loss: 8.71 | Entropy loss: -0.0003  | Total Loss: 8.67 | Total Steps: 42\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2048/129000  | Episode Reward: 4  | Average Reward 6.51  | Actor loss: -0.76 | Critic loss: 9.71 | Entropy loss: -0.0038  | Total Loss: 8.94 | Total Steps: 68\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2049/129000  | Episode Reward: 3  | Average Reward 6.51  | Actor loss: -0.42 | Critic loss: 9.05 | Entropy loss: -0.0028  | Total Loss: 8.63 | Total Steps: 54\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2050/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.01 | Critic loss: 3.81 | Entropy loss: -0.0000  | Total Loss: 3.82 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2051/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: -0.84 | Critic loss: 8.48 | Entropy loss: -0.0087  | Total Loss: 7.63 | Total Steps: 88\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2052/129000  | Episode Reward: 10  | Average Reward 6.57  | Actor loss: 0.18 | Critic loss: 3.57 | Entropy loss: -0.0006  | Total Loss: 3.75 | Total Steps: 29\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2053/129000  | Episode Reward: -3  | Average Reward 6.52  | Actor loss: -1.23 | Critic loss: 13.92 | Entropy loss: -0.0239  | Total Loss: 12.67 | Total Steps: 176\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2054/129000  | Episode Reward: 4  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 5.02 | Entropy loss: -0.0039  | Total Loss: 5.04 | Total Steps: 34\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2055/129000  | Episode Reward: 7  | Average Reward 6.52  | Actor loss: 0.14 | Critic loss: 9.52 | Entropy loss: -0.0007  | Total Loss: 9.65 | Total Steps: 30\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2056/129000  | Episode Reward: 7  | Average Reward 6.52  | Actor loss: -0.78 | Critic loss: 5.19 | Entropy loss: -0.0023  | Total Loss: 4.41 | Total Steps: 30\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2057/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 10.08 | Entropy loss: -0.0000  | Total Loss: 10.10 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2058/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 1.70 | Critic loss: 10.07 | Entropy loss: -0.0019  | Total Loss: 11.77 | Total Steps: 12\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2059/129000  | Episode Reward: -7  | Average Reward 6.48  | Actor loss: -1.00 | Critic loss: 18.84 | Entropy loss: -0.0191  | Total Loss: 17.83 | Total Steps: 162\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2060/129000  | Episode Reward: 4  | Average Reward 6.46  | Actor loss: 0.03 | Critic loss: 6.03 | Entropy loss: -0.0004  | Total Loss: 6.06 | Total Steps: 42\n",
      "---cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2061/129000  | Episode Reward: 10  | Average Reward 6.46  | Actor loss: 0.02 | Critic loss: 11.12 | Entropy loss: -0.0000  | Total Loss: 11.14 | Total Steps: 6\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2062/129000  | Episode Reward: 10  | Average Reward 6.54  | Actor loss: 0.20 | Critic loss: 1.73 | Entropy loss: -0.0014  | Total Loss: 1.93 | Total Steps: 34\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2063/129000  | Episode Reward: 7  | Average Reward 6.54  | Actor loss: 0.08 | Critic loss: 6.71 | Entropy loss: -0.0003  | Total Loss: 6.79 | Total Steps: 30\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2064/129000  | Episode Reward: 10  | Average Reward 6.55  | Actor loss: 0.01 | Critic loss: 1.43 | Entropy loss: -0.0000  | Total Loss: 1.44 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2065/129000  | Episode Reward: 4  | Average Reward 6.53  | Actor loss: -0.12 | Critic loss: 5.77 | Entropy loss: -0.0023  | Total Loss: 5.65 | Total Steps: 52\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2066/129000  | Episode Reward: -2  | Average Reward 6.46  | Actor loss: -1.04 | Critic loss: 8.58 | Entropy loss: -0.0078  | Total Loss: 7.53 | Total Steps: 72\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2067/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.12 | Critic loss: 4.38 | Entropy loss: -0.0006  | Total Loss: 4.50 | Total Steps: 30\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2068/129000  | Episode Reward: 7  | Average Reward 6.46  | Actor loss: 0.29 | Critic loss: 6.24 | Entropy loss: -0.0026  | Total Loss: 6.53 | Total Steps: 39\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2069/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.09 | Critic loss: 7.06 | Entropy loss: -0.0014  | Total Loss: 7.15 | Total Steps: 44\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2070/129000  | Episode Reward: 7  | Average Reward 6.50  | Actor loss: 0.02 | Critic loss: 8.26 | Entropy loss: -0.0001  | Total Loss: 8.28 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2071/129000  | Episode Reward: -2  | Average Reward 6.46  | Actor loss: -0.46 | Critic loss: 18.39 | Entropy loss: -0.0023  | Total Loss: 17.93 | Total Steps: 76\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2072/129000  | Episode Reward: 10  | Average Reward 6.50  | Actor loss: -0.14 | Critic loss: 6.50 | Entropy loss: -0.0048  | Total Loss: 6.36 | Total Steps: 31\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2073/129000  | Episode Reward: -2  | Average Reward 6.43  | Actor loss: -1.07 | Critic loss: 15.33 | Entropy loss: -0.0053  | Total Loss: 14.26 | Total Steps: 56\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2074/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.02 | Critic loss: 5.32 | Entropy loss: -0.0000  | Total Loss: 5.34 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2075/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: -1.24 | Critic loss: 7.65 | Entropy loss: -0.0064  | Total Loss: 6.40 | Total Steps: 56\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2076/129000  | Episode Reward: 10  | Average Reward 6.43  | Actor loss: 0.22 | Critic loss: 1.70 | Entropy loss: -0.0003  | Total Loss: 1.92 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2077/129000  | Episode Reward: -5  | Average Reward 6.36  | Actor loss: -1.03 | Critic loss: 19.65 | Entropy loss: -0.0062  | Total Loss: 18.61 | Total Steps: 88\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2078/129000  | Episode Reward: -5  | Average Reward 6.33  | Actor loss: -0.50 | Critic loss: 15.08 | Entropy loss: -0.0061  | Total Loss: 14.57 | Total Steps: 93\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2079/129000  | Episode Reward: -5  | Average Reward 6.28  | Actor loss: -1.28 | Critic loss: 16.42 | Entropy loss: -0.0059  | Total Loss: 15.13 | Total Steps: 67\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2080/129000  | Episode Reward: 7  | Average Reward 6.26  | Actor loss: 0.08 | Critic loss: 7.00 | Entropy loss: -0.0003  | Total Loss: 7.08 | Total Steps: 30\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2081/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.03 | Critic loss: 3.79 | Entropy loss: -0.0000  | Total Loss: 3.83 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2082/129000  | Episode Reward: 10  | Average Reward 6.34  | Actor loss: 0.71 | Critic loss: 2.70 | Entropy loss: -0.0012  | Total Loss: 3.41 | Total Steps: 9\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2083/129000  | Episode Reward: 10  | Average Reward 6.36  | Actor loss: 1.19 | Critic loss: 3.27 | Entropy loss: -0.0018  | Total Loss: 4.46 | Total Steps: 10\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2084/129000  | Episode Reward: 10  | Average Reward 6.40  | Actor loss: 0.03 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2085/129000  | Episode Reward: 7  | Average Reward 6.42  | Actor loss: -0.02 | Critic loss: 9.85 | Entropy loss: -0.0064  | Total Loss: 9.82 | Total Steps: 42\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2086/129000  | Episode Reward: 4  | Average Reward 6.42  | Actor loss: 0.02 | Critic loss: 9.72 | Entropy loss: -0.0035  | Total Loss: 9.74 | Total Steps: 56\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2087/129000  | Episode Reward: 7  | Average Reward 6.40  | Actor loss: -0.41 | Critic loss: 10.56 | Entropy loss: -0.0029  | Total Loss: 10.15 | Total Steps: 33\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2088/129000  | Episode Reward: 4  | Average Reward 6.40  | Actor loss: -0.17 | Critic loss: 6.62 | Entropy loss: -0.0006  | Total Loss: 6.45 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2089/129000  | Episode Reward: 4  | Average Reward 6.40  | Actor loss: -0.09 | Critic loss: 7.36 | Entropy loss: -0.0004  | Total Loss: 7.27 | Total Steps: 43\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2090/129000  | Episode Reward: 7  | Average Reward 6.38  | Actor loss: 0.17 | Critic loss: 4.46 | Entropy loss: -0.0008  | Total Loss: 4.63 | Total Steps: 29\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2091/129000  | Episode Reward: 1  | Average Reward 6.37  | Actor loss: -0.28 | Critic loss: 11.93 | Entropy loss: -0.0015  | Total Loss: 11.65 | Total Steps: 43\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2092/129000  | Episode Reward: -5  | Average Reward 6.33  | Actor loss: -1.06 | Critic loss: 20.02 | Entropy loss: -0.0059  | Total Loss: 18.96 | Total Steps: 96\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2093/129000  | Episode Reward: 1  | Average Reward 6.28  | Actor loss: -0.10 | Critic loss: 8.37 | Entropy loss: -0.0008  | Total Loss: 8.28 | Total Steps: 53\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2094/129000  | Episode Reward: -14  | Average Reward 6.16  | Actor loss: -0.38 | Critic loss: 17.41 | Entropy loss: -0.0044  | Total Loss: 17.03 | Total Steps: 153\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2095/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: 0.27 | Critic loss: 8.23 | Entropy loss: -0.0062  | Total Loss: 8.49 | Total Steps: 67\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2096/129000  | Episode Reward: 4  | Average Reward 6.14  | Actor loss: -1.34 | Critic loss: 5.26 | Entropy loss: -0.0083  | Total Loss: 3.91 | Total Steps: 58\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2097/129000  | Episode Reward: 10  | Average Reward 6.16  | Actor loss: 0.02 | Critic loss: 7.72 | Entropy loss: -0.0000  | Total Loss: 7.73 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2098/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.01 | Critic loss: 1.97 | Entropy loss: -0.0000  | Total Loss: 1.98 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2099/129000  | Episode Reward: 10  | Average Reward 6.26  | Actor loss: 0.03 | Critic loss: 2.73 | Entropy loss: -0.0000  | Total Loss: 2.76 | Total Steps: 6\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2100/129000  | Episode Reward: -8  | Average Reward 6.21  | Actor loss: -1.43 | Critic loss: 19.55 | Entropy loss: -0.0055  | Total Loss: 18.11 | Total Steps: 86\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2101/129000  | Episode Reward: 4  | Average Reward 6.19  | Actor loss: 0.03 | Critic loss: 7.66 | Entropy loss: -0.0011  | Total Loss: 7.69 | Total Steps: 48\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: 10  | Average Reward 2.98  | Actor loss: 1.37 | Critic loss: 11.88 | Entropy loss: -0.0071  | Total Loss: 13.25 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 7  | Average Reward 2.96  | Actor loss: 0.01 | Critic loss: 10.01 | Entropy loss: -0.0109  | Total Loss: 10.01 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: 7  | Average Reward 2.98  | Actor loss: 0.12 | Critic loss: 11.91 | Entropy loss: -0.0059  | Total Loss: 12.03 | Total Steps: 32\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 4/100  | Episode Reward: -37  | Average Reward 2.76  | Actor loss: -1.74 | Critic loss: 70.70 | Entropy loss: -0.0388  | Total Loss: 68.92 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 4  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 3.60 | Entropy loss: -0.0010  | Total Loss: 3.62 | Total Steps: 46\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 6/100  | Episode Reward: 4  | Average Reward 2.73  | Actor loss: 0.04 | Critic loss: 7.35 | Entropy loss: -0.0021  | Total Loss: 7.39 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 4  | Average Reward 2.70  | Actor loss: 0.43 | Critic loss: 4.46 | Entropy loss: -0.0221  | Total Loss: 4.87 | Total Steps: 46\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 10  | Average Reward 2.75  | Actor loss: 0.74 | Critic loss: 4.80 | Entropy loss: -0.0299  | Total Loss: 5.51 | Total Steps: 14\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 8.69 | Entropy loss: -0.0075  | Total Loss: 8.69 | Total Steps: 36\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 10  | Average Reward 2.76  | Actor loss: 0.05 | Critic loss: 5.84 | Entropy loss: -0.0019  | Total Loss: 5.88 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 4  | Average Reward 2.77  | Actor loss: 0.00 | Critic loss: 5.53 | Entropy loss: -0.0026  | Total Loss: 5.53 | Total Steps: 43\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Average Reward 2.76  | Actor loss: 0.04 | Critic loss: 10.49 | Entropy loss: -0.0261  | Total Loss: 10.50 | Total Steps: 38\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 1  | Average Reward 2.75  | Actor loss: 0.05 | Critic loss: 8.19 | Entropy loss: -0.0139  | Total Loss: 8.23 | Total Steps: 53\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 10  | Average Reward 2.76  | Actor loss: 0.01 | Critic loss: 8.11 | Entropy loss: -0.0032  | Total Loss: 8.11 | Total Steps: 61\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 4  | Average Reward 2.73  | Actor loss: 0.01 | Critic loss: 5.00 | Entropy loss: -0.0065  | Total Loss: 5.01 | Total Steps: 43\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 7  | Average Reward 2.73  | Actor loss: 0.10 | Critic loss: 8.20 | Entropy loss: -0.0309  | Total Loss: 8.28 | Total Steps: 56\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 1  | Average Reward 2.71  | Actor loss: 0.01 | Critic loss: 5.56 | Entropy loss: -0.0154  | Total Loss: 5.55 | Total Steps: 36\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 10  | Average Reward 2.75  | Actor loss: 0.16 | Critic loss: 5.10 | Entropy loss: -0.0082  | Total Loss: 5.25 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 2.76  | Actor loss: 0.01 | Critic loss: 6.58 | Entropy loss: -0.0022  | Total Loss: 6.59 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 20/100  | Episode Reward: 10  | Average Reward 2.81  | Actor loss: 0.01 | Critic loss: 1.58 | Entropy loss: -0.0009  | Total Loss: 1.58 | Total Steps: 6\n",
      "TEST: ---red---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 4  | Average Reward 2.82  | Actor loss: 3.12 | Critic loss: 20.10 | Entropy loss: -0.0185  | Total Loss: 23.21 | Total Steps: 40\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 7  | Average Reward 2.81  | Actor loss: 0.01 | Critic loss: 15.60 | Entropy loss: -0.0021  | Total Loss: 15.61 | Total Steps: 29\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: 10  | Average Reward 2.82  | Actor loss: 0.37 | Critic loss: 6.71 | Entropy loss: -0.0638  | Total Loss: 7.01 | Total Steps: 7\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 10  | Average Reward 2.85  | Actor loss: 0.03 | Critic loss: 2.86 | Entropy loss: -0.0124  | Total Loss: 2.87 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 1  | Average Reward 2.83  | Actor loss: 0.00 | Critic loss: 5.45 | Entropy loss: -0.0140  | Total Loss: 5.44 | Total Steps: 50\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 10  | Average Reward 2.83  | Actor loss: 0.01 | Critic loss: 4.01 | Entropy loss: -0.0043  | Total Loss: 4.01 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 7  | Average Reward 2.87  | Actor loss: 0.03 | Critic loss: 24.80 | Entropy loss: -0.0344  | Total Loss: 24.79 | Total Steps: 74\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 28/100  | Episode Reward: 1  | Average Reward 2.85  | Actor loss: 0.52 | Critic loss: 12.03 | Entropy loss: -0.0142  | Total Loss: 12.54 | Total Steps: 52\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 7  | Average Reward 2.88  | Actor loss: 0.00 | Critic loss: 6.81 | Entropy loss: -0.0098  | Total Loss: 6.80 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 7  | Average Reward 2.90  | Actor loss: 0.06 | Critic loss: 8.71 | Entropy loss: -0.0008  | Total Loss: 8.76 | Total Steps: 38\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 4  | Average Reward 2.92  | Actor loss: 0.05 | Critic loss: 7.98 | Entropy loss: -0.0007  | Total Loss: 8.03 | Total Steps: 42\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 7  | Average Reward 2.96  | Actor loss: 0.73 | Critic loss: 15.09 | Entropy loss: -0.0050  | Total Loss: 15.82 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 10  | Average Reward 2.96  | Actor loss: 0.04 | Critic loss: 3.15 | Entropy loss: -0.0031  | Total Loss: 3.19 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 4  | Average Reward 2.94  | Actor loss: 0.00 | Critic loss: 5.54 | Entropy loss: -0.0027  | Total Loss: 5.54 | Total Steps: 43\n",
      "TEST: ---blue---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 35/100  | Episode Reward: -10  | Average Reward 2.84  | Actor loss: -0.00 | Critic loss: 74.24 | Entropy loss: -0.0001  | Total Loss: 74.24 | Total Steps: 500\n",
      "TEST: ---black---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 36/100  | Episode Reward: -10  | Average Reward 2.77  | Actor loss: -0.00 | Critic loss: 74.81 | Entropy loss: -0.0000  | Total Loss: 74.81 | Total Steps: 500\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 37/100  | Episode Reward: -67  | Average Reward 2.42  | Actor loss: -0.04 | Critic loss: 79.73 | Entropy loss: -0.0332  | Total Loss: 79.66 | Total Steps: 500\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: 10  | Average Reward 2.42  | Actor loss: 0.05 | Critic loss: 5.81 | Entropy loss: -0.0063  | Total Loss: 5.85 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 10  | Average Reward 2.48  | Actor loss: 0.06 | Critic loss: 11.76 | Entropy loss: -0.0004  | Total Loss: 11.82 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 2.52  | Actor loss: 0.53 | Critic loss: 3.94 | Entropy loss: -0.0310  | Total Loss: 4.44 | Total Steps: 20\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 7  | Average Reward 2.53  | Actor loss: 0.11 | Critic loss: 7.52 | Entropy loss: -0.0119  | Total Loss: 7.61 | Total Steps: 35\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 42/100  | Episode Reward: 10  | Average Reward 2.54  | Actor loss: 0.01 | Critic loss: 11.08 | Entropy loss: -0.0027  | Total Loss: 11.09 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 1  | Average Reward 2.50  | Actor loss: 0.01 | Critic loss: 3.70 | Entropy loss: -0.0112  | Total Loss: 3.70 | Total Steps: 41\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 10  | Average Reward 2.52  | Actor loss: 0.09 | Critic loss: 2.43 | Entropy loss: -0.0085  | Total Loss: 2.51 | Total Steps: 8\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 2.52  | Actor loss: 0.51 | Critic loss: 18.79 | Entropy loss: -0.0365  | Total Loss: 19.26 | Total Steps: 68\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 2.53  | Actor loss: 0.01 | Critic loss: 11.44 | Entropy loss: -0.0068  | Total Loss: 11.45 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 7  | Average Reward 2.52  | Actor loss: 2.42 | Critic loss: 15.72 | Entropy loss: -0.0038  | Total Loss: 18.13 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 48/100  | Episode Reward: 10  | Average Reward 2.53  | Actor loss: 0.01 | Critic loss: 1.52 | Entropy loss: -0.0012  | Total Loss: 1.52 | Total Steps: 6\n",
      "TEST: ---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 7  | Average Reward 2.54  | Actor loss: 0.01 | Critic loss: 3.62 | Entropy loss: -0.0015  | Total Loss: 3.63 | Total Steps: 38\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: -2  | Average Reward 2.52  | Actor loss: 0.01 | Critic loss: 15.25 | Entropy loss: -0.0060  | Total Loss: 15.25 | Total Steps: 85\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Average Reward 2.52  | Actor loss: 4.96 | Critic loss: 11.85 | Entropy loss: -0.0753  | Total Loss: 16.73 | Total Steps: 13\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 52/100  | Episode Reward: 4  | Average Reward 2.52  | Actor loss: 0.36 | Critic loss: 20.73 | Entropy loss: -0.0134  | Total Loss: 21.07 | Total Steps: 48\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 10  | Average Reward 2.54  | Actor loss: 6.50 | Critic loss: 13.76 | Entropy loss: -0.0257  | Total Loss: 20.23 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Average Reward 2.56  | Actor loss: 7.23 | Critic loss: 19.53 | Entropy loss: -0.0261  | Total Loss: 26.73 | Total Steps: 47\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 55/100  | Episode Reward: 7  | Average Reward 2.57  | Actor loss: 0.01 | Critic loss: 7.75 | Entropy loss: -0.0112  | Total Loss: 7.75 | Total Steps: 30\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: 10  | Average Reward 2.58  | Actor loss: 0.01 | Critic loss: 3.89 | Entropy loss: -0.0028  | Total Loss: 3.90 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: 10  | Average Reward 2.62  | Actor loss: 0.01 | Critic loss: 10.03 | Entropy loss: -0.0015  | Total Loss: 10.04 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 1  | Average Reward 2.57  | Actor loss: 0.08 | Critic loss: 8.17 | Entropy loss: -0.0036  | Total Loss: 8.24 | Total Steps: 52\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 59/100  | Episode Reward: -2  | Average Reward 2.51  | Actor loss: 0.23 | Critic loss: 9.95 | Entropy loss: -0.0487  | Total Loss: 10.13 | Total Steps: 105\n",
      "TEST: ---red---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 7  | Average Reward 2.53  | Actor loss: 0.02 | Critic loss: 8.47 | Entropy loss: -0.0126  | Total Loss: 8.47 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 7  | Average Reward 2.53  | Actor loss: 0.00 | Critic loss: 6.29 | Entropy loss: -0.0047  | Total Loss: 6.29 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 1  | Average Reward 2.52  | Actor loss: 0.15 | Critic loss: 9.86 | Entropy loss: -0.0018  | Total Loss: 10.01 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 10  | Average Reward 2.53  | Actor loss: 0.01 | Critic loss: 3.78 | Entropy loss: -0.0012  | Total Loss: 3.79 | Total Steps: 6\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: -35  | Average Reward 2.31  | Actor loss: 14.28 | Critic loss: 18.96 | Entropy loss: -0.0272  | Total Loss: 33.21 | Total Steps: 347\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 65/100  | Episode Reward: 10  | Average Reward 2.33  | Actor loss: 0.55 | Critic loss: 11.00 | Entropy loss: -0.0512  | Total Loss: 11.49 | Total Steps: 10\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 7  | Average Reward 2.33  | Actor loss: 0.02 | Critic loss: 10.90 | Entropy loss: -0.0033  | Total Loss: 10.91 | Total Steps: 30\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 1  | Average Reward 2.35  | Actor loss: 0.00 | Critic loss: 3.83 | Entropy loss: -0.0032  | Total Loss: 3.83 | Total Steps: 74\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 68/100  | Episode Reward: 4  | Average Reward 2.32  | Actor loss: 0.49 | Critic loss: 20.20 | Entropy loss: -0.0080  | Total Loss: 20.68 | Total Steps: 50\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: -9  | Average Reward 2.23  | Actor loss: 0.80 | Critic loss: 11.20 | Entropy loss: -0.0354  | Total Loss: 11.97 | Total Steps: 180\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward 2.23  | Actor loss: 0.26 | Critic loss: 9.82 | Entropy loss: -0.0251  | Total Loss: 10.05 | Total Steps: 8\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 1  | Average Reward 2.19  | Actor loss: 1.96 | Critic loss: 16.18 | Entropy loss: -0.0408  | Total Loss: 18.10 | Total Steps: 71\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 7  | Average Reward 2.19  | Actor loss: 0.60 | Critic loss: 12.16 | Entropy loss: -0.0065  | Total Loss: 12.75 | Total Steps: 31\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 10  | Average Reward 2.19  | Actor loss: 0.26 | Critic loss: 4.68 | Entropy loss: -0.0471  | Total Loss: 4.89 | Total Steps: 14\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 1  | Average Reward 2.18  | Actor loss: 0.02 | Critic loss: 3.71 | Entropy loss: -0.0016  | Total Loss: 3.73 | Total Steps: 50\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 10  | Average Reward 2.19  | Actor loss: 0.30 | Critic loss: 7.62 | Entropy loss: -0.0602  | Total Loss: 7.86 | Total Steps: 10\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 7  | Average Reward 2.18  | Actor loss: 2.42 | Critic loss: 15.68 | Entropy loss: -0.0055  | Total Loss: 18.09 | Total Steps: 31\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.06 | Critic loss: 11.76 | Entropy loss: -0.0004  | Total Loss: 11.82 | Total Steps: 31\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.92 | Critic loss: 12.35 | Entropy loss: -0.0107  | Total Loss: 13.26 | Total Steps: 8\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 10  | Average Reward 2.26  | Actor loss: 0.04 | Critic loss: 38.56 | Entropy loss: -0.0450  | Total Loss: 38.55 | Total Steps: 20\n",
      "TEST: ---black---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 10  | Average Reward 2.26  | Actor loss: 0.01 | Critic loss: 4.98 | Entropy loss: -0.0083  | Total Loss: 4.99 | Total Steps: 6\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward 2.29  | Actor loss: 0.01 | Critic loss: 1.57 | Entropy loss: -0.0020  | Total Loss: 1.58 | Total Steps: 6\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 7  | Average Reward 2.33  | Actor loss: 0.00 | Critic loss: 4.76 | Entropy loss: -0.0013  | Total Loss: 4.76 | Total Steps: 38\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 1  | Average Reward 2.37  | Actor loss: 0.00 | Critic loss: 8.10 | Entropy loss: -0.0011  | Total Loss: 8.10 | Total Steps: 53\n",
      "TEST: ---green---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 84/100  | Episode Reward: 10  | Average Reward 2.37  | Actor loss: 0.01 | Critic loss: 4.29 | Entropy loss: -0.0028  | Total Loss: 4.30 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 85/100  | Episode Reward: 4  | Average Reward 2.33  | Actor loss: 0.02 | Critic loss: 5.00 | Entropy loss: -0.0071  | Total Loss: 5.01 | Total Steps: 43\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 7  | Average Reward 2.32  | Actor loss: 0.00 | Critic loss: 6.74 | Entropy loss: -0.0042  | Total Loss: 6.74 | Total Steps: 34\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: 7  | Average Reward 2.32  | Actor loss: 0.16 | Critic loss: 12.54 | Entropy loss: -0.0051  | Total Loss: 12.69 | Total Steps: 30\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward 2.32  | Actor loss: 0.12 | Critic loss: 3.10 | Entropy loss: -0.0185  | Total Loss: 3.21 | Total Steps: 9\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 7  | Average Reward 2.33  | Actor loss: 0.22 | Critic loss: 17.16 | Entropy loss: -0.0074  | Total Loss: 17.38 | Total Steps: 48\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 10  | Average Reward 2.38  | Actor loss: 0.10 | Critic loss: 2.66 | Entropy loss: -0.0217  | Total Loss: 2.74 | Total Steps: 13\n",
      "TEST: ---yellow---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 91/100  | Episode Reward: -34  | Average Reward 2.26  | Actor loss: -0.14 | Critic loss: 69.82 | Entropy loss: -0.0458  | Total Loss: 69.64 | Total Steps: 500\n",
      "TEST: ---red---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 10  | Average Reward 2.27  | Actor loss: 0.01 | Critic loss: 2.31 | Entropy loss: -0.0086  | Total Loss: 2.31 | Total Steps: 6\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: -2  | Average Reward 2.22  | Actor loss: 0.05 | Critic loss: 3.23 | Entropy loss: -0.0310  | Total Loss: 3.25 | Total Steps: 97\n",
      "TEST: ---blue---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 7  | Average Reward 2.23  | Actor loss: 0.18 | Critic loss: 11.70 | Entropy loss: -0.0014  | Total Loss: 11.88 | Total Steps: 38\n",
      "TEST: ---green---\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward 2.25  | Actor loss: 0.00 | Critic loss: 3.94 | Entropy loss: -0.0031  | Total Loss: 3.94 | Total Steps: 215\n",
      "TEST: ---black---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 4  | Average Reward 2.33  | Actor loss: 0.22 | Critic loss: 5.91 | Entropy loss: -0.0004  | Total Loss: 6.13 | Total Steps: 47\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 10  | Average Reward 2.36  | Actor loss: 0.03 | Critic loss: 3.08 | Entropy loss: -0.0021  | Total Loss: 3.11 | Total Steps: 6\n",
      "TEST: ---blue---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward 2.36  | Actor loss: 0.57 | Critic loss: 20.09 | Entropy loss: -0.0139  | Total Loss: 20.65 | Total Steps: 31\n",
      "TEST: ---green---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 4  | Average Reward 2.33  | Actor loss: 0.04 | Critic loss: 9.68 | Entropy loss: -0.0045  | Total Loss: 9.72 | Total Steps: 46\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 7  | Average Reward 2.36  | Actor loss: 0.01 | Critic loss: 3.30 | Entropy loss: -0.0032  | Total Loss: 3.31 | Total Steps: 43\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2102/129000  | Episode Reward: 7  | Average Reward 6.22  | Actor loss: 0.20 | Critic loss: 5.19 | Entropy loss: -0.0015  | Total Loss: 5.38 | Total Steps: 37\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2103/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 0.02 | Critic loss: 1.28 | Entropy loss: -0.0000  | Total Loss: 1.30 | Total Steps: 6\n",
      "---yellow---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2104/129000  | Episode Reward: 10  | Average Reward 6.25  | Actor loss: 0.25 | Critic loss: 3.43 | Entropy loss: -0.0003  | Total Loss: 3.68 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2105/129000  | Episode Reward: 4  | Average Reward 6.22  | Actor loss: -0.16 | Critic loss: 6.10 | Entropy loss: -0.0019  | Total Loss: 5.94 | Total Steps: 53\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2106/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.98 | Critic loss: 3.63 | Entropy loss: -0.0021  | Total Loss: 4.61 | Total Steps: 11\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2107/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.33 | Critic loss: 3.31 | Entropy loss: -0.0004  | Total Loss: 3.64 | Total Steps: 8\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2108/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.25 | Critic loss: 1.84 | Entropy loss: -0.0005  | Total Loss: 2.09 | Total Steps: 8\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2109/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.16 | Critic loss: 2.30 | Entropy loss: -0.0002  | Total Loss: 2.46 | Total Steps: 8\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2110/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: -0.01 | Critic loss: 4.05 | Entropy loss: -0.0003  | Total Loss: 4.04 | Total Steps: 47\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2111/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.01 | Critic loss: 2.67 | Entropy loss: -0.0000  | Total Loss: 2.68 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2112/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.01 | Critic loss: 1.31 | Entropy loss: -0.0000  | Total Loss: 1.33 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2113/129000  | Episode Reward: 7  | Average Reward 6.32  | Actor loss: -0.60 | Critic loss: 2.83 | Entropy loss: -0.0150  | Total Loss: 2.22 | Total Steps: 78\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2114/129000  | Episode Reward: 4  | Average Reward 6.30  | Actor loss: -0.62 | Critic loss: 4.73 | Entropy loss: -0.0086  | Total Loss: 4.10 | Total Steps: 77\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2115/129000  | Episode Reward: 4  | Average Reward 6.27  | Actor loss: 0.00 | Critic loss: 7.27 | Entropy loss: -0.0003  | Total Loss: 7.27 | Total Steps: 47\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2116/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: -0.45 | Critic loss: 6.00 | Entropy loss: -0.0025  | Total Loss: 5.55 | Total Steps: 46\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2117/129000  | Episode Reward: 7  | Average Reward 6.27  | Actor loss: 0.21 | Critic loss: 7.46 | Entropy loss: -0.0010  | Total Loss: 7.66 | Total Steps: 34\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2118/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: -0.52 | Critic loss: 6.70 | Entropy loss: -0.0073  | Total Loss: 6.17 | Total Steps: 103\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2119/129000  | Episode Reward: 7  | Average Reward 6.24  | Actor loss: -0.62 | Critic loss: 3.81 | Entropy loss: -0.0108  | Total Loss: 3.18 | Total Steps: 69\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2120/129000  | Episode Reward: 1  | Average Reward 6.21  | Actor loss: -0.02 | Critic loss: 11.54 | Entropy loss: -0.0003  | Total Loss: 11.52 | Total Steps: 53\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2121/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.16 | Critic loss: 4.69 | Entropy loss: -0.0006  | Total Loss: 4.85 | Total Steps: 29\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2122/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: 0.01 | Critic loss: 2.79 | Entropy loss: -0.0000  | Total Loss: 2.79 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2123/129000  | Episode Reward: 7  | Average Reward 6.18  | Actor loss: 0.10 | Critic loss: 4.15 | Entropy loss: -0.0005  | Total Loss: 4.24 | Total Steps: 30\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2124/129000  | Episode Reward: 4  | Average Reward 6.15  | Actor loss: 0.01 | Critic loss: 6.50 | Entropy loss: -0.0002  | Total Loss: 6.51 | Total Steps: 47\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2125/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: -0.00 | Critic loss: 4.13 | Entropy loss: -0.0003  | Total Loss: 4.13 | Total Steps: 31\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2126/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: -0.02 | Critic loss: 4.99 | Entropy loss: -0.0001  | Total Loss: 4.98 | Total Steps: 43\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2127/129000  | Episode Reward: 7  | Average Reward 6.15  | Actor loss: -0.02 | Critic loss: 2.59 | Entropy loss: -0.0004  | Total Loss: 2.57 | Total Steps: 49\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2128/129000  | Episode Reward: 7  | Average Reward 6.15  | Actor loss: -0.24 | Critic loss: 3.49 | Entropy loss: -0.0019  | Total Loss: 3.25 | Total Steps: 47\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2129/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.13 | Critic loss: 2.26 | Entropy loss: -0.0002  | Total Loss: 2.39 | Total Steps: 8\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2130/129000  | Episode Reward: 4  | Average Reward 6.15  | Actor loss: -1.13 | Critic loss: 8.54 | Entropy loss: -0.0073  | Total Loss: 7.40 | Total Steps: 51\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2131/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: -0.27 | Critic loss: 3.19 | Entropy loss: -0.0065  | Total Loss: 2.92 | Total Steps: 51\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2132/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: -0.23 | Critic loss: 2.89 | Entropy loss: -0.0098  | Total Loss: 2.65 | Total Steps: 66\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2133/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.11 | Critic loss: 5.73 | Entropy loss: -0.0033  | Total Loss: 5.84 | Total Steps: 37\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2134/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.28 | Critic loss: 6.06 | Entropy loss: -0.0011  | Total Loss: 6.34 | Total Steps: 29\n",
      "---red---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2135/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.87 | Critic loss: 4.04 | Entropy loss: -0.0075  | Total Loss: 3.16 | Total Steps: 64\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2136/129000  | Episode Reward: 4  | Average Reward 6.17  | Actor loss: -0.03 | Critic loss: 6.25 | Entropy loss: -0.0006  | Total Loss: 6.22 | Total Steps: 49\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2137/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: 0.05 | Critic loss: 4.81 | Entropy loss: -0.0004  | Total Loss: 4.86 | Total Steps: 34\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2138/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: 0.07 | Critic loss: 5.42 | Entropy loss: -0.0003  | Total Loss: 5.49 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2139/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: 1.70 | Critic loss: 2.49 | Entropy loss: -0.0022  | Total Loss: 4.19 | Total Steps: 11\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2140/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.01 | Critic loss: 4.78 | Entropy loss: -0.0000  | Total Loss: 4.79 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2141/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 1.19 | Critic loss: 7.30 | Entropy loss: -0.0010  | Total Loss: 8.48 | Total Steps: 9\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2142/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -0.58 | Critic loss: 6.25 | Entropy loss: -0.0029  | Total Loss: 5.67 | Total Steps: 48\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2143/129000  | Episode Reward: 4  | Average Reward 6.18  | Actor loss: -0.03 | Critic loss: 8.59 | Entropy loss: -0.0003  | Total Loss: 8.56 | Total Steps: 44\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2144/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.23 | Critic loss: 1.86 | Entropy loss: -0.0002  | Total Loss: 2.09 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2145/129000  | Episode Reward: 10  | Average Reward 6.22  | Actor loss: 1.31 | Critic loss: 4.17 | Entropy loss: -0.0023  | Total Loss: 5.47 | Total Steps: 10\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2146/129000  | Episode Reward: 10  | Average Reward 6.24  | Actor loss: 0.01 | Critic loss: 3.17 | Entropy loss: -0.0000  | Total Loss: 3.18 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2147/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.28 | Critic loss: 2.17 | Entropy loss: -0.0010  | Total Loss: 2.45 | Total Steps: 13\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2148/129000  | Episode Reward: 4  | Average Reward 6.27  | Actor loss: -0.04 | Critic loss: 4.30 | Entropy loss: -0.0003  | Total Loss: 4.27 | Total Steps: 42\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2149/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.04 | Critic loss: 1.68 | Entropy loss: -0.0002  | Total Loss: 1.71 | Total Steps: 34\n",
      "---red---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2150/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: -0.04 | Critic loss: 4.74 | Entropy loss: -0.0003  | Total Loss: 4.70 | Total Steps: 43\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2151/129000  | Episode Reward: 7  | Average Reward 6.27  | Actor loss: -0.24 | Critic loss: 3.58 | Entropy loss: -0.0018  | Total Loss: 3.34 | Total Steps: 49\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2152/129000  | Episode Reward: 7  | Average Reward 6.27  | Actor loss: -0.41 | Critic loss: 5.89 | Entropy loss: -0.0088  | Total Loss: 5.46 | Total Steps: 55\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2153/129000  | Episode Reward: -2  | Average Reward 6.22  | Actor loss: -0.94 | Critic loss: 13.33 | Entropy loss: -0.0050  | Total Loss: 12.38 | Total Steps: 55\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2154/129000  | Episode Reward: 4  | Average Reward 6.21  | Actor loss: -0.02 | Critic loss: 3.76 | Entropy loss: -0.0002  | Total Loss: 3.74 | Total Steps: 42\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2155/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.01 | Critic loss: 2.54 | Entropy loss: -0.0000  | Total Loss: 2.55 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2156/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: -0.73 | Critic loss: 4.25 | Entropy loss: -0.0046  | Total Loss: 3.52 | Total Steps: 44\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2157/129000  | Episode Reward: 7  | Average Reward 6.21  | Actor loss: 0.05 | Critic loss: 5.76 | Entropy loss: -0.0002  | Total Loss: 5.81 | Total Steps: 30\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2158/129000  | Episode Reward: 7  | Average Reward 6.20  | Actor loss: -0.77 | Critic loss: 7.30 | Entropy loss: -0.0097  | Total Loss: 6.52 | Total Steps: 100\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2159/129000  | Episode Reward: 10  | Average Reward 6.20  | Actor loss: -0.08 | Critic loss: 2.87 | Entropy loss: -0.0016  | Total Loss: 2.78 | Total Steps: 124\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2160/129000  | Episode Reward: 10  | Average Reward 6.21  | Actor loss: 0.52 | Critic loss: 4.12 | Entropy loss: -0.0014  | Total Loss: 4.64 | Total Steps: 17\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2161/129000  | Episode Reward: 7  | Average Reward 6.23  | Actor loss: -0.46 | Critic loss: 1.75 | Entropy loss: -0.0045  | Total Loss: 1.28 | Total Steps: 56\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2162/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.02 | Critic loss: 2.03 | Entropy loss: -0.0000  | Total Loss: 2.05 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2163/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.00 | Critic loss: 0.70 | Entropy loss: -0.0000  | Total Loss: 0.70 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2164/129000  | Episode Reward: 7  | Average Reward 6.26  | Actor loss: 0.17 | Critic loss: 6.76 | Entropy loss: -0.0014  | Total Loss: 6.93 | Total Steps: 45\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2165/129000  | Episode Reward: 7  | Average Reward 6.28  | Actor loss: 0.04 | Critic loss: 8.39 | Entropy loss: -0.0003  | Total Loss: 8.43 | Total Steps: 30\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2166/129000  | Episode Reward: 10  | Average Reward 6.32  | Actor loss: 0.00 | Critic loss: 0.88 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---black---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2167/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.00 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2168/129000  | Episode Reward: 7  | Average Reward 6.32  | Actor loss: 0.03 | Critic loss: 6.67 | Entropy loss: -0.0013  | Total Loss: 6.70 | Total Steps: 54\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2169/129000  | Episode Reward: 10  | Average Reward 6.35  | Actor loss: 0.00 | Critic loss: 0.52 | Entropy loss: -0.0000  | Total Loss: 0.53 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2170/129000  | Episode Reward: 4  | Average Reward 6.32  | Actor loss: -0.06 | Critic loss: 9.33 | Entropy loss: -0.0004  | Total Loss: 9.27 | Total Steps: 47\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2171/129000  | Episode Reward: 10  | Average Reward 6.35  | Actor loss: 0.02 | Critic loss: 0.48 | Entropy loss: -0.0000  | Total Loss: 0.50 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2172/129000  | Episode Reward: 7  | Average Reward 6.35  | Actor loss: 0.04 | Critic loss: 5.21 | Entropy loss: -0.0002  | Total Loss: 5.25 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2173/129000  | Episode Reward: 1  | Average Reward 6.30  | Actor loss: -0.66 | Critic loss: 10.60 | Entropy loss: -0.0037  | Total Loss: 9.93 | Total Steps: 44\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2174/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: 0.03 | Critic loss: 5.36 | Entropy loss: -0.0008  | Total Loss: 5.39 | Total Steps: 47\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2175/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 1.33 | Entropy loss: -0.0000  | Total Loss: 1.34 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2176/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.05 | Critic loss: 3.05 | Entropy loss: -0.0003  | Total Loss: 3.10 | Total Steps: 36\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2177/129000  | Episode Reward: 7  | Average Reward 6.29  | Actor loss: -0.08 | Critic loss: 2.29 | Entropy loss: -0.0006  | Total Loss: 2.21 | Total Steps: 43\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2178/129000  | Episode Reward: 10  | Average Reward 6.29  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0000  | Total Loss: 0.73 | Total Steps: 6\n",
      "---blue---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2179/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0000  | Total Loss: 0.74 | Total Steps: 6\n",
      "---black---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2180/129000  | Episode Reward: 10  | Average Reward 6.30  | Actor loss: 0.00 | Critic loss: 0.45 | Entropy loss: -0.0000  | Total Loss: 0.45 | Total Steps: 6\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2181/129000  | Episode Reward: 10  | Average Reward 6.33  | Actor loss: 0.01 | Critic loss: 0.33 | Entropy loss: -0.0000  | Total Loss: 0.34 | Total Steps: 6\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2182/129000  | Episode Reward: 1  | Average Reward 6.29  | Actor loss: -0.71 | Critic loss: 12.14 | Entropy loss: -0.0029  | Total Loss: 11.43 | Total Steps: 53\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2183/129000  | Episode Reward: 1  | Average Reward 6.25  | Actor loss: -0.68 | Critic loss: 12.99 | Entropy loss: -0.0033  | Total Loss: 12.30 | Total Steps: 58\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2184/129000  | Episode Reward: 7  | Average Reward 6.25  | Actor loss: 0.12 | Critic loss: 6.25 | Entropy loss: -0.0008  | Total Loss: 6.37 | Total Steps: 36\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2185/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.01 | Critic loss: 3.66 | Entropy loss: -0.0000  | Total Loss: 3.68 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2186/129000  | Episode Reward: 4  | Average Reward 6.25  | Actor loss: -0.14 | Critic loss: 5.68 | Entropy loss: -0.0006  | Total Loss: 5.54 | Total Steps: 43\n",
      "---yellow---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2187/129000  | Episode Reward: 10  | Average Reward 6.28  | Actor loss: 0.01 | Critic loss: 0.48 | Entropy loss: -0.0000  | Total Loss: 0.49 | Total Steps: 6\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2188/129000  | Episode Reward: -17  | Average Reward 6.16  | Actor loss: -0.59 | Critic loss: 23.71 | Entropy loss: -0.0043  | Total Loss: 23.11 | Total Steps: 137\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2189/129000  | Episode Reward: 4  | Average Reward 6.14  | Actor loss: 0.20 | Critic loss: 5.00 | Entropy loss: -0.0020  | Total Loss: 5.19 | Total Steps: 48\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2190/129000  | Episode Reward: 10  | Average Reward 6.17  | Actor loss: 0.01 | Critic loss: 0.55 | Entropy loss: -0.0000  | Total Loss: 0.55 | Total Steps: 6\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2191/129000  | Episode Reward: 10  | Average Reward 6.18  | Actor loss: 0.01 | Critic loss: 1.62 | Entropy loss: -0.0000  | Total Loss: 1.62 | Total Steps: 6\n",
      "---black---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2192/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: -0.07 | Critic loss: 3.93 | Entropy loss: -0.0015  | Total Loss: 3.86 | Total Steps: 184\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2193/129000  | Episode Reward: 7  | Average Reward 6.17  | Actor loss: 0.22 | Critic loss: 8.53 | Entropy loss: -0.0007  | Total Loss: 8.75 | Total Steps: 29\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2194/129000  | Episode Reward: 4  | Average Reward 6.14  | Actor loss: -0.17 | Critic loss: 10.22 | Entropy loss: -0.0011  | Total Loss: 10.05 | Total Steps: 71\n",
      "---green---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2195/129000  | Episode Reward: 10  | Average Reward 6.14  | Actor loss: 0.01 | Critic loss: 0.75 | Entropy loss: -0.0000  | Total Loss: 0.76 | Total Steps: 6\n",
      "---yellow---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2196/129000  | Episode Reward: 7  | Average Reward 6.13  | Actor loss: 0.20 | Critic loss: 7.67 | Entropy loss: -0.0019  | Total Loss: 7.87 | Total Steps: 40\n",
      "---red---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2197/129000  | Episode Reward: 10  | Average Reward 6.13  | Actor loss: 0.06 | Critic loss: 0.94 | Entropy loss: -0.0001  | Total Loss: 1.00 | Total Steps: 6\n",
      "---green---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2198/129000  | Episode Reward: 7  | Average Reward 6.16  | Actor loss: 0.06 | Critic loss: 7.96 | Entropy loss: -0.0002  | Total Loss: 8.02 | Total Steps: 30\n",
      "---green---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2199/129000  | Episode Reward: 4  | Average Reward 6.13  | Actor loss: -0.14 | Critic loss: 9.44 | Entropy loss: -0.0026  | Total Loss: 9.30 | Total Steps: 32\n",
      "---blue---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2200/129000  | Episode Reward: 7  | Average Reward 6.14  | Actor loss: 0.00 | Critic loss: 4.38 | Entropy loss: -0.0002  | Total Loss: 4.38 | Total Steps: 47\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Step: 250\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 2201/129000  | Episode Reward: -52  | Average Reward 5.83  | Actor loss: -0.36 | Critic loss: 18.41 | Entropy loss: -0.0044  | Total Loss: 18.05 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -1.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 1/100  | Episode Reward: -27  | Average Reward 2.21  | Actor loss: 7.40 | Critic loss: 18.35 | Entropy loss: -0.0222  | Total Loss: 25.72 | Total Steps: 215\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 2/100  | Episode Reward: 10  | Average Reward 2.21  | Actor loss: 0.01 | Critic loss: 1.55 | Entropy loss: -0.0048  | Total Loss: 1.55 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 3/100  | Episode Reward: -32  | Average Reward 2.01  | Actor loss: 0.09 | Critic loss: 15.71 | Entropy loss: -0.0145  | Total Loss: 15.79 | Total Steps: 205\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 4/100  | Episode Reward: 7  | Average Reward 2.01  | Actor loss: 0.01 | Critic loss: 15.09 | Entropy loss: -0.0024  | Total Loss: 15.10 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 5/100  | Episode Reward: 4  | Average Reward 2.47  | Actor loss: 3.16 | Critic loss: 12.69 | Entropy loss: -0.0076  | Total Loss: 15.84 | Total Steps: 43\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 6/100  | Episode Reward: -10  | Average Reward 2.40  | Actor loss: -0.00 | Critic loss: 71.15 | Entropy loss: -0.0000  | Total Loss: 71.15 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 7/100  | Episode Reward: 1  | Average Reward 2.39  | Actor loss: 0.09 | Critic loss: 9.49 | Entropy loss: -0.0405  | Total Loss: 9.54 | Total Steps: 68\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 8/100  | Episode Reward: 10  | Average Reward 2.42  | Actor loss: 0.04 | Critic loss: 17.41 | Entropy loss: -0.0027  | Total Loss: 17.45 | Total Steps: 31\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 9/100  | Episode Reward: 7  | Average Reward 2.40  | Actor loss: 0.02 | Critic loss: 2.26 | Entropy loss: -0.0058  | Total Loss: 2.27 | Total Steps: 34\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 10/100  | Episode Reward: 1  | Average Reward 2.39  | Actor loss: 0.03 | Critic loss: 4.20 | Entropy loss: -0.0090  | Total Loss: 4.22 | Total Steps: 53\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 11/100  | Episode Reward: 1  | Average Reward 2.35  | Actor loss: 2.28 | Critic loss: 19.26 | Entropy loss: -0.0113  | Total Loss: 21.53 | Total Steps: 52\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 12/100  | Episode Reward: 7  | Average Reward 2.49  | Actor loss: 0.09 | Critic loss: 10.72 | Entropy loss: -0.0054  | Total Loss: 10.81 | Total Steps: 44\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 13/100  | Episode Reward: 4  | Average Reward 2.56  | Actor loss: 0.63 | Critic loss: 21.40 | Entropy loss: -0.0008  | Total Loss: 22.04 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 14/100  | Episode Reward: 10  | Average Reward 2.60  | Actor loss: 0.02 | Critic loss: 7.56 | Entropy loss: -0.0002  | Total Loss: 7.57 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 15/100  | Episode Reward: 1  | Average Reward 2.58  | Actor loss: 0.01 | Critic loss: 14.07 | Entropy loss: -0.0016  | Total Loss: 14.08 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 16/100  | Episode Reward: 7  | Average Reward 2.59  | Actor loss: 0.01 | Critic loss: 14.47 | Entropy loss: -0.0177  | Total Loss: 14.46 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 17/100  | Episode Reward: 7  | Average Reward 2.60  | Actor loss: 0.01 | Critic loss: 8.47 | Entropy loss: -0.0067  | Total Loss: 8.47 | Total Steps: 43\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 18/100  | Episode Reward: 4  | Average Reward 2.59  | Actor loss: 0.20 | Critic loss: 13.11 | Entropy loss: -0.0008  | Total Loss: 13.31 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 19/100  | Episode Reward: 10  | Average Reward 2.60  | Actor loss: 7.32 | Critic loss: 16.72 | Entropy loss: -0.0230  | Total Loss: 24.01 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing  | Episode: 20/100  | Episode Reward: 4  | Average Reward 2.59  | Actor loss: 0.39 | Critic loss: 13.72 | Entropy loss: -0.0079  | Total Loss: 14.10 | Total Steps: 39\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 21/100  | Episode Reward: 10  | Average Reward 2.59  | Actor loss: 0.05 | Critic loss: 17.37 | Entropy loss: -0.0031  | Total Loss: 17.42 | Total Steps: 91\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 22/100  | Episode Reward: 4  | Average Reward 2.58  | Actor loss: 0.01 | Critic loss: 3.72 | Entropy loss: -0.0007  | Total Loss: 3.73 | Total Steps: 42\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 23/100  | Episode Reward: -2  | Average Reward 2.54  | Actor loss: 0.04 | Critic loss: 16.67 | Entropy loss: -0.0103  | Total Loss: 16.70 | Total Steps: 74\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 24/100  | Episode Reward: 7  | Average Reward 2.54  | Actor loss: 0.02 | Critic loss: 14.84 | Entropy loss: -0.0009  | Total Loss: 14.85 | Total Steps: 34\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 25/100  | Episode Reward: 7  | Average Reward 2.54  | Actor loss: 0.98 | Critic loss: 27.22 | Entropy loss: -0.0127  | Total Loss: 28.18 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 26/100  | Episode Reward: 4  | Average Reward 2.52  | Actor loss: 0.60 | Critic loss: 15.65 | Entropy loss: -0.0115  | Total Loss: 16.24 | Total Steps: 63\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 27/100  | Episode Reward: 1  | Average Reward 2.47  | Actor loss: 1.57 | Critic loss: 20.77 | Entropy loss: -0.0167  | Total Loss: 22.32 | Total Steps: 54\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 28/100  | Episode Reward: -112  | Average Reward 1.89  | Actor loss: -0.03 | Critic loss: 156.25 | Entropy loss: -0.0070  | Total Loss: 156.22 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 29/100  | Episode Reward: 1  | Average Reward 1.89  | Actor loss: 11.90 | Critic loss: 19.40 | Entropy loss: -0.0166  | Total Loss: 31.29 | Total Steps: 80\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 30/100  | Episode Reward: 10  | Average Reward 1.96  | Actor loss: 0.05 | Critic loss: 23.98 | Entropy loss: -0.0037  | Total Loss: 24.02 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 31/100  | Episode Reward: 10  | Average Reward 1.97  | Actor loss: 0.00 | Critic loss: 5.12 | Entropy loss: -0.0008  | Total Loss: 5.12 | Total Steps: 6\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 32/100  | Episode Reward: 10  | Average Reward 2.00  | Actor loss: 0.05 | Critic loss: 23.38 | Entropy loss: -0.0012  | Total Loss: 23.43 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 33/100  | Episode Reward: 7  | Average Reward 2.00  | Actor loss: 0.01 | Critic loss: 2.41 | Entropy loss: -0.0010  | Total Loss: 2.42 | Total Steps: 34\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 34/100  | Episode Reward: 10  | Average Reward 2.02  | Actor loss: 0.05 | Critic loss: 22.67 | Entropy loss: -0.0012  | Total Loss: 22.71 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 35/100  | Episode Reward: 10  | Average Reward 2.10  | Actor loss: 1.25 | Critic loss: 11.53 | Entropy loss: -0.0523  | Total Loss: 12.73 | Total Steps: 7\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 36/100  | Episode Reward: 7  | Average Reward 2.10  | Actor loss: 0.87 | Critic loss: 19.32 | Entropy loss: -0.0033  | Total Loss: 20.19 | Total Steps: 38\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 37/100  | Episode Reward: 4  | Average Reward 2.08  | Actor loss: 0.02 | Critic loss: 18.18 | Entropy loss: -0.0067  | Total Loss: 18.19 | Total Steps: 48\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 38/100  | Episode Reward: -50  | Average Reward 2.44  | Actor loss: 0.02 | Critic loss: 16.92 | Entropy loss: -0.0029  | Total Loss: 16.93 | Total Steps: 419\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 39/100  | Episode Reward: 7  | Average Reward 2.42  | Actor loss: 0.07 | Critic loss: 9.77 | Entropy loss: -0.0146  | Total Loss: 9.83 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 40/100  | Episode Reward: 10  | Average Reward 2.42  | Actor loss: 0.00 | Critic loss: 7.52 | Entropy loss: -0.0027  | Total Loss: 7.52 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 41/100  | Episode Reward: 10  | Average Reward 2.45  | Actor loss: 0.01 | Critic loss: 1.71 | Entropy loss: -0.0015  | Total Loss: 1.72 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 42/100  | Episode Reward: -118  | Average Reward 1.82  | Actor loss: -0.89 | Critic loss: 82.39 | Entropy loss: -0.0163  | Total Loss: 81.48 | Total Steps: 500\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 43/100  | Episode Reward: 7  | Average Reward 1.82  | Actor loss: 0.85 | Critic loss: 19.27 | Entropy loss: -0.0020  | Total Loss: 20.12 | Total Steps: 38\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 44/100  | Episode Reward: 1  | Average Reward 1.79  | Actor loss: 1.36 | Critic loss: 10.80 | Entropy loss: -0.0251  | Total Loss: 12.14 | Total Steps: 50\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 45/100  | Episode Reward: 10  | Average Reward 1.79  | Actor loss: 0.04 | Critic loss: 17.82 | Entropy loss: -0.0041  | Total Loss: 17.85 | Total Steps: 91\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 46/100  | Episode Reward: 10  | Average Reward 1.79  | Actor loss: 0.01 | Critic loss: 8.63 | Entropy loss: -0.0009  | Total Loss: 8.64 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 47/100  | Episode Reward: 4  | Average Reward 1.79  | Actor loss: 0.01 | Critic loss: 12.95 | Entropy loss: -0.0216  | Total Loss: 12.94 | Total Steps: 33\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 48/100  | Episode Reward: -121  | Average Reward 1.16  | Actor loss: -0.00 | Critic loss: 73.80 | Entropy loss: -0.0083  | Total Loss: 73.78 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 49/100  | Episode Reward: 4  | Average Reward 1.12  | Actor loss: 3.51 | Critic loss: 12.40 | Entropy loss: -0.0052  | Total Loss: 15.91 | Total Steps: 43\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 50/100  | Episode Reward: 10  | Average Reward 1.14  | Actor loss: 0.00 | Critic loss: 5.12 | Entropy loss: -0.0011  | Total Loss: 5.12 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 51/100  | Episode Reward: 10  | Average Reward 1.22  | Actor loss: 0.01 | Critic loss: 8.32 | Entropy loss: -0.0004  | Total Loss: 8.33 | Total Steps: 6\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 52/100  | Episode Reward: -124  | Average Reward 0.55  | Actor loss: -0.00 | Critic loss: 74.74 | Entropy loss: -0.0100  | Total Loss: 74.73 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 53/100  | Episode Reward: 7  | Average Reward 0.55  | Actor loss: 0.01 | Critic loss: 4.47 | Entropy loss: -0.0007  | Total Loss: 4.49 | Total Steps: 38\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 54/100  | Episode Reward: 7  | Average Reward 0.56  | Actor loss: 0.00 | Critic loss: 5.58 | Entropy loss: -0.0091  | Total Loss: 5.57 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 55/100  | Episode Reward: -112  | Average Reward -0.03  | Actor loss: -11.68 | Critic loss: 82.85 | Entropy loss: -0.0239  | Total Loss: 71.14 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 56/100  | Episode Reward: -71  | Average Reward -0.43  | Actor loss: 0.06 | Critic loss: 4.99 | Entropy loss: -0.0122  | Total Loss: 5.04 | Total Steps: 372\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 57/100  | Episode Reward: -11  | Average Reward -0.54  | Actor loss: 0.99 | Critic loss: 10.09 | Entropy loss: -0.0228  | Total Loss: 11.06 | Total Steps: 161\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 58/100  | Episode Reward: 1  | Average Reward -0.58  | Actor loss: 0.02 | Critic loss: 3.97 | Entropy loss: -0.0016  | Total Loss: 3.98 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 59/100  | Episode Reward: -109  | Average Reward -1.14  | Actor loss: -3.81 | Critic loss: 77.63 | Entropy loss: -0.0169  | Total Loss: 73.81 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 60/100  | Episode Reward: 10  | Average Reward -1.12  | Actor loss: 0.06 | Critic loss: 24.13 | Entropy loss: -0.0040  | Total Loss: 24.19 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 61/100  | Episode Reward: 10  | Average Reward -1.12  | Actor loss: 0.85 | Critic loss: 20.24 | Entropy loss: -0.0020  | Total Loss: 21.09 | Total Steps: 31\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 62/100  | Episode Reward: 4  | Average Reward -1.03  | Actor loss: 0.01 | Critic loss: 15.43 | Entropy loss: -0.0009  | Total Loss: 15.44 | Total Steps: 42\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 63/100  | Episode Reward: 7  | Average Reward -1.00  | Actor loss: 0.00 | Critic loss: 6.13 | Entropy loss: -0.0028  | Total Loss: 6.13 | Total Steps: 29\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 64/100  | Episode Reward: 7  | Average Reward -1.02  | Actor loss: 0.01 | Critic loss: 15.10 | Entropy loss: -0.0019  | Total Loss: 15.11 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Step: 100\n",
      "TEST: Step: 200\n",
      "TEST: Step: 300\n",
      "TEST: Step: 400\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 65/100  | Episode Reward: -10  | Average Reward -0.56  | Actor loss: -0.00 | Critic loss: 71.98 | Entropy loss: -0.0000  | Total Loss: 71.98 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 66/100  | Episode Reward: 4  | Average Reward -0.54  | Actor loss: 0.01 | Critic loss: 3.55 | Entropy loss: -0.0019  | Total Loss: 3.56 | Total Steps: 42\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 67/100  | Episode Reward: 4  | Average Reward -0.56  | Actor loss: 0.01 | Critic loss: 3.66 | Entropy loss: -0.0007  | Total Loss: 3.67 | Total Steps: 42\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 68/100  | Episode Reward: -109  | Average Reward -1.15  | Actor loss: -10.89 | Critic loss: 75.16 | Entropy loss: -0.0188  | Total Loss: 64.25 | Total Steps: 500\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 69/100  | Episode Reward: 7  | Average Reward -1.14  | Actor loss: 0.02 | Critic loss: 16.02 | Entropy loss: -0.0008  | Total Loss: 16.04 | Total Steps: 36\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 70/100  | Episode Reward: 10  | Average Reward -1.09  | Actor loss: 0.00 | Critic loss: 7.26 | Entropy loss: -0.0004  | Total Loss: 7.26 | Total Steps: 6\n",
      "TEST: ---cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 71/100  | Episode Reward: 4  | Average Reward -1.07  | Actor loss: 0.21 | Critic loss: 14.36 | Entropy loss: -0.0017  | Total Loss: 14.56 | Total Steps: 47\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 72/100  | Episode Reward: 10  | Average Reward -1.03  | Actor loss: 0.08 | Critic loss: 5.82 | Entropy loss: -0.0013  | Total Loss: 5.90 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 73/100  | Episode Reward: 4  | Average Reward -1.04  | Actor loss: 0.01 | Critic loss: 10.29 | Entropy loss: -0.0006  | Total Loss: 10.30 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 74/100  | Episode Reward: 10  | Average Reward -0.79  | Actor loss: 0.03 | Critic loss: 2.61 | Entropy loss: -0.0010  | Total Loss: 2.64 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 75/100  | Episode Reward: 7  | Average Reward -0.81  | Actor loss: 0.11 | Critic loss: 10.40 | Entropy loss: -0.0041  | Total Loss: 10.50 | Total Steps: 30\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 76/100  | Episode Reward: 7  | Average Reward -0.79  | Actor loss: 2.27 | Critic loss: 14.77 | Entropy loss: -0.0048  | Total Loss: 17.04 | Total Steps: 31\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 77/100  | Episode Reward: -23  | Average Reward -0.94  | Actor loss: 0.45 | Critic loss: 14.75 | Entropy loss: -0.0249  | Total Loss: 15.18 | Total Steps: 208\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 78/100  | Episode Reward: 4  | Average Reward -0.97  | Actor loss: 0.01 | Critic loss: 9.73 | Entropy loss: -0.0012  | Total Loss: 9.74 | Total Steps: 47\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 79/100  | Episode Reward: 7  | Average Reward -0.94  | Actor loss: 0.10 | Critic loss: 9.06 | Entropy loss: -0.0040  | Total Loss: 9.16 | Total Steps: 30\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 80/100  | Episode Reward: 7  | Average Reward -0.93  | Actor loss: 0.02 | Critic loss: 15.47 | Entropy loss: -0.0041  | Total Loss: 15.48 | Total Steps: 68\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 81/100  | Episode Reward: 10  | Average Reward -0.91  | Actor loss: 0.01 | Critic loss: 9.71 | Entropy loss: -0.0009  | Total Loss: 9.72 | Total Steps: 6\n",
      "TEST: ---cube---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 82/100  | Episode Reward: 10  | Average Reward -0.91  | Actor loss: 0.01 | Critic loss: 10.78 | Entropy loss: -0.0002  | Total Loss: 10.79 | Total Steps: 31\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 83/100  | Episode Reward: 1  | Average Reward -0.90  | Actor loss: 3.32 | Critic loss: 6.48 | Entropy loss: -0.0147  | Total Loss: 9.79 | Total Steps: 51\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 84/100  | Episode Reward: -124  | Average Reward -1.49  | Actor loss: -2.98 | Critic loss: 103.90 | Entropy loss: -0.0092  | Total Loss: 100.91 | Total Steps: 500\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 200\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 300\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 400\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Max Step Reward: -10\n",
      "TEST: Step: 500\n",
      "Testing  | Episode: 85/100  | Episode Reward: -109  | Average Reward -2.08  | Actor loss: -0.00 | Critic loss: 75.39 | Entropy loss: -0.0192  | Total Loss: 75.36 | Total Steps: 500\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 86/100  | Episode Reward: 1  | Average Reward -2.10  | Actor loss: 0.03 | Critic loss: 3.04 | Entropy loss: -0.0088  | Total Loss: 3.06 | Total Steps: 53\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Step: 100\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 87/100  | Episode Reward: -20  | Average Reward -2.18  | Actor loss: 0.11 | Critic loss: 16.48 | Entropy loss: -0.0110  | Total Loss: 16.58 | Total Steps: 153\n",
      "TEST: ---sphere---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 88/100  | Episode Reward: 10  | Average Reward -2.17  | Actor loss: 4.74 | Critic loss: 36.82 | Entropy loss: -0.0269  | Total Loss: 41.52 | Total Steps: 30\n",
      "TEST: ---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 89/100  | Episode Reward: 10  | Average Reward -2.17  | Actor loss: 0.22 | Critic loss: 19.60 | Entropy loss: -0.0073  | Total Loss: 19.81 | Total Steps: 29\n",
      "TEST: ---capsule---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 90/100  | Episode Reward: 1  | Average Reward -2.15  | Actor loss: 0.24 | Critic loss: 15.94 | Entropy loss: -0.0025  | Total Loss: 16.17 | Total Steps: 53\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 91/100  | Episode Reward: 4  | Average Reward -2.13  | Actor loss: 0.00 | Critic loss: 7.19 | Entropy loss: -0.0006  | Total Loss: 7.19 | Total Steps: 42\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 92/100  | Episode Reward: 7  | Average Reward -1.69  | Actor loss: 0.01 | Critic loss: 14.89 | Entropy loss: -0.0021  | Total Loss: 14.90 | Total Steps: 29\n",
      "TEST: ---cylinder---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 93/100  | Episode Reward: 10  | Average Reward -1.69  | Actor loss: 0.04 | Critic loss: 2.93 | Entropy loss: -0.0016  | Total Loss: 2.97 | Total Steps: 6\n",
      "TEST: ---cylinder---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 94/100  | Episode Reward: 7  | Average Reward -1.68  | Actor loss: 0.10 | Critic loss: 9.11 | Entropy loss: -0.0041  | Total Loss: 9.21 | Total Steps: 30\n",
      "TEST: ---cube---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 95/100  | Episode Reward: 7  | Average Reward -1.68  | Actor loss: 0.03 | Critic loss: 14.68 | Entropy loss: -0.0278  | Total Loss: 14.69 | Total Steps: 31\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 96/100  | Episode Reward: 7  | Average Reward -1.68  | Actor loss: 0.00 | Critic loss: 6.14 | Entropy loss: -0.0036  | Total Loss: 6.14 | Total Steps: 29\n",
      "TEST: ---prism---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 97/100  | Episode Reward: 1  | Average Reward -1.71  | Actor loss: 0.57 | Critic loss: 15.46 | Entropy loss: -0.0084  | Total Loss: 16.02 | Total Steps: 60\n",
      "TEST: ---capsule---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 98/100  | Episode Reward: 10  | Average Reward -1.69  | Actor loss: 0.04 | Critic loss: 25.15 | Entropy loss: -0.0024  | Total Loss: 25.19 | Total Steps: 6\n",
      "TEST: ---sphere---\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Decision Step reward: -3.0\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 99/100  | Episode Reward: 4  | Average Reward -1.71  | Actor loss: 0.41 | Critic loss: 12.16 | Entropy loss: -0.0083  | Total Loss: 12.56 | Total Steps: 46\n",
      "TEST: ---yellow---\n",
      "TEST: Agent in terminal steps\n",
      "TEST: Terminal Step reward: 10.0\n",
      "Testing  | Episode: 100/100  | Episode Reward: 10  | Average Reward -1.66  | Actor loss: 2.39 | Critic loss: 16.49 | Entropy loss: -0.0309  | Total Loss: 18.85 | Total Steps: 73\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2202/129000  | Episode Reward: 4  | Average Reward 5.83  | Actor loss: -0.03 | Critic loss: 3.42 | Entropy loss: -0.0002  | Total Loss: 3.39 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2203/129000  | Episode Reward: 7  | Average Reward 5.82  | Actor loss: 0.08 | Critic loss: 6.65 | Entropy loss: -0.0002  | Total Loss: 6.74 | Total Steps: 29\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2204/129000  | Episode Reward: 10  | Average Reward 5.87  | Actor loss: 0.01 | Critic loss: 1.83 | Entropy loss: -0.0000  | Total Loss: 1.83 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2205/129000  | Episode Reward: -8  | Average Reward 5.78  | Actor loss: -0.46 | Critic loss: 16.80 | Entropy loss: -0.0060  | Total Loss: 16.34 | Total Steps: 111\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2206/129000  | Episode Reward: 1  | Average Reward 5.75  | Actor loss: -0.04 | Critic loss: 7.03 | Entropy loss: -0.0004  | Total Loss: 6.99 | Total Steps: 53\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2207/129000  | Episode Reward: 7  | Average Reward 5.73  | Actor loss: -0.05 | Critic loss: 6.57 | Entropy loss: -0.0005  | Total Loss: 6.52 | Total Steps: 47\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2208/129000  | Episode Reward: 10  | Average Reward 5.76  | Actor loss: 0.02 | Critic loss: 8.77 | Entropy loss: -0.0000  | Total Loss: 8.79 | Total Steps: 6\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2209/129000  | Episode Reward: 10  | Average Reward 5.76  | Actor loss: 0.03 | Critic loss: 2.23 | Entropy loss: -0.0000  | Total Loss: 2.26 | Total Steps: 6\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2210/129000  | Episode Reward: 10  | Average Reward 5.76  | Actor loss: 0.00 | Critic loss: 0.90 | Entropy loss: -0.0000  | Total Loss: 0.90 | Total Steps: 6\n",
      "---sphere---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2211/129000  | Episode Reward: 7  | Average Reward 5.76  | Actor loss: 0.29 | Critic loss: 11.22 | Entropy loss: -0.0009  | Total Loss: 11.50 | Total Steps: 29\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2212/129000  | Episode Reward: 7  | Average Reward 5.78  | Actor loss: 0.08 | Critic loss: 9.60 | Entropy loss: -0.0004  | Total Loss: 9.69 | Total Steps: 32\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2213/129000  | Episode Reward: 7  | Average Reward 5.80  | Actor loss: 0.03 | Critic loss: 5.78 | Entropy loss: -0.0002  | Total Loss: 5.81 | Total Steps: 34\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2214/129000  | Episode Reward: 10  | Average Reward 5.83  | Actor loss: -0.01 | Critic loss: 2.87 | Entropy loss: -0.0009  | Total Loss: 2.87 | Total Steps: 42\n",
      "---sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2215/129000  | Episode Reward: 10  | Average Reward 5.85  | Actor loss: 0.71 | Critic loss: 2.87 | Entropy loss: -0.0017  | Total Loss: 3.58 | Total Steps: 10\n",
      "---prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2216/129000  | Episode Reward: 10  | Average Reward 5.85  | Actor loss: 0.01 | Critic loss: 1.81 | Entropy loss: -0.0000  | Total Loss: 1.82 | Total Steps: 6\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2217/129000  | Episode Reward: 4  | Average Reward 5.83  | Actor loss: -0.07 | Critic loss: 9.59 | Entropy loss: -0.0002  | Total Loss: 9.52 | Total Steps: 43\n",
      "---prism---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2218/129000  | Episode Reward: 7  | Average Reward 5.85  | Actor loss: 0.42 | Critic loss: 7.32 | Entropy loss: -0.0034  | Total Loss: 7.74 | Total Steps: 45\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2219/129000  | Episode Reward: 10  | Average Reward 5.85  | Actor loss: 2.68 | Critic loss: 5.22 | Entropy loss: -0.0025  | Total Loss: 7.90 | Total Steps: 6\n",
      "---sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2220/129000  | Episode Reward: 4  | Average Reward 5.89  | Actor loss: -0.57 | Critic loss: 10.13 | Entropy loss: -0.0125  | Total Loss: 9.55 | Total Steps: 112\n",
      "---cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2221/129000  | Episode Reward: 10  | Average Reward 5.90  | Actor loss: 0.02 | Critic loss: 1.42 | Entropy loss: -0.0001  | Total Loss: 1.44 | Total Steps: 6\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2222/129000  | Episode Reward: 10  | Average Reward 5.90  | Actor loss: 0.28 | Critic loss: 16.54 | Entropy loss: -0.0001  | Total Loss: 16.81 | Total Steps: 6\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2223/129000  | Episode Reward: 4  | Average Reward 5.92  | Actor loss: -0.02 | Critic loss: 8.81 | Entropy loss: -0.0003  | Total Loss: 8.79 | Total Steps: 42\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2224/129000  | Episode Reward: 4  | Average Reward 5.88  | Actor loss: -0.32 | Critic loss: 8.95 | Entropy loss: -0.0018  | Total Loss: 8.63 | Total Steps: 47\n",
      "---cube---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2225/129000  | Episode Reward: 7  | Average Reward 5.90  | Actor loss: 0.19 | Critic loss: 8.68 | Entropy loss: -0.0010  | Total Loss: 8.86 | Total Steps: 32\n",
      "---cylinder---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2226/129000  | Episode Reward: 7  | Average Reward 5.90  | Actor loss: 0.01 | Critic loss: 5.73 | Entropy loss: -0.0019  | Total Loss: 5.74 | Total Steps: 42\n",
      "---capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2227/129000  | Episode Reward: 10  | Average Reward 5.92  | Actor loss: 0.08 | Critic loss: 13.92 | Entropy loss: -0.0000  | Total Loss: 14.00 | Total Steps: 6\n",
      "---capsule---\n",
      "Decision Step reward: -3.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2228/129000  | Episode Reward: 7  | Average Reward 5.92  | Actor loss: 0.00 | Critic loss: 9.00 | Entropy loss: -0.0002  | Total Loss: 9.00 | Total Steps: 29\n",
      "---sphere---\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "entropy_term = 0\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "\n",
    "S0_ALG_NAME = 'S1ab_final'\n",
    "S0_ENV_ID = '3'\n",
    "S0_episode = 32000\n",
    "\n",
    "ALG_NAME = 'S1ab_final'\n",
    "ENV_ID = '4'\n",
    "TRAIN_EPISODES = 129000  # number of overall episodes for training  # number of overall episodes for testing\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "env_per_iteration = 100\n",
    "lr = 2.5e-5  #0.00005 \n",
    "speed = 3\n",
    "num_steps = 250 # the step for updating the network\n",
    "test_episode = 0\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = float('-inf')\n",
    "    hashmap = {\n",
    "        0: 'capsule',\n",
    "        1: 'cube',\n",
    "        2: 'cylinder',\n",
    "        3: 'prism',\n",
    "        4: 'sphere',\n",
    "        5: 'red',\n",
    "        6: 'green',\n",
    "        7: 'blue',\n",
    "        8: 'yellow',\n",
    "        9: 'black'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        test_episode_reward = []\n",
    "        test_average_reward = []\n",
    "        test_steps = []\n",
    "        test_actor_loss = []\n",
    "        test_critic_loss = []\n",
    "        test_entropy_loss = []\n",
    "        test_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        test_episode = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            env_index = (episode // env_per_iteration) % 2\n",
    "            if env_index == 0: env = env1_train\n",
    "            else: env = env2_train\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = int(decision_steps.obs[1])\n",
    "            if env_index: index = index + 5\n",
    "            print(f'---{hashmap[index]}---')\n",
    "\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                    # value = value.detach()\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % num_steps == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.1 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(float(episode_reward))\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(float(actor_loss))\n",
    "            all_critic_loss.append(float(critic_loss))\n",
    "            all_entropy_loss.append(float(entropy_loss))\n",
    "            all_total_loss.append(float(ac_loss))\n",
    "            if episode >= 200:\n",
    "                avg_score = np.mean(all_episode_reward[-200:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 200 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%500 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "            if episode%100 == 0:\n",
    "                test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss = test(agent,test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss)\n",
    "\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}_train.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "        \n",
    "        test_data = {\n",
    "                    'all_average_reward': test_average_reward,\n",
    "                    'all_episode_reward': test_episode_reward,\n",
    "                    'all_actor_loss': test_actor_loss,\n",
    "                    'all_critic_loss': test_critic_loss,\n",
    "                    'all_entropy_loss': test_entropy_loss,\n",
    "                    'all_total_loss': test_total_loss,\n",
    "                    'all_steps': test_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}_test.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(test_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T01:05:48.467421Z",
     "start_time": "2023-07-29T01:05:48.016471Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "            'all_average_reward': all_average_reward,\n",
    "            'all_episode_reward': all_episode_reward,\n",
    "            'all_actor_loss': all_actor_loss,\n",
    "            'all_critic_loss': all_critic_loss,\n",
    "            'all_entropy_loss': all_entropy_loss,\n",
    "            'all_total_loss': all_total_loss,\n",
    "            'all_steps': all_steps,\n",
    "        } \n",
    "file_path = f'result/{ALG_NAME}_{ENV_ID}_train.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "test_data = {\n",
    "            'all_average_reward': test_average_reward,\n",
    "            'all_episode_reward': test_episode_reward,\n",
    "            'all_actor_loss': test_actor_loss,\n",
    "            'all_critic_loss': test_critic_loss,\n",
    "            'all_entropy_loss': test_entropy_loss,\n",
    "            'all_total_loss': test_total_loss,\n",
    "            'all_steps': test_steps,\n",
    "        } \n",
    "file_path = f'result/{ALG_NAME}_{ENV_ID}_test.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(test_data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
