{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T04:38:43.167772Z",
     "start_time": "2023-07-20T04:38:43.155616Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# imports and initialise env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T07:22:27.611738Z",
     "start_time": "2023-07-19T07:22:18.814253Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# imports and initialise env \n",
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n",
    "\n",
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "# file_name = \"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S2_test 180723\\\\build\"\n",
    "file_name = \"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S2 180723\\\\build\"\n",
    "\n",
    "env =  UE(file_name=file_name,seed=1,side_channels=[],worker_id=5,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "# for i in range(len(behavior_names)):\n",
    "#     print(behavior_names[i])\n",
    "#     print(\"obs:\",behavior_value[i].observation_specs, \"   act:\", behavior_value[0].action_spec)\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agentsNum = len(DecisionSteps.agent_id)\n",
    "# print(\"exist:\",DecisionSteps.agent_id,\"   Dead:\",TerminalSteps.agent_id)\n",
    "# print(\"reward:\",DecisionSteps.reward,\"reward_dead:\",TerminalSteps.reward)\n",
    "# print(\"obs:\",DecisionSteps.obs,\"DeadObs:\",TerminalSteps.obs)\n",
    "# print(\"interrupted:\", TerminalSteps.interrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T07:22:40.728450Z",
     "start_time": "2023-07-19T07:22:40.701622Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-19T07:23:31.354927Z",
     "start_time": "2023-07-19T07:22:44.701950Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- target colour: red, target object: prism ---\n",
      "Decision Step reward: -3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\new_mlagents\\lib\\site-packages\\ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 0, Episode reward: 7.0\n",
      "--- target colour: yellow, target object: sphere ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 1, Episode reward: -11.0\n",
      "--- target colour: blue, target object: cube ---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 2, Episode reward: 10.0\n",
      "--- target colour: black, target object: capsule ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 3, Episode reward: -5.0\n",
      "--- target colour: green, target object: prism ---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 4, Episode reward: 10.0\n",
      "--- target colour: blue, target object: sphere ---\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 5, Episode reward: 7.0\n",
      "--- target colour: yellow, target object: cylinder ---\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 6, Episode reward: 7.0\n",
      "--- target colour: black, target object: cylinder ---\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 7, Episode reward: 7.0\n",
      "--- target colour: green, target object: cylinder ---\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 8, Episode reward: 7.0\n",
      "--- target colour: black, target object: cube ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 9, Episode reward: -2.0\n",
      "--- target colour: red, target object: prism ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Max Step Reward: -10\n",
      "Episode: 10, Episode reward: -103.0\n",
      "--- target colour: yellow, target object: cube ---\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 11, Episode reward: 7.0\n",
      "--- target colour: red, target object: sphere ---\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 12, Episode reward: 7.0\n",
      "--- target colour: yellow, target object: cylinder ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 13, Episode reward: -8.0\n",
      "--- target colour: yellow, target object: cube ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Max Step Reward: -10\n",
      "Episode: 14, Episode reward: -37.0\n",
      "--- target colour: red, target object: sphere ---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 15, Episode reward: 10.0\n",
      "--- target colour: red, target object: prism ---\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n",
      "Decision Step reward: -3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7e7232d21b4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0maction_tuple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_discrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscrete_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[0mdecision_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\site-packages\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"communicator.exchange\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has exited.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[1;31m# Got an acknowledgment from the connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    857\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test loop\n",
    "#-1(20000): -5 # -2.5 (10000): -6.73  #-5 (5000):-9.46  # -7.5 (5000): -7.02  #-10 (5000):-10(circle same place)\n",
    "# S1_4:12996      S1_13:6495           S1_14:4587        S1_15: 4073            S1_16:3085\n",
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cpu\")\n",
    "episode = 57991\n",
    "speed = 3\n",
    "MAX_STEPS = 500\n",
    "TEST_EPISODES = 100\n",
    "ALG_NAME = 'S2'\n",
    "ENV_ID = '5' \n",
    "tracked_agent = -1\n",
    "env.reset()\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode,ALG_NAME,ENV_ID)\n",
    "average = 0\n",
    "\n",
    "object_hashmap = {\n",
    "0: 'capsule',\n",
    "1: 'cube',\n",
    "2: 'cylinder',\n",
    "3: 'prism',\n",
    "4: 'sphere'}\n",
    "colour_hashmap = {\n",
    "0: 'red',\n",
    "1: 'green',\n",
    "2: 'blue',\n",
    "3: 'yellow',\n",
    "4: 'black'}\n",
    "\n",
    "for episode in range(TEST_EPISODES):\n",
    "            STEPS = 0\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            \n",
    "            object_index = int(decision_steps.obs[1][0][0])\n",
    "            colour_index = int(decision_steps.obs[1][0][1])\n",
    "            print(f'--- target colour: {colour_hashmap[colour_index]}, target object: {object_hashmap[object_index]} ---')\n",
    "            # objects: 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "            # colours: 0-red,1-green,2-blue,3-yellow,4-black \n",
    "            \n",
    "#             lt = torch.eye(num_words)[:, index].to(device) #one hot encoder language vector\n",
    "            lt = torch.zeros(35).to(device)\n",
    "            \n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while not done:\n",
    "                STEPS += 1                \n",
    "                lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                # value = value.detach()\n",
    "                dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                \n",
    "\n",
    "                action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                action = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    if reward<0:\n",
    "                        if reward==-2.5:\n",
    "                            reward=-3\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "\n",
    "                if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "\n",
    "                episode_reward = episode_reward + reward\n",
    "                vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                vt = vt_new\n",
    "            average += episode_reward / TEST_EPISODES\n",
    "            print(f'Episode: {episode}, Episode reward: {episode_reward}')\n",
    "print(f'Average Episode Reward: {average}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
