{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:32:45.517361Z",
     "start_time": "2023-07-30T16:32:45.375116Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:32:45.533450Z",
     "start_time": "2023-07-30T16:32:45.518324Z"
    }
   },
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:32:48.695023Z",
     "start_time": "2023-07-30T16:32:45.534435Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "file_name = \"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\build_envs\\\\windows\\\\S2 180723\\\\build\"\n",
    "env =  UE(file_name=file_name,seed=1,side_channels=[],worker_id=2,no_graphics = False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:32:49.182536Z",
     "start_time": "2023-07-30T16:32:48.696070Z"
    },
    "code_folding": [
     0,
     16,
     48,
     58,
     69,
     73
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\transformers_mlagents\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "bert_output_dim = 768  # output dim of BERT\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent1(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent1, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:32:49.564661Z",
     "start_time": "2023-07-30T16:32:49.183536Z"
    },
    "code_folding": [
     0,
     16,
     46,
     60,
     77,
     96,
     106
    ]
   },
   "outputs": [],
   "source": [
    "# model w Bert \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "bert_output_dim = 768  # output dim of BERT\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "# class LanguageModule(nn.Module): \n",
    "#     def __init__(self, num_words, embedding_dim):\n",
    "#         super(LanguageModule, self).__init__()\n",
    "#         self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "#     def forward(self, lt):\n",
    "#         embedded_lt = self.embedding(lt)\n",
    "#         return embedded_lt\n",
    "\n",
    "# bert encoder \n",
    "from transformers import BertTokenizer,BertModel\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertEncoder,self).__init__()\n",
    "        self.bert_tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "    def forward(self,input_caption):\n",
    "        tokens=self.bert_tokenizer.tokenize(input_caption)\n",
    "        tokens=[\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "#         n_pad_to_add=self.max_words-len(tokens)\n",
    "#         tokens+=[\"[PAD]\"]*n_pad_to_add\n",
    "        attention_mask=[1 if token!='[PAD]' else 0 for token in tokens]\n",
    "        token_ids=self.bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "        torch_token_ids=torch.tensor(token_ids).unsqueeze(0)\n",
    "        torch_attention_mask=torch.tensor(attention_mask).unsqueeze(0)\n",
    "        return torch_token_ids,torch_attention_mask\n",
    "        \n",
    "    \n",
    "class BertModule(nn.Module):\n",
    "    def __init__(self,bert_output_dim,embedding_dim):\n",
    "        super(BertModule,self).__init__()\n",
    "#         self.max_words=max_words\n",
    "        self.bert_model=BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.embedding=nn.Linear(bert_output_dim,embedding_dim)\n",
    "\n",
    "        # Freeze the weights of the BERT model\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self,torch_token_ids,torch_attention_mask):\n",
    "        output=self.bert_model(torch_token_ids,torch_attention_mask)\n",
    "        output=output[1].view(-1)\n",
    "        output=self.embedding(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent2(nn.Module):\n",
    "    def __init__(self, bert_output_dim, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent2, self).__init__()\n",
    "        self.bert_language_module = BertModule(bert_output_dim, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, torch_token_ids, torch_attention_mask, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.bert_language_module(torch_token_ids, torch_attention_mask)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-30T16:49:19.153Z"
    },
    "code_folding": [
     0,
     44
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "---black capsule---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\transformers_mlagents\\lib\\site-packages\\ipykernel_launcher.py:136: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1/175267  | Episode Reward: 5.0  | Actor loss: -0.15 | Critic loss: 8.13 | Entropy loss: -0.0032  | Total Loss: 7.98 | Total Steps: 44\n",
      "Model has been saved\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2/175267  | Episode Reward: 10.0  | Actor loss: 0.02 | Critic loss: 6.68 | Entropy loss: -0.0000  | Total Loss: 6.69 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 3/175267  | Episode Reward: 5.0  | Actor loss: 0.05 | Critic loss: 6.50 | Entropy loss: -0.0027  | Total Loss: 6.55 | Total Steps: 56\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 4/175267  | Episode Reward: 7.5  | Actor loss: 0.30 | Critic loss: 4.33 | Entropy loss: -0.0025  | Total Loss: 4.64 | Total Steps: 44\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 5/175267  | Episode Reward: 7.5  | Actor loss: -0.00 | Critic loss: 4.42 | Entropy loss: -0.0004  | Total Loss: 4.42 | Total Steps: 46\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 6/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.54 | Entropy loss: -0.0000  | Total Loss: 1.55 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 7/175267  | Episode Reward: 7.5  | Actor loss: 0.21 | Critic loss: 5.30 | Entropy loss: -0.0009  | Total Loss: 5.51 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 8/175267  | Episode Reward: 7.5  | Actor loss: 0.07 | Critic loss: 5.02 | Entropy loss: -0.0005  | Total Loss: 5.09 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 9/175267  | Episode Reward: 6.5  | Actor loss: -0.40 | Critic loss: 6.23 | Entropy loss: -0.0023  | Total Loss: 5.82 | Total Steps: 44\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 10/175267  | Episode Reward: 10.0  | Actor loss: 0.12 | Critic loss: 4.57 | Entropy loss: -0.0006  | Total Loss: 4.70 | Total Steps: 30\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 11/175267  | Episode Reward: 7.5  | Actor loss: 0.01 | Critic loss: 3.40 | Entropy loss: -0.0008  | Total Loss: 3.42 | Total Steps: 43\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 12/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0000  | Total Loss: 1.66 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 13/175267  | Episode Reward: 6.5  | Actor loss: -0.18 | Critic loss: 3.98 | Entropy loss: -0.0055  | Total Loss: 3.80 | Total Steps: 57\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 14/175267  | Episode Reward: 7.5  | Actor loss: 0.09 | Critic loss: 6.27 | Entropy loss: -0.0004  | Total Loss: 6.36 | Total Steps: 30\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 15/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.02 | Entropy loss: -0.0000  | Total Loss: 1.02 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 16/175267  | Episode Reward: 7.5  | Actor loss: 0.06 | Critic loss: 3.23 | Entropy loss: -0.0015  | Total Loss: 3.30 | Total Steps: 43\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 17/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.68 | Entropy loss: -0.0000  | Total Loss: 1.69 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 18/175267  | Episode Reward: 5.0  | Actor loss: -0.32 | Critic loss: 9.62 | Entropy loss: -0.0017  | Total Loss: 9.30 | Total Steps: 54\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 19/175267  | Episode Reward: 10.0  | Actor loss: 0.02 | Critic loss: 0.36 | Entropy loss: -0.0001  | Total Loss: 0.38 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 20/175267  | Episode Reward: 10.0  | Actor loss: 0.04 | Critic loss: 4.26 | Entropy loss: -0.0005  | Total Loss: 4.30 | Total Steps: 30\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 21/175267  | Episode Reward: 5.0  | Actor loss: -0.40 | Critic loss: 3.20 | Entropy loss: -0.0026  | Total Loss: 2.80 | Total Steps: 44\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 22/175267  | Episode Reward: 10.0  | Actor loss: 3.20 | Critic loss: 9.13 | Entropy loss: -0.0025  | Total Loss: 12.33 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 23/175267  | Episode Reward: -17.5  | Actor loss: -0.91 | Critic loss: 22.00 | Entropy loss: -0.0110  | Total Loss: 21.08 | Total Steps: 172\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 24/175267  | Episode Reward: 10.0  | Actor loss: 0.68 | Critic loss: 5.63 | Entropy loss: -0.0029  | Total Loss: 6.30 | Total Steps: 32\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 25/175267  | Episode Reward: 10.0  | Actor loss: 0.07 | Critic loss: 5.13 | Entropy loss: -0.0019  | Total Loss: 5.20 | Total Steps: 36\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 26/175267  | Episode Reward: 7.5  | Actor loss: -0.17 | Critic loss: 7.37 | Entropy loss: -0.0041  | Total Loss: 7.19 | Total Steps: 55\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 27/175267  | Episode Reward: 4.0  | Actor loss: -0.47 | Critic loss: 7.41 | Entropy loss: -0.0044  | Total Loss: 6.94 | Total Steps: 62\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 28/175267  | Episode Reward: 7.5  | Actor loss: 0.30 | Critic loss: 5.71 | Entropy loss: -0.0009  | Total Loss: 6.01 | Total Steps: 30\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 29/175267  | Episode Reward: 5.0  | Actor loss: -0.23 | Critic loss: 5.01 | Entropy loss: -0.0016  | Total Loss: 4.78 | Total Steps: 54\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 30/175267  | Episode Reward: 5.0  | Actor loss: -0.31 | Critic loss: 6.06 | Entropy loss: -0.0042  | Total Loss: 5.74 | Total Steps: 51\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 31/175267  | Episode Reward: 5.0  | Actor loss: 0.14 | Critic loss: 3.79 | Entropy loss: -0.0015  | Total Loss: 3.93 | Total Steps: 48\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 32/175267  | Episode Reward: 5.0  | Actor loss: -0.09 | Critic loss: 2.91 | Entropy loss: -0.0022  | Total Loss: 2.82 | Total Steps: 32\n",
      "---black cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 33/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.27 | Entropy loss: -0.0000  | Total Loss: 1.27 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 34/175267  | Episode Reward: 6.5  | Actor loss: -0.35 | Critic loss: 5.43 | Entropy loss: -0.0027  | Total Loss: 5.08 | Total Steps: 45\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 35/175267  | Episode Reward: 10.0  | Actor loss: -0.25 | Critic loss: 1.09 | Entropy loss: -0.0020  | Total Loss: 0.84 | Total Steps: 7\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 36/175267  | Episode Reward: 7.5  | Actor loss: -0.23 | Critic loss: 2.49 | Entropy loss: -0.0042  | Total Loss: 2.26 | Total Steps: 75\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 37/175267  | Episode Reward: 5.0  | Actor loss: -0.51 | Critic loss: 7.78 | Entropy loss: -0.0062  | Total Loss: 7.27 | Total Steps: 73\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 38/175267  | Episode Reward: 10.0  | Actor loss: 0.05 | Critic loss: 6.73 | Entropy loss: -0.0001  | Total Loss: 6.77 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 39/175267  | Episode Reward: 10.0  | Actor loss: -0.57 | Critic loss: 3.92 | Entropy loss: -0.0038  | Total Loss: 3.35 | Total Steps: 49\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 40/175267  | Episode Reward: 10.0  | Actor loss: 0.07 | Critic loss: 4.54 | Entropy loss: -0.0005  | Total Loss: 4.61 | Total Steps: 31\n",
      "---green capsule---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 41/175267  | Episode Reward: 6.5  | Actor loss: -0.31 | Critic loss: 2.95 | Entropy loss: -0.0040  | Total Loss: 2.64 | Total Steps: 82\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 42/175267  | Episode Reward: 0.0  | Actor loss: -0.60 | Critic loss: 9.74 | Entropy loss: -0.0076  | Total Loss: 9.13 | Total Steps: 86\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 43/175267  | Episode Reward: 2.5  | Actor loss: -0.16 | Critic loss: 4.97 | Entropy loss: -0.0054  | Total Loss: 4.81 | Total Steps: 43\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 44/175267  | Episode Reward: 7.5  | Actor loss: 0.41 | Critic loss: 4.04 | Entropy loss: -0.0029  | Total Loss: 4.45 | Total Steps: 32\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 45/175267  | Episode Reward: 10.0  | Actor loss: 0.28 | Critic loss: 4.44 | Entropy loss: -0.0030  | Total Loss: 4.71 | Total Steps: 32\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 46/175267  | Episode Reward: 7.5  | Actor loss: -0.12 | Critic loss: 7.07 | Entropy loss: -0.0040  | Total Loss: 6.95 | Total Steps: 56\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 47/175267  | Episode Reward: 5.0  | Actor loss: -0.24 | Critic loss: 4.11 | Entropy loss: -0.0038  | Total Loss: 3.87 | Total Steps: 54\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 48/175267  | Episode Reward: 10.0  | Actor loss: 0.00 | Critic loss: 4.30 | Entropy loss: -0.0009  | Total Loss: 4.30 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 49/175267  | Episode Reward: 7.5  | Actor loss: -0.01 | Critic loss: 3.36 | Entropy loss: -0.0006  | Total Loss: 3.35 | Total Steps: 44\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 50/175267  | Episode Reward: 7.5  | Actor loss: 0.05 | Critic loss: 3.85 | Entropy loss: -0.0004  | Total Loss: 3.90 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 51/175267  | Episode Reward: 10.0  | Actor loss: 0.04 | Critic loss: 7.17 | Entropy loss: -0.0000  | Total Loss: 7.21 | Total Steps: 6\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 52/175267  | Episode Reward: 10.0  | Actor loss: 0.02 | Critic loss: 1.49 | Entropy loss: -0.0000  | Total Loss: 1.51 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 53/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 0.74 | Entropy loss: -0.0000  | Total Loss: 0.75 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 54/175267  | Episode Reward: 4.0  | Actor loss: -0.22 | Critic loss: 11.41 | Entropy loss: -0.0050  | Total Loss: 11.18 | Total Steps: 66\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 55/175267  | Episode Reward: 7.5  | Actor loss: -0.05 | Critic loss: 2.30 | Entropy loss: -0.0011  | Total Loss: 2.25 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 56/175267  | Episode Reward: 10.0  | Actor loss: 0.05 | Critic loss: 3.97 | Entropy loss: -0.0002  | Total Loss: 4.02 | Total Steps: 29\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 57/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 0.52 | Entropy loss: -0.0000  | Total Loss: 0.53 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 58/175267  | Episode Reward: 2.5  | Actor loss: -0.45 | Critic loss: 7.04 | Entropy loss: -0.0031  | Total Loss: 6.58 | Total Steps: 53\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 59/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 0.26 | Entropy loss: -0.0000  | Total Loss: 0.27 | Total Steps: 6\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 60/175267  | Episode Reward: 10.0  | Actor loss: -0.10 | Critic loss: 4.00 | Entropy loss: -0.0024  | Total Loss: 3.90 | Total Steps: 46\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 61/175267  | Episode Reward: 5.0  | Actor loss: 0.06 | Critic loss: 8.77 | Entropy loss: -0.0009  | Total Loss: 8.83 | Total Steps: 43\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 62/175267  | Episode Reward: 10.0  | Actor loss: 0.04 | Critic loss: 2.69 | Entropy loss: -0.0012  | Total Loss: 2.72 | Total Steps: 61\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 63/175267  | Episode Reward: 7.5  | Actor loss: 0.04 | Critic loss: 7.41 | Entropy loss: -0.0002  | Total Loss: 7.45 | Total Steps: 29\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 64/175267  | Episode Reward: 1.5  | Actor loss: -0.51 | Critic loss: 13.29 | Entropy loss: -0.0039  | Total Loss: 12.77 | Total Steps: 54\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 65/175267  | Episode Reward: 7.5  | Actor loss: 0.17 | Critic loss: 7.41 | Entropy loss: -0.0006  | Total Loss: 7.58 | Total Steps: 30\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 66/175267  | Episode Reward: 10.0  | Actor loss: 0.11 | Critic loss: 2.96 | Entropy loss: -0.0005  | Total Loss: 3.07 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 67/175267  | Episode Reward: 7.5  | Actor loss: 0.07 | Critic loss: 3.99 | Entropy loss: -0.0004  | Total Loss: 4.06 | Total Steps: 34\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 68/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 0.86 | Entropy loss: -0.0000  | Total Loss: 0.87 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 69/175267  | Episode Reward: 4.0  | Actor loss: -0.37 | Critic loss: 7.06 | Entropy loss: -0.0033  | Total Loss: 6.69 | Total Steps: 45\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 70/175267  | Episode Reward: 7.5  | Actor loss: -0.07 | Critic loss: 4.60 | Entropy loss: -0.0023  | Total Loss: 4.53 | Total Steps: 49\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 71/175267  | Episode Reward: 5.0  | Actor loss: -0.04 | Critic loss: 4.08 | Entropy loss: -0.0012  | Total Loss: 4.04 | Total Steps: 42\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 72/175267  | Episode Reward: 5.0  | Actor loss: -0.08 | Critic loss: 8.09 | Entropy loss: -0.0019  | Total Loss: 8.00 | Total Steps: 85\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 73/175267  | Episode Reward: 5.0  | Actor loss: -0.50 | Critic loss: 4.82 | Entropy loss: -0.0069  | Total Loss: 4.31 | Total Steps: 83\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 74/175267  | Episode Reward: 5.0  | Actor loss: -0.34 | Critic loss: 6.06 | Entropy loss: -0.0062  | Total Loss: 5.72 | Total Steps: 94\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 75/175267  | Episode Reward: 7.5  | Actor loss: 0.32 | Critic loss: 8.54 | Entropy loss: -0.0010  | Total Loss: 8.86 | Total Steps: 32\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 76/175267  | Episode Reward: 10.0  | Actor loss: 0.12 | Critic loss: 3.03 | Entropy loss: -0.0025  | Total Loss: 3.15 | Total Steps: 37\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 77/175267  | Episode Reward: 7.5  | Actor loss: 0.08 | Critic loss: 3.74 | Entropy loss: -0.0005  | Total Loss: 3.82 | Total Steps: 29\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 78/175267  | Episode Reward: 7.5  | Actor loss: -0.01 | Critic loss: 0.74 | Entropy loss: -0.0027  | Total Loss: 0.74 | Total Steps: 40\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 79/175267  | Episode Reward: 10.0  | Actor loss: 0.05 | Critic loss: 11.90 | Entropy loss: -0.0000  | Total Loss: 11.96 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 80/175267  | Episode Reward: 7.5  | Actor loss: 0.19 | Critic loss: 3.05 | Entropy loss: -0.0021  | Total Loss: 3.24 | Total Steps: 42\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 81/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 0.92 | Entropy loss: -0.0000  | Total Loss: 0.93 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 82/175267  | Episode Reward: 7.5  | Actor loss: 0.29 | Critic loss: 4.02 | Entropy loss: -0.0016  | Total Loss: 4.31 | Total Steps: 30\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 83/175267  | Episode Reward: 10.0  | Actor loss: -0.15 | Critic loss: 1.20 | Entropy loss: -0.0031  | Total Loss: 1.05 | Total Steps: 47\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 84/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.95 | Entropy loss: -0.0000  | Total Loss: 1.96 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 85/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.01 | Entropy loss: -0.0000  | Total Loss: 1.02 | Total Steps: 6\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 86/175267  | Episode Reward: 10.0  | Actor loss: 0.04 | Critic loss: 6.90 | Entropy loss: -0.0000  | Total Loss: 6.93 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 87/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 0.41 | Entropy loss: -0.0000  | Total Loss: 0.42 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 88/175267  | Episode Reward: 10.0  | Actor loss: 0.01 | Critic loss: 1.02 | Entropy loss: -0.0000  | Total Loss: 1.03 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 89/175267  | Episode Reward: 7.5  | Actor loss: 0.31 | Critic loss: 5.90 | Entropy loss: -0.0014  | Total Loss: 6.21 | Total Steps: 29\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 90/175267  | Episode Reward: 7.5  | Actor loss: 0.07 | Critic loss: 4.88 | Entropy loss: -0.0004  | Total Loss: 4.95 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 91/175267  | Episode Reward: 5.0  | Actor loss: 0.23 | Critic loss: 7.90 | Entropy loss: -0.0022  | Total Loss: 8.12 | Total Steps: 32\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 92/175267  | Episode Reward: 7.5  | Actor loss: -0.45 | Critic loss: 4.13 | Entropy loss: -0.0075  | Total Loss: 3.67 | Total Steps: 123\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 93/175267  | Episode Reward: 10.0  | Actor loss: 0.00 | Critic loss: 0.28 | Entropy loss: -0.0000  | Total Loss: 0.29 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 94/175267  | Episode Reward: 7.5  | Actor loss: -0.41 | Critic loss: 2.58 | Entropy loss: -0.0042  | Total Loss: 2.16 | Total Steps: 52\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 95/175267  | Episode Reward: 7.5  | Actor loss: 0.07 | Critic loss: 6.68 | Entropy loss: -0.0011  | Total Loss: 6.75 | Total Steps: 32\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 96/175267  | Episode Reward: 10.0  | Actor loss: -1.04 | Critic loss: 4.71 | Entropy loss: -0.0026  | Total Loss: 3.67 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 97/175267  | Episode Reward: 7.5  | Actor loss: -0.02 | Critic loss: 0.86 | Entropy loss: -0.0007  | Total Loss: 0.84 | Total Steps: 38\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 98/175267  | Episode Reward: 7.5  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0004  | Total Loss: 0.96 | Total Steps: 38\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 99/175267  | Episode Reward: 10.0  | Actor loss: 0.03 | Critic loss: 0.44 | Entropy loss: -0.0001  | Total Loss: 0.47 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 100/175267  | Episode Reward: 7.5  | Actor loss: -0.85 | Critic loss: 7.67 | Entropy loss: -0.0041  | Total Loss: 6.82 | Total Steps: 48\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.495. Model has been saved-----\n",
      "Training  | Episode: 101/175267  | Episode Reward: 7.5  | Average Reward 7.50  | Actor loss: -0.36 | Critic loss: 3.80 | Entropy loss: -0.0020  | Total Loss: 3.43 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 102/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.00 | Critic loss: 0.43 | Entropy loss: -0.0000  | Total Loss: 0.44 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.52. Model has been saved-----\n",
      "Training  | Episode: 103/175267  | Episode Reward: 7.5  | Average Reward 7.52  | Actor loss: -0.52 | Critic loss: 4.65 | Entropy loss: -0.0018  | Total Loss: 4.13 | Total Steps: 35\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 104/175267  | Episode Reward: 2.5  | Average Reward 7.47  | Actor loss: -0.62 | Critic loss: 7.09 | Entropy loss: -0.0068  | Total Loss: 6.46 | Total Steps: 82\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 105/175267  | Episode Reward: 4.0  | Average Reward 7.43  | Actor loss: -0.88 | Critic loss: 9.47 | Entropy loss: -0.0085  | Total Loss: 8.58 | Total Steps: 89\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 106/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.02 | Critic loss: 3.35 | Entropy loss: -0.0001  | Total Loss: 3.37 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 107/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.00 | Critic loss: 0.31 | Entropy loss: -0.0000  | Total Loss: 0.32 | Total Steps: 6\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 108/175267  | Episode Reward: 10.0  | Average Reward 7.49  | Actor loss: 0.07 | Critic loss: 1.88 | Entropy loss: -0.0001  | Total Loss: 1.96 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 109/175267  | Episode Reward: 5.0  | Average Reward 7.47  | Actor loss: -0.05 | Critic loss: 8.09 | Entropy loss: -0.0005  | Total Loss: 8.04 | Total Steps: 43\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 110/175267  | Episode Reward: 7.5  | Average Reward 7.45  | Actor loss: -0.03 | Critic loss: 3.34 | Entropy loss: -0.0006  | Total Loss: 3.31 | Total Steps: 43\n",
      "---blue cube---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 111/175267  | Episode Reward: 6.5  | Average Reward 7.43  | Actor loss: -0.80 | Critic loss: 9.14 | Entropy loss: -0.0040  | Total Loss: 8.34 | Total Steps: 55\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 112/175267  | Episode Reward: 5.0  | Average Reward 7.38  | Actor loss: -0.05 | Critic loss: 4.29 | Entropy loss: -0.0041  | Total Loss: 4.24 | Total Steps: 58\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 113/175267  | Episode Reward: 10.0  | Average Reward 7.42  | Actor loss: 0.13 | Critic loss: 3.75 | Entropy loss: -0.0019  | Total Loss: 3.88 | Total Steps: 44\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 114/175267  | Episode Reward: 5.0  | Average Reward 7.39  | Actor loss: -0.13 | Critic loss: 8.61 | Entropy loss: -0.0013  | Total Loss: 8.48 | Total Steps: 53\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 115/175267  | Episode Reward: 10.0  | Average Reward 7.39  | Actor loss: 0.61 | Critic loss: 4.18 | Entropy loss: -0.0008  | Total Loss: 4.79 | Total Steps: 7\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 116/175267  | Episode Reward: 7.5  | Average Reward 7.39  | Actor loss: -0.03 | Critic loss: 6.31 | Entropy loss: -0.0014  | Total Loss: 6.28 | Total Steps: 44\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 117/175267  | Episode Reward: 7.5  | Average Reward 7.37  | Actor loss: 0.10 | Critic loss: 5.53 | Entropy loss: -0.0006  | Total Loss: 5.63 | Total Steps: 29\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 118/175267  | Episode Reward: 10.0  | Average Reward 7.42  | Actor loss: 0.02 | Critic loss: 3.29 | Entropy loss: -0.0002  | Total Loss: 3.31 | Total Steps: 31\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 119/175267  | Episode Reward: 10.0  | Average Reward 7.42  | Actor loss: 0.20 | Critic loss: 2.16 | Entropy loss: -0.0013  | Total Loss: 2.36 | Total Steps: 31\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 120/175267  | Episode Reward: 7.5  | Average Reward 7.39  | Actor loss: -0.22 | Critic loss: 3.63 | Entropy loss: -0.0028  | Total Loss: 3.41 | Total Steps: 31\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 121/175267  | Episode Reward: 10.0  | Average Reward 7.45  | Actor loss: 0.04 | Critic loss: 4.00 | Entropy loss: -0.0002  | Total Loss: 4.03 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 122/175267  | Episode Reward: 10.0  | Average Reward 7.45  | Actor loss: 0.02 | Critic loss: 4.53 | Entropy loss: -0.0000  | Total Loss: 4.54 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.695. Model has been saved-----\n",
      "Training  | Episode: 123/175267  | Episode Reward: 7.5  | Average Reward 7.70  | Actor loss: -0.01 | Critic loss: 2.32 | Entropy loss: -0.0009  | Total Loss: 2.30 | Total Steps: 43\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 124/175267  | Episode Reward: 10.0  | Average Reward 7.70  | Actor loss: 0.05 | Critic loss: 4.38 | Entropy loss: -0.0003  | Total Loss: 4.43 | Total Steps: 30\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 125/175267  | Episode Reward: 10.0  | Average Reward 7.70  | Actor loss: 0.00 | Critic loss: 0.41 | Entropy loss: -0.0000  | Total Loss: 0.41 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 126/175267  | Episode Reward: 7.5  | Average Reward 7.70  | Actor loss: -0.72 | Critic loss: 4.24 | Entropy loss: -0.0016  | Total Loss: 3.51 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 127/175267  | Episode Reward: 2.5  | Average Reward 7.68  | Actor loss: -0.03 | Critic loss: 5.35 | Entropy loss: -0.0035  | Total Loss: 5.31 | Total Steps: 55\n",
      "---red cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.705. Model has been saved-----\n",
      "Training  | Episode: 128/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.00 | Critic loss: 0.80 | Entropy loss: -0.0000  | Total Loss: 0.80 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.73. Model has been saved-----\n",
      "Training  | Episode: 129/175267  | Episode Reward: 7.5  | Average Reward 7.73  | Actor loss: 0.00 | Critic loss: 3.22 | Entropy loss: -0.0002  | Total Loss: 3.22 | Total Steps: 30\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.78. Model has been saved-----\n",
      "Training  | Episode: 130/175267  | Episode Reward: 10.0  | Average Reward 7.78  | Actor loss: 0.02 | Critic loss: 2.32 | Entropy loss: -0.0000  | Total Loss: 2.33 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.83. Model has been saved-----\n",
      "Training  | Episode: 131/175267  | Episode Reward: 10.0  | Average Reward 7.83  | Actor loss: 0.01 | Critic loss: 0.29 | Entropy loss: -0.0000  | Total Loss: 0.30 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.855. Model has been saved-----\n",
      "Training  | Episode: 132/175267  | Episode Reward: 7.5  | Average Reward 7.86  | Actor loss: 0.04 | Critic loss: 3.24 | Entropy loss: -0.0021  | Total Loss: 3.28 | Total Steps: 34\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 133/175267  | Episode Reward: -10.0  | Average Reward 7.66  | Actor loss: -0.53 | Critic loss: 25.90 | Entropy loss: -0.0059  | Total Loss: 25.36 | Total Steps: 165\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 134/175267  | Episode Reward: 10.0  | Average Reward 7.69  | Actor loss: 0.01 | Critic loss: 0.20 | Entropy loss: -0.0001  | Total Loss: 0.20 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 135/175267  | Episode Reward: 7.5  | Average Reward 7.67  | Actor loss: 0.54 | Critic loss: 4.43 | Entropy loss: -0.0025  | Total Loss: 4.96 | Total Steps: 31\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 136/175267  | Episode Reward: 7.5  | Average Reward 7.67  | Actor loss: 0.11 | Critic loss: 7.95 | Entropy loss: -0.0004  | Total Loss: 8.06 | Total Steps: 29\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 137/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.18 | Critic loss: 4.13 | Entropy loss: -0.0009  | Total Loss: 4.31 | Total Steps: 31\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 138/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.01 | Critic loss: 1.00 | Entropy loss: -0.0000  | Total Loss: 1.01 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 139/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.01 | Critic loss: 2.07 | Entropy loss: -0.0000  | Total Loss: 2.08 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 140/175267  | Episode Reward: 7.5  | Average Reward 7.69  | Actor loss: -0.09 | Critic loss: 7.26 | Entropy loss: -0.0010  | Total Loss: 7.17 | Total Steps: 52\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 141/175267  | Episode Reward: 5.0  | Average Reward 7.67  | Actor loss: -0.26 | Critic loss: 10.10 | Entropy loss: -0.0030  | Total Loss: 9.84 | Total Steps: 54\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 142/175267  | Episode Reward: 7.5  | Average Reward 7.75  | Actor loss: 0.03 | Critic loss: 2.86 | Entropy loss: -0.0072  | Total Loss: 2.89 | Total Steps: 47\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 143/175267  | Episode Reward: 10.0  | Average Reward 7.83  | Actor loss: 0.05 | Critic loss: 2.76 | Entropy loss: -0.0007  | Total Loss: 2.81 | Total Steps: 36\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 144/175267  | Episode Reward: 7.5  | Average Reward 7.83  | Actor loss: 0.08 | Critic loss: 6.37 | Entropy loss: -0.0012  | Total Loss: 6.45 | Total Steps: 44\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 145/175267  | Episode Reward: 10.0  | Average Reward 7.83  | Actor loss: 0.01 | Critic loss: 1.91 | Entropy loss: -0.0000  | Total Loss: 1.92 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 146/175267  | Episode Reward: 10.0  | Average Reward 7.85  | Actor loss: 0.00 | Critic loss: 0.51 | Entropy loss: -0.0000  | Total Loss: 0.52 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -1.0\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.865. Model has been saved-----\n",
      "Training  | Episode: 147/175267  | Episode Reward: 6.5  | Average Reward 7.87  | Actor loss: -0.58 | Critic loss: 7.83 | Entropy loss: -0.0098  | Total Loss: 7.24 | Total Steps: 388\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 148/175267  | Episode Reward: 10.0  | Average Reward 7.87  | Actor loss: 0.03 | Critic loss: 1.03 | Entropy loss: -0.0001  | Total Loss: 1.06 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 149/175267  | Episode Reward: 7.5  | Average Reward 7.87  | Actor loss: 0.03 | Critic loss: 1.74 | Entropy loss: -0.0022  | Total Loss: 1.77 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "-----The best score for averaging previous 200 episode reward is 7.89. Model has been saved-----\n",
      "Training  | Episode: 150/175267  | Episode Reward: 10.0  | Average Reward 7.89  | Actor loss: 0.24 | Critic loss: 2.12 | Entropy loss: -0.0028  | Total Loss: 2.36 | Total Steps: 60\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 151/175267  | Episode Reward: 5.0  | Average Reward 7.84  | Actor loss: 0.17 | Critic loss: 9.37 | Entropy loss: -0.0016  | Total Loss: 9.54 | Total Steps: 44\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 152/175267  | Episode Reward: 2.5  | Average Reward 7.76  | Actor loss: -0.47 | Critic loss: 8.41 | Entropy loss: -0.0020  | Total Loss: 7.93 | Total Steps: 52\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 153/175267  | Episode Reward: 10.0  | Average Reward 7.76  | Actor loss: 0.01 | Critic loss: 1.49 | Entropy loss: -0.0000  | Total Loss: 1.50 | Total Steps: 6\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 154/175267  | Episode Reward: 10.0  | Average Reward 7.83  | Actor loss: 0.38 | Critic loss: 3.44 | Entropy loss: -0.0016  | Total Loss: 3.81 | Total Steps: 31\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 155/175267  | Episode Reward: 0.0  | Average Reward 7.75  | Actor loss: -0.61 | Critic loss: 11.61 | Entropy loss: -0.0031  | Total Loss: 11.00 | Total Steps: 55\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 156/175267  | Episode Reward: 7.5  | Average Reward 7.72  | Actor loss: 0.20 | Critic loss: 4.70 | Entropy loss: -0.0009  | Total Loss: 4.90 | Total Steps: 29\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 157/175267  | Episode Reward: 7.5  | Average Reward 7.70  | Actor loss: 0.10 | Critic loss: 5.85 | Entropy loss: -0.0007  | Total Loss: 5.96 | Total Steps: 30\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 158/175267  | Episode Reward: 7.5  | Average Reward 7.75  | Actor loss: 0.10 | Critic loss: 4.61 | Entropy loss: -0.0004  | Total Loss: 4.71 | Total Steps: 30\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 159/175267  | Episode Reward: 5.0  | Average Reward 7.70  | Actor loss: -0.05 | Critic loss: 7.77 | Entropy loss: -0.0008  | Total Loss: 7.72 | Total Steps: 66\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 160/175267  | Episode Reward: 2.5  | Average Reward 7.62  | Actor loss: -0.50 | Critic loss: 11.65 | Entropy loss: -0.0062  | Total Loss: 11.15 | Total Steps: 148\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 161/175267  | Episode Reward: 7.5  | Average Reward 7.65  | Actor loss: 0.12 | Critic loss: 6.87 | Entropy loss: -0.0004  | Total Loss: 6.99 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 162/175267  | Episode Reward: 10.0  | Average Reward 7.65  | Actor loss: 0.02 | Critic loss: 4.39 | Entropy loss: -0.0000  | Total Loss: 4.41 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 163/175267  | Episode Reward: 1.5  | Average Reward 7.59  | Actor loss: -0.38 | Critic loss: 11.75 | Entropy loss: -0.0051  | Total Loss: 11.37 | Total Steps: 55\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 164/175267  | Episode Reward: 2.5  | Average Reward 7.60  | Actor loss: -0.28 | Critic loss: 6.59 | Entropy loss: -0.0034  | Total Loss: 6.30 | Total Steps: 52\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 165/175267  | Episode Reward: 4.0  | Average Reward 7.57  | Actor loss: -0.31 | Critic loss: 7.35 | Entropy loss: -0.0023  | Total Loss: 7.03 | Total Steps: 55\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 166/175267  | Episode Reward: 10.0  | Average Reward 7.57  | Actor loss: -0.02 | Critic loss: 1.69 | Entropy loss: -0.0034  | Total Loss: 1.67 | Total Steps: 64\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 167/175267  | Episode Reward: 7.5  | Average Reward 7.57  | Actor loss: 0.91 | Critic loss: 7.28 | Entropy loss: -0.0039  | Total Loss: 8.18 | Total Steps: 32\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 168/175267  | Episode Reward: 10.0  | Average Reward 7.57  | Actor loss: 0.09 | Critic loss: 3.21 | Entropy loss: -0.0018  | Total Loss: 3.30 | Total Steps: 37\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 169/175267  | Episode Reward: 0.0  | Average Reward 7.53  | Actor loss: -0.40 | Critic loss: 14.71 | Entropy loss: -0.0039  | Total Loss: 14.31 | Total Steps: 85\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 170/175267  | Episode Reward: -2.5  | Average Reward 7.42  | Actor loss: -0.36 | Critic loss: 17.20 | Entropy loss: -0.0043  | Total Loss: 16.83 | Total Steps: 89\n",
      "---black capsule---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 171/175267  | Episode Reward: 9.0  | Average Reward 7.46  | Actor loss: -0.11 | Critic loss: 2.29 | Entropy loss: -0.0052  | Total Loss: 2.18 | Total Steps: 46\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 172/175267  | Episode Reward: -2.5  | Average Reward 7.39  | Actor loss: -0.31 | Critic loss: 13.17 | Entropy loss: -0.0047  | Total Loss: 12.85 | Total Steps: 77\n",
      "---black capsule---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 173/175267  | Episode Reward: 9.0  | Average Reward 7.43  | Actor loss: -0.08 | Critic loss: 3.30 | Entropy loss: -0.0059  | Total Loss: 3.22 | Total Steps: 93\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 174/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.02 | Critic loss: 2.11 | Entropy loss: -0.0000  | Total Loss: 2.12 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 175/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.07 | Critic loss: 5.18 | Entropy loss: -0.0003  | Total Loss: 5.26 | Total Steps: 29\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 176/175267  | Episode Reward: 7.5  | Average Reward 7.48  | Actor loss: 0.18 | Critic loss: 6.00 | Entropy loss: -0.0006  | Total Loss: 6.18 | Total Steps: 29\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 177/175267  | Episode Reward: 5.0  | Average Reward 7.46  | Actor loss: -0.51 | Critic loss: 8.78 | Entropy loss: -0.0052  | Total Loss: 8.27 | Total Steps: 46\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 178/175267  | Episode Reward: 7.5  | Average Reward 7.46  | Actor loss: 0.58 | Critic loss: 6.47 | Entropy loss: -0.0027  | Total Loss: 7.05 | Total Steps: 34\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 179/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.04 | Critic loss: 3.79 | Entropy loss: -0.0000  | Total Loss: 3.83 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 180/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.03 | Critic loss: 1.08 | Entropy loss: -0.0000  | Total Loss: 1.10 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 181/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.09 | Critic loss: 3.67 | Entropy loss: -0.0009  | Total Loss: 3.75 | Total Steps: 39\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 182/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.03 | Critic loss: 0.58 | Entropy loss: -0.0001  | Total Loss: 0.61 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 183/175267  | Episode Reward: 7.5  | Average Reward 7.48  | Actor loss: -0.53 | Critic loss: 5.53 | Entropy loss: -0.0044  | Total Loss: 5.00 | Total Steps: 43\n",
      "---black cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 184/175267  | Episode Reward: 5.0  | Average Reward 7.43  | Actor loss: -0.23 | Critic loss: 6.98 | Entropy loss: -0.0046  | Total Loss: 6.74 | Total Steps: 64\n",
      "---green capsule---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 185/175267  | Episode Reward: 9.0  | Average Reward 7.42  | Actor loss: -0.01 | Critic loss: 2.99 | Entropy loss: -0.0073  | Total Loss: 2.97 | Total Steps: 154\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 186/175267  | Episode Reward: 1.5  | Average Reward 7.33  | Actor loss: -0.98 | Critic loss: 5.29 | Entropy loss: -0.0055  | Total Loss: 4.31 | Total Steps: 50\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 187/175267  | Episode Reward: 7.5  | Average Reward 7.31  | Actor loss: -0.08 | Critic loss: 3.42 | Entropy loss: -0.0045  | Total Loss: 3.34 | Total Steps: 125\n",
      "---black cube---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 188/175267  | Episode Reward: 9.0  | Average Reward 7.30  | Actor loss: 0.23 | Critic loss: 3.83 | Entropy loss: -0.0023  | Total Loss: 4.07 | Total Steps: 32\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 189/175267  | Episode Reward: 7.5  | Average Reward 7.30  | Actor loss: 0.15 | Critic loss: 5.94 | Entropy loss: -0.0015  | Total Loss: 6.09 | Total Steps: 44\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 190/175267  | Episode Reward: 0.0  | Average Reward 7.22  | Actor loss: -0.63 | Critic loss: 7.95 | Entropy loss: -0.0065  | Total Loss: 7.31 | Total Steps: 116\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 191/175267  | Episode Reward: 2.5  | Average Reward 7.20  | Actor loss: -0.36 | Critic loss: 11.40 | Entropy loss: -0.0064  | Total Loss: 11.03 | Total Steps: 133\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 192/175267  | Episode Reward: 7.5  | Average Reward 7.20  | Actor loss: 0.09 | Critic loss: 4.79 | Entropy loss: -0.0004  | Total Loss: 4.87 | Total Steps: 34\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 193/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.06 | Critic loss: 10.76 | Entropy loss: -0.0000  | Total Loss: 10.82 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 194/175267  | Episode Reward: 5.0  | Average Reward 7.17  | Actor loss: -0.13 | Critic loss: 7.30 | Entropy loss: -0.0009  | Total Loss: 7.17 | Total Steps: 47\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 195/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.02 | Critic loss: 3.22 | Entropy loss: -0.0000  | Total Loss: 3.24 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 196/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: -0.08 | Critic loss: 4.69 | Entropy loss: -0.0014  | Total Loss: 4.61 | Total Steps: 42\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 197/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.02 | Critic loss: 2.89 | Entropy loss: -0.0002  | Total Loss: 2.91 | Total Steps: 31\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 198/175267  | Episode Reward: 5.0  | Average Reward 7.17  | Actor loss: -0.30 | Critic loss: 6.94 | Entropy loss: -0.0024  | Total Loss: 6.64 | Total Steps: 51\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 199/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.01 | Critic loss: 1.19 | Entropy loss: -0.0000  | Total Loss: 1.19 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 200/175267  | Episode Reward: 5.0  | Average Reward 7.15  | Actor loss: -0.06 | Critic loss: 6.59 | Entropy loss: -0.0008  | Total Loss: 6.53 | Total Steps: 53\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 201/175267  | Episode Reward: -2.5  | Average Reward 7.05  | Actor loss: -0.69 | Critic loss: 7.79 | Entropy loss: -0.0046  | Total Loss: 7.10 | Total Steps: 54\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 202/175267  | Episode Reward: 7.5  | Average Reward 7.03  | Actor loss: 0.24 | Critic loss: 4.75 | Entropy loss: -0.0025  | Total Loss: 4.99 | Total Steps: 31\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 203/175267  | Episode Reward: 7.5  | Average Reward 7.03  | Actor loss: 0.30 | Critic loss: 4.42 | Entropy loss: -0.0022  | Total Loss: 4.72 | Total Steps: 31\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 204/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: 0.01 | Critic loss: 3.06 | Entropy loss: -0.0004  | Total Loss: 3.07 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 205/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.08 | Critic loss: 2.38 | Entropy loss: -0.0007  | Total Loss: 2.46 | Total Steps: 43\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 206/175267  | Episode Reward: 0.0  | Average Reward 7.04  | Actor loss: -0.57 | Critic loss: 13.31 | Entropy loss: -0.0033  | Total Loss: 12.75 | Total Steps: 54\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 207/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.02 | Critic loss: 4.33 | Entropy loss: -0.0000  | Total Loss: 4.35 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 208/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: 0.05 | Critic loss: 2.07 | Entropy loss: -0.0027  | Total Loss: 2.12 | Total Steps: 42\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 209/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: 0.10 | Critic loss: 3.52 | Entropy loss: -0.0010  | Total Loss: 3.62 | Total Steps: 42\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 210/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: -0.25 | Critic loss: 3.70 | Entropy loss: -0.0023  | Total Loss: 3.44 | Total Steps: 52\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 211/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: -0.01 | Critic loss: 5.88 | Entropy loss: -0.0009  | Total Loss: 5.87 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 212/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: 0.62 | Critic loss: 3.18 | Entropy loss: -0.0037  | Total Loss: 3.79 | Total Steps: 32\n",
      "---green sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 213/175267  | Episode Reward: 6.5  | Average Reward 7.04  | Actor loss: -0.18 | Critic loss: 2.50 | Entropy loss: -0.0046  | Total Loss: 2.32 | Total Steps: 58\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 214/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.04 | Critic loss: 7.91 | Entropy loss: -0.0000  | Total Loss: 7.95 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 215/175267  | Episode Reward: 5.0  | Average Reward 7.04  | Actor loss: 0.00 | Critic loss: 2.87 | Entropy loss: -0.0012  | Total Loss: 2.87 | Total Steps: 42\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 216/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.05 | Critic loss: 3.36 | Entropy loss: -0.0003  | Total Loss: 3.41 | Total Steps: 29\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 217/175267  | Episode Reward: -10.0  | Average Reward 6.88  | Actor loss: -0.52 | Critic loss: 16.88 | Entropy loss: -0.0074  | Total Loss: 16.35 | Total Steps: 172\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 218/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.01 | Critic loss: 3.47 | Entropy loss: -0.0000  | Total Loss: 3.49 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 219/175267  | Episode Reward: -12.5  | Average Reward 6.66  | Actor loss: -0.54 | Critic loss: 15.20 | Entropy loss: -0.0069  | Total Loss: 14.66 | Total Steps: 130\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 220/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: 0.20 | Critic loss: 7.55 | Entropy loss: -0.0007  | Total Loss: 7.75 | Total Steps: 29\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 221/175267  | Episode Reward: 7.5  | Average Reward 6.63  | Actor loss: -0.03 | Critic loss: 7.58 | Entropy loss: -0.0013  | Total Loss: 7.55 | Total Steps: 32\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 222/175267  | Episode Reward: 4.0  | Average Reward 6.58  | Actor loss: -0.20 | Critic loss: 5.11 | Entropy loss: -0.0026  | Total Loss: 4.90 | Total Steps: 44\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 223/175267  | Episode Reward: 5.0  | Average Reward 6.55  | Actor loss: -0.95 | Critic loss: 8.20 | Entropy loss: -0.0025  | Total Loss: 7.25 | Total Steps: 37\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 224/175267  | Episode Reward: 2.5  | Average Reward 6.47  | Actor loss: -0.24 | Critic loss: 7.01 | Entropy loss: -0.0020  | Total Loss: 6.77 | Total Steps: 52\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 225/175267  | Episode Reward: 10.0  | Average Reward 6.47  | Actor loss: 0.03 | Critic loss: 4.09 | Entropy loss: -0.0003  | Total Loss: 4.12 | Total Steps: 34\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 226/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.21 | Critic loss: 4.60 | Entropy loss: -0.0007  | Total Loss: 4.81 | Total Steps: 29\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 227/175267  | Episode Reward: 7.5  | Average Reward 6.55  | Actor loss: 0.05 | Critic loss: 5.45 | Entropy loss: -0.0010  | Total Loss: 5.50 | Total Steps: 30\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 228/175267  | Episode Reward: 10.0  | Average Reward 6.55  | Actor loss: 0.04 | Critic loss: 2.30 | Entropy loss: -0.0001  | Total Loss: 2.34 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 229/175267  | Episode Reward: -10.0  | Average Reward 6.38  | Actor loss: -0.47 | Critic loss: 23.32 | Entropy loss: -0.0035  | Total Loss: 22.85 | Total Steps: 119\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 230/175267  | Episode Reward: 10.0  | Average Reward 6.38  | Actor loss: 0.02 | Critic loss: 2.37 | Entropy loss: -0.0000  | Total Loss: 2.39 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 231/175267  | Episode Reward: 7.5  | Average Reward 6.35  | Actor loss: -0.05 | Critic loss: 3.00 | Entropy loss: -0.0011  | Total Loss: 2.95 | Total Steps: 42\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 232/175267  | Episode Reward: 10.0  | Average Reward 6.38  | Actor loss: 0.01 | Critic loss: 1.61 | Entropy loss: -0.0000  | Total Loss: 1.62 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 233/175267  | Episode Reward: -8.5  | Average Reward 6.39  | Actor loss: 0.11 | Critic loss: 4.75 | Entropy loss: -0.0002  | Total Loss: 4.86 | Total Steps: 260\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 234/175267  | Episode Reward: 10.0  | Average Reward 6.39  | Actor loss: 0.02 | Critic loss: 4.34 | Entropy loss: -0.0000  | Total Loss: 4.36 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 235/175267  | Episode Reward: 10.0  | Average Reward 6.42  | Actor loss: 0.19 | Critic loss: 4.92 | Entropy loss: -0.0008  | Total Loss: 5.11 | Total Steps: 29\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 236/175267  | Episode Reward: 7.5  | Average Reward 6.42  | Actor loss: 0.10 | Critic loss: 5.29 | Entropy loss: -0.0005  | Total Loss: 5.39 | Total Steps: 29\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 237/175267  | Episode Reward: 7.5  | Average Reward 6.39  | Actor loss: 0.02 | Critic loss: 7.63 | Entropy loss: -0.0001  | Total Loss: 7.65 | Total Steps: 29\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 238/175267  | Episode Reward: 7.5  | Average Reward 6.37  | Actor loss: -0.07 | Critic loss: 5.55 | Entropy loss: -0.0003  | Total Loss: 5.48 | Total Steps: 34\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 239/175267  | Episode Reward: 7.5  | Average Reward 6.34  | Actor loss: -0.14 | Critic loss: 2.85 | Entropy loss: -0.0018  | Total Loss: 2.71 | Total Steps: 82\n",
      "---green sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 240/175267  | Episode Reward: 5.0  | Average Reward 6.32  | Actor loss: 0.03 | Critic loss: 3.93 | Entropy loss: -0.0027  | Total Loss: 3.96 | Total Steps: 42\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 241/175267  | Episode Reward: 10.0  | Average Reward 6.37  | Actor loss: 0.72 | Critic loss: 0.83 | Entropy loss: -0.0015  | Total Loss: 1.55 | Total Steps: 7\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 242/175267  | Episode Reward: 10.0  | Average Reward 6.39  | Actor loss: 0.06 | Critic loss: 3.45 | Entropy loss: -0.0003  | Total Loss: 3.52 | Total Steps: 31\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 243/175267  | Episode Reward: 7.5  | Average Reward 6.37  | Actor loss: 0.00 | Critic loss: 4.38 | Entropy loss: -0.0016  | Total Loss: 4.38 | Total Steps: 31\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 244/175267  | Episode Reward: 7.5  | Average Reward 6.37  | Actor loss: -0.07 | Critic loss: 4.35 | Entropy loss: -0.0007  | Total Loss: 4.27 | Total Steps: 42\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 245/175267  | Episode Reward: 10.0  | Average Reward 6.37  | Actor loss: 0.02 | Critic loss: 2.80 | Entropy loss: -0.0002  | Total Loss: 2.83 | Total Steps: 29\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 246/175267  | Episode Reward: 10.0  | Average Reward 6.37  | Actor loss: 0.13 | Critic loss: 4.56 | Entropy loss: -0.0007  | Total Loss: 4.69 | Total Steps: 29\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 247/175267  | Episode Reward: 10.0  | Average Reward 6.40  | Actor loss: 0.03 | Critic loss: 1.17 | Entropy loss: -0.0000  | Total Loss: 1.20 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 248/175267  | Episode Reward: 10.0  | Average Reward 6.40  | Actor loss: 0.01 | Critic loss: 0.78 | Entropy loss: -0.0000  | Total Loss: 0.79 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 249/175267  | Episode Reward: -1.0  | Average Reward 6.32  | Actor loss: -0.21 | Critic loss: 13.96 | Entropy loss: -0.0036  | Total Loss: 13.75 | Total Steps: 87\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 250/175267  | Episode Reward: 5.0  | Average Reward 6.26  | Actor loss: -0.21 | Critic loss: 3.63 | Entropy loss: -0.0070  | Total Loss: 3.41 | Total Steps: 138\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 251/175267  | Episode Reward: 10.0  | Average Reward 6.32  | Actor loss: 0.11 | Critic loss: 4.04 | Entropy loss: -0.0006  | Total Loss: 4.15 | Total Steps: 29\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 252/175267  | Episode Reward: 5.0  | Average Reward 6.34  | Actor loss: -0.33 | Critic loss: 2.93 | Entropy loss: -0.0021  | Total Loss: 2.60 | Total Steps: 41\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 253/175267  | Episode Reward: 2.5  | Average Reward 6.26  | Actor loss: -0.07 | Critic loss: 8.07 | Entropy loss: -0.0045  | Total Loss: 7.99 | Total Steps: 102\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 254/175267  | Episode Reward: 7.5  | Average Reward 6.24  | Actor loss: 0.12 | Critic loss: 3.86 | Entropy loss: -0.0008  | Total Loss: 3.98 | Total Steps: 29\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 255/175267  | Episode Reward: 7.5  | Average Reward 6.32  | Actor loss: -0.08 | Critic loss: 2.69 | Entropy loss: -0.0009  | Total Loss: 2.61 | Total Steps: 42\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 256/175267  | Episode Reward: 7.5  | Average Reward 6.32  | Actor loss: 0.02 | Critic loss: 6.45 | Entropy loss: -0.0002  | Total Loss: 6.46 | Total Steps: 36\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 257/175267  | Episode Reward: 7.5  | Average Reward 6.32  | Actor loss: 0.22 | Critic loss: 6.50 | Entropy loss: -0.0018  | Total Loss: 6.71 | Total Steps: 32\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 258/175267  | Episode Reward: 10.0  | Average Reward 6.34  | Actor loss: 0.03 | Critic loss: 2.38 | Entropy loss: -0.0000  | Total Loss: 2.41 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 259/175267  | Episode Reward: 10.0  | Average Reward 6.39  | Actor loss: 0.01 | Critic loss: 1.29 | Entropy loss: -0.0000  | Total Loss: 1.30 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 260/175267  | Episode Reward: 10.0  | Average Reward 6.46  | Actor loss: 0.01 | Critic loss: 2.35 | Entropy loss: -0.0000  | Total Loss: 2.37 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 261/175267  | Episode Reward: 7.5  | Average Reward 6.46  | Actor loss: 0.04 | Critic loss: 6.54 | Entropy loss: -0.0004  | Total Loss: 6.58 | Total Steps: 30\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 262/175267  | Episode Reward: 10.0  | Average Reward 6.46  | Actor loss: 0.06 | Critic loss: 0.61 | Entropy loss: -0.0014  | Total Loss: 0.67 | Total Steps: 7\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 263/175267  | Episode Reward: 7.5  | Average Reward 6.53  | Actor loss: -0.21 | Critic loss: 6.08 | Entropy loss: -0.0032  | Total Loss: 5.88 | Total Steps: 54\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 264/175267  | Episode Reward: 10.0  | Average Reward 6.60  | Actor loss: 0.01 | Critic loss: 0.38 | Entropy loss: -0.0000  | Total Loss: 0.39 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 265/175267  | Episode Reward: 7.5  | Average Reward 6.63  | Actor loss: 0.16 | Critic loss: 3.41 | Entropy loss: -0.0012  | Total Loss: 3.57 | Total Steps: 31\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 266/175267  | Episode Reward: -2.5  | Average Reward 6.51  | Actor loss: -1.06 | Critic loss: 18.62 | Entropy loss: -0.0050  | Total Loss: 17.56 | Total Steps: 91\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 267/175267  | Episode Reward: 10.0  | Average Reward 6.54  | Actor loss: 0.08 | Critic loss: 0.80 | Entropy loss: -0.0015  | Total Loss: 0.88 | Total Steps: 7\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 268/175267  | Episode Reward: 7.5  | Average Reward 6.51  | Actor loss: -0.08 | Critic loss: 5.30 | Entropy loss: -0.0007  | Total Loss: 5.22 | Total Steps: 43\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 269/175267  | Episode Reward: 10.0  | Average Reward 6.61  | Actor loss: 0.01 | Critic loss: 0.89 | Entropy loss: -0.0000  | Total Loss: 0.90 | Total Steps: 6\n",
      "---green sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 270/175267  | Episode Reward: 5.0  | Average Reward 6.68  | Actor loss: -0.07 | Critic loss: 4.68 | Entropy loss: -0.0011  | Total Loss: 4.61 | Total Steps: 43\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 271/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.00 | Critic loss: 0.72 | Entropy loss: -0.0000  | Total Loss: 0.73 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 272/175267  | Episode Reward: -5.0  | Average Reward 6.67  | Actor loss: -0.78 | Critic loss: 9.77 | Entropy loss: -0.0107  | Total Loss: 8.99 | Total Steps: 399\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 273/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: -0.27 | Critic loss: 6.28 | Entropy loss: -0.0061  | Total Loss: 6.00 | Total Steps: 166\n",
      "---red sphere---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 274/175267  | Episode Reward: 6.5  | Average Reward 6.62  | Actor loss: -0.47 | Critic loss: 5.79 | Entropy loss: -0.0071  | Total Loss: 5.32 | Total Steps: 94\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 275/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.06 | Critic loss: 2.98 | Entropy loss: -0.0006  | Total Loss: 3.03 | Total Steps: 30\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 276/175267  | Episode Reward: 7.5  | Average Reward 6.62  | Actor loss: 0.05 | Critic loss: 7.58 | Entropy loss: -0.0003  | Total Loss: 7.63 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 277/175267  | Episode Reward: 7.5  | Average Reward 6.64  | Actor loss: -0.20 | Critic loss: 3.48 | Entropy loss: -0.0077  | Total Loss: 3.27 | Total Steps: 189\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 278/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.05 | Critic loss: 0.63 | Entropy loss: -0.0002  | Total Loss: 0.67 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 279/175267  | Episode Reward: 5.0  | Average Reward 6.62  | Actor loss: 0.03 | Critic loss: 5.68 | Entropy loss: -0.0017  | Total Loss: 5.71 | Total Steps: 53\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 280/175267  | Episode Reward: 5.0  | Average Reward 6.57  | Actor loss: -0.46 | Critic loss: 4.47 | Entropy loss: -0.0024  | Total Loss: 4.01 | Total Steps: 45\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 281/175267  | Episode Reward: 10.0  | Average Reward 6.57  | Actor loss: 0.01 | Critic loss: 1.99 | Entropy loss: -0.0000  | Total Loss: 2.01 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 282/175267  | Episode Reward: 10.0  | Average Reward 6.57  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0000  | Total Loss: 0.97 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 283/175267  | Episode Reward: 7.5  | Average Reward 6.57  | Actor loss: 0.19 | Critic loss: 5.64 | Entropy loss: -0.0008  | Total Loss: 5.83 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 284/175267  | Episode Reward: 7.5  | Average Reward 6.59  | Actor loss: -0.07 | Critic loss: 3.05 | Entropy loss: -0.0009  | Total Loss: 2.97 | Total Steps: 42\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 285/175267  | Episode Reward: 10.0  | Average Reward 6.61  | Actor loss: -0.41 | Critic loss: 3.76 | Entropy loss: -0.0051  | Total Loss: 3.34 | Total Steps: 35\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 286/175267  | Episode Reward: 10.0  | Average Reward 6.69  | Actor loss: 0.04 | Critic loss: 3.89 | Entropy loss: -0.0002  | Total Loss: 3.93 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 287/175267  | Episode Reward: 10.0  | Average Reward 6.71  | Actor loss: 0.01 | Critic loss: 1.74 | Entropy loss: -0.0000  | Total Loss: 1.75 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 288/175267  | Episode Reward: 10.0  | Average Reward 6.72  | Actor loss: 0.01 | Critic loss: 1.23 | Entropy loss: -0.0000  | Total Loss: 1.24 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 289/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.01 | Critic loss: 1.52 | Entropy loss: -0.0000  | Total Loss: 1.53 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 290/175267  | Episode Reward: 2.5  | Average Reward 6.78  | Actor loss: -0.28 | Critic loss: 11.36 | Entropy loss: -0.0021  | Total Loss: 11.07 | Total Steps: 54\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 291/175267  | Episode Reward: 10.0  | Average Reward 6.85  | Actor loss: 0.05 | Critic loss: 3.56 | Entropy loss: -0.0002  | Total Loss: 3.61 | Total Steps: 29\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 292/175267  | Episode Reward: 5.0  | Average Reward 6.83  | Actor loss: -0.31 | Critic loss: 5.28 | Entropy loss: -0.0024  | Total Loss: 4.97 | Total Steps: 51\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 293/175267  | Episode Reward: 10.0  | Average Reward 6.83  | Actor loss: 0.02 | Critic loss: 3.66 | Entropy loss: -0.0000  | Total Loss: 3.68 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 294/175267  | Episode Reward: 7.5  | Average Reward 6.85  | Actor loss: -0.09 | Critic loss: 4.61 | Entropy loss: -0.0008  | Total Loss: 4.51 | Total Steps: 50\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 295/175267  | Episode Reward: 7.5  | Average Reward 6.83  | Actor loss: -0.12 | Critic loss: 5.10 | Entropy loss: -0.0016  | Total Loss: 4.98 | Total Steps: 52\n",
      "---yellow cylinder---\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 296/175267  | Episode Reward: 7.5  | Average Reward 6.83  | Actor loss: 0.00 | Critic loss: 2.47 | Entropy loss: -0.0029  | Total Loss: 2.47 | Total Steps: 322\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 297/175267  | Episode Reward: 10.0  | Average Reward 6.83  | Actor loss: 0.03 | Critic loss: 5.00 | Entropy loss: -0.0002  | Total Loss: 5.03 | Total Steps: 29\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 298/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.05 | Critic loss: 2.33 | Entropy loss: -0.0010  | Total Loss: 2.38 | Total Steps: 44\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 299/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: -0.13 | Critic loss: 4.32 | Entropy loss: -0.0008  | Total Loss: 4.19 | Total Steps: 31\n",
      "---blue sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 300/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.01 | Critic loss: 0.39 | Entropy loss: -0.0000  | Total Loss: 0.39 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 301/175267  | Episode Reward: 5.0  | Average Reward 7.00  | Actor loss: -0.14 | Critic loss: 7.35 | Entropy loss: -0.0012  | Total Loss: 7.21 | Total Steps: 52\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 302/175267  | Episode Reward: 10.0  | Average Reward 7.03  | Actor loss: 0.06 | Critic loss: 3.76 | Entropy loss: -0.0005  | Total Loss: 3.83 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 303/175267  | Episode Reward: 10.0  | Average Reward 7.05  | Actor loss: 0.02 | Critic loss: 3.73 | Entropy loss: -0.0011  | Total Loss: 3.75 | Total Steps: 43\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 304/175267  | Episode Reward: 7.5  | Average Reward 7.05  | Actor loss: 0.15 | Critic loss: 6.08 | Entropy loss: -0.0015  | Total Loss: 6.23 | Total Steps: 47\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 305/175267  | Episode Reward: 7.5  | Average Reward 7.03  | Actor loss: 0.04 | Critic loss: 7.58 | Entropy loss: -0.0002  | Total Loss: 7.62 | Total Steps: 29\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 306/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.03 | Critic loss: 2.12 | Entropy loss: -0.0000  | Total Loss: 2.16 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 307/175267  | Episode Reward: 7.5  | Average Reward 7.10  | Actor loss: -0.06 | Critic loss: 3.49 | Entropy loss: -0.0042  | Total Loss: 3.43 | Total Steps: 57\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 308/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.02 | Critic loss: 2.80 | Entropy loss: -0.0000  | Total Loss: 2.82 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 309/175267  | Episode Reward: 7.5  | Average Reward 7.12  | Actor loss: 0.06 | Critic loss: 7.57 | Entropy loss: -0.0003  | Total Loss: 7.63 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 310/175267  | Episode Reward: 7.5  | Average Reward 7.12  | Actor loss: -0.63 | Critic loss: 3.41 | Entropy loss: -0.0029  | Total Loss: 2.78 | Total Steps: 44\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 311/175267  | Episode Reward: 7.5  | Average Reward 7.12  | Actor loss: 0.03 | Critic loss: 5.14 | Entropy loss: -0.0003  | Total Loss: 5.17 | Total Steps: 29\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 312/175267  | Episode Reward: 5.0  | Average Reward 7.10  | Actor loss: -0.07 | Critic loss: 5.65 | Entropy loss: -0.0006  | Total Loss: 5.58 | Total Steps: 42\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 313/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: -0.12 | Critic loss: 3.67 | Entropy loss: -0.0017  | Total Loss: 3.55 | Total Steps: 30\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 314/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.06 | Critic loss: 3.89 | Entropy loss: -0.0006  | Total Loss: 3.96 | Total Steps: 32\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 315/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.01 | Critic loss: 0.84 | Entropy loss: -0.0000  | Total Loss: 0.85 | Total Steps: 6\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 316/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.05 | Critic loss: 1.08 | Entropy loss: -0.0001  | Total Loss: 1.14 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 317/175267  | Episode Reward: 7.5  | Average Reward 7.33  | Actor loss: 0.07 | Critic loss: 6.91 | Entropy loss: -0.0004  | Total Loss: 6.98 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 318/175267  | Episode Reward: 5.0  | Average Reward 7.29  | Actor loss: -0.01 | Critic loss: 7.02 | Entropy loss: -0.0028  | Total Loss: 7.00 | Total Steps: 55\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 319/175267  | Episode Reward: 0.0  | Average Reward 7.41  | Actor loss: -0.36 | Critic loss: 12.38 | Entropy loss: -0.0025  | Total Loss: 12.02 | Total Steps: 82\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 320/175267  | Episode Reward: 5.0  | Average Reward 7.38  | Actor loss: -0.65 | Critic loss: 4.48 | Entropy loss: -0.0064  | Total Loss: 3.83 | Total Steps: 64\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 321/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: 0.01 | Critic loss: 0.98 | Entropy loss: -0.0000  | Total Loss: 0.99 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 322/175267  | Episode Reward: 7.5  | Average Reward 7.45  | Actor loss: 0.10 | Critic loss: 7.46 | Entropy loss: -0.0004  | Total Loss: 7.56 | Total Steps: 29\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 323/175267  | Episode Reward: 7.5  | Average Reward 7.47  | Actor loss: 0.03 | Critic loss: 3.83 | Entropy loss: -0.0007  | Total Loss: 3.86 | Total Steps: 30\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 324/175267  | Episode Reward: 7.5  | Average Reward 7.52  | Actor loss: 0.03 | Critic loss: 5.54 | Entropy loss: -0.0003  | Total Loss: 5.57 | Total Steps: 34\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 325/175267  | Episode Reward: 7.5  | Average Reward 7.50  | Actor loss: -0.20 | Critic loss: 3.60 | Entropy loss: -0.0026  | Total Loss: 3.39 | Total Steps: 47\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 326/175267  | Episode Reward: 7.5  | Average Reward 7.47  | Actor loss: 0.06 | Critic loss: 6.23 | Entropy loss: -0.0004  | Total Loss: 6.28 | Total Steps: 34\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 327/175267  | Episode Reward: 7.5  | Average Reward 7.47  | Actor loss: 0.04 | Critic loss: 6.89 | Entropy loss: -0.0002  | Total Loss: 6.92 | Total Steps: 29\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 328/175267  | Episode Reward: 10.0  | Average Reward 7.47  | Actor loss: -0.38 | Critic loss: 3.52 | Entropy loss: -0.0025  | Total Loss: 3.13 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 329/175267  | Episode Reward: -2.5  | Average Reward 7.54  | Actor loss: -0.41 | Critic loss: 13.01 | Entropy loss: -0.0063  | Total Loss: 12.60 | Total Steps: 204\n",
      "---blue prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 330/175267  | Episode Reward: 10.0  | Average Reward 7.54  | Actor loss: 0.02 | Critic loss: 2.26 | Entropy loss: -0.0000  | Total Loss: 2.28 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 331/175267  | Episode Reward: 7.5  | Average Reward 7.54  | Actor loss: 0.04 | Critic loss: 2.20 | Entropy loss: -0.0061  | Total Loss: 2.23 | Total Steps: 69\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 332/175267  | Episode Reward: 7.5  | Average Reward 7.52  | Actor loss: 0.21 | Critic loss: 3.93 | Entropy loss: -0.0050  | Total Loss: 4.13 | Total Steps: 58\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 333/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.01 | Critic loss: 2.07 | Entropy loss: -0.0000  | Total Loss: 2.08 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 334/175267  | Episode Reward: 5.0  | Average Reward 7.66  | Actor loss: -0.34 | Critic loss: 7.57 | Entropy loss: -0.0050  | Total Loss: 7.22 | Total Steps: 34\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 335/175267  | Episode Reward: 10.0  | Average Reward 7.66  | Actor loss: 0.03 | Critic loss: 2.97 | Entropy loss: -0.0019  | Total Loss: 3.00 | Total Steps: 90\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 336/175267  | Episode Reward: 10.0  | Average Reward 7.68  | Actor loss: 0.22 | Critic loss: 1.66 | Entropy loss: -0.0022  | Total Loss: 1.88 | Total Steps: 32\n",
      "---yellow capsule---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 337/175267  | Episode Reward: 9.0  | Average Reward 7.70  | Actor loss: -0.06 | Critic loss: 3.51 | Entropy loss: -0.0055  | Total Loss: 3.44 | Total Steps: 56\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 338/175267  | Episode Reward: 7.5  | Average Reward 7.70  | Actor loss: 0.04 | Critic loss: 5.41 | Entropy loss: -0.0006  | Total Loss: 5.46 | Total Steps: 43\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 339/175267  | Episode Reward: 10.0  | Average Reward 7.72  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 340/175267  | Episode Reward: 5.0  | Average Reward 7.72  | Actor loss: -0.15 | Critic loss: 7.35 | Entropy loss: -0.0010  | Total Loss: 7.19 | Total Steps: 52\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 341/175267  | Episode Reward: 10.0  | Average Reward 7.72  | Actor loss: 0.02 | Critic loss: 4.27 | Entropy loss: -0.0000  | Total Loss: 4.28 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 342/175267  | Episode Reward: 5.0  | Average Reward 7.67  | Actor loss: 0.04 | Critic loss: 7.57 | Entropy loss: -0.0009  | Total Loss: 7.61 | Total Steps: 40\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 343/175267  | Episode Reward: 10.0  | Average Reward 7.70  | Actor loss: 0.01 | Critic loss: 1.39 | Entropy loss: -0.0000  | Total Loss: 1.39 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 344/175267  | Episode Reward: 10.0  | Average Reward 7.72  | Actor loss: 0.07 | Critic loss: 3.18 | Entropy loss: -0.0007  | Total Loss: 3.26 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 345/175267  | Episode Reward: 7.5  | Average Reward 7.70  | Actor loss: 0.06 | Critic loss: 2.73 | Entropy loss: -0.0005  | Total Loss: 2.80 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 346/175267  | Episode Reward: 10.0  | Average Reward 7.70  | Actor loss: -0.01 | Critic loss: 2.07 | Entropy loss: -0.0005  | Total Loss: 2.06 | Total Steps: 43\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 347/175267  | Episode Reward: 10.0  | Average Reward 7.70  | Actor loss: 0.01 | Critic loss: 1.04 | Entropy loss: -0.0000  | Total Loss: 1.05 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 348/175267  | Episode Reward: 4.0  | Average Reward 7.63  | Actor loss: -0.22 | Critic loss: 6.73 | Entropy loss: -0.0021  | Total Loss: 6.51 | Total Steps: 45\n",
      "---black cube---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 349/175267  | Episode Reward: 9.0  | Average Reward 7.74  | Actor loss: 0.10 | Critic loss: 3.94 | Entropy loss: -0.0048  | Total Loss: 4.03 | Total Steps: 45\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 350/175267  | Episode Reward: 10.0  | Average Reward 7.79  | Actor loss: 0.02 | Critic loss: 0.76 | Entropy loss: -0.0001  | Total Loss: 0.78 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 351/175267  | Episode Reward: 7.5  | Average Reward 7.76  | Actor loss: 0.01 | Critic loss: 5.71 | Entropy loss: -0.0007  | Total Loss: 5.72 | Total Steps: 43\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 352/175267  | Episode Reward: 7.5  | Average Reward 7.79  | Actor loss: 0.05 | Critic loss: 2.45 | Entropy loss: -0.0004  | Total Loss: 2.50 | Total Steps: 30\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 353/175267  | Episode Reward: 2.5  | Average Reward 7.79  | Actor loss: -0.06 | Critic loss: 6.54 | Entropy loss: -0.0032  | Total Loss: 6.47 | Total Steps: 54\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 354/175267  | Episode Reward: 10.0  | Average Reward 7.81  | Actor loss: 0.01 | Critic loss: 1.25 | Entropy loss: -0.0000  | Total Loss: 1.26 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 355/175267  | Episode Reward: 7.5  | Average Reward 7.81  | Actor loss: 0.02 | Critic loss: 2.41 | Entropy loss: -0.0008  | Total Loss: 2.42 | Total Steps: 43\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 356/175267  | Episode Reward: 5.0  | Average Reward 7.79  | Actor loss: -0.49 | Critic loss: 9.26 | Entropy loss: -0.0042  | Total Loss: 8.76 | Total Steps: 82\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 357/175267  | Episode Reward: 2.5  | Average Reward 7.74  | Actor loss: -0.04 | Critic loss: 6.67 | Entropy loss: -0.0025  | Total Loss: 6.63 | Total Steps: 54\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 358/175267  | Episode Reward: 10.0  | Average Reward 7.74  | Actor loss: 0.04 | Critic loss: 3.21 | Entropy loss: -0.0003  | Total Loss: 3.24 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 359/175267  | Episode Reward: 10.0  | Average Reward 7.74  | Actor loss: 0.00 | Critic loss: 3.22 | Entropy loss: -0.0004  | Total Loss: 3.23 | Total Steps: 49\n",
      "---yellow cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 360/175267  | Episode Reward: 5.0  | Average Reward 7.68  | Actor loss: 0.04 | Critic loss: 5.80 | Entropy loss: -0.0016  | Total Loss: 5.84 | Total Steps: 44\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 361/175267  | Episode Reward: 5.0  | Average Reward 7.66  | Actor loss: -0.02 | Critic loss: 8.50 | Entropy loss: -0.0003  | Total Loss: 8.48 | Total Steps: 42\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 362/175267  | Episode Reward: 10.0  | Average Reward 7.66  | Actor loss: 0.02 | Critic loss: 1.61 | Entropy loss: -0.0000  | Total Loss: 1.63 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 363/175267  | Episode Reward: 10.0  | Average Reward 7.68  | Actor loss: 0.01 | Critic loss: 2.54 | Entropy loss: -0.0000  | Total Loss: 2.55 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 364/175267  | Episode Reward: 10.0  | Average Reward 7.68  | Actor loss: 0.03 | Critic loss: 2.61 | Entropy loss: -0.0000  | Total Loss: 2.64 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 365/175267  | Episode Reward: 5.0  | Average Reward 7.66  | Actor loss: 0.03 | Critic loss: 6.18 | Entropy loss: -0.0018  | Total Loss: 6.21 | Total Steps: 54\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 366/175267  | Episode Reward: 0.0  | Average Reward 7.68  | Actor loss: -0.74 | Critic loss: 17.72 | Entropy loss: -0.0041  | Total Loss: 16.98 | Total Steps: 83\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 367/175267  | Episode Reward: 10.0  | Average Reward 7.68  | Actor loss: 0.06 | Critic loss: 2.23 | Entropy loss: -0.0022  | Total Loss: 2.29 | Total Steps: 60\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 368/175267  | Episode Reward: 7.5  | Average Reward 7.68  | Actor loss: -0.53 | Critic loss: 4.84 | Entropy loss: -0.0018  | Total Loss: 4.32 | Total Steps: 42\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 369/175267  | Episode Reward: 7.5  | Average Reward 7.66  | Actor loss: 0.33 | Critic loss: 3.96 | Entropy loss: -0.0019  | Total Loss: 4.29 | Total Steps: 37\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 370/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.01 | Critic loss: 1.21 | Entropy loss: -0.0000  | Total Loss: 1.21 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 371/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.01 | Critic loss: 1.53 | Entropy loss: -0.0000  | Total Loss: 1.54 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 372/175267  | Episode Reward: 7.5  | Average Reward 7.83  | Actor loss: 0.16 | Critic loss: 7.68 | Entropy loss: -0.0007  | Total Loss: 7.84 | Total Steps: 32\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 373/175267  | Episode Reward: 5.0  | Average Reward 7.81  | Actor loss: -0.04 | Critic loss: 4.41 | Entropy loss: -0.0043  | Total Loss: 4.36 | Total Steps: 56\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 374/175267  | Episode Reward: 10.0  | Average Reward 7.84  | Actor loss: 0.03 | Critic loss: 1.61 | Entropy loss: -0.0001  | Total Loss: 1.64 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 375/175267  | Episode Reward: 5.0  | Average Reward 7.79  | Actor loss: -0.35 | Critic loss: 8.13 | Entropy loss: -0.0017  | Total Loss: 7.78 | Total Steps: 54\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 376/175267  | Episode Reward: 7.5  | Average Reward 7.79  | Actor loss: 0.02 | Critic loss: 7.25 | Entropy loss: -0.0002  | Total Loss: 7.27 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 377/175267  | Episode Reward: 5.0  | Average Reward 7.77  | Actor loss: -0.28 | Critic loss: 4.50 | Entropy loss: -0.0037  | Total Loss: 4.22 | Total Steps: 113\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 378/175267  | Episode Reward: 7.5  | Average Reward 7.75  | Actor loss: -0.01 | Critic loss: 2.44 | Entropy loss: -0.0011  | Total Loss: 2.43 | Total Steps: 43\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 379/175267  | Episode Reward: 5.0  | Average Reward 7.75  | Actor loss: -0.39 | Critic loss: 7.31 | Entropy loss: -0.0023  | Total Loss: 6.93 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 380/175267  | Episode Reward: 10.0  | Average Reward 7.79  | Actor loss: 0.01 | Critic loss: 1.91 | Entropy loss: -0.0000  | Total Loss: 1.92 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 381/175267  | Episode Reward: 10.0  | Average Reward 7.79  | Actor loss: 0.19 | Critic loss: 3.43 | Entropy loss: -0.0002  | Total Loss: 3.62 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 382/175267  | Episode Reward: 10.0  | Average Reward 7.79  | Actor loss: 0.01 | Critic loss: 3.15 | Entropy loss: -0.0000  | Total Loss: 3.16 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 383/175267  | Episode Reward: 2.5  | Average Reward 7.75  | Actor loss: -0.34 | Critic loss: 6.67 | Entropy loss: -0.0044  | Total Loss: 6.32 | Total Steps: 143\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 384/175267  | Episode Reward: 0.0  | Average Reward 7.67  | Actor loss: -0.39 | Critic loss: 9.22 | Entropy loss: -0.0037  | Total Loss: 8.83 | Total Steps: 57\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 385/175267  | Episode Reward: 7.5  | Average Reward 7.64  | Actor loss: 0.48 | Critic loss: 5.91 | Entropy loss: -0.0016  | Total Loss: 6.39 | Total Steps: 37\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 386/175267  | Episode Reward: 5.0  | Average Reward 7.59  | Actor loss: 0.10 | Critic loss: 6.80 | Entropy loss: -0.0010  | Total Loss: 6.89 | Total Steps: 44\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 387/175267  | Episode Reward: 5.0  | Average Reward 7.54  | Actor loss: -0.03 | Critic loss: 6.12 | Entropy loss: -0.0005  | Total Loss: 6.09 | Total Steps: 42\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 388/175267  | Episode Reward: 7.5  | Average Reward 7.52  | Actor loss: -0.07 | Critic loss: 1.34 | Entropy loss: -0.0018  | Total Loss: 1.27 | Total Steps: 39\n",
      "---blue sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 389/175267  | Episode Reward: 7.5  | Average Reward 7.50  | Actor loss: -0.09 | Critic loss: 2.93 | Entropy loss: -0.0033  | Total Loss: 2.84 | Total Steps: 33\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 390/175267  | Episode Reward: 6.5  | Average Reward 7.54  | Actor loss: -0.18 | Critic loss: 2.89 | Entropy loss: -0.0041  | Total Loss: 2.71 | Total Steps: 109\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 391/175267  | Episode Reward: 10.0  | Average Reward 7.54  | Actor loss: 0.08 | Critic loss: 4.30 | Entropy loss: -0.0024  | Total Loss: 4.38 | Total Steps: 37\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 392/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.01 | Critic loss: 1.21 | Entropy loss: -0.0000  | Total Loss: 1.21 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 393/175267  | Episode Reward: 7.5  | Average Reward 7.56  | Actor loss: 0.07 | Critic loss: 6.15 | Entropy loss: -0.0005  | Total Loss: 6.22 | Total Steps: 30\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 394/175267  | Episode Reward: 5.0  | Average Reward 7.54  | Actor loss: -0.07 | Critic loss: 6.98 | Entropy loss: -0.0010  | Total Loss: 6.91 | Total Steps: 43\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 395/175267  | Episode Reward: 5.0  | Average Reward 7.51  | Actor loss: 0.16 | Critic loss: 7.62 | Entropy loss: -0.0032  | Total Loss: 7.78 | Total Steps: 44\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 396/175267  | Episode Reward: 10.0  | Average Reward 7.54  | Actor loss: 0.02 | Critic loss: 3.82 | Entropy loss: -0.0000  | Total Loss: 3.84 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 397/175267  | Episode Reward: 7.5  | Average Reward 7.51  | Actor loss: 0.08 | Critic loss: 2.43 | Entropy loss: -0.0022  | Total Loss: 2.50 | Total Steps: 44\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 398/175267  | Episode Reward: 10.0  | Average Reward 7.51  | Actor loss: 0.03 | Critic loss: 1.82 | Entropy loss: -0.0026  | Total Loss: 1.84 | Total Steps: 45\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 399/175267  | Episode Reward: -10.0  | Average Reward 7.31  | Actor loss: -0.76 | Critic loss: 20.16 | Entropy loss: -0.0083  | Total Loss: 19.39 | Total Steps: 130\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 400/175267  | Episode Reward: 10.0  | Average Reward 7.31  | Actor loss: 0.01 | Critic loss: 2.97 | Entropy loss: -0.0000  | Total Loss: 2.98 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 401/175267  | Episode Reward: 7.5  | Average Reward 7.33  | Actor loss: 0.05 | Critic loss: 3.96 | Entropy loss: -0.0004  | Total Loss: 4.01 | Total Steps: 30\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 402/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.11 | Critic loss: 1.04 | Entropy loss: -0.0002  | Total Loss: 1.15 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 403/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.23 | Critic loss: 1.54 | Entropy loss: -0.0025  | Total Loss: 1.77 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 404/175267  | Episode Reward: 5.0  | Average Reward 7.31  | Actor loss: -0.26 | Critic loss: 6.94 | Entropy loss: -0.0021  | Total Loss: 6.68 | Total Steps: 53\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 405/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.04 | Critic loss: 3.87 | Entropy loss: -0.0003  | Total Loss: 3.91 | Total Steps: 34\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 406/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.01 | Critic loss: 0.97 | Entropy loss: -0.0000  | Total Loss: 0.98 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 407/175267  | Episode Reward: -25.0  | Average Reward 7.01  | Actor loss: -0.73 | Critic loss: 25.98 | Entropy loss: -0.0106  | Total Loss: 25.24 | Total Steps: 196\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 408/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: 0.02 | Critic loss: 3.30 | Entropy loss: -0.0001  | Total Loss: 3.31 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 409/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 1.16 | Entropy loss: -0.0000  | Total Loss: 1.17 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 410/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: -0.38 | Critic loss: 3.25 | Entropy loss: -0.0038  | Total Loss: 2.86 | Total Steps: 53\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 411/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 0.86 | Entropy loss: -0.0000  | Total Loss: 0.86 | Total Steps: 6\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 412/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.02 | Critic loss: 0.32 | Entropy loss: -0.0001  | Total Loss: 0.34 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 413/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.01 | Critic loss: 1.28 | Entropy loss: -0.0000  | Total Loss: 1.28 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 414/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: -0.09 | Critic loss: 3.68 | Entropy loss: -0.0010  | Total Loss: 3.60 | Total Steps: 43\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 415/175267  | Episode Reward: 0.0  | Average Reward 7.01  | Actor loss: -0.30 | Critic loss: 9.43 | Entropy loss: -0.0018  | Total Loss: 9.13 | Total Steps: 69\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 416/175267  | Episode Reward: 2.5  | Average Reward 6.93  | Actor loss: -0.66 | Critic loss: 4.64 | Entropy loss: -0.0045  | Total Loss: 3.98 | Total Steps: 52\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 417/175267  | Episode Reward: 5.0  | Average Reward 6.91  | Actor loss: -0.03 | Critic loss: 3.77 | Entropy loss: -0.0007  | Total Loss: 3.73 | Total Steps: 43\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 418/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: 0.06 | Critic loss: 7.78 | Entropy loss: -0.0003  | Total Loss: 7.84 | Total Steps: 29\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 419/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: -0.45 | Critic loss: 5.10 | Entropy loss: -0.0035  | Total Loss: 4.64 | Total Steps: 53\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 420/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.02 | Critic loss: 1.96 | Entropy loss: -0.0000  | Total Loss: 1.97 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 421/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: 0.47 | Critic loss: 5.92 | Entropy loss: -0.0020  | Total Loss: 6.38 | Total Steps: 36\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 422/175267  | Episode Reward: 2.5  | Average Reward 6.96  | Actor loss: -0.04 | Critic loss: 6.32 | Entropy loss: -0.0010  | Total Loss: 6.27 | Total Steps: 53\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 423/175267  | Episode Reward: -5.0  | Average Reward 6.83  | Actor loss: -0.21 | Critic loss: 9.98 | Entropy loss: -0.0037  | Total Loss: 9.77 | Total Steps: 163\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 424/175267  | Episode Reward: 1.5  | Average Reward 6.78  | Actor loss: -0.81 | Critic loss: 11.73 | Entropy loss: -0.0063  | Total Loss: 10.92 | Total Steps: 59\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 425/175267  | Episode Reward: 5.0  | Average Reward 6.75  | Actor loss: -0.20 | Critic loss: 5.95 | Entropy loss: -0.0029  | Total Loss: 5.74 | Total Steps: 72\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 426/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 0.11 | Critic loss: 5.42 | Entropy loss: -0.0010  | Total Loss: 5.53 | Total Steps: 53\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 427/175267  | Episode Reward: 10.0  | Average Reward 6.78  | Actor loss: 0.07 | Critic loss: 3.89 | Entropy loss: -0.0004  | Total Loss: 3.96 | Total Steps: 31\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 428/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 1.04 | Critic loss: 5.68 | Entropy loss: -0.0037  | Total Loss: 6.71 | Total Steps: 34\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 429/175267  | Episode Reward: 7.5  | Average Reward 6.85  | Actor loss: 0.06 | Critic loss: 6.48 | Entropy loss: -0.0022  | Total Loss: 6.54 | Total Steps: 52\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 430/175267  | Episode Reward: 7.5  | Average Reward 6.83  | Actor loss: 0.22 | Critic loss: 6.09 | Entropy loss: -0.0008  | Total Loss: 6.31 | Total Steps: 31\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 431/175267  | Episode Reward: 2.5  | Average Reward 6.78  | Actor loss: -0.20 | Critic loss: 7.32 | Entropy loss: -0.0024  | Total Loss: 7.12 | Total Steps: 51\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 432/175267  | Episode Reward: 10.0  | Average Reward 6.80  | Actor loss: 0.01 | Critic loss: 2.42 | Entropy loss: -0.0000  | Total Loss: 2.43 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 433/175267  | Episode Reward: 2.5  | Average Reward 6.72  | Actor loss: -0.02 | Critic loss: 5.90 | Entropy loss: -0.0016  | Total Loss: 5.88 | Total Steps: 54\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 434/175267  | Episode Reward: 10.0  | Average Reward 6.78  | Actor loss: 0.28 | Critic loss: 1.94 | Entropy loss: -0.0016  | Total Loss: 2.22 | Total Steps: 7\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 435/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 0.05 | Critic loss: 3.50 | Entropy loss: -0.0004  | Total Loss: 3.55 | Total Steps: 30\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 436/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.23 | Critic loss: 11.66 | Entropy loss: -0.0001  | Total Loss: 11.89 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 437/175267  | Episode Reward: 1.5  | Average Reward 6.67  | Actor loss: -0.42 | Critic loss: 6.41 | Entropy loss: -0.0026  | Total Loss: 5.99 | Total Steps: 57\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 438/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.02 | Critic loss: 3.62 | Entropy loss: -0.0000  | Total Loss: 3.63 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 439/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.01 | Critic loss: 4.25 | Entropy loss: -0.0000  | Total Loss: 4.26 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 440/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.01 | Critic loss: 2.43 | Entropy loss: -0.0000  | Total Loss: 2.44 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 441/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.01 | Critic loss: 5.08 | Entropy loss: -0.0015  | Total Loss: 5.08 | Total Steps: 55\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 442/175267  | Episode Reward: 6.5  | Average Reward 6.76  | Actor loss: -0.39 | Critic loss: 8.59 | Entropy loss: -0.0021  | Total Loss: 8.19 | Total Steps: 48\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 443/175267  | Episode Reward: 0.0  | Average Reward 6.67  | Actor loss: -0.14 | Critic loss: 6.13 | Entropy loss: -0.0038  | Total Loss: 5.98 | Total Steps: 146\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 444/175267  | Episode Reward: 5.0  | Average Reward 6.62  | Actor loss: 0.02 | Critic loss: 5.43 | Entropy loss: -0.0016  | Total Loss: 5.45 | Total Steps: 44\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 445/175267  | Episode Reward: 7.5  | Average Reward 6.62  | Actor loss: 0.03 | Critic loss: 5.49 | Entropy loss: -0.0003  | Total Loss: 5.52 | Total Steps: 37\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 446/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: -0.31 | Critic loss: 5.21 | Entropy loss: -0.0069  | Total Loss: 4.89 | Total Steps: 143\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 447/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.01 | Critic loss: 1.02 | Entropy loss: -0.0000  | Total Loss: 1.03 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 448/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.01 | Critic loss: 0.45 | Entropy loss: -0.0000  | Total Loss: 0.46 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 449/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: 0.16 | Critic loss: 2.13 | Entropy loss: -0.0029  | Total Loss: 2.28 | Total Steps: 44\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 450/175267  | Episode Reward: 5.0  | Average Reward 6.61  | Actor loss: -0.20 | Critic loss: 7.71 | Entropy loss: -0.0013  | Total Loss: 7.50 | Total Steps: 53\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 451/175267  | Episode Reward: 10.0  | Average Reward 6.63  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 452/175267  | Episode Reward: 10.0  | Average Reward 6.66  | Actor loss: 0.01 | Critic loss: 1.52 | Entropy loss: -0.0000  | Total Loss: 1.53 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 453/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: 0.02 | Critic loss: 2.59 | Entropy loss: -0.0000  | Total Loss: 2.60 | Total Steps: 6\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 454/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 455/175267  | Episode Reward: 2.5  | Average Reward 6.68  | Actor loss: -0.25 | Critic loss: 7.66 | Entropy loss: -0.0028  | Total Loss: 7.41 | Total Steps: 55\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 456/175267  | Episode Reward: 7.5  | Average Reward 6.71  | Actor loss: 0.14 | Critic loss: 7.11 | Entropy loss: -0.0006  | Total Loss: 7.26 | Total Steps: 30\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 457/175267  | Episode Reward: 7.5  | Average Reward 6.76  | Actor loss: 0.03 | Critic loss: 3.49 | Entropy loss: -0.0006  | Total Loss: 3.52 | Total Steps: 42\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 458/175267  | Episode Reward: 2.5  | Average Reward 6.68  | Actor loss: -0.39 | Critic loss: 7.27 | Entropy loss: -0.0029  | Total Loss: 6.88 | Total Steps: 55\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 459/175267  | Episode Reward: 5.0  | Average Reward 6.63  | Actor loss: -0.89 | Critic loss: 3.69 | Entropy loss: -0.0054  | Total Loss: 2.80 | Total Steps: 44\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 460/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: 0.13 | Critic loss: 3.63 | Entropy loss: -0.0038  | Total Loss: 3.76 | Total Steps: 68\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 461/175267  | Episode Reward: 10.0  | Average Reward 6.71  | Actor loss: 0.06 | Critic loss: 4.99 | Entropy loss: -0.0005  | Total Loss: 5.04 | Total Steps: 37\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 462/175267  | Episode Reward: 7.5  | Average Reward 6.68  | Actor loss: 0.02 | Critic loss: 7.68 | Entropy loss: -0.0002  | Total Loss: 7.70 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 463/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: 0.01 | Critic loss: 3.42 | Entropy loss: -0.0021  | Total Loss: 3.43 | Total Steps: 96\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 464/175267  | Episode Reward: 10.0  | Average Reward 6.66  | Actor loss: 0.09 | Critic loss: 2.67 | Entropy loss: -0.0030  | Total Loss: 2.76 | Total Steps: 92\n",
      "---red prism---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 465/175267  | Episode Reward: 6.5  | Average Reward 6.67  | Actor loss: -0.31 | Critic loss: 2.60 | Entropy loss: -0.0033  | Total Loss: 2.28 | Total Steps: 50\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 466/175267  | Episode Reward: 5.0  | Average Reward 6.72  | Actor loss: 0.02 | Critic loss: 8.01 | Entropy loss: -0.0018  | Total Loss: 8.03 | Total Steps: 44\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 467/175267  | Episode Reward: 7.5  | Average Reward 6.70  | Actor loss: 0.32 | Critic loss: 8.46 | Entropy loss: -0.0009  | Total Loss: 8.79 | Total Steps: 30\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 468/175267  | Episode Reward: 5.0  | Average Reward 6.67  | Actor loss: -0.37 | Critic loss: 10.28 | Entropy loss: -0.0041  | Total Loss: 9.91 | Total Steps: 36\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 469/175267  | Episode Reward: 7.5  | Average Reward 6.67  | Actor loss: 0.03 | Critic loss: 4.11 | Entropy loss: -0.0003  | Total Loss: 4.14 | Total Steps: 29\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 470/175267  | Episode Reward: 5.0  | Average Reward 6.62  | Actor loss: -0.12 | Critic loss: 4.11 | Entropy loss: -0.0015  | Total Loss: 3.99 | Total Steps: 43\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 471/175267  | Episode Reward: 2.5  | Average Reward 6.55  | Actor loss: -0.30 | Critic loss: 10.15 | Entropy loss: -0.0016  | Total Loss: 9.84 | Total Steps: 54\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 472/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.03 | Critic loss: 3.26 | Entropy loss: -0.0002  | Total Loss: 3.28 | Total Steps: 31\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 473/175267  | Episode Reward: 5.0  | Average Reward 6.58  | Actor loss: 0.26 | Critic loss: 3.13 | Entropy loss: -0.0028  | Total Loss: 3.38 | Total Steps: 44\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 474/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.52 | Critic loss: 4.77 | Entropy loss: -0.0006  | Total Loss: 5.29 | Total Steps: 7\n",
      "---blue cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 475/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.02 | Critic loss: 4.28 | Entropy loss: -0.0000  | Total Loss: 4.30 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 476/175267  | Episode Reward: 5.0  | Average Reward 6.60  | Actor loss: -0.04 | Critic loss: 6.16 | Entropy loss: -0.0006  | Total Loss: 6.12 | Total Steps: 53\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 477/175267  | Episode Reward: 10.0  | Average Reward 6.65  | Actor loss: 0.05 | Critic loss: 4.70 | Entropy loss: -0.0002  | Total Loss: 4.75 | Total Steps: 31\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 478/175267  | Episode Reward: 7.5  | Average Reward 6.65  | Actor loss: -0.06 | Critic loss: 4.61 | Entropy loss: -0.0005  | Total Loss: 4.55 | Total Steps: 53\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 479/175267  | Episode Reward: 7.5  | Average Reward 6.67  | Actor loss: -0.14 | Critic loss: 4.55 | Entropy loss: -0.0032  | Total Loss: 4.40 | Total Steps: 45\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 480/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.43 | Critic loss: 3.47 | Entropy loss: -0.0021  | Total Loss: 3.90 | Total Steps: 31\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 481/175267  | Episode Reward: 7.5  | Average Reward 6.65  | Actor loss: -0.07 | Critic loss: 3.36 | Entropy loss: -0.0010  | Total Loss: 3.29 | Total Steps: 42\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 482/175267  | Episode Reward: 10.0  | Average Reward 6.65  | Actor loss: 0.02 | Critic loss: 2.36 | Entropy loss: -0.0010  | Total Loss: 2.38 | Total Steps: 7\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 483/175267  | Episode Reward: 10.0  | Average Reward 6.72  | Actor loss: 0.01 | Critic loss: 1.63 | Entropy loss: -0.0000  | Total Loss: 1.65 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 484/175267  | Episode Reward: 5.0  | Average Reward 6.78  | Actor loss: -0.34 | Critic loss: 10.29 | Entropy loss: -0.0036  | Total Loss: 9.94 | Total Steps: 86\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 485/175267  | Episode Reward: -47.5  | Average Reward 6.22  | Actor loss: -0.85 | Critic loss: 21.04 | Entropy loss: -0.0121  | Total Loss: 20.18 | Total Steps: 500\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 486/175267  | Episode Reward: 6.5  | Average Reward 6.24  | Actor loss: 0.03 | Critic loss: 6.82 | Entropy loss: -0.0054  | Total Loss: 6.85 | Total Steps: 56\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 487/175267  | Episode Reward: 7.5  | Average Reward 6.26  | Actor loss: 0.01 | Critic loss: 7.74 | Entropy loss: -0.0001  | Total Loss: 7.75 | Total Steps: 36\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 488/175267  | Episode Reward: 10.0  | Average Reward 6.29  | Actor loss: 0.04 | Critic loss: 4.15 | Entropy loss: -0.0005  | Total Loss: 4.19 | Total Steps: 42\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 489/175267  | Episode Reward: 5.0  | Average Reward 6.26  | Actor loss: -0.01 | Critic loss: 7.16 | Entropy loss: -0.0005  | Total Loss: 7.15 | Total Steps: 42\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 490/175267  | Episode Reward: 5.0  | Average Reward 6.25  | Actor loss: -0.08 | Critic loss: 5.27 | Entropy loss: -0.0037  | Total Loss: 5.18 | Total Steps: 54\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 491/175267  | Episode Reward: 7.5  | Average Reward 6.22  | Actor loss: -0.00 | Critic loss: 3.94 | Entropy loss: -0.0007  | Total Loss: 3.94 | Total Steps: 42\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 492/175267  | Episode Reward: 7.5  | Average Reward 6.20  | Actor loss: 0.08 | Critic loss: 6.64 | Entropy loss: -0.0004  | Total Loss: 6.72 | Total Steps: 39\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 493/175267  | Episode Reward: 10.0  | Average Reward 6.22  | Actor loss: 0.02 | Critic loss: 4.77 | Entropy loss: -0.0001  | Total Loss: 4.79 | Total Steps: 29\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 494/175267  | Episode Reward: 7.5  | Average Reward 6.25  | Actor loss: -0.21 | Critic loss: 2.50 | Entropy loss: -0.0018  | Total Loss: 2.28 | Total Steps: 42\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 495/175267  | Episode Reward: 5.0  | Average Reward 6.25  | Actor loss: -0.01 | Critic loss: 6.66 | Entropy loss: -0.0007  | Total Loss: 6.65 | Total Steps: 43\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 496/175267  | Episode Reward: 10.0  | Average Reward 6.25  | Actor loss: 0.05 | Critic loss: 0.78 | Entropy loss: -0.0001  | Total Loss: 0.83 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 497/175267  | Episode Reward: 7.5  | Average Reward 6.25  | Actor loss: -0.04 | Critic loss: 6.24 | Entropy loss: -0.0012  | Total Loss: 6.20 | Total Steps: 30\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 498/175267  | Episode Reward: 2.5  | Average Reward 6.17  | Actor loss: -0.28 | Critic loss: 6.84 | Entropy loss: -0.0027  | Total Loss: 6.56 | Total Steps: 70\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 499/175267  | Episode Reward: 10.0  | Average Reward 6.38  | Actor loss: 0.01 | Critic loss: 3.29 | Entropy loss: -0.0012  | Total Loss: 3.30 | Total Steps: 51\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 500/175267  | Episode Reward: 10.0  | Average Reward 6.38  | Actor loss: 0.09 | Critic loss: 2.48 | Entropy loss: -0.0013  | Total Loss: 2.57 | Total Steps: 47\n",
      "---blue capsule---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 501/175267  | Episode Reward: 4.0  | Average Reward 6.34  | Actor loss: -0.09 | Critic loss: 3.50 | Entropy loss: -0.0080  | Total Loss: 3.41 | Total Steps: 229\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 502/175267  | Episode Reward: 7.5  | Average Reward 6.32  | Actor loss: 0.04 | Critic loss: 5.96 | Entropy loss: -0.0004  | Total Loss: 6.00 | Total Steps: 29\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 503/175267  | Episode Reward: 7.5  | Average Reward 6.29  | Actor loss: 0.15 | Critic loss: 3.84 | Entropy loss: -0.0007  | Total Loss: 3.99 | Total Steps: 36\n",
      "---blue sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 504/175267  | Episode Reward: 10.0  | Average Reward 6.34  | Actor loss: 0.16 | Critic loss: 3.15 | Entropy loss: -0.0002  | Total Loss: 3.31 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 505/175267  | Episode Reward: 7.5  | Average Reward 6.32  | Actor loss: 0.10 | Critic loss: 8.49 | Entropy loss: -0.0003  | Total Loss: 8.59 | Total Steps: 30\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 506/175267  | Episode Reward: 10.0  | Average Reward 6.32  | Actor loss: 0.03 | Critic loss: 4.37 | Entropy loss: -0.0000  | Total Loss: 4.40 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 507/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.02 | Critic loss: 3.44 | Entropy loss: -0.0002  | Total Loss: 3.46 | Total Steps: 34\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 508/175267  | Episode Reward: 5.0  | Average Reward 6.62  | Actor loss: -0.38 | Critic loss: 7.55 | Entropy loss: -0.0030  | Total Loss: 7.17 | Total Steps: 80\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 509/175267  | Episode Reward: 7.5  | Average Reward 6.59  | Actor loss: 0.20 | Critic loss: 4.72 | Entropy loss: -0.0010  | Total Loss: 4.92 | Total Steps: 29\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 510/175267  | Episode Reward: 2.5  | Average Reward 6.54  | Actor loss: -0.25 | Critic loss: 9.83 | Entropy loss: -0.0012  | Total Loss: 9.57 | Total Steps: 53\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 511/175267  | Episode Reward: 7.5  | Average Reward 6.51  | Actor loss: 0.13 | Critic loss: 7.70 | Entropy loss: -0.0005  | Total Loss: 7.83 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 512/175267  | Episode Reward: 2.5  | Average Reward 6.44  | Actor loss: -0.22 | Critic loss: 10.36 | Entropy loss: -0.0051  | Total Loss: 10.13 | Total Steps: 101\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 513/175267  | Episode Reward: 10.0  | Average Reward 6.44  | Actor loss: 0.18 | Critic loss: 5.03 | Entropy loss: -0.0007  | Total Loss: 5.21 | Total Steps: 30\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 514/175267  | Episode Reward: 2.5  | Average Reward 6.39  | Actor loss: -0.52 | Critic loss: 7.65 | Entropy loss: -0.0046  | Total Loss: 7.13 | Total Steps: 56\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 515/175267  | Episode Reward: 7.5  | Average Reward 6.46  | Actor loss: -0.27 | Critic loss: 5.70 | Entropy loss: -0.0039  | Total Loss: 5.43 | Total Steps: 46\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 516/175267  | Episode Reward: 10.0  | Average Reward 6.54  | Actor loss: 0.03 | Critic loss: 3.31 | Entropy loss: -0.0000  | Total Loss: 3.34 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 517/175267  | Episode Reward: 10.0  | Average Reward 6.59  | Actor loss: 0.02 | Critic loss: 1.85 | Entropy loss: -0.0000  | Total Loss: 1.87 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 518/175267  | Episode Reward: 7.5  | Average Reward 6.59  | Actor loss: 0.64 | Critic loss: 7.06 | Entropy loss: -0.0051  | Total Loss: 7.69 | Total Steps: 34\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 519/175267  | Episode Reward: 10.0  | Average Reward 6.64  | Actor loss: 0.01 | Critic loss: 1.55 | Entropy loss: -0.0000  | Total Loss: 1.55 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 520/175267  | Episode Reward: 7.5  | Average Reward 6.62  | Actor loss: -0.16 | Critic loss: 2.06 | Entropy loss: -0.0021  | Total Loss: 1.90 | Total Steps: 43\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 521/175267  | Episode Reward: 10.0  | Average Reward 6.64  | Actor loss: 0.02 | Critic loss: 0.61 | Entropy loss: -0.0001  | Total Loss: 0.62 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 522/175267  | Episode Reward: 10.0  | Average Reward 6.71  | Actor loss: 0.01 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.14 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 523/175267  | Episode Reward: 7.5  | Average Reward 6.84  | Actor loss: 0.02 | Critic loss: 6.35 | Entropy loss: -0.0002  | Total Loss: 6.37 | Total Steps: 36\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 524/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.02 | Critic loss: 3.19 | Entropy loss: -0.0000  | Total Loss: 3.21 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 525/175267  | Episode Reward: 2.5  | Average Reward 6.90  | Actor loss: -0.43 | Critic loss: 5.67 | Entropy loss: -0.0025  | Total Loss: 5.23 | Total Steps: 42\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 526/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.07 | Critic loss: 7.37 | Entropy loss: -0.0001  | Total Loss: 7.44 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 527/175267  | Episode Reward: -15.0  | Average Reward 6.67  | Actor loss: -0.05 | Critic loss: 10.46 | Entropy loss: -0.0015  | Total Loss: 10.41 | Total Steps: 341\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 528/175267  | Episode Reward: 5.0  | Average Reward 6.65  | Actor loss: -0.66 | Critic loss: 6.49 | Entropy loss: -0.0023  | Total Loss: 5.83 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 529/175267  | Episode Reward: 0.0  | Average Reward 6.58  | Actor loss: -0.19 | Critic loss: 11.43 | Entropy loss: -0.0057  | Total Loss: 11.23 | Total Steps: 182\n",
      "---green cylinder---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 530/175267  | Episode Reward: -3.5  | Average Reward 6.46  | Actor loss: -0.57 | Critic loss: 9.28 | Entropy loss: -0.0107  | Total Loss: 8.71 | Total Steps: 192\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 531/175267  | Episode Reward: 7.5  | Average Reward 6.51  | Actor loss: 0.46 | Critic loss: 5.23 | Entropy loss: -0.0036  | Total Loss: 5.68 | Total Steps: 44\n",
      "---blue prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 532/175267  | Episode Reward: 10.0  | Average Reward 6.51  | Actor loss: 0.02 | Critic loss: 4.20 | Entropy loss: -0.0000  | Total Loss: 4.23 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 533/175267  | Episode Reward: 5.0  | Average Reward 6.54  | Actor loss: -0.06 | Critic loss: 4.79 | Entropy loss: -0.0025  | Total Loss: 4.73 | Total Steps: 44\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 534/175267  | Episode Reward: 7.5  | Average Reward 6.51  | Actor loss: 0.01 | Critic loss: 2.71 | Entropy loss: -0.0005  | Total Loss: 2.72 | Total Steps: 47\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 535/175267  | Episode Reward: 10.0  | Average Reward 6.54  | Actor loss: 0.01 | Critic loss: 1.71 | Entropy loss: -0.0000  | Total Loss: 1.72 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 536/175267  | Episode Reward: 7.5  | Average Reward 6.51  | Actor loss: 0.01 | Critic loss: 6.87 | Entropy loss: -0.0001  | Total Loss: 6.89 | Total Steps: 29\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 537/175267  | Episode Reward: 7.5  | Average Reward 6.58  | Actor loss: 0.04 | Critic loss: 7.86 | Entropy loss: -0.0002  | Total Loss: 7.90 | Total Steps: 29\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 538/175267  | Episode Reward: -5.0  | Average Reward 6.42  | Actor loss: -0.27 | Critic loss: 16.39 | Entropy loss: -0.0034  | Total Loss: 16.12 | Total Steps: 123\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 539/175267  | Episode Reward: 7.5  | Average Reward 6.40  | Actor loss: -0.04 | Critic loss: 4.94 | Entropy loss: -0.0007  | Total Loss: 4.90 | Total Steps: 48\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 540/175267  | Episode Reward: 5.0  | Average Reward 6.35  | Actor loss: -0.19 | Critic loss: 5.91 | Entropy loss: -0.0045  | Total Loss: 5.72 | Total Steps: 62\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 541/175267  | Episode Reward: 10.0  | Average Reward 6.35  | Actor loss: 0.01 | Critic loss: 2.01 | Entropy loss: -0.0000  | Total Loss: 2.01 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 542/175267  | Episode Reward: 7.5  | Average Reward 6.36  | Actor loss: 0.13 | Critic loss: 3.71 | Entropy loss: -0.0008  | Total Loss: 3.84 | Total Steps: 30\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 543/175267  | Episode Reward: 7.5  | Average Reward 6.43  | Actor loss: 0.00 | Critic loss: 3.05 | Entropy loss: -0.0006  | Total Loss: 3.05 | Total Steps: 47\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 544/175267  | Episode Reward: 7.5  | Average Reward 6.46  | Actor loss: 0.04 | Critic loss: 4.61 | Entropy loss: -0.0002  | Total Loss: 4.65 | Total Steps: 29\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 545/175267  | Episode Reward: 2.5  | Average Reward 6.41  | Actor loss: -0.15 | Critic loss: 10.22 | Entropy loss: -0.0011  | Total Loss: 10.07 | Total Steps: 53\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 546/175267  | Episode Reward: 10.0  | Average Reward 6.41  | Actor loss: 0.01 | Critic loss: 0.63 | Entropy loss: -0.0000  | Total Loss: 0.64 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 547/175267  | Episode Reward: 7.5  | Average Reward 6.38  | Actor loss: -0.06 | Critic loss: 3.32 | Entropy loss: -0.0022  | Total Loss: 3.26 | Total Steps: 120\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 548/175267  | Episode Reward: 10.0  | Average Reward 6.38  | Actor loss: 0.01 | Critic loss: 3.83 | Entropy loss: -0.0001  | Total Loss: 3.84 | Total Steps: 36\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 549/175267  | Episode Reward: 10.0  | Average Reward 6.41  | Actor loss: 0.18 | Critic loss: 5.85 | Entropy loss: -0.0005  | Total Loss: 6.03 | Total Steps: 29\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 550/175267  | Episode Reward: 5.0  | Average Reward 6.41  | Actor loss: -0.00 | Critic loss: 2.81 | Entropy loss: -0.0009  | Total Loss: 2.81 | Total Steps: 42\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 551/175267  | Episode Reward: 4.0  | Average Reward 6.35  | Actor loss: -0.24 | Critic loss: 4.10 | Entropy loss: -0.0016  | Total Loss: 3.86 | Total Steps: 47\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 552/175267  | Episode Reward: 0.0  | Average Reward 6.25  | Actor loss: -0.64 | Critic loss: 11.08 | Entropy loss: -0.0069  | Total Loss: 10.43 | Total Steps: 128\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 553/175267  | Episode Reward: 10.0  | Average Reward 6.25  | Actor loss: 0.01 | Critic loss: 0.61 | Entropy loss: -0.0001  | Total Loss: 0.62 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 554/175267  | Episode Reward: 10.0  | Average Reward 6.25  | Actor loss: 0.10 | Critic loss: 2.55 | Entropy loss: -0.0006  | Total Loss: 2.65 | Total Steps: 34\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 555/175267  | Episode Reward: 2.5  | Average Reward 6.25  | Actor loss: -0.09 | Critic loss: 4.23 | Entropy loss: -0.0012  | Total Loss: 4.14 | Total Steps: 45\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 556/175267  | Episode Reward: 10.0  | Average Reward 6.28  | Actor loss: 0.06 | Critic loss: 5.54 | Entropy loss: -0.0005  | Total Loss: 5.60 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 557/175267  | Episode Reward: 7.5  | Average Reward 6.28  | Actor loss: -0.03 | Critic loss: 4.98 | Entropy loss: -0.0007  | Total Loss: 4.95 | Total Steps: 53\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 558/175267  | Episode Reward: 10.0  | Average Reward 6.35  | Actor loss: 0.01 | Critic loss: 1.37 | Entropy loss: -0.0000  | Total Loss: 1.39 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 559/175267  | Episode Reward: 10.0  | Average Reward 6.40  | Actor loss: 0.01 | Critic loss: 1.26 | Entropy loss: -0.0000  | Total Loss: 1.27 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 560/175267  | Episode Reward: 7.5  | Average Reward 6.40  | Actor loss: 0.01 | Critic loss: 5.37 | Entropy loss: -0.0004  | Total Loss: 5.38 | Total Steps: 43\n",
      "---black capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 561/175267  | Episode Reward: 10.0  | Average Reward 6.40  | Actor loss: 0.01 | Critic loss: 1.05 | Entropy loss: -0.0000  | Total Loss: 1.06 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 562/175267  | Episode Reward: 10.0  | Average Reward 6.42  | Actor loss: 0.01 | Critic loss: 2.24 | Entropy loss: -0.0000  | Total Loss: 2.25 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 563/175267  | Episode Reward: 7.5  | Average Reward 6.42  | Actor loss: 0.06 | Critic loss: 2.82 | Entropy loss: -0.0005  | Total Loss: 2.88 | Total Steps: 30\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 564/175267  | Episode Reward: 10.0  | Average Reward 6.42  | Actor loss: 0.01 | Critic loss: 0.90 | Entropy loss: -0.0000  | Total Loss: 0.91 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 565/175267  | Episode Reward: 10.0  | Average Reward 6.46  | Actor loss: 0.01 | Critic loss: 1.19 | Entropy loss: -0.0000  | Total Loss: 1.20 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 566/175267  | Episode Reward: 7.5  | Average Reward 6.49  | Actor loss: 0.30 | Critic loss: 8.32 | Entropy loss: -0.0012  | Total Loss: 8.62 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 567/175267  | Episode Reward: 7.5  | Average Reward 6.49  | Actor loss: 0.25 | Critic loss: 6.70 | Entropy loss: -0.0009  | Total Loss: 6.94 | Total Steps: 29\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 568/175267  | Episode Reward: 10.0  | Average Reward 6.54  | Actor loss: 0.01 | Critic loss: 1.43 | Entropy loss: -0.0000  | Total Loss: 1.44 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 569/175267  | Episode Reward: 10.0  | Average Reward 6.56  | Actor loss: 0.00 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 570/175267  | Episode Reward: 6.5  | Average Reward 6.58  | Actor loss: -0.13 | Critic loss: 3.08 | Entropy loss: -0.0051  | Total Loss: 2.94 | Total Steps: 46\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 571/175267  | Episode Reward: -30.0  | Average Reward 6.25  | Actor loss: 0.05 | Critic loss: 3.90 | Entropy loss: -0.0001  | Total Loss: 3.95 | Total Steps: 255\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 572/175267  | Episode Reward: 10.0  | Average Reward 6.25  | Actor loss: 0.01 | Critic loss: 0.70 | Entropy loss: -0.0000  | Total Loss: 0.70 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 573/175267  | Episode Reward: 7.5  | Average Reward 6.28  | Actor loss: -0.20 | Critic loss: 2.79 | Entropy loss: -0.0015  | Total Loss: 2.58 | Total Steps: 46\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 574/175267  | Episode Reward: 7.5  | Average Reward 6.25  | Actor loss: 0.26 | Critic loss: 3.57 | Entropy loss: -0.0023  | Total Loss: 3.83 | Total Steps: 33\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 575/175267  | Episode Reward: 10.0  | Average Reward 6.25  | Actor loss: 0.10 | Critic loss: 0.94 | Entropy loss: -0.0003  | Total Loss: 1.05 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 576/175267  | Episode Reward: 5.0  | Average Reward 6.25  | Actor loss: -0.02 | Critic loss: 4.34 | Entropy loss: -0.0013  | Total Loss: 4.32 | Total Steps: 47\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 577/175267  | Episode Reward: 10.0  | Average Reward 6.25  | Actor loss: 0.02 | Critic loss: 0.70 | Entropy loss: -0.0001  | Total Loss: 0.73 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 578/175267  | Episode Reward: 10.0  | Average Reward 6.28  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0000  | Total Loss: 1.67 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 579/175267  | Episode Reward: 7.5  | Average Reward 6.28  | Actor loss: -0.03 | Critic loss: 5.55 | Entropy loss: -0.0010  | Total Loss: 5.53 | Total Steps: 44\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 580/175267  | Episode Reward: 0.0  | Average Reward 6.17  | Actor loss: -0.15 | Critic loss: 9.44 | Entropy loss: -0.0010  | Total Loss: 9.29 | Total Steps: 53\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 581/175267  | Episode Reward: 10.0  | Average Reward 6.20  | Actor loss: 0.00 | Critic loss: 0.74 | Entropy loss: -0.0000  | Total Loss: 0.74 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 582/175267  | Episode Reward: 7.5  | Average Reward 6.17  | Actor loss: 0.05 | Critic loss: 4.93 | Entropy loss: -0.0021  | Total Loss: 4.97 | Total Steps: 47\n",
      "---red cube---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 583/175267  | Episode Reward: 3.0  | Average Reward 6.11  | Actor loss: -0.68 | Critic loss: 6.87 | Entropy loss: -0.0039  | Total Loss: 6.18 | Total Steps: 51\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 584/175267  | Episode Reward: 2.5  | Average Reward 6.08  | Actor loss: -0.34 | Critic loss: 5.82 | Entropy loss: -0.0051  | Total Loss: 5.47 | Total Steps: 113\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 585/175267  | Episode Reward: 2.5  | Average Reward 6.58  | Actor loss: 0.12 | Critic loss: 8.49 | Entropy loss: -0.0015  | Total Loss: 8.60 | Total Steps: 53\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 586/175267  | Episode Reward: 7.5  | Average Reward 6.59  | Actor loss: -0.06 | Critic loss: 2.38 | Entropy loss: -0.0008  | Total Loss: 2.32 | Total Steps: 47\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 587/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.03 | Critic loss: 6.59 | Entropy loss: -0.0000  | Total Loss: 6.62 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 588/175267  | Episode Reward: 5.0  | Average Reward 6.57  | Actor loss: -0.59 | Critic loss: 4.19 | Entropy loss: -0.0038  | Total Loss: 3.60 | Total Steps: 54\n",
      "---black cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 589/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.01 | Critic loss: 1.72 | Entropy loss: -0.0000  | Total Loss: 1.73 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 590/175267  | Episode Reward: 7.5  | Average Reward 6.64  | Actor loss: -0.32 | Critic loss: 2.64 | Entropy loss: -0.0021  | Total Loss: 2.32 | Total Steps: 45\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 591/175267  | Episode Reward: 2.5  | Average Reward 6.59  | Actor loss: -0.16 | Critic loss: 10.93 | Entropy loss: -0.0009  | Total Loss: 10.76 | Total Steps: 53\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 592/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.26 | Critic loss: 3.84 | Entropy loss: -0.0025  | Total Loss: 4.10 | Total Steps: 31\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 593/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.01 | Critic loss: 0.62 | Entropy loss: -0.0000  | Total Loss: 0.64 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 594/175267  | Episode Reward: 10.0  | Average Reward 6.64  | Actor loss: -0.03 | Critic loss: 2.74 | Entropy loss: -0.0013  | Total Loss: 2.70 | Total Steps: 61\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 595/175267  | Episode Reward: 7.5  | Average Reward 6.67  | Actor loss: -0.05 | Critic loss: 4.45 | Entropy loss: -0.0030  | Total Loss: 4.40 | Total Steps: 44\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 596/175267  | Episode Reward: 2.5  | Average Reward 6.59  | Actor loss: -0.17 | Critic loss: 9.52 | Entropy loss: -0.0025  | Total Loss: 9.34 | Total Steps: 64\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 597/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.01 | Critic loss: 3.27 | Entropy loss: -0.0000  | Total Loss: 3.28 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 598/175267  | Episode Reward: 7.5  | Average Reward 6.67  | Actor loss: 0.12 | Critic loss: 7.07 | Entropy loss: -0.0007  | Total Loss: 7.19 | Total Steps: 34\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 599/175267  | Episode Reward: 4.0  | Average Reward 6.61  | Actor loss: -0.23 | Critic loss: 7.67 | Entropy loss: -0.0019  | Total Loss: 7.44 | Total Steps: 55\n",
      "---black cube---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 600/175267  | Episode Reward: 6.5  | Average Reward 6.57  | Actor loss: -0.23 | Critic loss: 3.70 | Entropy loss: -0.0067  | Total Loss: 3.47 | Total Steps: 155\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 601/175267  | Episode Reward: 10.0  | Average Reward 6.63  | Actor loss: 0.05 | Critic loss: 2.42 | Entropy loss: -0.0001  | Total Loss: 2.47 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 602/175267  | Episode Reward: 10.0  | Average Reward 6.66  | Actor loss: 0.02 | Critic loss: 3.52 | Entropy loss: -0.0000  | Total Loss: 3.54 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 603/175267  | Episode Reward: 5.0  | Average Reward 6.63  | Actor loss: -0.12 | Critic loss: 7.07 | Entropy loss: -0.0032  | Total Loss: 6.94 | Total Steps: 45\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 604/175267  | Episode Reward: 10.0  | Average Reward 6.63  | Actor loss: 0.01 | Critic loss: 3.59 | Entropy loss: -0.0000  | Total Loss: 3.61 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 605/175267  | Episode Reward: 10.0  | Average Reward 6.66  | Actor loss: 0.01 | Critic loss: 1.69 | Entropy loss: -0.0000  | Total Loss: 1.70 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 606/175267  | Episode Reward: 2.5  | Average Reward 6.58  | Actor loss: -0.21 | Critic loss: 4.66 | Entropy loss: -0.0016  | Total Loss: 4.45 | Total Steps: 51\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 607/175267  | Episode Reward: 7.5  | Average Reward 6.55  | Actor loss: -0.05 | Critic loss: 4.20 | Entropy loss: -0.0004  | Total Loss: 4.15 | Total Steps: 43\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 608/175267  | Episode Reward: 10.0  | Average Reward 6.61  | Actor loss: 0.08 | Critic loss: 5.07 | Entropy loss: -0.0004  | Total Loss: 5.15 | Total Steps: 30\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 609/175267  | Episode Reward: 10.0  | Average Reward 6.63  | Actor loss: 0.02 | Critic loss: 2.84 | Entropy loss: -0.0002  | Total Loss: 2.85 | Total Steps: 31\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 610/175267  | Episode Reward: 4.0  | Average Reward 6.64  | Actor loss: -0.37 | Critic loss: 10.85 | Entropy loss: -0.0036  | Total Loss: 10.48 | Total Steps: 93\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 611/175267  | Episode Reward: 2.5  | Average Reward 6.59  | Actor loss: -0.17 | Critic loss: 6.14 | Entropy loss: -0.0066  | Total Loss: 5.97 | Total Steps: 229\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 612/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.03 | Critic loss: 2.53 | Entropy loss: -0.0001  | Total Loss: 2.57 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 613/175267  | Episode Reward: 5.0  | Average Reward 6.62  | Actor loss: -0.02 | Critic loss: 3.85 | Entropy loss: -0.0011  | Total Loss: 3.83 | Total Steps: 48\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 614/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.10 | Critic loss: 5.87 | Entropy loss: -0.0004  | Total Loss: 5.97 | Total Steps: 30\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 615/175267  | Episode Reward: -3.5  | Average Reward 6.58  | Actor loss: -1.55 | Critic loss: 15.96 | Entropy loss: -0.0114  | Total Loss: 14.40 | Total Steps: 100\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 616/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.01 | Critic loss: 1.76 | Entropy loss: -0.0001  | Total Loss: 1.77 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 617/175267  | Episode Reward: 7.5  | Average Reward 6.56  | Actor loss: -0.05 | Critic loss: 5.74 | Entropy loss: -0.0009  | Total Loss: 5.69 | Total Steps: 42\n",
      "---blue cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 618/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.03 | Critic loss: 6.97 | Entropy loss: -0.0000  | Total Loss: 7.00 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 619/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.01 | Critic loss: 0.78 | Entropy loss: -0.0000  | Total Loss: 0.78 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 620/175267  | Episode Reward: 7.5  | Average Reward 6.58  | Actor loss: 0.11 | Critic loss: 6.12 | Entropy loss: -0.0008  | Total Loss: 6.23 | Total Steps: 34\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 621/175267  | Episode Reward: 7.5  | Average Reward 6.56  | Actor loss: -0.15 | Critic loss: 3.85 | Entropy loss: -0.0013  | Total Loss: 3.70 | Total Steps: 44\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 622/175267  | Episode Reward: 0.0  | Average Reward 6.46  | Actor loss: -0.26 | Critic loss: 8.48 | Entropy loss: -0.0041  | Total Loss: 8.21 | Total Steps: 144\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 623/175267  | Episode Reward: 4.0  | Average Reward 6.42  | Actor loss: -0.31 | Critic loss: 7.25 | Entropy loss: -0.0058  | Total Loss: 6.94 | Total Steps: 56\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 624/175267  | Episode Reward: 10.0  | Average Reward 6.42  | Actor loss: 0.05 | Critic loss: 5.38 | Entropy loss: -0.0003  | Total Loss: 5.42 | Total Steps: 30\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 625/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 2.67 | Entropy loss: -0.0000  | Total Loss: 2.68 | Total Steps: 6\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 626/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.07 | Critic loss: 4.92 | Entropy loss: -0.0006  | Total Loss: 5.00 | Total Steps: 34\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 627/175267  | Episode Reward: 4.0  | Average Reward 6.69  | Actor loss: -0.26 | Critic loss: 3.94 | Entropy loss: -0.0039  | Total Loss: 3.68 | Total Steps: 57\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 628/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: 0.06 | Critic loss: 2.77 | Entropy loss: -0.0037  | Total Loss: 2.82 | Total Steps: 211\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 629/175267  | Episode Reward: 7.5  | Average Reward 6.82  | Actor loss: 0.04 | Critic loss: 8.52 | Entropy loss: -0.0002  | Total Loss: 8.56 | Total Steps: 29\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 630/175267  | Episode Reward: 10.0  | Average Reward 6.95  | Actor loss: 0.04 | Critic loss: 4.69 | Entropy loss: -0.0003  | Total Loss: 4.73 | Total Steps: 34\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 631/175267  | Episode Reward: 5.0  | Average Reward 6.92  | Actor loss: 0.01 | Critic loss: 7.82 | Entropy loss: -0.0003  | Total Loss: 7.82 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 632/175267  | Episode Reward: 7.5  | Average Reward 6.90  | Actor loss: -0.11 | Critic loss: 2.14 | Entropy loss: -0.0021  | Total Loss: 2.03 | Total Steps: 72\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 633/175267  | Episode Reward: 7.5  | Average Reward 6.92  | Actor loss: 0.03 | Critic loss: 6.17 | Entropy loss: -0.0004  | Total Loss: 6.20 | Total Steps: 36\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 634/175267  | Episode Reward: 7.5  | Average Reward 6.92  | Actor loss: 0.39 | Critic loss: 6.11 | Entropy loss: -0.0026  | Total Loss: 6.49 | Total Steps: 43\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 635/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.01 | Critic loss: 0.76 | Entropy loss: -0.0005  | Total Loss: 0.77 | Total Steps: 49\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 636/175267  | Episode Reward: 10.0  | Average Reward 6.95  | Actor loss: 0.02 | Critic loss: 0.94 | Entropy loss: -0.0000  | Total Loss: 0.95 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 637/175267  | Episode Reward: 5.0  | Average Reward 6.92  | Actor loss: -0.26 | Critic loss: 5.35 | Entropy loss: -0.0028  | Total Loss: 5.09 | Total Steps: 54\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 638/175267  | Episode Reward: 7.5  | Average Reward 7.05  | Actor loss: 0.15 | Critic loss: 5.40 | Entropy loss: -0.0009  | Total Loss: 5.54 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 639/175267  | Episode Reward: 7.5  | Average Reward 7.05  | Actor loss: -0.43 | Critic loss: 6.83 | Entropy loss: -0.0048  | Total Loss: 6.40 | Total Steps: 82\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 640/175267  | Episode Reward: 2.5  | Average Reward 7.03  | Actor loss: -0.27 | Critic loss: 8.39 | Entropy loss: -0.0024  | Total Loss: 8.12 | Total Steps: 84\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 641/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.05 | Critic loss: 5.24 | Entropy loss: -0.0004  | Total Loss: 5.19 | Total Steps: 43\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 642/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.33 | Critic loss: 4.64 | Entropy loss: -0.0048  | Total Loss: 4.31 | Total Steps: 53\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 643/175267  | Episode Reward: 10.0  | Average Reward 7.03  | Actor loss: -0.02 | Critic loss: 1.24 | Entropy loss: -0.0008  | Total Loss: 1.23 | Total Steps: 49\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 644/175267  | Episode Reward: 10.0  | Average Reward 7.05  | Actor loss: 0.03 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.18 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 645/175267  | Episode Reward: 5.0  | Average Reward 7.08  | Actor loss: -0.12 | Critic loss: 7.36 | Entropy loss: -0.0011  | Total Loss: 7.23 | Total Steps: 49\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 646/175267  | Episode Reward: 5.0  | Average Reward 7.03  | Actor loss: -0.17 | Critic loss: 4.65 | Entropy loss: -0.0012  | Total Loss: 4.48 | Total Steps: 79\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 647/175267  | Episode Reward: 10.0  | Average Reward 7.05  | Actor loss: 0.86 | Critic loss: 1.20 | Entropy loss: -0.0016  | Total Loss: 2.06 | Total Steps: 7\n",
      "---yellow sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 648/175267  | Episode Reward: 7.5  | Average Reward 7.03  | Actor loss: 0.15 | Critic loss: 4.52 | Entropy loss: -0.0009  | Total Loss: 4.68 | Total Steps: 29\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 649/175267  | Episode Reward: 10.0  | Average Reward 7.03  | Actor loss: 0.30 | Critic loss: 5.57 | Entropy loss: -0.0012  | Total Loss: 5.86 | Total Steps: 30\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 650/175267  | Episode Reward: 7.5  | Average Reward 7.05  | Actor loss: -0.18 | Critic loss: 4.63 | Entropy loss: -0.0011  | Total Loss: 4.45 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 651/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.01 | Critic loss: 2.03 | Entropy loss: -0.0000  | Total Loss: 2.04 | Total Steps: 6\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 652/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.00 | Critic loss: 1.20 | Entropy loss: -0.0000  | Total Loss: 1.21 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 653/175267  | Episode Reward: -5.0  | Average Reward 7.06  | Actor loss: -0.44 | Critic loss: 16.25 | Entropy loss: -0.0069  | Total Loss: 15.80 | Total Steps: 185\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 654/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.13 | Critic loss: 3.68 | Entropy loss: -0.0014  | Total Loss: 3.81 | Total Steps: 30\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 655/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: -0.01 | Critic loss: 6.14 | Entropy loss: -0.0006  | Total Loss: 6.13 | Total Steps: 53\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 656/175267  | Episode Reward: 5.0  | Average Reward 7.06  | Actor loss: -0.10 | Critic loss: 8.12 | Entropy loss: -0.0011  | Total Loss: 8.01 | Total Steps: 47\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 657/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: 0.02 | Critic loss: 7.51 | Entropy loss: -0.0002  | Total Loss: 7.53 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 658/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: -0.05 | Critic loss: 4.44 | Entropy loss: -0.0006  | Total Loss: 4.39 | Total Steps: 50\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 659/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 1.00 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 660/175267  | Episode Reward: 0.0  | Average Reward 6.96  | Actor loss: -0.06 | Critic loss: 13.93 | Entropy loss: -0.0045  | Total Loss: 13.87 | Total Steps: 80\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 661/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 1.35 | Entropy loss: -0.0000  | Total Loss: 1.36 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 662/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.00 | Critic loss: 1.05 | Entropy loss: -0.0000  | Total Loss: 1.05 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 663/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: 0.18 | Critic loss: 6.42 | Entropy loss: -0.0009  | Total Loss: 6.60 | Total Steps: 37\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 664/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.00 | Critic loss: 0.83 | Entropy loss: -0.0000  | Total Loss: 0.83 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 665/175267  | Episode Reward: 5.0  | Average Reward 6.91  | Actor loss: -0.11 | Critic loss: 7.90 | Entropy loss: -0.0021  | Total Loss: 7.79 | Total Steps: 53\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 666/175267  | Episode Reward: 10.0  | Average Reward 6.93  | Actor loss: 0.01 | Critic loss: 0.89 | Entropy loss: -0.0000  | Total Loss: 0.90 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 667/175267  | Episode Reward: 4.0  | Average Reward 6.90  | Actor loss: -0.46 | Critic loss: 6.79 | Entropy loss: -0.0047  | Total Loss: 6.32 | Total Steps: 60\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 668/175267  | Episode Reward: 10.0  | Average Reward 6.90  | Actor loss: 0.01 | Critic loss: 0.84 | Entropy loss: -0.0000  | Total Loss: 0.85 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 669/175267  | Episode Reward: 10.0  | Average Reward 6.90  | Actor loss: 0.01 | Critic loss: 0.58 | Entropy loss: -0.0000  | Total Loss: 0.59 | Total Steps: 6\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 670/175267  | Episode Reward: 10.0  | Average Reward 6.93  | Actor loss: 0.02 | Critic loss: 3.57 | Entropy loss: -0.0000  | Total Loss: 3.59 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 671/175267  | Episode Reward: 5.0  | Average Reward 7.29  | Actor loss: -0.09 | Critic loss: 5.24 | Entropy loss: -0.0013  | Total Loss: 5.15 | Total Steps: 52\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 672/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.06 | Critic loss: 6.73 | Entropy loss: -0.0003  | Total Loss: 6.79 | Total Steps: 29\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 673/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.01 | Critic loss: 0.61 | Entropy loss: -0.0000  | Total Loss: 0.63 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 674/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: 0.12 | Critic loss: 7.71 | Entropy loss: -0.0005  | Total Loss: 7.83 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 675/175267  | Episode Reward: 5.0  | Average Reward 7.24  | Actor loss: -0.04 | Critic loss: 6.38 | Entropy loss: -0.0004  | Total Loss: 6.34 | Total Steps: 42\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 676/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.06 | Critic loss: 6.98 | Entropy loss: -0.0004  | Total Loss: 7.03 | Total Steps: 37\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 677/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.02 | Critic loss: 2.43 | Entropy loss: -0.0016  | Total Loss: 2.45 | Total Steps: 49\n",
      "---blue cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 678/175267  | Episode Reward: 7.5  | Average Reward 7.24  | Actor loss: 0.13 | Critic loss: 3.52 | Entropy loss: -0.0031  | Total Loss: 3.65 | Total Steps: 31\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 679/175267  | Episode Reward: 7.5  | Average Reward 7.24  | Actor loss: 0.04 | Critic loss: 3.87 | Entropy loss: -0.0005  | Total Loss: 3.91 | Total Steps: 30\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 680/175267  | Episode Reward: 7.5  | Average Reward 7.31  | Actor loss: 0.00 | Critic loss: 0.77 | Entropy loss: -0.0004  | Total Loss: 0.78 | Total Steps: 38\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 681/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: 0.10 | Critic loss: 5.71 | Entropy loss: -0.0015  | Total Loss: 5.81 | Total Steps: 47\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 682/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: -0.14 | Critic loss: 5.51 | Entropy loss: -0.0018  | Total Loss: 5.37 | Total Steps: 47\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 683/175267  | Episode Reward: 10.0  | Average Reward 7.36  | Actor loss: 0.01 | Critic loss: 1.27 | Entropy loss: -0.0000  | Total Loss: 1.28 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 684/175267  | Episode Reward: 5.0  | Average Reward 7.38  | Actor loss: -0.21 | Critic loss: 9.18 | Entropy loss: -0.0037  | Total Loss: 8.96 | Total Steps: 82\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 685/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.01 | Critic loss: 1.63 | Entropy loss: -0.0000  | Total Loss: 1.63 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 686/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.02 | Critic loss: 3.74 | Entropy loss: -0.0000  | Total Loss: 3.76 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 687/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.29 | Critic loss: 4.46 | Entropy loss: -0.0020  | Total Loss: 4.75 | Total Steps: 37\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 688/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.03 | Critic loss: 0.63 | Entropy loss: -0.0001  | Total Loss: 0.66 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 689/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.01 | Critic loss: 1.57 | Entropy loss: -0.0000  | Total Loss: 1.58 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 690/175267  | Episode Reward: 5.0  | Average Reward 7.50  | Actor loss: -0.02 | Critic loss: 7.87 | Entropy loss: -0.0004  | Total Loss: 7.85 | Total Steps: 52\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 691/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.24 | Critic loss: 0.04 | Entropy loss: -0.0015  | Total Loss: 0.27 | Total Steps: 7\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 692/175267  | Episode Reward: 2.5  | Average Reward 7.50  | Actor loss: -0.24 | Critic loss: 10.40 | Entropy loss: -0.0029  | Total Loss: 10.15 | Total Steps: 44\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 693/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.00 | Critic loss: 0.63 | Entropy loss: -0.0000  | Total Loss: 0.64 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 694/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.01 | Critic loss: 0.83 | Entropy loss: -0.0000  | Total Loss: 0.84 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 695/175267  | Episode Reward: 7.5  | Average Reward 7.50  | Actor loss: 0.03 | Critic loss: 5.08 | Entropy loss: -0.0005  | Total Loss: 5.11 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 696/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.01 | Critic loss: 1.65 | Entropy loss: -0.0000  | Total Loss: 1.66 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 697/175267  | Episode Reward: 5.0  | Average Reward 7.53  | Actor loss: 0.19 | Critic loss: 3.63 | Entropy loss: -0.0037  | Total Loss: 3.81 | Total Steps: 42\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 698/175267  | Episode Reward: 7.5  | Average Reward 7.53  | Actor loss: -0.03 | Critic loss: 4.62 | Entropy loss: -0.0006  | Total Loss: 4.59 | Total Steps: 43\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 699/175267  | Episode Reward: 7.5  | Average Reward 7.57  | Actor loss: 0.01 | Critic loss: 6.64 | Entropy loss: -0.0001  | Total Loss: 6.65 | Total Steps: 34\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 700/175267  | Episode Reward: 7.5  | Average Reward 7.58  | Actor loss: -0.08 | Critic loss: 5.68 | Entropy loss: -0.0015  | Total Loss: 5.60 | Total Steps: 43\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 701/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.01 | Critic loss: 0.92 | Entropy loss: -0.0000  | Total Loss: 0.92 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 702/175267  | Episode Reward: 2.5  | Average Reward 7.50  | Actor loss: -0.17 | Critic loss: 11.05 | Entropy loss: -0.0009  | Total Loss: 10.88 | Total Steps: 53\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 703/175267  | Episode Reward: 10.0  | Average Reward 7.55  | Actor loss: 0.03 | Critic loss: 4.72 | Entropy loss: -0.0002  | Total Loss: 4.74 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 704/175267  | Episode Reward: 7.5  | Average Reward 7.53  | Actor loss: -0.07 | Critic loss: 3.22 | Entropy loss: -0.0060  | Total Loss: 3.15 | Total Steps: 160\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 705/175267  | Episode Reward: 2.5  | Average Reward 7.45  | Actor loss: -0.10 | Critic loss: 5.28 | Entropy loss: -0.0025  | Total Loss: 5.18 | Total Steps: 58\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 706/175267  | Episode Reward: 5.0  | Average Reward 7.47  | Actor loss: -0.06 | Critic loss: 8.29 | Entropy loss: -0.0005  | Total Loss: 8.23 | Total Steps: 53\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 707/175267  | Episode Reward: -1.0  | Average Reward 7.39  | Actor loss: -0.27 | Critic loss: 10.53 | Entropy loss: -0.0025  | Total Loss: 10.25 | Total Steps: 91\n",
      "---blue capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 708/175267  | Episode Reward: 10.0  | Average Reward 7.39  | Actor loss: 0.04 | Critic loss: 2.31 | Entropy loss: -0.0023  | Total Loss: 2.34 | Total Steps: 90\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 709/175267  | Episode Reward: 10.0  | Average Reward 7.39  | Actor loss: -0.17 | Critic loss: 6.90 | Entropy loss: -0.0050  | Total Loss: 6.73 | Total Steps: 168\n",
      "---green cylinder---\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 710/175267  | Episode Reward: 7.5  | Average Reward 7.42  | Actor loss: 0.11 | Critic loss: 3.46 | Entropy loss: -0.0017  | Total Loss: 3.58 | Total Steps: 308\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 711/175267  | Episode Reward: 5.0  | Average Reward 7.45  | Actor loss: -0.28 | Critic loss: 8.33 | Entropy loss: -0.0030  | Total Loss: 8.05 | Total Steps: 81\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 712/175267  | Episode Reward: 10.0  | Average Reward 7.45  | Actor loss: 0.19 | Critic loss: 4.14 | Entropy loss: -0.0009  | Total Loss: 4.33 | Total Steps: 29\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 713/175267  | Episode Reward: 7.5  | Average Reward 7.47  | Actor loss: 0.11 | Critic loss: 3.41 | Entropy loss: -0.0006  | Total Loss: 3.52 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 714/175267  | Episode Reward: 7.5  | Average Reward 7.45  | Actor loss: -0.03 | Critic loss: 5.13 | Entropy loss: -0.0017  | Total Loss: 5.10 | Total Steps: 44\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 715/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.06 | Critic loss: 0.76 | Entropy loss: -0.0002  | Total Loss: 0.83 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 716/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.02 | Critic loss: 1.56 | Entropy loss: -0.0000  | Total Loss: 1.58 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 717/175267  | Episode Reward: 2.5  | Average Reward 7.54  | Actor loss: -0.10 | Critic loss: 9.87 | Entropy loss: -0.0027  | Total Loss: 9.77 | Total Steps: 54\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 718/175267  | Episode Reward: 7.5  | Average Reward 7.51  | Actor loss: 0.10 | Critic loss: 6.09 | Entropy loss: -0.0030  | Total Loss: 6.18 | Total Steps: 40\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 719/175267  | Episode Reward: 7.5  | Average Reward 7.49  | Actor loss: 0.68 | Critic loss: 4.88 | Entropy loss: -0.0029  | Total Loss: 5.55 | Total Steps: 37\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 720/175267  | Episode Reward: 10.0  | Average Reward 7.51  | Actor loss: 0.13 | Critic loss: 4.18 | Entropy loss: -0.0011  | Total Loss: 4.31 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 721/175267  | Episode Reward: 10.0  | Average Reward 7.54  | Actor loss: 0.01 | Critic loss: 2.60 | Entropy loss: -0.0000  | Total Loss: 2.61 | Total Steps: 6\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 722/175267  | Episode Reward: 10.0  | Average Reward 7.63  | Actor loss: 0.02 | Critic loss: 3.96 | Entropy loss: -0.0000  | Total Loss: 3.98 | Total Steps: 6\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 723/175267  | Episode Reward: 10.0  | Average Reward 7.70  | Actor loss: 0.01 | Critic loss: 2.36 | Entropy loss: -0.0000  | Total Loss: 2.37 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 724/175267  | Episode Reward: 7.5  | Average Reward 7.67  | Actor loss: 0.05 | Critic loss: 6.77 | Entropy loss: -0.0002  | Total Loss: 6.82 | Total Steps: 29\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 725/175267  | Episode Reward: 5.0  | Average Reward 7.62  | Actor loss: -0.31 | Critic loss: 8.63 | Entropy loss: -0.0025  | Total Loss: 8.31 | Total Steps: 74\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 726/175267  | Episode Reward: 5.0  | Average Reward 7.57  | Actor loss: -0.00 | Critic loss: 5.65 | Entropy loss: -0.0039  | Total Loss: 5.64 | Total Steps: 137\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 727/175267  | Episode Reward: 7.5  | Average Reward 7.61  | Actor loss: 0.08 | Critic loss: 6.34 | Entropy loss: -0.0025  | Total Loss: 6.41 | Total Steps: 53\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 728/175267  | Episode Reward: 7.5  | Average Reward 7.58  | Actor loss: -0.02 | Critic loss: 4.95 | Entropy loss: -0.0047  | Total Loss: 4.92 | Total Steps: 114\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 729/175267  | Episode Reward: 5.0  | Average Reward 7.55  | Actor loss: 0.04 | Critic loss: 7.40 | Entropy loss: -0.0014  | Total Loss: 7.44 | Total Steps: 77\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 730/175267  | Episode Reward: 5.0  | Average Reward 7.50  | Actor loss: -0.22 | Critic loss: 6.83 | Entropy loss: -0.0010  | Total Loss: 6.61 | Total Steps: 52\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 731/175267  | Episode Reward: 10.0  | Average Reward 7.55  | Actor loss: 0.10 | Critic loss: 2.87 | Entropy loss: -0.0001  | Total Loss: 2.96 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 732/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.01 | Critic loss: 1.85 | Entropy loss: -0.0000  | Total Loss: 1.86 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 733/175267  | Episode Reward: 10.0  | Average Reward 7.61  | Actor loss: 0.04 | Critic loss: 1.78 | Entropy loss: -0.0008  | Total Loss: 1.82 | Total Steps: 43\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 734/175267  | Episode Reward: 7.5  | Average Reward 7.61  | Actor loss: 0.09 | Critic loss: 5.86 | Entropy loss: -0.0009  | Total Loss: 5.95 | Total Steps: 39\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 735/175267  | Episode Reward: 7.5  | Average Reward 7.58  | Actor loss: 0.25 | Critic loss: 4.40 | Entropy loss: -0.0018  | Total Loss: 4.64 | Total Steps: 31\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 736/175267  | Episode Reward: 10.0  | Average Reward 7.58  | Actor loss: 0.01 | Critic loss: 0.54 | Entropy loss: -0.0000  | Total Loss: 0.56 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 737/175267  | Episode Reward: 10.0  | Average Reward 7.63  | Actor loss: 0.01 | Critic loss: 0.24 | Entropy loss: -0.0000  | Total Loss: 0.24 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 738/175267  | Episode Reward: 1.5  | Average Reward 7.57  | Actor loss: -0.38 | Critic loss: 6.46 | Entropy loss: -0.0037  | Total Loss: 6.07 | Total Steps: 50\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 739/175267  | Episode Reward: 10.0  | Average Reward 7.59  | Actor loss: 0.05 | Critic loss: 2.71 | Entropy loss: -0.0020  | Total Loss: 2.76 | Total Steps: 47\n",
      "---black cylinder---\n",
      "Step: 250\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 740/175267  | Episode Reward: 6.5  | Average Reward 7.63  | Actor loss: -0.14 | Critic loss: 2.76 | Entropy loss: -0.0064  | Total Loss: 2.61 | Total Steps: 488\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 741/175267  | Episode Reward: 6.5  | Average Reward 7.62  | Actor loss: -0.24 | Critic loss: 5.17 | Entropy loss: -0.0038  | Total Loss: 4.93 | Total Steps: 59\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 742/175267  | Episode Reward: -73.5  | Average Reward 6.82  | Actor loss: -0.92 | Critic loss: 25.11 | Entropy loss: -0.0111  | Total Loss: 24.18 | Total Steps: 500\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 743/175267  | Episode Reward: 10.0  | Average Reward 6.82  | Actor loss: 0.02 | Critic loss: 0.40 | Entropy loss: -0.0001  | Total Loss: 0.42 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 744/175267  | Episode Reward: 10.0  | Average Reward 6.82  | Actor loss: 0.01 | Critic loss: 0.93 | Entropy loss: -0.0000  | Total Loss: 0.94 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 745/175267  | Episode Reward: 10.0  | Average Reward 6.87  | Actor loss: 0.20 | Critic loss: 4.49 | Entropy loss: -0.0013  | Total Loss: 4.69 | Total Steps: 37\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 746/175267  | Episode Reward: 7.5  | Average Reward 6.89  | Actor loss: 0.14 | Critic loss: 5.60 | Entropy loss: -0.0028  | Total Loss: 5.74 | Total Steps: 36\n",
      "---black cube---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 747/175267  | Episode Reward: 6.5  | Average Reward 6.86  | Actor loss: -0.21 | Critic loss: 2.67 | Entropy loss: -0.0139  | Total Loss: 2.44 | Total Steps: 194\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 748/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.00 | Critic loss: 1.12 | Entropy loss: -0.0001  | Total Loss: 1.12 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 749/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.03 | Critic loss: 2.59 | Entropy loss: -0.0000  | Total Loss: 2.62 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 750/175267  | Episode Reward: 7.5  | Average Reward 6.88  | Actor loss: 0.31 | Critic loss: 5.04 | Entropy loss: -0.0016  | Total Loss: 5.35 | Total Steps: 31\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 751/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.47 | Critic loss: 5.69 | Entropy loss: -0.0022  | Total Loss: 6.16 | Total Steps: 30\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 752/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.01 | Critic loss: 1.18 | Entropy loss: -0.0000  | Total Loss: 1.19 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 753/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.01 | Critic loss: 2.38 | Entropy loss: -0.0012  | Total Loss: 2.37 | Total Steps: 42\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 754/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: -0.05 | Critic loss: 5.69 | Entropy loss: -0.0046  | Total Loss: 5.63 | Total Steps: 113\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 755/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: -0.10 | Critic loss: 5.05 | Entropy loss: -0.0008  | Total Loss: 4.95 | Total Steps: 47\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 756/175267  | Episode Reward: 10.0  | Average Reward 7.03  | Actor loss: 0.17 | Critic loss: 2.32 | Entropy loss: -0.0034  | Total Loss: 2.49 | Total Steps: 60\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 757/175267  | Episode Reward: 10.0  | Average Reward 7.05  | Actor loss: 0.02 | Critic loss: 1.45 | Entropy loss: -0.0000  | Total Loss: 1.46 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 758/175267  | Episode Reward: 0.0  | Average Reward 6.98  | Actor loss: -0.15 | Critic loss: 10.77 | Entropy loss: -0.0013  | Total Loss: 10.62 | Total Steps: 69\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 759/175267  | Episode Reward: 10.0  | Average Reward 6.98  | Actor loss: 0.25 | Critic loss: 3.98 | Entropy loss: -0.0028  | Total Loss: 4.22 | Total Steps: 33\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 760/175267  | Episode Reward: 7.5  | Average Reward 7.05  | Actor loss: 0.21 | Critic loss: 5.55 | Entropy loss: -0.0023  | Total Loss: 5.76 | Total Steps: 42\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 761/175267  | Episode Reward: 10.0  | Average Reward 7.05  | Actor loss: 0.01 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.14 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 762/175267  | Episode Reward: 7.5  | Average Reward 7.03  | Actor loss: 0.06 | Critic loss: 5.29 | Entropy loss: -0.0007  | Total Loss: 5.35 | Total Steps: 29\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 763/175267  | Episode Reward: 5.0  | Average Reward 7.00  | Actor loss: 0.02 | Critic loss: 3.78 | Entropy loss: -0.0032  | Total Loss: 3.81 | Total Steps: 54\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 764/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: 0.37 | Critic loss: 5.77 | Entropy loss: -0.0015  | Total Loss: 6.13 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 765/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.33 | Critic loss: 5.38 | Entropy loss: -0.0021  | Total Loss: 5.05 | Total Steps: 42\n",
      "---blue sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 766/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: 0.20 | Critic loss: 6.01 | Entropy loss: -0.0016  | Total Loss: 6.21 | Total Steps: 44\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 767/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 1.19 | Entropy loss: -0.0000  | Total Loss: 1.20 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 768/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0000  | Total Loss: 0.97 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 769/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: -0.01 | Critic loss: 7.85 | Entropy loss: -0.0035  | Total Loss: 7.83 | Total Steps: 54\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 770/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: -0.06 | Critic loss: 4.76 | Entropy loss: -0.0030  | Total Loss: 4.69 | Total Steps: 66\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 771/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 1.08 | Entropy loss: -0.0000  | Total Loss: 1.09 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 772/175267  | Episode Reward: 0.0  | Average Reward 6.96  | Actor loss: -0.37 | Critic loss: 7.08 | Entropy loss: -0.0031  | Total Loss: 6.71 | Total Steps: 81\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 773/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 1.56 | Entropy loss: -0.0000  | Total Loss: 1.58 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 774/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.03 | Critic loss: 4.12 | Entropy loss: -0.0000  | Total Loss: 4.14 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 775/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: 0.41 | Critic loss: 2.86 | Entropy loss: -0.0027  | Total Loss: 3.27 | Total Steps: 43\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 776/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: -0.12 | Critic loss: 5.79 | Entropy loss: -0.0007  | Total Loss: 5.66 | Total Steps: 42\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 777/175267  | Episode Reward: 7.5  | Average Reward 6.99  | Actor loss: -0.03 | Critic loss: 2.33 | Entropy loss: -0.0016  | Total Loss: 2.30 | Total Steps: 42\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 778/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: 0.00 | Critic loss: 0.77 | Entropy loss: -0.0000  | Total Loss: 0.77 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 779/175267  | Episode Reward: 2.5  | Average Reward 6.96  | Actor loss: -0.19 | Critic loss: 7.27 | Entropy loss: -0.0031  | Total Loss: 7.08 | Total Steps: 56\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 780/175267  | Episode Reward: 5.0  | Average Reward 6.94  | Actor loss: -0.01 | Critic loss: 7.46 | Entropy loss: -0.0006  | Total Loss: 7.45 | Total Steps: 44\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 781/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 0.51 | Entropy loss: -0.0000  | Total Loss: 0.51 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 782/175267  | Episode Reward: 5.0  | Average Reward 6.94  | Actor loss: -0.10 | Critic loss: 7.55 | Entropy loss: -0.0008  | Total Loss: 7.44 | Total Steps: 47\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 783/175267  | Episode Reward: 10.0  | Average Reward 6.94  | Actor loss: 0.00 | Critic loss: 0.12 | Entropy loss: -0.0001  | Total Loss: 0.12 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 784/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.00 | Critic loss: 0.69 | Entropy loss: -0.0000  | Total Loss: 0.69 | Total Steps: 6\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 785/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.06 | Critic loss: 1.26 | Entropy loss: -0.0001  | Total Loss: 1.32 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 786/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 1.15 | Entropy loss: -0.0000  | Total Loss: 1.16 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 787/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.00 | Critic loss: 0.56 | Entropy loss: -0.0000  | Total Loss: 0.56 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 788/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: -0.01 | Critic loss: 2.83 | Entropy loss: -0.0020  | Total Loss: 2.81 | Total Steps: 151\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 789/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: 0.06 | Critic loss: 8.32 | Entropy loss: -0.0003  | Total Loss: 8.38 | Total Steps: 30\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 790/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: -0.33 | Critic loss: 6.80 | Entropy loss: -0.0024  | Total Loss: 6.46 | Total Steps: 42\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 791/175267  | Episode Reward: 7.5  | Average Reward 6.94  | Actor loss: 0.04 | Critic loss: 7.62 | Entropy loss: -0.0002  | Total Loss: 7.66 | Total Steps: 29\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 792/175267  | Episode Reward: 7.5  | Average Reward 6.99  | Actor loss: 0.20 | Critic loss: 7.72 | Entropy loss: -0.0008  | Total Loss: 7.91 | Total Steps: 30\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 793/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 1.17 | Entropy loss: -0.0000  | Total Loss: 1.18 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 794/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: 0.10 | Critic loss: 3.65 | Entropy loss: -0.0008  | Total Loss: 3.74 | Total Steps: 29\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 795/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.00 | Critic loss: 0.67 | Entropy loss: -0.0000  | Total Loss: 0.67 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 796/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: 0.16 | Critic loss: 8.43 | Entropy loss: -0.0006  | Total Loss: 8.59 | Total Steps: 29\n",
      "---black capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 797/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: 0.02 | Critic loss: 3.06 | Entropy loss: -0.0001  | Total Loss: 3.08 | Total Steps: 31\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 798/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: -0.21 | Critic loss: 6.03 | Entropy loss: -0.0051  | Total Loss: 5.81 | Total Steps: 190\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 799/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: -0.09 | Critic loss: 6.49 | Entropy loss: -0.0026  | Total Loss: 6.40 | Total Steps: 112\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 800/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 0.24 | Entropy loss: -0.0002  | Total Loss: 0.24 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 801/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.06 | Critic loss: 3.74 | Entropy loss: -0.0030  | Total Loss: 3.81 | Total Steps: 53\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 802/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.01 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.03 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 803/175267  | Episode Reward: -45.0  | Average Reward 6.57  | Actor loss: -0.65 | Critic loss: 20.64 | Entropy loss: -0.0091  | Total Loss: 19.98 | Total Steps: 500\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 804/175267  | Episode Reward: 10.0  | Average Reward 6.59  | Actor loss: -0.03 | Critic loss: 6.75 | Entropy loss: -0.0004  | Total Loss: 6.72 | Total Steps: 29\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 805/175267  | Episode Reward: 2.5  | Average Reward 6.59  | Actor loss: -0.00 | Critic loss: 6.75 | Entropy loss: -0.0017  | Total Loss: 6.75 | Total Steps: 54\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 806/175267  | Episode Reward: 10.0  | Average Reward 6.64  | Actor loss: 0.01 | Critic loss: 1.29 | Entropy loss: -0.0000  | Total Loss: 1.30 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 807/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.01 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 808/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.01 | Critic loss: 1.09 | Entropy loss: -0.0000  | Total Loss: 1.09 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 809/175267  | Episode Reward: 7.5  | Average Reward 6.72  | Actor loss: 0.06 | Critic loss: 3.15 | Entropy loss: -0.0021  | Total Loss: 3.20 | Total Steps: 53\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 810/175267  | Episode Reward: 7.5  | Average Reward 6.72  | Actor loss: 0.04 | Critic loss: 4.53 | Entropy loss: -0.0015  | Total Loss: 4.56 | Total Steps: 30\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 811/175267  | Episode Reward: 10.0  | Average Reward 6.78  | Actor loss: 0.01 | Critic loss: 4.15 | Entropy loss: -0.0001  | Total Loss: 4.16 | Total Steps: 34\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 812/175267  | Episode Reward: 10.0  | Average Reward 6.78  | Actor loss: 0.04 | Critic loss: 1.33 | Entropy loss: -0.0001  | Total Loss: 1.37 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 813/175267  | Episode Reward: 7.5  | Average Reward 6.78  | Actor loss: 0.09 | Critic loss: 3.54 | Entropy loss: -0.0006  | Total Loss: 3.63 | Total Steps: 30\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 814/175267  | Episode Reward: 10.0  | Average Reward 6.80  | Actor loss: 0.36 | Critic loss: 3.70 | Entropy loss: -0.0015  | Total Loss: 4.05 | Total Steps: 36\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 815/175267  | Episode Reward: 2.5  | Average Reward 6.72  | Actor loss: -0.03 | Critic loss: 8.32 | Entropy loss: -0.0011  | Total Loss: 8.29 | Total Steps: 50\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 816/175267  | Episode Reward: 10.0  | Average Reward 6.72  | Actor loss: 0.08 | Critic loss: 5.97 | Entropy loss: -0.0005  | Total Loss: 6.05 | Total Steps: 32\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 817/175267  | Episode Reward: -17.5  | Average Reward 6.53  | Actor loss: -0.67 | Critic loss: 19.19 | Entropy loss: -0.0071  | Total Loss: 18.52 | Total Steps: 171\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 818/175267  | Episode Reward: 5.0  | Average Reward 6.50  | Actor loss: -0.31 | Critic loss: 9.04 | Entropy loss: -0.0040  | Total Loss: 8.72 | Total Steps: 66\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 819/175267  | Episode Reward: 7.5  | Average Reward 6.50  | Actor loss: 0.43 | Critic loss: 6.21 | Entropy loss: -0.0031  | Total Loss: 6.63 | Total Steps: 43\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 820/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.21 | Critic loss: 4.50 | Entropy loss: -0.0007  | Total Loss: 4.70 | Total Steps: 29\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 821/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 0.68 | Entropy loss: -0.0000  | Total Loss: 0.69 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 822/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.02 | Critic loss: 2.00 | Entropy loss: -0.0000  | Total Loss: 2.02 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 823/175267  | Episode Reward: 7.5  | Average Reward 6.47  | Actor loss: -0.05 | Critic loss: 5.38 | Entropy loss: -0.0005  | Total Loss: 5.33 | Total Steps: 49\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 824/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 0.75 | Entropy loss: -0.0000  | Total Loss: 0.75 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  | Episode: 825/175267  | Episode Reward: 5.0  | Average Reward 6.50  | Actor loss: -0.02 | Critic loss: 2.88 | Entropy loss: -0.0024  | Total Loss: 2.86 | Total Steps: 42\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 826/175267  | Episode Reward: 10.0  | Average Reward 6.55  | Actor loss: 0.02 | Critic loss: 2.70 | Entropy loss: -0.0012  | Total Loss: 2.71 | Total Steps: 35\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 827/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.42 | Critic loss: 4.23 | Entropy loss: -0.0019  | Total Loss: 4.65 | Total Steps: 30\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 828/175267  | Episode Reward: 10.0  | Average Reward 6.60  | Actor loss: 0.01 | Critic loss: 0.39 | Entropy loss: -0.0001  | Total Loss: 0.40 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 829/175267  | Episode Reward: 10.0  | Average Reward 6.65  | Actor loss: 0.02 | Critic loss: 0.74 | Entropy loss: -0.0000  | Total Loss: 0.76 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 830/175267  | Episode Reward: 7.5  | Average Reward 6.67  | Actor loss: 0.23 | Critic loss: 7.93 | Entropy loss: -0.0010  | Total Loss: 8.16 | Total Steps: 29\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 831/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.03 | Critic loss: 1.69 | Entropy loss: -0.0000  | Total Loss: 1.71 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 832/175267  | Episode Reward: 7.5  | Average Reward 6.65  | Actor loss: 0.07 | Critic loss: 6.98 | Entropy loss: -0.0006  | Total Loss: 7.05 | Total Steps: 34\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 833/175267  | Episode Reward: 5.0  | Average Reward 6.60  | Actor loss: -0.10 | Critic loss: 4.11 | Entropy loss: -0.0038  | Total Loss: 4.01 | Total Steps: 54\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 834/175267  | Episode Reward: 5.0  | Average Reward 6.58  | Actor loss: -0.22 | Critic loss: 5.86 | Entropy loss: -0.0014  | Total Loss: 5.64 | Total Steps: 77\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 835/175267  | Episode Reward: 10.0  | Average Reward 6.60  | Actor loss: 0.05 | Critic loss: 1.07 | Entropy loss: -0.0001  | Total Loss: 1.11 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 836/175267  | Episode Reward: 10.0  | Average Reward 6.60  | Actor loss: 0.12 | Critic loss: 3.28 | Entropy loss: -0.0008  | Total Loss: 3.40 | Total Steps: 29\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 837/175267  | Episode Reward: 10.0  | Average Reward 6.60  | Actor loss: 0.35 | Critic loss: 4.26 | Entropy loss: -0.0028  | Total Loss: 4.62 | Total Steps: 32\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 838/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: -0.20 | Critic loss: 4.73 | Entropy loss: -0.0024  | Total Loss: 4.53 | Total Steps: 83\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 839/175267  | Episode Reward: 7.5  | Average Reward 6.63  | Actor loss: -0.85 | Critic loss: 7.50 | Entropy loss: -0.0015  | Total Loss: 6.65 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 840/175267  | Episode Reward: 10.0  | Average Reward 6.67  | Actor loss: 0.00 | Critic loss: 0.55 | Entropy loss: -0.0000  | Total Loss: 0.55 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 841/175267  | Episode Reward: 5.0  | Average Reward 6.66  | Actor loss: 0.10 | Critic loss: 3.31 | Entropy loss: -0.0013  | Total Loss: 3.41 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 842/175267  | Episode Reward: 7.5  | Average Reward 7.46  | Actor loss: -0.01 | Critic loss: 5.68 | Entropy loss: -0.0004  | Total Loss: 5.67 | Total Steps: 43\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 843/175267  | Episode Reward: 4.0  | Average Reward 7.41  | Actor loss: -0.39 | Critic loss: 4.83 | Entropy loss: -0.0037  | Total Loss: 4.43 | Total Steps: 53\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 844/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: 0.01 | Critic loss: 3.36 | Entropy loss: -0.0003  | Total Loss: 3.37 | Total Steps: 29\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 845/175267  | Episode Reward: 7.5  | Average Reward 7.38  | Actor loss: -0.28 | Critic loss: 3.60 | Entropy loss: -0.0047  | Total Loss: 3.32 | Total Steps: 57\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 846/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: 0.00 | Critic loss: 0.26 | Entropy loss: -0.0000  | Total Loss: 0.26 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 847/175267  | Episode Reward: 2.5  | Average Reward 7.37  | Actor loss: -0.05 | Critic loss: 7.47 | Entropy loss: -0.0013  | Total Loss: 7.42 | Total Steps: 55\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 848/175267  | Episode Reward: 10.0  | Average Reward 7.37  | Actor loss: 0.00 | Critic loss: 0.16 | Entropy loss: -0.0002  | Total Loss: 0.17 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 849/175267  | Episode Reward: 7.5  | Average Reward 7.34  | Actor loss: -0.51 | Critic loss: 5.81 | Entropy loss: -0.0060  | Total Loss: 5.29 | Total Steps: 104\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 850/175267  | Episode Reward: 7.5  | Average Reward 7.34  | Actor loss: 0.06 | Critic loss: 7.68 | Entropy loss: -0.0006  | Total Loss: 7.74 | Total Steps: 32\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 851/175267  | Episode Reward: 10.0  | Average Reward 7.34  | Actor loss: 0.01 | Critic loss: 1.11 | Entropy loss: -0.0000  | Total Loss: 1.12 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 852/175267  | Episode Reward: 7.5  | Average Reward 7.32  | Actor loss: 0.02 | Critic loss: 3.21 | Entropy loss: -0.0020  | Total Loss: 3.23 | Total Steps: 44\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 853/175267  | Episode Reward: 2.5  | Average Reward 7.26  | Actor loss: -0.37 | Critic loss: 9.65 | Entropy loss: -0.0029  | Total Loss: 9.28 | Total Steps: 97\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 854/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.15 | Critic loss: 4.81 | Entropy loss: -0.0007  | Total Loss: 4.96 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 855/175267  | Episode Reward: 0.0  | Average Reward 7.19  | Actor loss: -0.19 | Critic loss: 12.67 | Entropy loss: -0.0048  | Total Loss: 12.47 | Total Steps: 182\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 856/175267  | Episode Reward: 10.0  | Average Reward 7.19  | Actor loss: 0.01 | Critic loss: 2.21 | Entropy loss: -0.0000  | Total Loss: 2.21 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 857/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: 0.06 | Critic loss: 3.87 | Entropy loss: -0.0006  | Total Loss: 3.92 | Total Steps: 32\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 858/175267  | Episode Reward: 2.5  | Average Reward 7.19  | Actor loss: -0.16 | Critic loss: 7.65 | Entropy loss: -0.0016  | Total Loss: 7.49 | Total Steps: 43\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 859/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: -0.41 | Critic loss: 7.83 | Entropy loss: -0.0012  | Total Loss: 7.42 | Total Steps: 36\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 860/175267  | Episode Reward: 10.0  | Average Reward 7.19  | Actor loss: 0.13 | Critic loss: 4.99 | Entropy loss: -0.0009  | Total Loss: 5.11 | Total Steps: 32\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 861/175267  | Episode Reward: 10.0  | Average Reward 7.19  | Actor loss: 0.02 | Critic loss: 4.74 | Entropy loss: -0.0000  | Total Loss: 4.76 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 862/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.02 | Critic loss: 4.02 | Entropy loss: -0.0001  | Total Loss: 4.05 | Total Steps: 29\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 863/175267  | Episode Reward: 7.5  | Average Reward 7.24  | Actor loss: -0.01 | Critic loss: 3.42 | Entropy loss: -0.0007  | Total Loss: 3.41 | Total Steps: 30\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 864/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.21 | Critic loss: 1.72 | Entropy loss: -0.0018  | Total Loss: 1.92 | Total Steps: 32\n",
      "---black cube---\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 865/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: -0.04 | Critic loss: 3.94 | Entropy loss: -0.0013  | Total Loss: 3.90 | Total Steps: 304\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 866/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.04 | Critic loss: 3.72 | Entropy loss: -0.0000  | Total Loss: 3.76 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 867/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.02 | Critic loss: 2.93 | Entropy loss: -0.0000  | Total Loss: 2.95 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 868/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.01 | Critic loss: 0.73 | Entropy loss: -0.0000  | Total Loss: 0.74 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 869/175267  | Episode Reward: 5.0  | Average Reward 7.29  | Actor loss: -0.08 | Critic loss: 8.18 | Entropy loss: -0.0006  | Total Loss: 8.10 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 870/175267  | Episode Reward: 2.5  | Average Reward 7.21  | Actor loss: -0.24 | Critic loss: 11.24 | Entropy loss: -0.0019  | Total Loss: 11.00 | Total Steps: 54\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 871/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.43 | Critic loss: 3.61 | Entropy loss: -0.0020  | Total Loss: 4.04 | Total Steps: 31\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 872/175267  | Episode Reward: 10.0  | Average Reward 7.32  | Actor loss: 0.07 | Critic loss: 2.40 | Entropy loss: -0.0005  | Total Loss: 2.47 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 873/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: 0.04 | Critic loss: 6.71 | Entropy loss: -0.0004  | Total Loss: 6.74 | Total Steps: 36\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 874/175267  | Episode Reward: 5.0  | Average Reward 7.24  | Actor loss: -0.04 | Critic loss: 4.15 | Entropy loss: -0.0027  | Total Loss: 4.11 | Total Steps: 47\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 875/175267  | Episode Reward: 5.0  | Average Reward 7.21  | Actor loss: 0.14 | Critic loss: 8.33 | Entropy loss: -0.0021  | Total Loss: 8.47 | Total Steps: 44\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 876/175267  | Episode Reward: 10.0  | Average Reward 7.24  | Actor loss: 0.01 | Critic loss: 1.07 | Entropy loss: -0.0000  | Total Loss: 1.08 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 877/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.01 | Critic loss: 0.93 | Entropy loss: -0.0000  | Total Loss: 0.93 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 878/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.05 | Critic loss: 3.69 | Entropy loss: -0.0002  | Total Loss: 3.74 | Total Steps: 29\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 879/175267  | Episode Reward: 7.5  | Average Reward 7.32  | Actor loss: 0.10 | Critic loss: 2.83 | Entropy loss: -0.0016  | Total Loss: 2.93 | Total Steps: 44\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 880/175267  | Episode Reward: 5.0  | Average Reward 7.32  | Actor loss: -0.05 | Critic loss: 3.94 | Entropy loss: -0.0010  | Total Loss: 3.89 | Total Steps: 49\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 881/175267  | Episode Reward: -1.0  | Average Reward 7.21  | Actor loss: -0.84 | Critic loss: 13.46 | Entropy loss: -0.0039  | Total Loss: 12.61 | Total Steps: 54\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 882/175267  | Episode Reward: -6.0  | Average Reward 7.09  | Actor loss: -0.78 | Critic loss: 17.23 | Entropy loss: -0.0059  | Total Loss: 16.44 | Total Steps: 127\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 883/175267  | Episode Reward: 10.0  | Average Reward 7.09  | Actor loss: 0.01 | Critic loss: 1.28 | Entropy loss: -0.0000  | Total Loss: 1.28 | Total Steps: 6\n",
      "---black prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 884/175267  | Episode Reward: 2.5  | Average Reward 7.02  | Actor loss: -0.14 | Critic loss: 7.68 | Entropy loss: -0.0010  | Total Loss: 7.54 | Total Steps: 52\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 885/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.21 | Critic loss: 3.62 | Entropy loss: -0.0022  | Total Loss: 3.40 | Total Steps: 64\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 886/175267  | Episode Reward: -2.5  | Average Reward 6.87  | Actor loss: -0.07 | Critic loss: 11.43 | Entropy loss: -0.0024  | Total Loss: 11.37 | Total Steps: 101\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 887/175267  | Episode Reward: 10.0  | Average Reward 6.87  | Actor loss: 0.02 | Critic loss: 0.71 | Entropy loss: -0.0001  | Total Loss: 0.73 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 888/175267  | Episode Reward: 10.0  | Average Reward 6.87  | Actor loss: 0.01 | Critic loss: 2.80 | Entropy loss: -0.0000  | Total Loss: 2.81 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 889/175267  | Episode Reward: 10.0  | Average Reward 6.89  | Actor loss: 0.01 | Critic loss: 1.12 | Entropy loss: -0.0000  | Total Loss: 1.13 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 890/175267  | Episode Reward: 5.0  | Average Reward 6.89  | Actor loss: -0.17 | Critic loss: 3.63 | Entropy loss: -0.0132  | Total Loss: 3.44 | Total Steps: 161\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 891/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.01 | Critic loss: 1.12 | Entropy loss: -0.0001  | Total Loss: 1.13 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 892/175267  | Episode Reward: 0.0  | Average Reward 6.84  | Actor loss: -0.36 | Critic loss: 5.69 | Entropy loss: -0.0084  | Total Loss: 5.32 | Total Steps: 234\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 893/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.03 | Critic loss: 0.74 | Entropy loss: -0.0001  | Total Loss: 0.76 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 894/175267  | Episode Reward: 1.5  | Average Reward 6.79  | Actor loss: -0.78 | Critic loss: 6.81 | Entropy loss: -0.0051  | Total Loss: 6.02 | Total Steps: 56\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 895/175267  | Episode Reward: 7.5  | Average Reward 6.76  | Actor loss: 0.02 | Critic loss: 4.07 | Entropy loss: -0.0006  | Total Loss: 4.09 | Total Steps: 30\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 896/175267  | Episode Reward: 7.5  | Average Reward 6.76  | Actor loss: -0.12 | Critic loss: 3.95 | Entropy loss: -0.0033  | Total Loss: 3.83 | Total Steps: 82\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 897/175267  | Episode Reward: 2.5  | Average Reward 6.68  | Actor loss: -0.69 | Critic loss: 9.25 | Entropy loss: -0.0036  | Total Loss: 8.56 | Total Steps: 53\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 898/175267  | Episode Reward: 7.5  | Average Reward 6.71  | Actor loss: -0.09 | Critic loss: 1.96 | Entropy loss: -0.0025  | Total Loss: 1.87 | Total Steps: 45\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 899/175267  | Episode Reward: 7.5  | Average Reward 6.68  | Actor loss: 0.07 | Critic loss: 7.96 | Entropy loss: -0.0005  | Total Loss: 8.02 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 900/175267  | Episode Reward: 5.0  | Average Reward 6.63  | Actor loss: -0.13 | Critic loss: 4.34 | Entropy loss: -0.0043  | Total Loss: 4.20 | Total Steps: 55\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 901/175267  | Episode Reward: 10.0  | Average Reward 6.63  | Actor loss: 0.25 | Critic loss: 4.90 | Entropy loss: -0.0009  | Total Loss: 5.16 | Total Steps: 29\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 902/175267  | Episode Reward: 10.0  | Average Reward 6.63  | Actor loss: -0.09 | Critic loss: 2.86 | Entropy loss: -0.0028  | Total Loss: 2.77 | Total Steps: 42\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 903/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.12 | Critic loss: 0.72 | Entropy loss: -0.0015  | Total Loss: 0.83 | Total Steps: 7\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 904/175267  | Episode Reward: 7.5  | Average Reward 7.16  | Actor loss: 0.22 | Critic loss: 4.63 | Entropy loss: -0.0008  | Total Loss: 4.85 | Total Steps: 30\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 905/175267  | Episode Reward: 10.0  | Average Reward 7.24  | Actor loss: 0.01 | Critic loss: 0.69 | Entropy loss: -0.0000  | Total Loss: 0.70 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 906/175267  | Episode Reward: 5.0  | Average Reward 7.18  | Actor loss: -0.15 | Critic loss: 8.42 | Entropy loss: -0.0012  | Total Loss: 8.27 | Total Steps: 42\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 907/175267  | Episode Reward: 7.5  | Average Reward 7.16  | Actor loss: 0.10 | Critic loss: 7.33 | Entropy loss: -0.0003  | Total Loss: 7.43 | Total Steps: 29\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 908/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.07 | Critic loss: 5.24 | Entropy loss: -0.0003  | Total Loss: 5.31 | Total Steps: 29\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 909/175267  | Episode Reward: -7.5  | Average Reward 7.01  | Actor loss: -0.31 | Critic loss: 12.16 | Entropy loss: -0.0065  | Total Loss: 11.84 | Total Steps: 153\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 910/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.02 | Critic loss: 0.13 | Entropy loss: -0.0001  | Total Loss: 0.15 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 911/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: 0.11 | Critic loss: 5.50 | Entropy loss: -0.0008  | Total Loss: 5.61 | Total Steps: 32\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 912/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: 0.04 | Critic loss: 4.39 | Entropy loss: -0.0002  | Total Loss: 4.43 | Total Steps: 29\n",
      "---blue capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 913/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.49 | Critic loss: 3.40 | Entropy loss: -0.0029  | Total Loss: 3.89 | Total Steps: 32\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 914/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: -0.04 | Critic loss: 4.58 | Entropy loss: -0.0010  | Total Loss: 4.54 | Total Steps: 30\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 915/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.25 | Critic loss: 3.40 | Entropy loss: -0.0034  | Total Loss: 3.64 | Total Steps: 31\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 916/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: -0.23 | Critic loss: 2.91 | Entropy loss: -0.0027  | Total Loss: 2.68 | Total Steps: 49\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 917/175267  | Episode Reward: -2.5  | Average Reward 7.21  | Actor loss: -0.40 | Critic loss: 13.82 | Entropy loss: -0.0029  | Total Loss: 13.42 | Total Steps: 92\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 918/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: -0.03 | Critic loss: 4.02 | Entropy loss: -0.0032  | Total Loss: 3.99 | Total Steps: 74\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 919/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.64 | Critic loss: 4.48 | Entropy loss: -0.0030  | Total Loss: 5.12 | Total Steps: 31\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 920/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.02 | Critic loss: 4.51 | Entropy loss: -0.0000  | Total Loss: 4.53 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 921/175267  | Episode Reward: 7.5  | Average Reward 7.24  | Actor loss: 0.08 | Critic loss: 4.92 | Entropy loss: -0.0031  | Total Loss: 5.00 | Total Steps: 56\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 922/175267  | Episode Reward: 10.0  | Average Reward 7.24  | Actor loss: 0.01 | Critic loss: 0.72 | Entropy loss: -0.0000  | Total Loss: 0.73 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 923/175267  | Episode Reward: -5.0  | Average Reward 7.11  | Actor loss: -0.26 | Critic loss: 10.93 | Entropy loss: -0.0042  | Total Loss: 10.67 | Total Steps: 183\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 924/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: -0.76 | Critic loss: 4.99 | Entropy loss: -0.0026  | Total Loss: 4.22 | Total Steps: 31\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 925/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.04 | Critic loss: 4.83 | Entropy loss: -0.0025  | Total Loss: 4.87 | Total Steps: 32\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 926/175267  | Episode Reward: 7.5  | Average Reward 7.13  | Actor loss: -0.20 | Critic loss: 3.74 | Entropy loss: -0.0021  | Total Loss: 3.53 | Total Steps: 55\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 927/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: 0.15 | Critic loss: 8.39 | Entropy loss: -0.0008  | Total Loss: 8.54 | Total Steps: 31\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 928/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.31 | Critic loss: 5.12 | Entropy loss: -0.0014  | Total Loss: 5.43 | Total Steps: 7\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 929/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: 0.03 | Critic loss: 3.47 | Entropy loss: -0.0005  | Total Loss: 3.50 | Total Steps: 43\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 930/175267  | Episode Reward: 5.0  | Average Reward 7.06  | Actor loss: 0.09 | Critic loss: 8.97 | Entropy loss: -0.0014  | Total Loss: 9.06 | Total Steps: 47\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 931/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 1.44 | Entropy loss: -0.0000  | Total Loss: 1.45 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 932/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: -0.13 | Critic loss: 4.23 | Entropy loss: -0.0013  | Total Loss: 4.09 | Total Steps: 52\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 933/175267  | Episode Reward: 5.0  | Average Reward 7.06  | Actor loss: 0.03 | Critic loss: 7.83 | Entropy loss: -0.0005  | Total Loss: 7.87 | Total Steps: 43\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 934/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.00 | Critic loss: 0.83 | Entropy loss: -0.0000  | Total Loss: 0.83 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 935/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.03 | Critic loss: 4.65 | Entropy loss: -0.0002  | Total Loss: 4.69 | Total Steps: 29\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 936/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.00 | Critic loss: 1.08 | Entropy loss: -0.0000  | Total Loss: 1.08 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 937/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: 0.08 | Critic loss: 2.61 | Entropy loss: -0.0014  | Total Loss: 2.69 | Total Steps: 49\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 938/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.18 | Critic loss: 4.55 | Entropy loss: -0.0010  | Total Loss: 4.72 | Total Steps: 29\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 939/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: -0.20 | Critic loss: 2.70 | Entropy loss: -0.0036  | Total Loss: 2.50 | Total Steps: 51\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 940/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.01 | Critic loss: 3.21 | Entropy loss: -0.0000  | Total Loss: 3.22 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 941/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.06 | Critic loss: 1.04 | Entropy loss: -0.0025  | Total Loss: 1.10 | Total Steps: 39\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 942/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.10 | Critic loss: 3.91 | Entropy loss: -0.0005  | Total Loss: 4.01 | Total Steps: 32\n",
      "---yellow capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 943/175267  | Episode Reward: -5.0  | Average Reward 7.09  | Actor loss: -1.03 | Critic loss: 25.05 | Entropy loss: -0.0038  | Total Loss: 24.02 | Total Steps: 85\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 944/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: 0.10 | Critic loss: 6.23 | Entropy loss: -0.0014  | Total Loss: 6.32 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 945/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: 0.29 | Critic loss: 5.79 | Entropy loss: -0.0009  | Total Loss: 6.08 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 946/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.02 | Critic loss: 3.42 | Entropy loss: -0.0001  | Total Loss: 3.45 | Total Steps: 34\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 947/175267  | Episode Reward: 7.5  | Average Reward 7.12  | Actor loss: 0.08 | Critic loss: 7.66 | Entropy loss: -0.0003  | Total Loss: 7.74 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Step: 250\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 948/175267  | Episode Reward: -11.0  | Average Reward 6.91  | Actor loss: -0.21 | Critic loss: 7.03 | Entropy loss: -0.0062  | Total Loss: 6.81 | Total Steps: 500\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 949/175267  | Episode Reward: 10.0  | Average Reward 6.93  | Actor loss: 0.01 | Critic loss: 0.66 | Entropy loss: -0.0001  | Total Loss: 0.67 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 950/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: -0.24 | Critic loss: 2.78 | Entropy loss: -0.0050  | Total Loss: 2.54 | Total Steps: 139\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 951/175267  | Episode Reward: 10.0  | Average Reward 6.93  | Actor loss: 0.01 | Critic loss: 2.85 | Entropy loss: -0.0000  | Total Loss: 2.86 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 952/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: -0.05 | Critic loss: 3.98 | Entropy loss: -0.0013  | Total Loss: 3.93 | Total Steps: 42\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 953/175267  | Episode Reward: -10.0  | Average Reward 6.81  | Actor loss: -0.98 | Critic loss: 21.29 | Entropy loss: -0.0075  | Total Loss: 20.31 | Total Steps: 127\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 954/175267  | Episode Reward: 10.0  | Average Reward 6.83  | Actor loss: 0.01 | Critic loss: 2.15 | Entropy loss: -0.0000  | Total Loss: 2.16 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 955/175267  | Episode Reward: 2.5  | Average Reward 6.86  | Actor loss: -0.07 | Critic loss: 11.31 | Entropy loss: -0.0028  | Total Loss: 11.24 | Total Steps: 53\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 956/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.00 | Critic loss: 0.10 | Entropy loss: -0.0001  | Total Loss: 0.10 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 957/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.01 | Critic loss: 2.48 | Entropy loss: -0.0000  | Total Loss: 2.50 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 958/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: -0.26 | Critic loss: 3.87 | Entropy loss: -0.0065  | Total Loss: 3.61 | Total Steps: 86\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 959/175267  | Episode Reward: 5.0  | Average Reward 6.91  | Actor loss: -0.06 | Critic loss: 4.67 | Entropy loss: -0.0022  | Total Loss: 4.60 | Total Steps: 54\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 960/175267  | Episode Reward: 2.5  | Average Reward 6.83  | Actor loss: -0.31 | Critic loss: 6.65 | Entropy loss: -0.0039  | Total Loss: 6.33 | Total Steps: 56\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 961/175267  | Episode Reward: 10.0  | Average Reward 6.83  | Actor loss: 0.30 | Critic loss: 6.17 | Entropy loss: -0.0009  | Total Loss: 6.47 | Total Steps: 36\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 962/175267  | Episode Reward: 7.5  | Average Reward 6.81  | Actor loss: 0.38 | Critic loss: 7.70 | Entropy loss: -0.0019  | Total Loss: 8.07 | Total Steps: 36\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 963/175267  | Episode Reward: 7.5  | Average Reward 6.81  | Actor loss: 0.07 | Critic loss: 3.90 | Entropy loss: -0.0010  | Total Loss: 3.97 | Total Steps: 48\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 964/175267  | Episode Reward: 10.0  | Average Reward 6.81  | Actor loss: 0.03 | Critic loss: 4.72 | Entropy loss: -0.0001  | Total Loss: 4.75 | Total Steps: 29\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 965/175267  | Episode Reward: -2.5  | Average Reward 6.71  | Actor loss: -0.79 | Critic loss: 12.32 | Entropy loss: -0.0100  | Total Loss: 11.52 | Total Steps: 111\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 966/175267  | Episode Reward: 0.0  | Average Reward 6.61  | Actor loss: -0.26 | Critic loss: 11.94 | Entropy loss: -0.0019  | Total Loss: 11.68 | Total Steps: 91\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 967/175267  | Episode Reward: 7.5  | Average Reward 6.58  | Actor loss: 0.48 | Critic loss: 7.82 | Entropy loss: -0.0016  | Total Loss: 8.30 | Total Steps: 31\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 968/175267  | Episode Reward: 5.0  | Average Reward 6.54  | Actor loss: -0.05 | Critic loss: 6.84 | Entropy loss: -0.0011  | Total Loss: 6.78 | Total Steps: 53\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 969/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.03 | Critic loss: 3.65 | Entropy loss: -0.0003  | Total Loss: 3.68 | Total Steps: 31\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 970/175267  | Episode Reward: 2.5  | Average Reward 6.58  | Actor loss: -0.53 | Critic loss: 10.09 | Entropy loss: -0.0051  | Total Loss: 9.55 | Total Steps: 55\n",
      "---green cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 971/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.00 | Critic loss: 1.24 | Entropy loss: -0.0000  | Total Loss: 1.24 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 972/175267  | Episode Reward: 5.0  | Average Reward 6.54  | Actor loss: -0.32 | Critic loss: 6.12 | Entropy loss: -0.0049  | Total Loss: 5.80 | Total Steps: 112\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 973/175267  | Episode Reward: 2.5  | Average Reward 6.49  | Actor loss: -0.15 | Critic loss: 7.33 | Entropy loss: -0.0009  | Total Loss: 7.18 | Total Steps: 53\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 974/175267  | Episode Reward: 10.0  | Average Reward 6.54  | Actor loss: 0.01 | Critic loss: 2.19 | Entropy loss: -0.0000  | Total Loss: 2.20 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 975/175267  | Episode Reward: 7.5  | Average Reward 6.56  | Actor loss: 0.00 | Critic loss: 6.06 | Entropy loss: -0.0008  | Total Loss: 6.06 | Total Steps: 42\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 976/175267  | Episode Reward: 10.0  | Average Reward 6.56  | Actor loss: 0.01 | Critic loss: 4.04 | Entropy loss: -0.0000  | Total Loss: 4.05 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 977/175267  | Episode Reward: 5.0  | Average Reward 6.51  | Actor loss: -0.15 | Critic loss: 4.76 | Entropy loss: -0.0015  | Total Loss: 4.61 | Total Steps: 51\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 978/175267  | Episode Reward: 10.0  | Average Reward 6.51  | Actor loss: 0.14 | Critic loss: 5.17 | Entropy loss: -0.0005  | Total Loss: 5.31 | Total Steps: 30\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 979/175267  | Episode Reward: 10.0  | Average Reward 6.54  | Actor loss: 0.02 | Critic loss: 1.61 | Entropy loss: -0.0000  | Total Loss: 1.63 | Total Steps: 6\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 980/175267  | Episode Reward: 10.0  | Average Reward 6.58  | Actor loss: 0.01 | Critic loss: 1.14 | Entropy loss: -0.0000  | Total Loss: 1.15 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 981/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.01 | Critic loss: 1.17 | Entropy loss: -0.0000  | Total Loss: 1.18 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 982/175267  | Episode Reward: 5.0  | Average Reward 6.80  | Actor loss: -0.12 | Critic loss: 6.32 | Entropy loss: -0.0033  | Total Loss: 6.20 | Total Steps: 55\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 983/175267  | Episode Reward: 10.0  | Average Reward 6.80  | Actor loss: 0.03 | Critic loss: 3.89 | Entropy loss: -0.0002  | Total Loss: 3.92 | Total Steps: 29\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 984/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.06 | Critic loss: 5.55 | Entropy loss: -0.0004  | Total Loss: 5.61 | Total Steps: 32\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 985/175267  | Episode Reward: 10.0  | Average Reward 6.91  | Actor loss: 0.01 | Critic loss: 0.95 | Entropy loss: -0.0000  | Total Loss: 0.96 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 986/175267  | Episode Reward: 10.0  | Average Reward 7.03  | Actor loss: 0.01 | Critic loss: 1.96 | Entropy loss: -0.0000  | Total Loss: 1.98 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 987/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.02 | Critic loss: 6.15 | Entropy loss: -0.0007  | Total Loss: 6.13 | Total Steps: 43\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 988/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.02 | Critic loss: 3.19 | Entropy loss: -0.0003  | Total Loss: 3.20 | Total Steps: 29\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 989/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: -0.08 | Critic loss: 5.04 | Entropy loss: -0.0016  | Total Loss: 4.96 | Total Steps: 30\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 990/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: 0.06 | Critic loss: 6.02 | Entropy loss: -0.0029  | Total Loss: 6.07 | Total Steps: 37\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 991/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: -0.14 | Critic loss: 6.17 | Entropy loss: -0.0038  | Total Loss: 6.03 | Total Steps: 32\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 992/175267  | Episode Reward: 7.5  | Average Reward 7.03  | Actor loss: 0.07 | Critic loss: 5.54 | Entropy loss: -0.0026  | Total Loss: 5.61 | Total Steps: 54\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 993/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: 0.07 | Critic loss: 4.53 | Entropy loss: -0.0008  | Total Loss: 4.61 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 994/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: 0.04 | Critic loss: 4.72 | Entropy loss: -0.0005  | Total Loss: 4.76 | Total Steps: 39\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 995/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: 0.16 | Critic loss: 7.51 | Entropy loss: -0.0005  | Total Loss: 7.66 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 996/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: -0.90 | Critic loss: 7.32 | Entropy loss: -0.0021  | Total Loss: 6.42 | Total Steps: 30\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 997/175267  | Episode Reward: 5.0  | Average Reward 7.09  | Actor loss: -0.09 | Critic loss: 5.81 | Entropy loss: -0.0007  | Total Loss: 5.71 | Total Steps: 43\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 998/175267  | Episode Reward: 7.5  | Average Reward 7.09  | Actor loss: 0.19 | Critic loss: 4.10 | Entropy loss: -0.0008  | Total Loss: 4.29 | Total Steps: 30\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 999/175267  | Episode Reward: 7.5  | Average Reward 7.09  | Actor loss: 0.10 | Critic loss: 3.36 | Entropy loss: -0.0006  | Total Loss: 3.46 | Total Steps: 30\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1000/175267  | Episode Reward: 7.5  | Average Reward 7.12  | Actor loss: 0.07 | Critic loss: 4.31 | Entropy loss: -0.0004  | Total Loss: 4.38 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1001/175267  | Episode Reward: -2.5  | Average Reward 6.99  | Actor loss: -0.59 | Critic loss: 16.65 | Entropy loss: -0.0045  | Total Loss: 16.06 | Total Steps: 91\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1002/175267  | Episode Reward: 2.5  | Average Reward 6.92  | Actor loss: -0.58 | Critic loss: 6.47 | Entropy loss: -0.0035  | Total Loss: 5.89 | Total Steps: 44\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1003/175267  | Episode Reward: 7.5  | Average Reward 6.89  | Actor loss: 0.09 | Critic loss: 4.66 | Entropy loss: -0.0004  | Total Loss: 4.75 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1004/175267  | Episode Reward: 5.0  | Average Reward 6.87  | Actor loss: -0.08 | Critic loss: 6.24 | Entropy loss: -0.0015  | Total Loss: 6.16 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1005/175267  | Episode Reward: 2.5  | Average Reward 6.79  | Actor loss: -0.48 | Critic loss: 10.69 | Entropy loss: -0.0048  | Total Loss: 10.20 | Total Steps: 77\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1006/175267  | Episode Reward: 7.5  | Average Reward 6.82  | Actor loss: 0.09 | Critic loss: 2.30 | Entropy loss: -0.0014  | Total Loss: 2.39 | Total Steps: 46\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1007/175267  | Episode Reward: 7.5  | Average Reward 6.82  | Actor loss: 0.02 | Critic loss: 7.59 | Entropy loss: -0.0009  | Total Loss: 7.60 | Total Steps: 43\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1008/175267  | Episode Reward: 4.0  | Average Reward 6.75  | Actor loss: -0.00 | Critic loss: 5.73 | Entropy loss: -0.0035  | Total Loss: 5.72 | Total Steps: 54\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1009/175267  | Episode Reward: 10.0  | Average Reward 6.93  | Actor loss: 0.01 | Critic loss: 1.70 | Entropy loss: -0.0000  | Total Loss: 1.71 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1010/175267  | Episode Reward: 10.0  | Average Reward 6.93  | Actor loss: 0.07 | Critic loss: 3.20 | Entropy loss: -0.0003  | Total Loss: 3.28 | Total Steps: 29\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1011/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.08 | Critic loss: 3.94 | Entropy loss: -0.0004  | Total Loss: 4.01 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1012/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 2.47 | Entropy loss: -0.0000  | Total Loss: 2.48 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1013/175267  | Episode Reward: 0.0  | Average Reward 6.86  | Actor loss: -1.11 | Critic loss: 8.28 | Entropy loss: -0.0073  | Total Loss: 7.16 | Total Steps: 59\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1014/175267  | Episode Reward: 7.5  | Average Reward 6.86  | Actor loss: -0.30 | Critic loss: 3.09 | Entropy loss: -0.0016  | Total Loss: 2.79 | Total Steps: 30\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1015/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: -0.03 | Critic loss: 2.55 | Entropy loss: -0.0029  | Total Loss: 2.51 | Total Steps: 61\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1016/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.10 | Critic loss: 11.30 | Entropy loss: -0.0001  | Total Loss: 11.40 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1017/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.03 | Critic loss: 3.81 | Entropy loss: -0.0002  | Total Loss: 3.84 | Total Steps: 34\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1018/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.14 | Critic loss: 2.46 | Entropy loss: -0.0002  | Total Loss: 2.60 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1019/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: 0.05 | Critic loss: 5.27 | Entropy loss: -0.0003  | Total Loss: 5.32 | Total Steps: 30\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1020/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: -0.38 | Critic loss: 3.97 | Entropy loss: -0.0014  | Total Loss: 3.59 | Total Steps: 35\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1021/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.25 | Critic loss: 2.72 | Entropy loss: -0.0011  | Total Loss: 2.97 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1022/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: -0.13 | Critic loss: 2.10 | Entropy loss: -0.0015  | Total Loss: 1.97 | Total Steps: 68\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1023/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: -0.10 | Critic loss: 3.32 | Entropy loss: -0.0012  | Total Loss: 3.21 | Total Steps: 43\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1024/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.01 | Critic loss: 1.47 | Entropy loss: -0.0000  | Total Loss: 1.48 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1025/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: 0.12 | Critic loss: 7.51 | Entropy loss: -0.0005  | Total Loss: 7.62 | Total Steps: 30\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1026/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.02 | Critic loss: 4.06 | Entropy loss: -0.0010  | Total Loss: 4.08 | Total Steps: 44\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1027/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.02 | Critic loss: 3.64 | Entropy loss: -0.0001  | Total Loss: 3.67 | Total Steps: 29\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1028/175267  | Episode Reward: 2.5  | Average Reward 7.05  | Actor loss: -0.06 | Critic loss: 10.93 | Entropy loss: -0.0010  | Total Loss: 10.88 | Total Steps: 52\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1029/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: -0.09 | Critic loss: 1.02 | Entropy loss: -0.0023  | Total Loss: 0.93 | Total Steps: 49\n",
      "---black cylinder---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1030/175267  | Episode Reward: 3.0  | Average Reward 7.06  | Actor loss: -0.56 | Critic loss: 5.73 | Entropy loss: -0.0073  | Total Loss: 5.15 | Total Steps: 143\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1031/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 2.92 | Entropy loss: -0.0000  | Total Loss: 2.93 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1032/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: 0.08 | Critic loss: 3.66 | Entropy loss: -0.0004  | Total Loss: 3.74 | Total Steps: 30\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1033/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.01 | Critic loss: 1.26 | Entropy loss: -0.0000  | Total Loss: 1.26 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1034/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: -0.04 | Critic loss: 5.23 | Entropy loss: -0.0003  | Total Loss: 5.19 | Total Steps: 52\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1035/175267  | Episode Reward: 5.0  | Average Reward 7.04  | Actor loss: -0.00 | Critic loss: 8.28 | Entropy loss: -0.0005  | Total Loss: 8.27 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1036/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: 0.24 | Critic loss: 6.83 | Entropy loss: -0.0009  | Total Loss: 7.07 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1037/175267  | Episode Reward: 2.5  | Average Reward 6.96  | Actor loss: -0.47 | Critic loss: 12.71 | Entropy loss: -0.0040  | Total Loss: 12.23 | Total Steps: 106\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1038/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.02 | Critic loss: 4.61 | Entropy loss: -0.0005  | Total Loss: 4.63 | Total Steps: 30\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1039/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: -0.43 | Critic loss: 3.55 | Entropy loss: -0.0030  | Total Loss: 3.11 | Total Steps: 43\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1040/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 2.80 | Entropy loss: -0.0000  | Total Loss: 2.81 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1041/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.05 | Critic loss: 7.13 | Entropy loss: -0.0000  | Total Loss: 7.18 | Total Steps: 6\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1042/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.49 | Critic loss: 3.25 | Entropy loss: -0.0015  | Total Loss: 3.73 | Total Steps: 7\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1043/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: -0.75 | Critic loss: 3.21 | Entropy loss: -0.0033  | Total Loss: 2.46 | Total Steps: 43\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1044/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.20 | Critic loss: 2.81 | Entropy loss: -0.0018  | Total Loss: 3.00 | Total Steps: 30\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1045/175267  | Episode Reward: 2.5  | Average Reward 7.08  | Actor loss: -0.54 | Critic loss: 7.02 | Entropy loss: -0.0071  | Total Loss: 6.47 | Total Steps: 81\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1046/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.01 | Critic loss: 0.80 | Entropy loss: -0.0000  | Total Loss: 0.80 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1047/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: 0.07 | Critic loss: 6.02 | Entropy loss: -0.0006  | Total Loss: 6.09 | Total Steps: 32\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1048/175267  | Episode Reward: 4.0  | Average Reward 7.24  | Actor loss: 0.04 | Critic loss: 4.05 | Entropy loss: -0.0042  | Total Loss: 4.09 | Total Steps: 45\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1049/175267  | Episode Reward: 10.0  | Average Reward 7.24  | Actor loss: 0.08 | Critic loss: 1.21 | Entropy loss: -0.0002  | Total Loss: 1.29 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1050/175267  | Episode Reward: 7.5  | Average Reward 7.24  | Actor loss: 0.23 | Critic loss: 2.35 | Entropy loss: -0.0013  | Total Loss: 2.58 | Total Steps: 31\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1051/175267  | Episode Reward: 0.0  | Average Reward 7.13  | Actor loss: -0.29 | Critic loss: 15.87 | Entropy loss: -0.0015  | Total Loss: 15.58 | Total Steps: 71\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1052/175267  | Episode Reward: 5.0  | Average Reward 7.11  | Actor loss: -0.28 | Critic loss: 2.74 | Entropy loss: -0.0025  | Total Loss: 2.46 | Total Steps: 32\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1053/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: -0.54 | Critic loss: 6.88 | Entropy loss: -0.0060  | Total Loss: 6.34 | Total Steps: 85\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1054/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.01 | Critic loss: 3.47 | Entropy loss: -0.0043  | Total Loss: 3.47 | Total Steps: 156\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1055/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.09 | Critic loss: 14.26 | Entropy loss: -0.0001  | Total Loss: 14.35 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1056/175267  | Episode Reward: -25.0  | Average Reward 6.99  | Actor loss: -1.02 | Critic loss: 16.94 | Entropy loss: -0.0086  | Total Loss: 15.92 | Total Steps: 382\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1057/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 1.57 | Entropy loss: -0.0000  | Total Loss: 1.59 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1058/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: -0.18 | Critic loss: 9.62 | Entropy loss: -0.0039  | Total Loss: 9.44 | Total Steps: 87\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1059/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: 0.00 | Critic loss: 0.83 | Entropy loss: -0.0000  | Total Loss: 0.84 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1060/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: 0.14 | Critic loss: 3.11 | Entropy loss: -0.0009  | Total Loss: 3.25 | Total Steps: 31\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1061/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 1.37 | Entropy loss: -0.0000  | Total Loss: 1.38 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1062/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: -0.25 | Critic loss: 4.07 | Entropy loss: -0.0039  | Total Loss: 3.81 | Total Steps: 65\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1063/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: 0.13 | Critic loss: 4.04 | Entropy loss: -0.0007  | Total Loss: 4.17 | Total Steps: 29\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1064/175267  | Episode Reward: 5.0  | Average Reward 7.01  | Actor loss: -0.03 | Critic loss: 6.21 | Entropy loss: -0.0003  | Total Loss: 6.18 | Total Steps: 43\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1065/175267  | Episode Reward: 2.5  | Average Reward 7.06  | Actor loss: -0.13 | Critic loss: 10.76 | Entropy loss: -0.0009  | Total Loss: 10.63 | Total Steps: 54\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1066/175267  | Episode Reward: 5.0  | Average Reward 7.11  | Actor loss: -0.50 | Critic loss: 3.70 | Entropy loss: -0.0033  | Total Loss: 3.20 | Total Steps: 47\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1067/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.00 | Critic loss: 1.04 | Entropy loss: -0.0000  | Total Loss: 1.04 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1068/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.14 | Critic loss: 4.06 | Entropy loss: -0.0005  | Total Loss: 4.20 | Total Steps: 29\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1069/175267  | Episode Reward: 5.0  | Average Reward 7.13  | Actor loss: -0.04 | Critic loss: 5.50 | Entropy loss: -0.0005  | Total Loss: 5.46 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1070/175267  | Episode Reward: 5.0  | Average Reward 7.16  | Actor loss: -0.11 | Critic loss: 8.52 | Entropy loss: -0.0012  | Total Loss: 8.41 | Total Steps: 73\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1071/175267  | Episode Reward: -42.5  | Average Reward 6.63  | Actor loss: -0.20 | Critic loss: 9.81 | Entropy loss: -0.0067  | Total Loss: 9.60 | Total Steps: 454\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1072/175267  | Episode Reward: 10.0  | Average Reward 6.68  | Actor loss: 0.02 | Critic loss: 0.99 | Entropy loss: -0.0001  | Total Loss: 1.01 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1073/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: 0.05 | Critic loss: 8.40 | Entropy loss: -0.0003  | Total Loss: 8.44 | Total Steps: 30\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1074/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: 0.05 | Critic loss: 4.65 | Entropy loss: -0.0005  | Total Loss: 4.69 | Total Steps: 31\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1075/175267  | Episode Reward: 10.0  | Average Reward 6.76  | Actor loss: 0.35 | Critic loss: 2.23 | Entropy loss: -0.0021  | Total Loss: 2.57 | Total Steps: 32\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1076/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: 0.01 | Critic loss: 2.61 | Entropy loss: -0.0006  | Total Loss: 2.61 | Total Steps: 30\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1077/175267  | Episode Reward: 10.0  | Average Reward 6.79  | Actor loss: 0.04 | Critic loss: 5.35 | Entropy loss: -0.0000  | Total Loss: 5.40 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1078/175267  | Episode Reward: 10.0  | Average Reward 6.79  | Actor loss: 0.01 | Critic loss: 2.51 | Entropy loss: -0.0000  | Total Loss: 2.52 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1079/175267  | Episode Reward: 10.0  | Average Reward 6.79  | Actor loss: 0.01 | Critic loss: 1.33 | Entropy loss: -0.0000  | Total Loss: 1.34 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1080/175267  | Episode Reward: 7.5  | Average Reward 6.76  | Actor loss: -0.19 | Critic loss: 2.42 | Entropy loss: -0.0036  | Total Loss: 2.22 | Total Steps: 45\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1081/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: -0.00 | Critic loss: 4.79 | Entropy loss: -0.0018  | Total Loss: 4.79 | Total Steps: 42\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1082/175267  | Episode Reward: 4.0  | Average Reward 6.72  | Actor loss: -0.94 | Critic loss: 5.31 | Entropy loss: -0.0066  | Total Loss: 4.37 | Total Steps: 52\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1083/175267  | Episode Reward: 10.0  | Average Reward 6.72  | Actor loss: 0.01 | Critic loss: 0.23 | Entropy loss: -0.0001  | Total Loss: 0.24 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1084/175267  | Episode Reward: 5.0  | Average Reward 6.67  | Actor loss: 0.16 | Critic loss: 5.67 | Entropy loss: -0.0012  | Total Loss: 5.82 | Total Steps: 48\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1085/175267  | Episode Reward: 2.5  | Average Reward 6.60  | Actor loss: -0.27 | Critic loss: 9.98 | Entropy loss: -0.0017  | Total Loss: 9.71 | Total Steps: 52\n",
      "---red cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1086/175267  | Episode Reward: 10.0  | Average Reward 6.60  | Actor loss: 0.01 | Critic loss: 3.66 | Entropy loss: -0.0000  | Total Loss: 3.68 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1087/175267  | Episode Reward: 7.5  | Average Reward 6.60  | Actor loss: -0.05 | Critic loss: 1.06 | Entropy loss: -0.0011  | Total Loss: 1.01 | Total Steps: 68\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1088/175267  | Episode Reward: 6.5  | Average Reward 6.57  | Actor loss: -0.32 | Critic loss: 7.33 | Entropy loss: -0.0020  | Total Loss: 7.01 | Total Steps: 45\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1089/175267  | Episode Reward: 5.0  | Average Reward 6.54  | Actor loss: -0.01 | Critic loss: 3.82 | Entropy loss: -0.0009  | Total Loss: 3.81 | Total Steps: 43\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1090/175267  | Episode Reward: 10.0  | Average Reward 6.57  | Actor loss: 0.21 | Critic loss: 5.10 | Entropy loss: -0.0010  | Total Loss: 5.31 | Total Steps: 29\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1091/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 2.82 | Critic loss: 10.73 | Entropy loss: -0.0014  | Total Loss: 13.55 | Total Steps: 7\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1092/175267  | Episode Reward: 5.0  | Average Reward 6.59  | Actor loss: 0.23 | Critic loss: 6.85 | Entropy loss: -0.0007  | Total Loss: 7.08 | Total Steps: 278\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1093/175267  | Episode Reward: 7.5  | Average Reward 6.59  | Actor loss: 0.04 | Critic loss: 4.77 | Entropy loss: -0.0005  | Total Loss: 4.81 | Total Steps: 32\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1094/175267  | Episode Reward: 7.5  | Average Reward 6.59  | Actor loss: -0.19 | Critic loss: 2.97 | Entropy loss: -0.0023  | Total Loss: 2.77 | Total Steps: 72\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1095/175267  | Episode Reward: 10.0  | Average Reward 6.62  | Actor loss: 0.01 | Critic loss: 0.81 | Entropy loss: -0.0000  | Total Loss: 0.82 | Total Steps: 6\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1096/175267  | Episode Reward: 10.0  | Average Reward 6.64  | Actor loss: 0.02 | Critic loss: 0.32 | Entropy loss: -0.0000  | Total Loss: 0.34 | Total Steps: 6\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1097/175267  | Episode Reward: 10.0  | Average Reward 6.69  | Actor loss: 0.02 | Critic loss: 0.29 | Entropy loss: -0.0001  | Total Loss: 0.31 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1098/175267  | Episode Reward: 5.0  | Average Reward 6.67  | Actor loss: 0.01 | Critic loss: 7.35 | Entropy loss: -0.0005  | Total Loss: 7.36 | Total Steps: 43\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1099/175267  | Episode Reward: 2.5  | Average Reward 6.62  | Actor loss: -0.12 | Critic loss: 8.12 | Entropy loss: -0.0015  | Total Loss: 8.00 | Total Steps: 53\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1100/175267  | Episode Reward: 10.0  | Average Reward 6.64  | Actor loss: 0.05 | Critic loss: 4.57 | Entropy loss: -0.0005  | Total Loss: 4.62 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1101/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: 0.06 | Critic loss: 1.85 | Entropy loss: -0.0016  | Total Loss: 1.90 | Total Steps: 33\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1102/175267  | Episode Reward: 7.5  | Average Reward 6.79  | Actor loss: 0.06 | Critic loss: 6.89 | Entropy loss: -0.0003  | Total Loss: 6.95 | Total Steps: 34\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1103/175267  | Episode Reward: 5.0  | Average Reward 6.76  | Actor loss: -0.53 | Critic loss: 9.93 | Entropy loss: -0.0062  | Total Loss: 9.40 | Total Steps: 126\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1104/175267  | Episode Reward: 5.0  | Average Reward 6.76  | Actor loss: -0.80 | Critic loss: 4.64 | Entropy loss: -0.0035  | Total Loss: 3.84 | Total Steps: 47\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1105/175267  | Episode Reward: 7.5  | Average Reward 6.82  | Actor loss: -0.01 | Critic loss: 6.07 | Entropy loss: -0.0003  | Total Loss: 6.06 | Total Steps: 42\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1106/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.03 | Critic loss: 4.19 | Entropy loss: -0.0000  | Total Loss: 4.22 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1107/175267  | Episode Reward: 7.5  | Average Reward 6.84  | Actor loss: 0.04 | Critic loss: 3.08 | Entropy loss: -0.0011  | Total Loss: 3.12 | Total Steps: 53\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1108/175267  | Episode Reward: 10.0  | Average Reward 6.90  | Actor loss: -0.35 | Critic loss: 7.00 | Entropy loss: -0.0059  | Total Loss: 6.64 | Total Steps: 109\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1109/175267  | Episode Reward: 5.0  | Average Reward 6.85  | Actor loss: -0.19 | Critic loss: 8.31 | Entropy loss: -0.0019  | Total Loss: 8.13 | Total Steps: 54\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1110/175267  | Episode Reward: 10.0  | Average Reward 6.85  | Actor loss: 0.01 | Critic loss: 1.37 | Entropy loss: -0.0000  | Total Loss: 1.38 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1111/175267  | Episode Reward: 10.0  | Average Reward 6.85  | Actor loss: 0.01 | Critic loss: 2.77 | Entropy loss: -0.0000  | Total Loss: 2.78 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1112/175267  | Episode Reward: 5.0  | Average Reward 6.80  | Actor loss: -0.07 | Critic loss: 7.64 | Entropy loss: -0.0007  | Total Loss: 7.57 | Total Steps: 52\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1113/175267  | Episode Reward: 10.0  | Average Reward 6.90  | Actor loss: 0.10 | Critic loss: 2.73 | Entropy loss: -0.0006  | Total Loss: 2.83 | Total Steps: 30\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1114/175267  | Episode Reward: 7.5  | Average Reward 6.90  | Actor loss: 0.02 | Critic loss: 4.40 | Entropy loss: -0.0010  | Total Loss: 4.42 | Total Steps: 53\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1115/175267  | Episode Reward: 5.0  | Average Reward 6.85  | Actor loss: -0.45 | Critic loss: 2.72 | Entropy loss: -0.0028  | Total Loss: 2.27 | Total Steps: 41\n",
      "---green sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1116/175267  | Episode Reward: 5.0  | Average Reward 6.80  | Actor loss: -0.08 | Critic loss: 3.28 | Entropy loss: -0.0013  | Total Loss: 3.20 | Total Steps: 42\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1117/175267  | Episode Reward: 10.0  | Average Reward 6.80  | Actor loss: 0.01 | Critic loss: 2.68 | Entropy loss: -0.0000  | Total Loss: 2.69 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1118/175267  | Episode Reward: 2.5  | Average Reward 6.72  | Actor loss: -0.02 | Critic loss: 9.36 | Entropy loss: -0.0022  | Total Loss: 9.34 | Total Steps: 50\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1119/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.01 | Critic loss: 1.32 | Entropy loss: -0.0000  | Total Loss: 1.33 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1120/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: -0.07 | Critic loss: 5.53 | Entropy loss: -0.0011  | Total Loss: 5.46 | Total Steps: 54\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1121/175267  | Episode Reward: 7.5  | Average Reward 6.72  | Actor loss: -0.06 | Critic loss: 2.56 | Entropy loss: -0.0007  | Total Loss: 2.50 | Total Steps: 42\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1122/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.06 | Critic loss: 7.04 | Entropy loss: -0.0000  | Total Loss: 7.10 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1123/175267  | Episode Reward: 5.0  | Average Reward 6.72  | Actor loss: -0.14 | Critic loss: 3.08 | Entropy loss: -0.0013  | Total Loss: 2.93 | Total Steps: 42\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1124/175267  | Episode Reward: -5.0  | Average Reward 6.58  | Actor loss: -0.52 | Critic loss: 7.67 | Entropy loss: -0.0091  | Total Loss: 7.15 | Total Steps: 394\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1125/175267  | Episode Reward: 7.5  | Average Reward 6.58  | Actor loss: -1.05 | Critic loss: 8.39 | Entropy loss: -0.0024  | Total Loss: 7.34 | Total Steps: 35\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1126/175267  | Episode Reward: 5.0  | Average Reward 6.53  | Actor loss: 0.15 | Critic loss: 2.13 | Entropy loss: -0.0031  | Total Loss: 2.28 | Total Steps: 32\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1127/175267  | Episode Reward: 5.0  | Average Reward 6.47  | Actor loss: -0.09 | Critic loss: 2.96 | Entropy loss: -0.0042  | Total Loss: 2.87 | Total Steps: 45\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1128/175267  | Episode Reward: 5.0  | Average Reward 6.50  | Actor loss: 0.06 | Critic loss: 6.70 | Entropy loss: -0.0009  | Total Loss: 6.75 | Total Steps: 44\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1129/175267  | Episode Reward: 10.0  | Average Reward 6.50  | Actor loss: 0.01 | Critic loss: 0.46 | Entropy loss: -0.0000  | Total Loss: 0.47 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1130/175267  | Episode Reward: 7.5  | Average Reward 6.54  | Actor loss: -0.05 | Critic loss: 6.37 | Entropy loss: -0.0010  | Total Loss: 6.31 | Total Steps: 42\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1131/175267  | Episode Reward: 7.5  | Average Reward 6.52  | Actor loss: 0.19 | Critic loss: 4.82 | Entropy loss: -0.0009  | Total Loss: 5.01 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1132/175267  | Episode Reward: 7.5  | Average Reward 6.52  | Actor loss: 0.18 | Critic loss: 7.31 | Entropy loss: -0.0024  | Total Loss: 7.48 | Total Steps: 62\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1133/175267  | Episode Reward: 7.5  | Average Reward 6.50  | Actor loss: -0.14 | Critic loss: 3.88 | Entropy loss: -0.0029  | Total Loss: 3.74 | Total Steps: 44\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1134/175267  | Episode Reward: 5.0  | Average Reward 6.47  | Actor loss: -0.27 | Critic loss: 7.28 | Entropy loss: -0.0018  | Total Loss: 7.01 | Total Steps: 42\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1135/175267  | Episode Reward: 10.0  | Average Reward 6.52  | Actor loss: 0.03 | Critic loss: 4.51 | Entropy loss: -0.0001  | Total Loss: 4.54 | Total Steps: 29\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1136/175267  | Episode Reward: 7.5  | Average Reward 6.52  | Actor loss: 0.12 | Critic loss: 3.91 | Entropy loss: -0.0012  | Total Loss: 4.03 | Total Steps: 30\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1137/175267  | Episode Reward: 7.5  | Average Reward 6.57  | Actor loss: 0.11 | Critic loss: 8.71 | Entropy loss: -0.0003  | Total Loss: 8.82 | Total Steps: 29\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1138/175267  | Episode Reward: 7.5  | Average Reward 6.54  | Actor loss: -0.02 | Critic loss: 5.15 | Entropy loss: -0.0003  | Total Loss: 5.12 | Total Steps: 42\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1139/175267  | Episode Reward: 2.5  | Average Reward 6.50  | Actor loss: -0.55 | Critic loss: 6.55 | Entropy loss: -0.0028  | Total Loss: 6.00 | Total Steps: 53\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1140/175267  | Episode Reward: 7.5  | Average Reward 6.47  | Actor loss: 0.02 | Critic loss: 4.93 | Entropy loss: -0.0004  | Total Loss: 4.95 | Total Steps: 43\n",
      "---green capsule---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1141/175267  | Episode Reward: 9.0  | Average Reward 6.46  | Actor loss: -0.71 | Critic loss: 2.98 | Entropy loss: -0.0064  | Total Loss: 2.26 | Total Steps: 77\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1142/175267  | Episode Reward: 7.5  | Average Reward 6.43  | Actor loss: 0.13 | Critic loss: 5.23 | Entropy loss: -0.0028  | Total Loss: 5.36 | Total Steps: 50\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1143/175267  | Episode Reward: 10.0  | Average Reward 6.43  | Actor loss: 0.06 | Critic loss: 3.91 | Entropy loss: -0.0012  | Total Loss: 3.96 | Total Steps: 54\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1144/175267  | Episode Reward: 7.5  | Average Reward 6.41  | Actor loss: 0.09 | Critic loss: 2.98 | Entropy loss: -0.0009  | Total Loss: 3.06 | Total Steps: 32\n",
      "---black capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1145/175267  | Episode Reward: 7.5  | Average Reward 6.46  | Actor loss: 0.02 | Critic loss: 6.71 | Entropy loss: -0.0001  | Total Loss: 6.72 | Total Steps: 34\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1146/175267  | Episode Reward: 5.0  | Average Reward 6.41  | Actor loss: -0.05 | Critic loss: 8.06 | Entropy loss: -0.0019  | Total Loss: 8.01 | Total Steps: 90\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1147/175267  | Episode Reward: 5.0  | Average Reward 6.38  | Actor loss: -0.17 | Critic loss: 4.51 | Entropy loss: -0.0013  | Total Loss: 4.34 | Total Steps: 52\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1148/175267  | Episode Reward: 10.0  | Average Reward 6.45  | Actor loss: 0.02 | Critic loss: 5.18 | Entropy loss: -0.0000  | Total Loss: 5.20 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1149/175267  | Episode Reward: 10.0  | Average Reward 6.45  | Actor loss: -0.25 | Critic loss: 3.89 | Entropy loss: -0.0030  | Total Loss: 3.63 | Total Steps: 35\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1150/175267  | Episode Reward: 10.0  | Average Reward 6.47  | Actor loss: 0.02 | Critic loss: 0.24 | Entropy loss: -0.0001  | Total Loss: 0.26 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1151/175267  | Episode Reward: 10.0  | Average Reward 6.57  | Actor loss: 0.02 | Critic loss: 1.99 | Entropy loss: -0.0024  | Total Loss: 2.01 | Total Steps: 42\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1152/175267  | Episode Reward: 5.0  | Average Reward 6.57  | Actor loss: -0.02 | Critic loss: 6.45 | Entropy loss: -0.0003  | Total Loss: 6.43 | Total Steps: 42\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1153/175267  | Episode Reward: 7.5  | Average Reward 6.57  | Actor loss: 0.03 | Critic loss: 4.14 | Entropy loss: -0.0003  | Total Loss: 4.17 | Total Steps: 30\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1154/175267  | Episode Reward: -2.5  | Average Reward 6.47  | Actor loss: -0.46 | Critic loss: 15.07 | Entropy loss: -0.0036  | Total Loss: 14.61 | Total Steps: 86\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1155/175267  | Episode Reward: 1.5  | Average Reward 6.38  | Actor loss: -0.23 | Critic loss: 7.05 | Entropy loss: -0.0018  | Total Loss: 6.82 | Total Steps: 54\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1156/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: -0.40 | Critic loss: 3.54 | Entropy loss: -0.0026  | Total Loss: 3.14 | Total Steps: 48\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1157/175267  | Episode Reward: 7.5  | Average Reward 6.71  | Actor loss: -0.13 | Critic loss: 3.25 | Entropy loss: -0.0009  | Total Loss: 3.12 | Total Steps: 53\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1158/175267  | Episode Reward: 10.0  | Average Reward 6.76  | Actor loss: 0.01 | Critic loss: 2.66 | Entropy loss: -0.0000  | Total Loss: 2.67 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1159/175267  | Episode Reward: 10.0  | Average Reward 6.76  | Actor loss: 0.02 | Critic loss: 2.40 | Entropy loss: -0.0000  | Total Loss: 2.42 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1160/175267  | Episode Reward: 5.0  | Average Reward 6.74  | Actor loss: -0.02 | Critic loss: 7.74 | Entropy loss: -0.0002  | Total Loss: 7.72 | Total Steps: 42\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1161/175267  | Episode Reward: 2.5  | Average Reward 6.66  | Actor loss: -0.13 | Critic loss: 8.32 | Entropy loss: -0.0007  | Total Loss: 8.19 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1162/175267  | Episode Reward: 7.5  | Average Reward 6.66  | Actor loss: 0.12 | Critic loss: 4.95 | Entropy loss: -0.0018  | Total Loss: 5.07 | Total Steps: 61\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1163/175267  | Episode Reward: 10.0  | Average Reward 6.68  | Actor loss: 0.01 | Critic loss: 0.57 | Entropy loss: -0.0000  | Total Loss: 0.58 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1164/175267  | Episode Reward: 0.0  | Average Reward 6.63  | Actor loss: -0.35 | Critic loss: 7.55 | Entropy loss: -0.0053  | Total Loss: 7.19 | Total Steps: 82\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1165/175267  | Episode Reward: 10.0  | Average Reward 6.71  | Actor loss: 0.15 | Critic loss: 4.61 | Entropy loss: -0.0015  | Total Loss: 4.77 | Total Steps: 30\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1166/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: 0.11 | Critic loss: 3.73 | Entropy loss: -0.0011  | Total Loss: 3.84 | Total Steps: 37\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1167/175267  | Episode Reward: 7.5  | Average Reward 6.71  | Actor loss: 0.06 | Critic loss: 5.49 | Entropy loss: -0.0005  | Total Loss: 5.55 | Total Steps: 32\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1168/175267  | Episode Reward: 10.0  | Average Reward 6.71  | Actor loss: 0.03 | Critic loss: 3.32 | Entropy loss: -0.0005  | Total Loss: 3.34 | Total Steps: 30\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1169/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: 0.07 | Critic loss: 3.97 | Entropy loss: -0.0007  | Total Loss: 4.04 | Total Steps: 32\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1170/175267  | Episode Reward: 10.0  | Average Reward 6.79  | Actor loss: 0.12 | Critic loss: 3.71 | Entropy loss: -0.0005  | Total Loss: 3.83 | Total Steps: 30\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1171/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: 0.09 | Critic loss: 4.58 | Entropy loss: -0.0008  | Total Loss: 4.67 | Total Steps: 36\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1172/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.13 | Critic loss: 8.15 | Entropy loss: -0.0005  | Total Loss: 8.28 | Total Steps: 31\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1173/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.16 | Critic loss: 14.32 | Entropy loss: -0.0001  | Total Loss: 14.48 | Total Steps: 6\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1174/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.04 | Critic loss: 8.83 | Entropy loss: -0.0000  | Total Loss: 8.87 | Total Steps: 6\n",
      "---red sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1175/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.09 | Critic loss: 3.24 | Entropy loss: -0.0009  | Total Loss: 3.32 | Total Steps: 7\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1176/175267  | Episode Reward: 10.0  | Average Reward 7.31  | Actor loss: 0.00 | Critic loss: 0.66 | Entropy loss: -0.0000  | Total Loss: 0.67 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1177/175267  | Episode Reward: 5.0  | Average Reward 7.26  | Actor loss: -0.03 | Critic loss: 7.98 | Entropy loss: -0.0003  | Total Loss: 7.95 | Total Steps: 43\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1178/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.02 | Critic loss: 0.37 | Entropy loss: -0.0001  | Total Loss: 0.39 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1179/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.01 | Critic loss: 1.04 | Entropy loss: -0.0000  | Total Loss: 1.05 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1180/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.20 | Critic loss: 2.16 | Entropy loss: -0.0021  | Total Loss: 2.37 | Total Steps: 49\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1181/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: 0.22 | Critic loss: 3.78 | Entropy loss: -0.0025  | Total Loss: 4.00 | Total Steps: 33\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1182/175267  | Episode Reward: 5.0  | Average Reward 7.27  | Actor loss: -0.04 | Critic loss: 7.99 | Entropy loss: -0.0004  | Total Loss: 7.95 | Total Steps: 49\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1183/175267  | Episode Reward: 7.5  | Average Reward 7.25  | Actor loss: -0.04 | Critic loss: 2.36 | Entropy loss: -0.0009  | Total Loss: 2.33 | Total Steps: 42\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1184/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.00 | Critic loss: 4.62 | Entropy loss: -0.0003  | Total Loss: 4.63 | Total Steps: 30\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1185/175267  | Episode Reward: 10.0  | Average Reward 7.37  | Actor loss: -0.45 | Critic loss: 4.86 | Entropy loss: -0.0045  | Total Loss: 4.41 | Total Steps: 38\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1186/175267  | Episode Reward: 2.5  | Average Reward 7.29  | Actor loss: -0.67 | Critic loss: 10.45 | Entropy loss: -0.0039  | Total Loss: 9.77 | Total Steps: 53\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1187/175267  | Episode Reward: 6.5  | Average Reward 7.29  | Actor loss: -0.18 | Critic loss: 6.71 | Entropy loss: -0.0015  | Total Loss: 6.53 | Total Steps: 48\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1188/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: 0.15 | Critic loss: 6.18 | Entropy loss: -0.0005  | Total Loss: 6.32 | Total Steps: 29\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1189/175267  | Episode Reward: 2.5  | Average Reward 7.27  | Actor loss: -0.07 | Critic loss: 7.66 | Entropy loss: -0.0016  | Total Loss: 7.59 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1190/175267  | Episode Reward: 5.0  | Average Reward 7.22  | Actor loss: -0.25 | Critic loss: 3.98 | Entropy loss: -0.0070  | Total Loss: 3.72 | Total Steps: 158\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1191/175267  | Episode Reward: 7.5  | Average Reward 7.20  | Actor loss: 0.03 | Critic loss: 7.02 | Entropy loss: -0.0002  | Total Loss: 7.04 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1192/175267  | Episode Reward: 2.5  | Average Reward 7.17  | Actor loss: -0.35 | Critic loss: 10.98 | Entropy loss: -0.0028  | Total Loss: 10.63 | Total Steps: 92\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1193/175267  | Episode Reward: 0.0  | Average Reward 7.09  | Actor loss: -0.16 | Critic loss: 13.37 | Entropy loss: -0.0011  | Total Loss: 13.20 | Total Steps: 82\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1194/175267  | Episode Reward: 5.0  | Average Reward 7.07  | Actor loss: -0.11 | Critic loss: 3.54 | Entropy loss: -0.0028  | Total Loss: 3.43 | Total Steps: 44\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1195/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.02 | Critic loss: 3.45 | Entropy loss: -0.0000  | Total Loss: 3.47 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1196/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.12 | Critic loss: 4.97 | Entropy loss: -0.0010  | Total Loss: 5.09 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1197/175267  | Episode Reward: 5.0  | Average Reward 7.02  | Actor loss: -0.06 | Critic loss: 7.82 | Entropy loss: -0.0015  | Total Loss: 7.76 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1198/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.15 | Critic loss: 4.22 | Entropy loss: -0.0005  | Total Loss: 4.38 | Total Steps: 36\n",
      "---black cube---\n",
      "Step: 250\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1199/175267  | Episode Reward: 5.5  | Average Reward 7.10  | Actor loss: -0.18 | Critic loss: 3.20 | Entropy loss: -0.0080  | Total Loss: 3.02 | Total Steps: 488\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1200/175267  | Episode Reward: 10.0  | Average Reward 7.10  | Actor loss: -0.07 | Critic loss: 2.71 | Entropy loss: -0.0011  | Total Loss: 2.64 | Total Steps: 47\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1201/175267  | Episode Reward: 7.5  | Average Reward 7.10  | Actor loss: -0.31 | Critic loss: 2.21 | Entropy loss: -0.0037  | Total Loss: 1.89 | Total Steps: 49\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1202/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.03 | Critic loss: 2.05 | Entropy loss: -0.0000  | Total Loss: 2.07 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1203/175267  | Episode Reward: 2.5  | Average Reward 7.10  | Actor loss: -0.16 | Critic loss: 5.10 | Entropy loss: -0.0018  | Total Loss: 4.93 | Total Steps: 53\n",
      "---blue cube---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1204/175267  | Episode Reward: 5.0  | Average Reward 7.10  | Actor loss: -0.13 | Critic loss: 9.37 | Entropy loss: -0.0048  | Total Loss: 9.23 | Total Steps: 55\n",
      "---black prism---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1205/175267  | Episode Reward: 4.0  | Average Reward 7.07  | Actor loss: -0.20 | Critic loss: 2.27 | Entropy loss: -0.0090  | Total Loss: 2.06 | Total Steps: 225\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1206/175267  | Episode Reward: 5.0  | Average Reward 7.01  | Actor loss: 0.58 | Critic loss: 7.84 | Entropy loss: -0.0036  | Total Loss: 8.42 | Total Steps: 54\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1207/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 1.36 | Entropy loss: -0.0000  | Total Loss: 1.37 | Total Steps: 6\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1208/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 2.11 | Entropy loss: -0.0000  | Total Loss: 2.11 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1209/175267  | Episode Reward: 10.0  | Average Reward 7.09  | Actor loss: 0.01 | Critic loss: 1.84 | Entropy loss: -0.0000  | Total Loss: 1.85 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1210/175267  | Episode Reward: 2.5  | Average Reward 7.01  | Actor loss: -0.14 | Critic loss: 9.21 | Entropy loss: -0.0017  | Total Loss: 9.07 | Total Steps: 55\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1211/175267  | Episode Reward: 7.5  | Average Reward 6.99  | Actor loss: 0.03 | Critic loss: 6.83 | Entropy loss: -0.0001  | Total Loss: 6.85 | Total Steps: 29\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1212/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 1.51 | Entropy loss: -0.0000  | Total Loss: 1.51 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1213/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: 0.07 | Critic loss: 4.26 | Entropy loss: -0.0021  | Total Loss: 4.32 | Total Steps: 44\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1214/175267  | Episode Reward: 10.0  | Average Reward 7.01  | Actor loss: 0.16 | Critic loss: 3.76 | Entropy loss: -0.0008  | Total Loss: 3.92 | Total Steps: 7\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1215/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.01 | Critic loss: 0.23 | Entropy loss: -0.0002  | Total Loss: 0.24 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1216/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.04 | Critic loss: 3.87 | Entropy loss: -0.0002  | Total Loss: 3.91 | Total Steps: 31\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1217/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: -0.13 | Critic loss: 3.67 | Entropy loss: -0.0031  | Total Loss: 3.53 | Total Steps: 55\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1218/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: -0.09 | Critic loss: 3.43 | Entropy loss: -0.0030  | Total Loss: 3.33 | Total Steps: 47\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1219/175267  | Episode Reward: 7.5  | Average Reward 7.14  | Actor loss: -0.14 | Critic loss: 2.78 | Entropy loss: -0.0019  | Total Loss: 2.64 | Total Steps: 53\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1220/175267  | Episode Reward: 7.5  | Average Reward 7.14  | Actor loss: 0.14 | Critic loss: 8.27 | Entropy loss: -0.0004  | Total Loss: 8.40 | Total Steps: 29\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1221/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.01 | Critic loss: 3.16 | Entropy loss: -0.0000  | Total Loss: 3.18 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1222/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.01 | Critic loss: 1.09 | Entropy loss: -0.0000  | Total Loss: 1.10 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1223/175267  | Episode Reward: 7.5  | Average Reward 7.19  | Actor loss: -0.13 | Critic loss: 3.66 | Entropy loss: -0.0027  | Total Loss: 3.53 | Total Steps: 66\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1224/175267  | Episode Reward: 10.0  | Average Reward 7.34  | Actor loss: 0.01 | Critic loss: 1.12 | Entropy loss: -0.0000  | Total Loss: 1.12 | Total Steps: 6\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1225/175267  | Episode Reward: 10.0  | Average Reward 7.37  | Actor loss: 0.31 | Critic loss: 0.19 | Entropy loss: -0.0012  | Total Loss: 0.50 | Total Steps: 7\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1226/175267  | Episode Reward: 10.0  | Average Reward 7.42  | Actor loss: 0.03 | Critic loss: 2.94 | Entropy loss: -0.0000  | Total Loss: 2.96 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1227/175267  | Episode Reward: 7.5  | Average Reward 7.44  | Actor loss: -0.77 | Critic loss: 7.94 | Entropy loss: -0.0023  | Total Loss: 7.17 | Total Steps: 30\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1228/175267  | Episode Reward: 10.0  | Average Reward 7.49  | Actor loss: 0.01 | Critic loss: 0.67 | Entropy loss: -0.0000  | Total Loss: 0.68 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1229/175267  | Episode Reward: 5.0  | Average Reward 7.44  | Actor loss: 0.07 | Critic loss: 1.92 | Entropy loss: -0.0022  | Total Loss: 1.99 | Total Steps: 35\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1230/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.06 | Critic loss: 0.58 | Entropy loss: -0.0002  | Total Loss: 0.64 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1231/175267  | Episode Reward: 5.0  | Average Reward 7.44  | Actor loss: -0.10 | Critic loss: 7.38 | Entropy loss: -0.0009  | Total Loss: 7.28 | Total Steps: 54\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1232/175267  | Episode Reward: 7.5  | Average Reward 7.44  | Actor loss: 0.03 | Critic loss: 6.53 | Entropy loss: -0.0003  | Total Loss: 6.57 | Total Steps: 34\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1233/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.01 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.04 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1234/175267  | Episode Reward: 10.0  | Average Reward 7.51  | Actor loss: 0.43 | Critic loss: 2.11 | Entropy loss: -0.0008  | Total Loss: 2.55 | Total Steps: 7\n",
      "---red cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1235/175267  | Episode Reward: 7.5  | Average Reward 7.49  | Actor loss: -0.12 | Critic loss: 6.74 | Entropy loss: -0.0007  | Total Loss: 6.62 | Total Steps: 52\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1236/175267  | Episode Reward: 7.5  | Average Reward 7.49  | Actor loss: -0.01 | Critic loss: 6.19 | Entropy loss: -0.0006  | Total Loss: 6.18 | Total Steps: 43\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1237/175267  | Episode Reward: 7.5  | Average Reward 7.49  | Actor loss: 0.30 | Critic loss: 3.10 | Entropy loss: -0.0035  | Total Loss: 3.40 | Total Steps: 32\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1238/175267  | Episode Reward: 10.0  | Average Reward 7.51  | Actor loss: 0.01 | Critic loss: 2.32 | Entropy loss: -0.0000  | Total Loss: 2.33 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1239/175267  | Episode Reward: 6.5  | Average Reward 7.55  | Actor loss: -1.14 | Critic loss: 7.83 | Entropy loss: -0.0054  | Total Loss: 6.69 | Total Steps: 55\n",
      "---black cylinder---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1240/175267  | Episode Reward: 4.0  | Average Reward 7.52  | Actor loss: -0.00 | Critic loss: 1.85 | Entropy loss: -0.0003  | Total Loss: 1.84 | Total Steps: 265\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1241/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.01 | Critic loss: 1.26 | Entropy loss: -0.0000  | Total Loss: 1.28 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1242/175267  | Episode Reward: 5.0  | Average Reward 7.50  | Actor loss: -0.87 | Critic loss: 11.39 | Entropy loss: -0.0038  | Total Loss: 10.52 | Total Steps: 83\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1243/175267  | Episode Reward: 7.5  | Average Reward 7.48  | Actor loss: -0.08 | Critic loss: 4.19 | Entropy loss: -0.0029  | Total Loss: 4.10 | Total Steps: 54\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1244/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.00 | Critic loss: 0.13 | Entropy loss: -0.0001  | Total Loss: 0.13 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1245/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.39 | Critic loss: 5.34 | Entropy loss: -0.0040  | Total Loss: 5.72 | Total Steps: 33\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1246/175267  | Episode Reward: -1.0  | Average Reward 7.47  | Actor loss: -0.41 | Critic loss: 9.14 | Entropy loss: -0.0041  | Total Loss: 8.72 | Total Steps: 107\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1247/175267  | Episode Reward: 10.0  | Average Reward 7.52  | Actor loss: 0.01 | Critic loss: 0.15 | Entropy loss: -0.0001  | Total Loss: 0.15 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1248/175267  | Episode Reward: 7.5  | Average Reward 7.50  | Actor loss: 0.11 | Critic loss: 3.97 | Entropy loss: -0.0013  | Total Loss: 4.08 | Total Steps: 43\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1249/175267  | Episode Reward: 2.5  | Average Reward 7.42  | Actor loss: -0.27 | Critic loss: 9.26 | Entropy loss: -0.0023  | Total Loss: 8.98 | Total Steps: 54\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1250/175267  | Episode Reward: 5.0  | Average Reward 7.37  | Actor loss: 0.04 | Critic loss: 3.83 | Entropy loss: -0.0010  | Total Loss: 3.86 | Total Steps: 42\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1251/175267  | Episode Reward: 7.5  | Average Reward 7.34  | Actor loss: 0.07 | Critic loss: 4.24 | Entropy loss: -0.0049  | Total Loss: 4.31 | Total Steps: 68\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1252/175267  | Episode Reward: 2.5  | Average Reward 7.32  | Actor loss: -0.03 | Critic loss: 9.63 | Entropy loss: -0.0004  | Total Loss: 9.60 | Total Steps: 52\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1253/175267  | Episode Reward: 5.0  | Average Reward 7.29  | Actor loss: -0.20 | Critic loss: 6.75 | Entropy loss: -0.0031  | Total Loss: 6.55 | Total Steps: 44\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1254/175267  | Episode Reward: 10.0  | Average Reward 7.42  | Actor loss: 0.01 | Critic loss: 0.93 | Entropy loss: -0.0000  | Total Loss: 0.94 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1255/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.04 | Critic loss: 4.56 | Entropy loss: -0.0005  | Total Loss: 4.60 | Total Steps: 39\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1256/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.03 | Critic loss: 5.96 | Entropy loss: -0.0000  | Total Loss: 5.99 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1257/175267  | Episode Reward: 5.0  | Average Reward 7.48  | Actor loss: -0.08 | Critic loss: 6.82 | Entropy loss: -0.0026  | Total Loss: 6.74 | Total Steps: 56\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1258/175267  | Episode Reward: -12.5  | Average Reward 7.25  | Actor loss: -0.83 | Critic loss: 19.74 | Entropy loss: -0.0074  | Total Loss: 18.90 | Total Steps: 131\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1259/175267  | Episode Reward: 10.0  | Average Reward 7.25  | Actor loss: 0.01 | Critic loss: 1.68 | Entropy loss: -0.0000  | Total Loss: 1.69 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1260/175267  | Episode Reward: 10.0  | Average Reward 7.30  | Actor loss: 0.01 | Critic loss: 1.81 | Entropy loss: -0.0000  | Total Loss: 1.82 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1261/175267  | Episode Reward: 5.0  | Average Reward 7.33  | Actor loss: 0.06 | Critic loss: 8.27 | Entropy loss: -0.0011  | Total Loss: 8.33 | Total Steps: 44\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1262/175267  | Episode Reward: 7.5  | Average Reward 7.33  | Actor loss: 0.09 | Critic loss: 6.26 | Entropy loss: -0.0006  | Total Loss: 6.35 | Total Steps: 36\n",
      "---red sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1263/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.02 | Critic loss: 0.89 | Entropy loss: -0.0000  | Total Loss: 0.91 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1264/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.06 | Critic loss: 5.66 | Entropy loss: -0.0003  | Total Loss: 5.72 | Total Steps: 31\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1265/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.12 | Critic loss: 4.08 | Entropy loss: -0.0020  | Total Loss: 4.20 | Total Steps: 32\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1266/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.02 | Critic loss: 1.78 | Entropy loss: -0.0002  | Total Loss: 1.80 | Total Steps: 38\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1267/175267  | Episode Reward: 5.0  | Average Reward 7.43  | Actor loss: -0.01 | Critic loss: 3.98 | Entropy loss: -0.0019  | Total Loss: 3.98 | Total Steps: 44\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1268/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.01 | Critic loss: 1.47 | Entropy loss: -0.0000  | Total Loss: 1.48 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1269/175267  | Episode Reward: 7.5  | Average Reward 7.43  | Actor loss: -0.19 | Critic loss: 5.58 | Entropy loss: -0.0011  | Total Loss: 5.39 | Total Steps: 53\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1270/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.04 | Critic loss: 3.03 | Entropy loss: -0.0003  | Total Loss: 3.07 | Total Steps: 30\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1271/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.04 | Critic loss: 4.15 | Entropy loss: -0.0002  | Total Loss: 4.19 | Total Steps: 29\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1272/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.02 | Critic loss: 4.36 | Entropy loss: -0.0002  | Total Loss: 4.38 | Total Steps: 39\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1273/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: -0.29 | Critic loss: 3.29 | Entropy loss: -0.0019  | Total Loss: 3.00 | Total Steps: 31\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1274/175267  | Episode Reward: 5.0  | Average Reward 7.43  | Actor loss: -0.14 | Critic loss: 7.84 | Entropy loss: -0.0012  | Total Loss: 7.70 | Total Steps: 43\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1275/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.01 | Critic loss: 1.75 | Entropy loss: -0.0000  | Total Loss: 1.76 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1276/175267  | Episode Reward: 10.0  | Average Reward 7.43  | Actor loss: 0.01 | Critic loss: 0.62 | Entropy loss: -0.0000  | Total Loss: 0.62 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1277/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.07 | Critic loss: 3.30 | Entropy loss: -0.0023  | Total Loss: 3.37 | Total Steps: 45\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1278/175267  | Episode Reward: 10.0  | Average Reward 7.48  | Actor loss: 0.01 | Critic loss: 1.17 | Entropy loss: -0.0000  | Total Loss: 1.17 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1279/175267  | Episode Reward: 5.0  | Average Reward 7.43  | Actor loss: -0.12 | Critic loss: 8.04 | Entropy loss: -0.0015  | Total Loss: 7.91 | Total Steps: 44\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1280/175267  | Episode Reward: 4.0  | Average Reward 7.39  | Actor loss: -0.38 | Critic loss: 2.32 | Entropy loss: -0.0042  | Total Loss: 1.93 | Total Steps: 33\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1281/175267  | Episode Reward: 1.5  | Average Reward 7.33  | Actor loss: -0.27 | Critic loss: 11.28 | Entropy loss: -0.0052  | Total Loss: 11.00 | Total Steps: 63\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1282/175267  | Episode Reward: 10.0  | Average Reward 7.38  | Actor loss: 0.01 | Critic loss: 1.07 | Entropy loss: -0.0000  | Total Loss: 1.07 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1283/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: -0.10 | Critic loss: 0.43 | Entropy loss: -0.0006  | Total Loss: 0.33 | Total Steps: 7\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1284/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: -0.17 | Critic loss: 2.47 | Entropy loss: -0.0088  | Total Loss: 2.29 | Total Steps: 190\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1285/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0000  | Total Loss: 0.96 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1286/175267  | Episode Reward: 10.0  | Average Reward 7.49  | Actor loss: 0.00 | Critic loss: 0.29 | Entropy loss: -0.0000  | Total Loss: 0.30 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1287/175267  | Episode Reward: 5.0  | Average Reward 7.47  | Actor loss: -0.13 | Critic loss: 7.21 | Entropy loss: -0.0010  | Total Loss: 7.08 | Total Steps: 47\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1288/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.06 | Critic loss: 5.66 | Entropy loss: -0.0002  | Total Loss: 5.72 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1289/175267  | Episode Reward: 10.0  | Average Reward 7.57  | Actor loss: -0.03 | Critic loss: 2.60 | Entropy loss: -0.0005  | Total Loss: 2.57 | Total Steps: 53\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1290/175267  | Episode Reward: 10.0  | Average Reward 7.62  | Actor loss: -0.30 | Critic loss: 3.08 | Entropy loss: -0.0018  | Total Loss: 2.78 | Total Steps: 31\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1291/175267  | Episode Reward: 4.0  | Average Reward 7.58  | Actor loss: -0.82 | Critic loss: 4.72 | Entropy loss: -0.0078  | Total Loss: 3.89 | Total Steps: 58\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1292/175267  | Episode Reward: 7.5  | Average Reward 7.63  | Actor loss: 0.16 | Critic loss: 7.93 | Entropy loss: -0.0007  | Total Loss: 8.08 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1293/175267  | Episode Reward: 5.0  | Average Reward 7.68  | Actor loss: -0.01 | Critic loss: 6.85 | Entropy loss: -0.0017  | Total Loss: 6.84 | Total Steps: 52\n",
      "---red sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1294/175267  | Episode Reward: 2.5  | Average Reward 7.66  | Actor loss: -0.33 | Critic loss: 6.73 | Entropy loss: -0.0042  | Total Loss: 6.40 | Total Steps: 59\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1295/175267  | Episode Reward: 7.5  | Average Reward 7.63  | Actor loss: 0.14 | Critic loss: 3.35 | Entropy loss: -0.0009  | Total Loss: 3.49 | Total Steps: 29\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1296/175267  | Episode Reward: 5.0  | Average Reward 7.58  | Actor loss: 0.05 | Critic loss: 7.88 | Entropy loss: -0.0014  | Total Loss: 7.92 | Total Steps: 43\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1297/175267  | Episode Reward: 10.0  | Average Reward 7.63  | Actor loss: 0.01 | Critic loss: 0.84 | Entropy loss: -0.0000  | Total Loss: 0.84 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1298/175267  | Episode Reward: 10.0  | Average Reward 7.63  | Actor loss: 0.01 | Critic loss: 1.41 | Entropy loss: -0.0000  | Total Loss: 1.42 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1299/175267  | Episode Reward: 10.0  | Average Reward 7.68  | Actor loss: 0.02 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.05 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1300/175267  | Episode Reward: 5.0  | Average Reward 7.63  | Actor loss: -0.15 | Critic loss: 9.28 | Entropy loss: -0.0018  | Total Loss: 9.13 | Total Steps: 85\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1301/175267  | Episode Reward: 10.0  | Average Reward 7.66  | Actor loss: 0.00 | Critic loss: 0.74 | Entropy loss: -0.0000  | Total Loss: 0.75 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1302/175267  | Episode Reward: 7.5  | Average Reward 7.63  | Actor loss: -0.01 | Critic loss: 6.69 | Entropy loss: -0.0002  | Total Loss: 6.68 | Total Steps: 29\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1303/175267  | Episode Reward: 10.0  | Average Reward 7.71  | Actor loss: 0.02 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.01 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1304/175267  | Episode Reward: 10.0  | Average Reward 7.75  | Actor loss: 0.01 | Critic loss: 0.96 | Entropy loss: -0.0000  | Total Loss: 0.96 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1305/175267  | Episode Reward: 10.0  | Average Reward 7.82  | Actor loss: 0.31 | Critic loss: 4.54 | Entropy loss: -0.0009  | Total Loss: 4.85 | Total Steps: 31\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1306/175267  | Episode Reward: 5.0  | Average Reward 7.82  | Actor loss: -0.10 | Critic loss: 8.31 | Entropy loss: -0.0009  | Total Loss: 8.21 | Total Steps: 43\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1307/175267  | Episode Reward: 2.5  | Average Reward 7.74  | Actor loss: -0.39 | Critic loss: 11.25 | Entropy loss: -0.0016  | Total Loss: 10.86 | Total Steps: 53\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1308/175267  | Episode Reward: 5.0  | Average Reward 7.69  | Actor loss: 0.01 | Critic loss: 7.49 | Entropy loss: -0.0020  | Total Loss: 7.50 | Total Steps: 54\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1309/175267  | Episode Reward: 10.0  | Average Reward 7.69  | Actor loss: 0.05 | Critic loss: 5.15 | Entropy loss: -0.0002  | Total Loss: 5.19 | Total Steps: 29\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1310/175267  | Episode Reward: 5.0  | Average Reward 7.71  | Actor loss: -0.02 | Critic loss: 7.60 | Entropy loss: -0.0003  | Total Loss: 7.58 | Total Steps: 43\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1311/175267  | Episode Reward: 5.0  | Average Reward 7.69  | Actor loss: -0.23 | Critic loss: 3.03 | Entropy loss: -0.0017  | Total Loss: 2.80 | Total Steps: 41\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1312/175267  | Episode Reward: 10.0  | Average Reward 7.69  | Actor loss: 0.01 | Critic loss: 1.94 | Entropy loss: -0.0000  | Total Loss: 1.95 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1313/175267  | Episode Reward: 10.0  | Average Reward 7.74  | Actor loss: 0.03 | Critic loss: 4.44 | Entropy loss: -0.0002  | Total Loss: 4.47 | Total Steps: 34\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1314/175267  | Episode Reward: 10.0  | Average Reward 7.74  | Actor loss: 0.01 | Critic loss: 1.14 | Entropy loss: -0.0000  | Total Loss: 1.14 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1315/175267  | Episode Reward: 4.0  | Average Reward 7.68  | Actor loss: -0.03 | Critic loss: 5.84 | Entropy loss: -0.0044  | Total Loss: 5.80 | Total Steps: 52\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1316/175267  | Episode Reward: 5.0  | Average Reward 7.63  | Actor loss: -0.06 | Critic loss: 4.13 | Entropy loss: -0.0004  | Total Loss: 4.07 | Total Steps: 43\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1317/175267  | Episode Reward: 2.5  | Average Reward 7.55  | Actor loss: -0.14 | Critic loss: 7.46 | Entropy loss: -0.0007  | Total Loss: 7.31 | Total Steps: 52\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1318/175267  | Episode Reward: 5.0  | Average Reward 7.53  | Actor loss: -0.33 | Critic loss: 5.60 | Entropy loss: -0.0022  | Total Loss: 5.26 | Total Steps: 52\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1319/175267  | Episode Reward: 7.5  | Average Reward 7.53  | Actor loss: -0.04 | Critic loss: 2.62 | Entropy loss: -0.0005  | Total Loss: 2.58 | Total Steps: 49\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1320/175267  | Episode Reward: 2.5  | Average Reward 7.48  | Actor loss: -0.06 | Critic loss: 7.62 | Entropy loss: -0.0015  | Total Loss: 7.56 | Total Steps: 53\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1321/175267  | Episode Reward: 5.0  | Average Reward 7.43  | Actor loss: 0.01 | Critic loss: 4.35 | Entropy loss: -0.0012  | Total Loss: 4.36 | Total Steps: 42\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1322/175267  | Episode Reward: 7.5  | Average Reward 7.41  | Actor loss: 0.59 | Critic loss: 4.79 | Entropy loss: -0.0065  | Total Loss: 5.38 | Total Steps: 55\n",
      "---red sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1323/175267  | Episode Reward: 7.5  | Average Reward 7.41  | Actor loss: 0.18 | Critic loss: 2.46 | Entropy loss: -0.0020  | Total Loss: 2.64 | Total Steps: 40\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1324/175267  | Episode Reward: 2.5  | Average Reward 7.33  | Actor loss: -0.13 | Critic loss: 4.39 | Entropy loss: -0.0010  | Total Loss: 4.27 | Total Steps: 43\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1325/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.23 | Critic loss: 3.20 | Entropy loss: -0.0015  | Total Loss: 3.43 | Total Steps: 31\n",
      "---blue sphere---\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1326/175267  | Episode Reward: 6.5  | Average Reward 7.29  | Actor loss: -0.23 | Critic loss: 4.23 | Entropy loss: -0.0033  | Total Loss: 4.00 | Total Steps: 58\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1327/175267  | Episode Reward: 10.0  | Average Reward 7.32  | Actor loss: 0.01 | Critic loss: 1.68 | Entropy loss: -0.0000  | Total Loss: 1.69 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1328/175267  | Episode Reward: 10.0  | Average Reward 7.32  | Actor loss: 0.05 | Critic loss: 2.42 | Entropy loss: -0.0001  | Total Loss: 2.47 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1329/175267  | Episode Reward: 7.5  | Average Reward 7.34  | Actor loss: -0.03 | Critic loss: 4.99 | Entropy loss: -0.0002  | Total Loss: 4.97 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1330/175267  | Episode Reward: -10.0  | Average Reward 7.14  | Actor loss: -0.40 | Critic loss: 14.77 | Entropy loss: -0.0047  | Total Loss: 14.37 | Total Steps: 182\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1331/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: 0.04 | Critic loss: 2.94 | Entropy loss: -0.0017  | Total Loss: 2.98 | Total Steps: 35\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1332/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.11 | Critic loss: 5.53 | Entropy loss: -0.0003  | Total Loss: 5.64 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1333/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: 0.03 | Critic loss: 5.82 | Entropy loss: -0.0002  | Total Loss: 5.86 | Total Steps: 34\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1334/175267  | Episode Reward: 7.5  | Average Reward 7.14  | Actor loss: -0.53 | Critic loss: 3.97 | Entropy loss: -0.0029  | Total Loss: 3.45 | Total Steps: 60\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1335/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.03 | Critic loss: 4.38 | Entropy loss: -0.0000  | Total Loss: 4.41 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1336/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.00 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.13 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1337/175267  | Episode Reward: 10.0  | Average Reward 7.22  | Actor loss: -0.01 | Critic loss: 0.69 | Entropy loss: -0.0008  | Total Loss: 0.68 | Total Steps: 7\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1338/175267  | Episode Reward: 10.0  | Average Reward 7.22  | Actor loss: 0.02 | Critic loss: 4.28 | Entropy loss: -0.0000  | Total Loss: 4.30 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1339/175267  | Episode Reward: 10.0  | Average Reward 7.25  | Actor loss: 0.04 | Critic loss: 5.59 | Entropy loss: -0.0004  | Total Loss: 5.63 | Total Steps: 32\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1340/175267  | Episode Reward: 7.5  | Average Reward 7.29  | Actor loss: -1.60 | Critic loss: 5.82 | Entropy loss: -0.0043  | Total Loss: 4.22 | Total Steps: 42\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1341/175267  | Episode Reward: 7.5  | Average Reward 7.26  | Actor loss: -0.02 | Critic loss: 4.02 | Entropy loss: -0.0008  | Total Loss: 4.00 | Total Steps: 30\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1342/175267  | Episode Reward: 10.0  | Average Reward 7.32  | Actor loss: 0.07 | Critic loss: 4.72 | Entropy loss: -0.0005  | Total Loss: 4.79 | Total Steps: 31\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1343/175267  | Episode Reward: 7.5  | Average Reward 7.32  | Actor loss: 0.18 | Critic loss: 2.24 | Entropy loss: -0.0017  | Total Loss: 2.41 | Total Steps: 31\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1344/175267  | Episode Reward: 10.0  | Average Reward 7.32  | Actor loss: 0.13 | Critic loss: 3.75 | Entropy loss: -0.0029  | Total Loss: 3.88 | Total Steps: 50\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1345/175267  | Episode Reward: 5.0  | Average Reward 7.26  | Actor loss: -0.07 | Critic loss: 7.22 | Entropy loss: -0.0021  | Total Loss: 7.15 | Total Steps: 77\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1346/175267  | Episode Reward: 10.0  | Average Reward 7.38  | Actor loss: 0.02 | Critic loss: 4.14 | Entropy loss: -0.0001  | Total Loss: 4.16 | Total Steps: 32\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1347/175267  | Episode Reward: 6.5  | Average Reward 7.34  | Actor loss: -0.16 | Critic loss: 3.26 | Entropy loss: -0.0026  | Total Loss: 3.10 | Total Steps: 55\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1348/175267  | Episode Reward: 5.0  | Average Reward 7.32  | Actor loss: -0.03 | Critic loss: 7.37 | Entropy loss: -0.0003  | Total Loss: 7.34 | Total Steps: 52\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1349/175267  | Episode Reward: 10.0  | Average Reward 7.39  | Actor loss: -0.07 | Critic loss: 4.07 | Entropy loss: -0.0020  | Total Loss: 4.00 | Total Steps: 54\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1350/175267  | Episode Reward: 10.0  | Average Reward 7.44  | Actor loss: 0.30 | Critic loss: 4.98 | Entropy loss: -0.0038  | Total Loss: 5.28 | Total Steps: 32\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1351/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.06 | Critic loss: 0.60 | Entropy loss: -0.0002  | Total Loss: 0.66 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1352/175267  | Episode Reward: 10.0  | Average Reward 7.54  | Actor loss: 0.04 | Critic loss: 4.41 | Entropy loss: -0.0002  | Total Loss: 4.45 | Total Steps: 31\n",
      "---blue capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1353/175267  | Episode Reward: 2.5  | Average Reward 7.51  | Actor loss: -0.32 | Critic loss: 6.88 | Entropy loss: -0.0024  | Total Loss: 6.56 | Total Steps: 44\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1354/175267  | Episode Reward: 7.5  | Average Reward 7.49  | Actor loss: 0.26 | Critic loss: 7.03 | Entropy loss: -0.0009  | Total Loss: 7.29 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1355/175267  | Episode Reward: 5.0  | Average Reward 7.44  | Actor loss: -0.00 | Critic loss: 4.55 | Entropy loss: -0.0011  | Total Loss: 4.54 | Total Steps: 43\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1356/175267  | Episode Reward: 10.0  | Average Reward 7.44  | Actor loss: 0.00 | Critic loss: 1.03 | Entropy loss: -0.0000  | Total Loss: 1.04 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1357/175267  | Episode Reward: 7.5  | Average Reward 7.46  | Actor loss: -0.06 | Critic loss: 6.72 | Entropy loss: -0.0019  | Total Loss: 6.66 | Total Steps: 56\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1358/175267  | Episode Reward: 10.0  | Average Reward 7.69  | Actor loss: 0.01 | Critic loss: 1.81 | Entropy loss: -0.0000  | Total Loss: 1.82 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1359/175267  | Episode Reward: 0.0  | Average Reward 7.59  | Actor loss: -0.15 | Critic loss: 9.38 | Entropy loss: -0.0017  | Total Loss: 9.23 | Total Steps: 69\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1360/175267  | Episode Reward: 7.5  | Average Reward 7.57  | Actor loss: 0.13 | Critic loss: 2.91 | Entropy loss: -0.0008  | Total Loss: 3.04 | Total Steps: 29\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1361/175267  | Episode Reward: 10.0  | Average Reward 7.62  | Actor loss: 0.01 | Critic loss: 1.46 | Entropy loss: -0.0000  | Total Loss: 1.47 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1362/175267  | Episode Reward: 5.0  | Average Reward 7.59  | Actor loss: -0.01 | Critic loss: 7.16 | Entropy loss: -0.0003  | Total Loss: 7.15 | Total Steps: 42\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1363/175267  | Episode Reward: 7.5  | Average Reward 7.57  | Actor loss: 0.05 | Critic loss: 4.71 | Entropy loss: -0.0003  | Total Loss: 4.75 | Total Steps: 34\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1364/175267  | Episode Reward: 7.5  | Average Reward 7.54  | Actor loss: 0.04 | Critic loss: 4.24 | Entropy loss: -0.0002  | Total Loss: 4.28 | Total Steps: 29\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1365/175267  | Episode Reward: -2.5  | Average Reward 7.42  | Actor loss: -0.41 | Critic loss: 18.98 | Entropy loss: -0.0025  | Total Loss: 18.57 | Total Steps: 90\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1366/175267  | Episode Reward: 10.0  | Average Reward 7.42  | Actor loss: 0.01 | Critic loss: 0.68 | Entropy loss: -0.0000  | Total Loss: 0.69 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1367/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.03 | Critic loss: 2.00 | Entropy loss: -0.0011  | Total Loss: 2.03 | Total Steps: 300\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1368/175267  | Episode Reward: -7.5  | Average Reward 7.29  | Actor loss: -1.16 | Critic loss: 17.65 | Entropy loss: -0.0073  | Total Loss: 16.49 | Total Steps: 112\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1369/175267  | Episode Reward: 10.0  | Average Reward 7.32  | Actor loss: 0.04 | Critic loss: 5.18 | Entropy loss: -0.0002  | Total Loss: 5.22 | Total Steps: 29\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1370/175267  | Episode Reward: 5.0  | Average Reward 7.26  | Actor loss: -0.06 | Critic loss: 6.79 | Entropy loss: -0.0008  | Total Loss: 6.73 | Total Steps: 53\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1371/175267  | Episode Reward: 7.5  | Average Reward 7.24  | Actor loss: 0.18 | Critic loss: 3.93 | Entropy loss: -0.0010  | Total Loss: 4.11 | Total Steps: 30\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1372/175267  | Episode Reward: 7.5  | Average Reward 7.21  | Actor loss: 0.13 | Critic loss: 5.94 | Entropy loss: -0.0004  | Total Loss: 6.07 | Total Steps: 29\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1373/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.01 | Critic loss: 0.31 | Entropy loss: -0.0001  | Total Loss: 0.32 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1374/175267  | Episode Reward: 2.5  | Average Reward 7.19  | Actor loss: -0.03 | Critic loss: 9.82 | Entropy loss: -0.0016  | Total Loss: 9.79 | Total Steps: 53\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1375/175267  | Episode Reward: 0.0  | Average Reward 7.09  | Actor loss: -0.23 | Critic loss: 9.05 | Entropy loss: -0.0016  | Total Loss: 8.82 | Total Steps: 55\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1376/175267  | Episode Reward: 5.0  | Average Reward 7.04  | Actor loss: 0.51 | Critic loss: 3.81 | Entropy loss: -0.0031  | Total Loss: 4.32 | Total Steps: 44\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1377/175267  | Episode Reward: 7.5  | Average Reward 7.01  | Actor loss: 0.12 | Critic loss: 3.05 | Entropy loss: -0.0014  | Total Loss: 3.17 | Total Steps: 47\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1378/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: 0.14 | Critic loss: 4.19 | Entropy loss: -0.0036  | Total Loss: 4.32 | Total Steps: 58\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1379/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: -0.01 | Critic loss: 7.12 | Entropy loss: -0.0003  | Total Loss: 7.11 | Total Steps: 42\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1380/175267  | Episode Reward: 10.0  | Average Reward 7.03  | Actor loss: 0.06 | Critic loss: 4.97 | Entropy loss: -0.0037  | Total Loss: 5.02 | Total Steps: 64\n",
      "---black capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1381/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.02 | Critic loss: 3.87 | Entropy loss: -0.0001  | Total Loss: 3.89 | Total Steps: 31\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1382/175267  | Episode Reward: 4.0  | Average Reward 7.05  | Actor loss: -0.47 | Critic loss: 10.92 | Entropy loss: -0.0058  | Total Loss: 10.44 | Total Steps: 57\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1383/175267  | Episode Reward: 10.0  | Average Reward 7.05  | Actor loss: 0.05 | Critic loss: 4.58 | Entropy loss: -0.0002  | Total Loss: 4.62 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1384/175267  | Episode Reward: 5.0  | Average Reward 7.00  | Actor loss: -0.19 | Critic loss: 2.98 | Entropy loss: -0.0034  | Total Loss: 2.79 | Total Steps: 111\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1385/175267  | Episode Reward: 5.0  | Average Reward 6.95  | Actor loss: -0.02 | Critic loss: 6.61 | Entropy loss: -0.0006  | Total Loss: 6.60 | Total Steps: 42\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1386/175267  | Episode Reward: 7.5  | Average Reward 6.92  | Actor loss: 0.24 | Critic loss: 5.42 | Entropy loss: -0.0011  | Total Loss: 5.66 | Total Steps: 29\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1387/175267  | Episode Reward: 5.0  | Average Reward 6.92  | Actor loss: -0.13 | Critic loss: 4.33 | Entropy loss: -0.0012  | Total Loss: 4.20 | Total Steps: 52\n",
      "---blue prism---\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1388/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: -0.21 | Critic loss: 2.70 | Entropy loss: -0.0105  | Total Loss: 2.48 | Total Steps: 374\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1389/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.01 | Critic loss: 2.83 | Entropy loss: -0.0000  | Total Loss: 2.83 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1390/175267  | Episode Reward: 7.5  | Average Reward 6.90  | Actor loss: 0.03 | Critic loss: 8.12 | Entropy loss: -0.0001  | Total Loss: 8.16 | Total Steps: 29\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1391/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: -0.07 | Critic loss: 4.57 | Entropy loss: -0.0009  | Total Loss: 4.50 | Total Steps: 43\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1392/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: 0.13 | Critic loss: 4.11 | Entropy loss: -0.0013  | Total Loss: 4.24 | Total Steps: 44\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1393/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: -0.03 | Critic loss: 4.10 | Entropy loss: -0.0006  | Total Loss: 4.07 | Total Steps: 30\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1394/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.13 | Critic loss: 5.46 | Entropy loss: -0.0004  | Total Loss: 5.59 | Total Steps: 29\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1395/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 4.87 | Entropy loss: -0.0003  | Total Loss: 4.88 | Total Steps: 43\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1396/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.07 | Critic loss: 5.67 | Entropy loss: -0.0004  | Total Loss: 5.74 | Total Steps: 32\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1397/175267  | Episode Reward: 5.0  | Average Reward 7.04  | Actor loss: -0.01 | Critic loss: 6.45 | Entropy loss: -0.0002  | Total Loss: 6.45 | Total Steps: 43\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1398/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.07 | Critic loss: 5.14 | Entropy loss: -0.0000  | Total Loss: 5.21 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1399/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: 0.00 | Critic loss: 6.93 | Entropy loss: -0.0003  | Total Loss: 6.93 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1400/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.12 | Critic loss: 5.21 | Entropy loss: -0.0003  | Total Loss: 5.32 | Total Steps: 29\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1401/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: -0.03 | Critic loss: 4.49 | Entropy loss: -0.0013  | Total Loss: 4.45 | Total Steps: 41\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1402/175267  | Episode Reward: 7.5  | Average Reward 6.99  | Actor loss: -0.07 | Critic loss: 3.69 | Entropy loss: -0.0008  | Total Loss: 3.62 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1403/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: -0.00 | Critic loss: 2.45 | Entropy loss: -0.0010  | Total Loss: 2.45 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1404/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 2.36 | Entropy loss: -0.0000  | Total Loss: 2.36 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1405/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: -0.05 | Critic loss: 4.63 | Entropy loss: -0.0006  | Total Loss: 4.58 | Total Steps: 30\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1406/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.01 | Critic loss: 2.24 | Entropy loss: -0.0000  | Total Loss: 2.26 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1407/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 2.67 | Entropy loss: -0.0000  | Total Loss: 2.68 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1408/175267  | Episode Reward: 0.0  | Average Reward 7.01  | Actor loss: -0.37 | Critic loss: 12.57 | Entropy loss: -0.0018  | Total Loss: 12.20 | Total Steps: 55\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1409/175267  | Episode Reward: 7.5  | Average Reward 6.99  | Actor loss: 0.19 | Critic loss: 6.99 | Entropy loss: -0.0008  | Total Loss: 7.18 | Total Steps: 32\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1410/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.01 | Critic loss: 2.08 | Entropy loss: -0.0000  | Total Loss: 2.09 | Total Steps: 6\n",
      "---green prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1411/175267  | Episode Reward: 7.5  | Average Reward 7.06  | Actor loss: 0.20 | Critic loss: 5.34 | Entropy loss: -0.0010  | Total Loss: 5.54 | Total Steps: 30\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1412/175267  | Episode Reward: 10.0  | Average Reward 7.06  | Actor loss: 0.01 | Critic loss: 1.88 | Entropy loss: -0.0000  | Total Loss: 1.89 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1413/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: 0.13 | Critic loss: 7.35 | Entropy loss: -0.0010  | Total Loss: 7.48 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1414/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.12 | Critic loss: 4.25 | Entropy loss: -0.0005  | Total Loss: 4.37 | Total Steps: 29\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1415/175267  | Episode Reward: 5.0  | Average Reward 7.04  | Actor loss: -0.57 | Critic loss: 10.28 | Entropy loss: -0.0041  | Total Loss: 9.70 | Total Steps: 58\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1416/175267  | Episode Reward: 7.5  | Average Reward 7.07  | Actor loss: 0.07 | Critic loss: 5.92 | Entropy loss: -0.0006  | Total Loss: 5.99 | Total Steps: 29\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1417/175267  | Episode Reward: 10.0  | Average Reward 7.14  | Actor loss: 0.17 | Critic loss: 3.18 | Entropy loss: -0.0006  | Total Loss: 3.35 | Total Steps: 29\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1418/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: -0.04 | Critic loss: 5.64 | Entropy loss: -0.0005  | Total Loss: 5.60 | Total Steps: 43\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1419/175267  | Episode Reward: -3.5  | Average Reward 7.06  | Actor loss: -0.75 | Critic loss: 20.12 | Entropy loss: -0.0070  | Total Loss: 19.36 | Total Steps: 102\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1420/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.03 | Critic loss: 3.68 | Entropy loss: -0.0001  | Total Loss: 3.72 | Total Steps: 6\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1421/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.00 | Critic loss: 3.22 | Entropy loss: -0.0014  | Total Loss: 3.22 | Total Steps: 91\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1422/175267  | Episode Reward: -1.0  | Average Reward 7.10  | Actor loss: -1.09 | Critic loss: 11.21 | Entropy loss: -0.0051  | Total Loss: 10.11 | Total Steps: 49\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1423/175267  | Episode Reward: -10.0  | Average Reward 6.92  | Actor loss: -0.82 | Critic loss: 21.57 | Entropy loss: -0.0079  | Total Loss: 20.75 | Total Steps: 117\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1424/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.02 | Critic loss: 2.50 | Entropy loss: -0.0000  | Total Loss: 2.52 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1425/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.02 | Critic loss: 2.33 | Entropy loss: -0.0000  | Total Loss: 2.35 | Total Steps: 6\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1426/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: -0.01 | Critic loss: 3.64 | Entropy loss: -0.0003  | Total Loss: 3.64 | Total Steps: 42\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1427/175267  | Episode Reward: 5.0  | Average Reward 6.99  | Actor loss: -0.09 | Critic loss: 7.62 | Entropy loss: -0.0028  | Total Loss: 7.53 | Total Steps: 36\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1428/175267  | Episode Reward: 10.0  | Average Reward 6.99  | Actor loss: 0.08 | Critic loss: 3.84 | Entropy loss: -0.0003  | Total Loss: 3.92 | Total Steps: 29\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1429/175267  | Episode Reward: 2.5  | Average Reward 6.93  | Actor loss: -0.35 | Critic loss: 10.02 | Entropy loss: -0.0026  | Total Loss: 9.67 | Total Steps: 52\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1430/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: -0.47 | Critic loss: 6.52 | Entropy loss: -0.0060  | Total Loss: 6.05 | Total Steps: 92\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1431/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: 0.07 | Critic loss: 4.71 | Entropy loss: -0.0008  | Total Loss: 4.77 | Total Steps: 47\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1432/175267  | Episode Reward: 5.0  | Average Reward 7.06  | Actor loss: -0.13 | Critic loss: 6.04 | Entropy loss: -0.0011  | Total Loss: 5.91 | Total Steps: 47\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1433/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.17 | Critic loss: 6.01 | Entropy loss: -0.0001  | Total Loss: 6.18 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1434/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.02 | Critic loss: 2.80 | Entropy loss: -0.0000  | Total Loss: 2.81 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1435/175267  | Episode Reward: 2.5  | Average Reward 7.04  | Actor loss: -0.07 | Critic loss: 9.72 | Entropy loss: -0.0016  | Total Loss: 9.65 | Total Steps: 70\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1436/175267  | Episode Reward: 0.0  | Average Reward 6.93  | Actor loss: -0.60 | Critic loss: 10.71 | Entropy loss: -0.0055  | Total Loss: 10.10 | Total Steps: 129\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1437/175267  | Episode Reward: 2.5  | Average Reward 6.86  | Actor loss: -0.17 | Critic loss: 5.56 | Entropy loss: -0.0038  | Total Loss: 5.38 | Total Steps: 70\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1438/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.03 | Critic loss: 4.24 | Entropy loss: -0.0004  | Total Loss: 4.27 | Total Steps: 31\n",
      "---red cylinder---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1439/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.05 | Critic loss: 3.93 | Entropy loss: -0.0002  | Total Loss: 3.98 | Total Steps: 29\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1440/175267  | Episode Reward: 5.0  | Average Reward 6.83  | Actor loss: -0.15 | Critic loss: 7.65 | Entropy loss: -0.0018  | Total Loss: 7.50 | Total Steps: 52\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1441/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.02 | Critic loss: 0.94 | Entropy loss: -0.0006  | Total Loss: 0.96 | Total Steps: 7\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1442/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.09 | Critic loss: 4.02 | Entropy loss: -0.0005  | Total Loss: 4.11 | Total Steps: 34\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1443/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.01 | Critic loss: 1.88 | Entropy loss: -0.0000  | Total Loss: 1.89 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1444/175267  | Episode Reward: 10.0  | Average Reward 6.88  | Actor loss: 0.01 | Critic loss: 3.86 | Entropy loss: -0.0000  | Total Loss: 3.87 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1445/175267  | Episode Reward: 7.5  | Average Reward 6.91  | Actor loss: -0.04 | Critic loss: 4.69 | Entropy loss: -0.0006  | Total Loss: 4.66 | Total Steps: 53\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1446/175267  | Episode Reward: 10.0  | Average Reward 6.91  | Actor loss: 0.10 | Critic loss: 3.97 | Entropy loss: -0.0008  | Total Loss: 4.07 | Total Steps: 32\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1447/175267  | Episode Reward: 5.0  | Average Reward 6.89  | Actor loss: 0.28 | Critic loss: 7.03 | Entropy loss: -0.0025  | Total Loss: 7.31 | Total Steps: 48\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1448/175267  | Episode Reward: 10.0  | Average Reward 6.95  | Actor loss: 0.01 | Critic loss: 1.84 | Entropy loss: -0.0000  | Total Loss: 1.85 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1449/175267  | Episode Reward: 5.0  | Average Reward 6.89  | Actor loss: -0.03 | Critic loss: 5.92 | Entropy loss: -0.0002  | Total Loss: 5.89 | Total Steps: 43\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1450/175267  | Episode Reward: 5.0  | Average Reward 6.84  | Actor loss: -0.26 | Critic loss: 5.05 | Entropy loss: -0.0027  | Total Loss: 4.79 | Total Steps: 49\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1451/175267  | Episode Reward: 5.0  | Average Reward 6.79  | Actor loss: 0.02 | Critic loss: 4.21 | Entropy loss: -0.0023  | Total Loss: 4.22 | Total Steps: 58\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1452/175267  | Episode Reward: 7.5  | Average Reward 6.77  | Actor loss: 0.22 | Critic loss: 7.27 | Entropy loss: -0.0007  | Total Loss: 7.49 | Total Steps: 29\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1453/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.01 | Critic loss: 2.46 | Entropy loss: -0.0000  | Total Loss: 2.47 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1454/175267  | Episode Reward: -5.0  | Average Reward 6.72  | Actor loss: -0.92 | Critic loss: 15.51 | Entropy loss: -0.0075  | Total Loss: 14.59 | Total Steps: 141\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1455/175267  | Episode Reward: 10.0  | Average Reward 6.77  | Actor loss: 0.01 | Critic loss: 2.93 | Entropy loss: -0.0000  | Total Loss: 2.94 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1456/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 0.07 | Critic loss: 3.85 | Entropy loss: -0.0005  | Total Loss: 3.91 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1457/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 0.02 | Critic loss: 3.35 | Entropy loss: -0.0006  | Total Loss: 3.37 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1458/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.16 | Critic loss: 3.03 | Entropy loss: -0.0019  | Total Loss: 3.19 | Total Steps: 30\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1459/175267  | Episode Reward: 2.5  | Average Reward 6.77  | Actor loss: -0.64 | Critic loss: 6.15 | Entropy loss: -0.0033  | Total Loss: 5.51 | Total Steps: 54\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1460/175267  | Episode Reward: 5.0  | Average Reward 6.75  | Actor loss: 0.42 | Critic loss: 2.10 | Entropy loss: -0.0034  | Total Loss: 2.52 | Total Steps: 33\n",
      "---red sphere---\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1461/175267  | Episode Reward: 9.0  | Average Reward 6.74  | Actor loss: 0.41 | Critic loss: 2.09 | Entropy loss: -0.0073  | Total Loss: 2.49 | Total Steps: 36\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1462/175267  | Episode Reward: 7.5  | Average Reward 6.76  | Actor loss: -0.16 | Critic loss: 3.17 | Entropy loss: -0.0013  | Total Loss: 3.00 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1463/175267  | Episode Reward: 5.0  | Average Reward 6.74  | Actor loss: 0.12 | Critic loss: 7.16 | Entropy loss: -0.0014  | Total Loss: 7.27 | Total Steps: 47\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1464/175267  | Episode Reward: 7.5  | Average Reward 6.74  | Actor loss: 0.02 | Critic loss: 5.07 | Entropy loss: -0.0004  | Total Loss: 5.09 | Total Steps: 42\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1465/175267  | Episode Reward: 5.0  | Average Reward 6.81  | Actor loss: -0.50 | Critic loss: 6.75 | Entropy loss: -0.0041  | Total Loss: 6.25 | Total Steps: 81\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1466/175267  | Episode Reward: 10.0  | Average Reward 6.81  | Actor loss: 0.01 | Critic loss: 2.59 | Entropy loss: -0.0000  | Total Loss: 2.60 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1467/175267  | Episode Reward: 7.5  | Average Reward 6.79  | Actor loss: 0.22 | Critic loss: 2.92 | Entropy loss: -0.0020  | Total Loss: 3.13 | Total Steps: 31\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1468/175267  | Episode Reward: 6.5  | Average Reward 6.92  | Actor loss: -0.31 | Critic loss: 3.80 | Entropy loss: -0.0030  | Total Loss: 3.49 | Total Steps: 55\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1469/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.03 | Critic loss: 3.29 | Entropy loss: -0.0003  | Total Loss: 3.33 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1470/175267  | Episode Reward: 7.5  | Average Reward 6.95  | Actor loss: -0.56 | Critic loss: 3.02 | Entropy loss: -0.0031  | Total Loss: 2.46 | Total Steps: 48\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1471/175267  | Episode Reward: 2.5  | Average Reward 6.90  | Actor loss: -0.35 | Critic loss: 6.89 | Entropy loss: -0.0021  | Total Loss: 6.54 | Total Steps: 52\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1472/175267  | Episode Reward: 7.5  | Average Reward 6.90  | Actor loss: 0.30 | Critic loss: 5.12 | Entropy loss: -0.0022  | Total Loss: 5.42 | Total Steps: 39\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1473/175267  | Episode Reward: 10.0  | Average Reward 6.90  | Actor loss: 0.01 | Critic loss: 1.46 | Entropy loss: -0.0000  | Total Loss: 1.47 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1474/175267  | Episode Reward: 10.0  | Average Reward 6.97  | Actor loss: 0.86 | Critic loss: 7.54 | Entropy loss: -0.0006  | Total Loss: 8.40 | Total Steps: 7\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1475/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.00 | Critic loss: 0.14 | Entropy loss: -0.0000  | Total Loss: 0.14 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1476/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.02 | Critic loss: 0.29 | Entropy loss: -0.0001  | Total Loss: 0.31 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1477/175267  | Episode Reward: 5.0  | Average Reward 7.10  | Actor loss: -0.55 | Critic loss: 10.31 | Entropy loss: -0.0037  | Total Loss: 9.76 | Total Steps: 83\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1478/175267  | Episode Reward: 10.0  | Average Reward 7.15  | Actor loss: 0.01 | Critic loss: 1.32 | Entropy loss: -0.0000  | Total Loss: 1.33 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1479/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.03 | Critic loss: 1.70 | Entropy loss: -0.0000  | Total Loss: 1.73 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1480/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: -0.26 | Critic loss: 3.51 | Entropy loss: -0.0036  | Total Loss: 3.25 | Total Steps: 55\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1481/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.01 | Critic loss: 1.66 | Entropy loss: -0.0000  | Total Loss: 1.67 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1482/175267  | Episode Reward: -2.5  | Average Reward 7.13  | Actor loss: -0.37 | Critic loss: 10.42 | Entropy loss: -0.0055  | Total Loss: 10.05 | Total Steps: 168\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1483/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: 0.05 | Critic loss: 8.44 | Entropy loss: -0.0005  | Total Loss: 8.49 | Total Steps: 29\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1484/175267  | Episode Reward: 7.5  | Average Reward 7.13  | Actor loss: -0.03 | Critic loss: 3.86 | Entropy loss: -0.0003  | Total Loss: 3.84 | Total Steps: 42\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1485/175267  | Episode Reward: 7.5  | Average Reward 7.16  | Actor loss: 0.02 | Critic loss: 5.56 | Entropy loss: -0.0001  | Total Loss: 5.59 | Total Steps: 29\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1486/175267  | Episode Reward: 7.5  | Average Reward 7.16  | Actor loss: 0.22 | Critic loss: 4.77 | Entropy loss: -0.0018  | Total Loss: 5.00 | Total Steps: 43\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1487/175267  | Episode Reward: 4.0  | Average Reward 7.15  | Actor loss: -0.39 | Critic loss: 5.99 | Entropy loss: -0.0021  | Total Loss: 5.59 | Total Steps: 54\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1488/175267  | Episode Reward: 10.0  | Average Reward 7.15  | Actor loss: 0.03 | Critic loss: 0.50 | Entropy loss: -0.0001  | Total Loss: 0.53 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1489/175267  | Episode Reward: 10.0  | Average Reward 7.15  | Actor loss: 0.02 | Critic loss: 3.56 | Entropy loss: -0.0000  | Total Loss: 3.58 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1490/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.04 | Critic loss: 0.24 | Entropy loss: -0.0001  | Total Loss: 0.28 | Total Steps: 6\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1491/175267  | Episode Reward: 7.5  | Average Reward 7.17  | Actor loss: 0.21 | Critic loss: 3.12 | Entropy loss: -0.0012  | Total Loss: 3.33 | Total Steps: 29\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1492/175267  | Episode Reward: 10.0  | Average Reward 7.20  | Actor loss: 0.52 | Critic loss: 4.66 | Entropy loss: -0.0017  | Total Loss: 5.18 | Total Steps: 32\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1493/175267  | Episode Reward: 10.0  | Average Reward 7.22  | Actor loss: 0.01 | Critic loss: 1.65 | Entropy loss: -0.0000  | Total Loss: 1.66 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1494/175267  | Episode Reward: 7.5  | Average Reward 7.20  | Actor loss: -0.24 | Critic loss: 3.15 | Entropy loss: -0.0028  | Total Loss: 2.90 | Total Steps: 73\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1495/175267  | Episode Reward: 7.5  | Average Reward 7.20  | Actor loss: 0.31 | Critic loss: 5.68 | Entropy loss: -0.0019  | Total Loss: 5.98 | Total Steps: 32\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1496/175267  | Episode Reward: 1.5  | Average Reward 7.12  | Actor loss: -0.27 | Critic loss: 10.89 | Entropy loss: -0.0017  | Total Loss: 10.63 | Total Steps: 54\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1497/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.01 | Critic loss: 1.79 | Entropy loss: -0.0000  | Total Loss: 1.79 | Total Steps: 6\n",
      "---green prism---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1498/175267  | Episode Reward: 5.0  | Average Reward 7.12  | Actor loss: -0.20 | Critic loss: 5.08 | Entropy loss: -0.0026  | Total Loss: 4.88 | Total Steps: 52\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1499/175267  | Episode Reward: 7.5  | Average Reward 7.14  | Actor loss: -0.15 | Critic loss: 5.84 | Entropy loss: -0.0012  | Total Loss: 5.70 | Total Steps: 53\n",
      "---red sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1500/175267  | Episode Reward: 10.0  | Average Reward 7.14  | Actor loss: 0.04 | Critic loss: 1.73 | Entropy loss: -0.0001  | Total Loss: 1.77 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1501/175267  | Episode Reward: 5.0  | Average Reward 7.14  | Actor loss: -0.49 | Critic loss: 5.66 | Entropy loss: -0.0032  | Total Loss: 5.16 | Total Steps: 55\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1502/175267  | Episode Reward: 10.0  | Average Reward 7.17  | Actor loss: 0.11 | Critic loss: 4.39 | Entropy loss: -0.0006  | Total Loss: 4.50 | Total Steps: 37\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1503/175267  | Episode Reward: 10.0  | Average Reward 7.19  | Actor loss: 0.02 | Critic loss: 4.01 | Entropy loss: -0.0000  | Total Loss: 4.03 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1504/175267  | Episode Reward: -5.0  | Average Reward 7.04  | Actor loss: -0.47 | Critic loss: 17.30 | Entropy loss: -0.0047  | Total Loss: 16.82 | Total Steps: 132\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1505/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.00 | Critic loss: 0.87 | Entropy loss: -0.0000  | Total Loss: 0.88 | Total Steps: 6\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1506/175267  | Episode Reward: 10.0  | Average Reward 7.07  | Actor loss: 0.06 | Critic loss: 7.70 | Entropy loss: -0.0000  | Total Loss: 7.77 | Total Steps: 6\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1507/175267  | Episode Reward: 4.0  | Average Reward 7.00  | Actor loss: -0.20 | Critic loss: 5.06 | Entropy loss: -0.0055  | Total Loss: 4.85 | Total Steps: 51\n",
      "---red cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1508/175267  | Episode Reward: 10.0  | Average Reward 7.11  | Actor loss: 0.01 | Critic loss: 0.82 | Entropy loss: -0.0002  | Total Loss: 0.83 | Total Steps: 38\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1509/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.09 | Critic loss: 9.20 | Entropy loss: -0.0000  | Total Loss: 9.29 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1510/175267  | Episode Reward: 7.5  | Average Reward 7.11  | Actor loss: 0.07 | Critic loss: 6.36 | Entropy loss: -0.0004  | Total Loss: 6.43 | Total Steps: 30\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1511/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.01 | Critic loss: 2.15 | Entropy loss: -0.0000  | Total Loss: 2.17 | Total Steps: 6\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1512/175267  | Episode Reward: 10.0  | Average Reward 7.13  | Actor loss: 0.03 | Critic loss: 4.00 | Entropy loss: -0.0000  | Total Loss: 4.03 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1513/175267  | Episode Reward: -5.0  | Average Reward 7.00  | Actor loss: -0.79 | Critic loss: 18.77 | Entropy loss: -0.0059  | Total Loss: 17.97 | Total Steps: 122\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1514/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.17 | Critic loss: 3.73 | Entropy loss: -0.0008  | Total Loss: 3.90 | Total Steps: 31\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1515/175267  | Episode Reward: 5.0  | Average Reward 7.00  | Actor loss: -0.46 | Critic loss: 6.08 | Entropy loss: -0.0039  | Total Loss: 5.61 | Total Steps: 54\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1516/175267  | Episode Reward: 7.5  | Average Reward 7.00  | Actor loss: -0.05 | Critic loss: 2.14 | Entropy loss: -0.0010  | Total Loss: 2.09 | Total Steps: 42\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1517/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.01 | Critic loss: 0.92 | Entropy loss: -0.0000  | Total Loss: 0.93 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1518/175267  | Episode Reward: 5.0  | Average Reward 6.98  | Actor loss: -0.14 | Critic loss: 4.71 | Entropy loss: -0.0006  | Total Loss: 4.57 | Total Steps: 47\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1519/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.01 | Critic loss: 1.85 | Entropy loss: -0.0000  | Total Loss: 1.86 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1520/175267  | Episode Reward: 10.0  | Average Reward 7.12  | Actor loss: 0.02 | Critic loss: 3.15 | Entropy loss: -0.0001  | Total Loss: 3.17 | Total Steps: 29\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1521/175267  | Episode Reward: 7.5  | Average Reward 7.09  | Actor loss: 0.08 | Critic loss: 8.64 | Entropy loss: -0.0025  | Total Loss: 8.72 | Total Steps: 55\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1522/175267  | Episode Reward: 0.0  | Average Reward 7.10  | Actor loss: -0.54 | Critic loss: 10.43 | Entropy loss: -0.0054  | Total Loss: 9.89 | Total Steps: 94\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1523/175267  | Episode Reward: 5.0  | Average Reward 7.25  | Actor loss: -0.16 | Critic loss: 3.90 | Entropy loss: -0.0027  | Total Loss: 3.74 | Total Steps: 52\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1524/175267  | Episode Reward: 2.5  | Average Reward 7.17  | Actor loss: -0.08 | Critic loss: 10.15 | Entropy loss: -0.0007  | Total Loss: 10.06 | Total Steps: 53\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1525/175267  | Episode Reward: 2.5  | Average Reward 7.10  | Actor loss: -0.19 | Critic loss: 6.33 | Entropy loss: -0.0014  | Total Loss: 6.14 | Total Steps: 53\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1526/175267  | Episode Reward: 10.0  | Average Reward 7.10  | Actor loss: 0.00 | Critic loss: 0.71 | Entropy loss: -0.0000  | Total Loss: 0.71 | Total Steps: 6\n",
      "---green capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1527/175267  | Episode Reward: 10.0  | Average Reward 7.15  | Actor loss: 0.01 | Critic loss: 0.71 | Entropy loss: -0.0000  | Total Loss: 0.72 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1528/175267  | Episode Reward: -5.0  | Average Reward 7.00  | Actor loss: -0.81 | Critic loss: 16.15 | Entropy loss: -0.0063  | Total Loss: 15.33 | Total Steps: 130\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1529/175267  | Episode Reward: 4.0  | Average Reward 7.01  | Actor loss: -0.45 | Critic loss: 5.47 | Entropy loss: -0.0047  | Total Loss: 5.02 | Total Steps: 83\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1530/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.06 | Critic loss: 4.13 | Entropy loss: -0.0003  | Total Loss: 4.20 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1531/175267  | Episode Reward: 7.5  | Average Reward 7.04  | Actor loss: 0.25 | Critic loss: 4.85 | Entropy loss: -0.0008  | Total Loss: 5.09 | Total Steps: 29\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1532/175267  | Episode Reward: -2.5  | Average Reward 6.96  | Actor loss: -0.34 | Critic loss: 10.09 | Entropy loss: -0.0062  | Total Loss: 9.75 | Total Steps: 169\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1533/175267  | Episode Reward: 7.5  | Average Reward 6.94  | Actor loss: 0.12 | Critic loss: 7.99 | Entropy loss: -0.0005  | Total Loss: 8.11 | Total Steps: 30\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1534/175267  | Episode Reward: 5.0  | Average Reward 6.89  | Actor loss: -0.19 | Critic loss: 7.22 | Entropy loss: -0.0011  | Total Loss: 7.04 | Total Steps: 53\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1535/175267  | Episode Reward: 7.5  | Average Reward 6.94  | Actor loss: 0.28 | Critic loss: 4.06 | Entropy loss: -0.0016  | Total Loss: 4.33 | Total Steps: 43\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1536/175267  | Episode Reward: 10.0  | Average Reward 7.04  | Actor loss: 0.02 | Critic loss: 0.72 | Entropy loss: -0.0001  | Total Loss: 0.75 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1537/175267  | Episode Reward: 7.5  | Average Reward 7.09  | Actor loss: 0.00 | Critic loss: 5.18 | Entropy loss: -0.0002  | Total Loss: 5.18 | Total Steps: 43\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1538/175267  | Episode Reward: 10.0  | Average Reward 7.09  | Actor loss: 0.02 | Critic loss: 2.46 | Entropy loss: -0.0000  | Total Loss: 2.48 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1539/175267  | Episode Reward: -2.5  | Average Reward 6.96  | Actor loss: 0.85 | Critic loss: 6.65 | Entropy loss: -0.0011  | Total Loss: 7.49 | Total Steps: 261\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1540/175267  | Episode Reward: 7.5  | Average Reward 6.99  | Actor loss: -0.19 | Critic loss: 5.84 | Entropy loss: -0.0035  | Total Loss: 5.64 | Total Steps: 55\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1541/175267  | Episode Reward: 5.0  | Average Reward 6.94  | Actor loss: -0.06 | Critic loss: 3.82 | Entropy loss: -0.0020  | Total Loss: 3.75 | Total Steps: 52\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1542/175267  | Episode Reward: 10.0  | Average Reward 6.94  | Actor loss: 0.03 | Critic loss: 3.74 | Entropy loss: -0.0001  | Total Loss: 3.77 | Total Steps: 29\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1543/175267  | Episode Reward: 10.0  | Average Reward 6.94  | Actor loss: 0.01 | Critic loss: 1.22 | Entropy loss: -0.0000  | Total Loss: 1.23 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1544/175267  | Episode Reward: 10.0  | Average Reward 6.94  | Actor loss: 0.05 | Critic loss: 3.89 | Entropy loss: -0.0003  | Total Loss: 3.94 | Total Steps: 32\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1545/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.02 | Critic loss: 4.65 | Entropy loss: -0.0000  | Total Loss: 4.67 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1546/175267  | Episode Reward: 7.5  | Average Reward 6.94  | Actor loss: -0.20 | Critic loss: 5.50 | Entropy loss: -0.0007  | Total Loss: 5.30 | Total Steps: 42\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1547/175267  | Episode Reward: 4.0  | Average Reward 6.93  | Actor loss: -0.33 | Critic loss: 7.07 | Entropy loss: -0.0033  | Total Loss: 6.73 | Total Steps: 55\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1548/175267  | Episode Reward: 7.5  | Average Reward 6.91  | Actor loss: 0.01 | Critic loss: 5.76 | Entropy loss: -0.0005  | Total Loss: 5.77 | Total Steps: 42\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1549/175267  | Episode Reward: 5.0  | Average Reward 6.91  | Actor loss: 0.13 | Critic loss: 7.73 | Entropy loss: -0.0023  | Total Loss: 7.86 | Total Steps: 50\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1550/175267  | Episode Reward: 10.0  | Average Reward 6.96  | Actor loss: 0.01 | Critic loss: 1.08 | Entropy loss: -0.0000  | Total Loss: 1.08 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1551/175267  | Episode Reward: 7.5  | Average Reward 6.98  | Actor loss: 0.16 | Critic loss: 4.19 | Entropy loss: -0.0021  | Total Loss: 4.35 | Total Steps: 43\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1552/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: -0.99 | Critic loss: 8.07 | Entropy loss: -0.0025  | Total Loss: 7.07 | Total Steps: 52\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1553/175267  | Episode Reward: 7.5  | Average Reward 6.93  | Actor loss: 0.18 | Critic loss: 4.15 | Entropy loss: -0.0009  | Total Loss: 4.33 | Total Steps: 29\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1554/175267  | Episode Reward: 2.5  | Average Reward 7.00  | Actor loss: -0.14 | Critic loss: 8.99 | Entropy loss: -0.0007  | Total Loss: 8.85 | Total Steps: 52\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Step: 250\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1555/175267  | Episode Reward: -7.5  | Average Reward 6.83  | Actor loss: -0.33 | Critic loss: 5.71 | Entropy loss: -0.0045  | Total Loss: 5.37 | Total Steps: 304\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1556/175267  | Episode Reward: 5.0  | Average Reward 6.80  | Actor loss: -0.19 | Critic loss: 5.68 | Entropy loss: -0.0055  | Total Loss: 5.48 | Total Steps: 121\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1557/175267  | Episode Reward: 10.0  | Average Reward 6.83  | Actor loss: 0.01 | Critic loss: 1.37 | Entropy loss: -0.0000  | Total Loss: 1.38 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1558/175267  | Episode Reward: 5.0  | Average Reward 6.78  | Actor loss: -0.13 | Critic loss: 3.91 | Entropy loss: -0.0018  | Total Loss: 3.77 | Total Steps: 52\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1559/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.29 | Critic loss: 5.68 | Entropy loss: -0.0030  | Total Loss: 5.97 | Total Steps: 32\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1560/175267  | Episode Reward: 10.0  | Average Reward 6.91  | Actor loss: 0.03 | Critic loss: 4.71 | Entropy loss: -0.0002  | Total Loss: 4.74 | Total Steps: 29\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1561/175267  | Episode Reward: 10.0  | Average Reward 6.92  | Actor loss: 0.03 | Critic loss: 4.65 | Entropy loss: -0.0002  | Total Loss: 4.68 | Total Steps: 29\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1562/175267  | Episode Reward: 10.0  | Average Reward 6.94  | Actor loss: 0.01 | Critic loss: 1.76 | Entropy loss: -0.0000  | Total Loss: 1.77 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1563/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: 0.13 | Critic loss: 4.75 | Entropy loss: -0.0011  | Total Loss: 4.88 | Total Steps: 37\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1564/175267  | Episode Reward: 7.5  | Average Reward 6.96  | Actor loss: -0.28 | Critic loss: 4.06 | Entropy loss: -0.0035  | Total Loss: 3.78 | Total Steps: 91\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1565/175267  | Episode Reward: 5.0  | Average Reward 6.96  | Actor loss: -0.11 | Critic loss: 3.32 | Entropy loss: -0.0019  | Total Loss: 3.20 | Total Steps: 102\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1566/175267  | Episode Reward: 5.0  | Average Reward 6.92  | Actor loss: -0.01 | Critic loss: 7.34 | Entropy loss: -0.0011  | Total Loss: 7.33 | Total Steps: 43\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1567/175267  | Episode Reward: 10.0  | Average Reward 6.94  | Actor loss: 0.04 | Critic loss: 4.99 | Entropy loss: -0.0003  | Total Loss: 5.03 | Total Steps: 39\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1568/175267  | Episode Reward: 10.0  | Average Reward 6.97  | Actor loss: 0.12 | Critic loss: 5.95 | Entropy loss: -0.0007  | Total Loss: 6.07 | Total Steps: 29\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1569/175267  | Episode Reward: 10.0  | Average Reward 6.97  | Actor loss: 0.00 | Critic loss: 1.12 | Entropy loss: -0.0000  | Total Loss: 1.12 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1570/175267  | Episode Reward: 10.0  | Average Reward 7.00  | Actor loss: 0.01 | Critic loss: 1.04 | Entropy loss: -0.0000  | Total Loss: 1.04 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1571/175267  | Episode Reward: 10.0  | Average Reward 7.08  | Actor loss: 0.18 | Critic loss: 1.62 | Entropy loss: -0.0018  | Total Loss: 1.80 | Total Steps: 68\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1572/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: 0.04 | Critic loss: 5.50 | Entropy loss: -0.0003  | Total Loss: 5.54 | Total Steps: 32\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1573/175267  | Episode Reward: 5.0  | Average Reward 7.03  | Actor loss: -0.32 | Critic loss: 9.07 | Entropy loss: -0.0019  | Total Loss: 8.74 | Total Steps: 81\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1574/175267  | Episode Reward: 2.5  | Average Reward 6.95  | Actor loss: -0.11 | Critic loss: 8.02 | Entropy loss: -0.0031  | Total Loss: 7.90 | Total Steps: 55\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1575/175267  | Episode Reward: 7.5  | Average Reward 6.92  | Actor loss: 0.01 | Critic loss: 6.07 | Entropy loss: -0.0001  | Total Loss: 6.08 | Total Steps: 29\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1576/175267  | Episode Reward: 7.5  | Average Reward 6.90  | Actor loss: -0.02 | Critic loss: 3.25 | Entropy loss: -0.0002  | Total Loss: 3.24 | Total Steps: 43\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1577/175267  | Episode Reward: 5.0  | Average Reward 6.90  | Actor loss: -0.49 | Critic loss: 4.89 | Entropy loss: -0.0025  | Total Loss: 4.40 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1578/175267  | Episode Reward: 10.0  | Average Reward 6.90  | Actor loss: 0.01 | Critic loss: 2.22 | Entropy loss: -0.0000  | Total Loss: 2.23 | Total Steps: 6\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1579/175267  | Episode Reward: 5.0  | Average Reward 6.85  | Actor loss: 0.29 | Critic loss: 6.76 | Entropy loss: -0.0017  | Total Loss: 7.05 | Total Steps: 43\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1580/175267  | Episode Reward: 10.0  | Average Reward 6.85  | Actor loss: 0.05 | Critic loss: 0.28 | Entropy loss: -0.0002  | Total Loss: 0.33 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1581/175267  | Episode Reward: 10.0  | Average Reward 6.85  | Actor loss: -0.12 | Critic loss: 4.14 | Entropy loss: -0.0041  | Total Loss: 4.02 | Total Steps: 46\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1582/175267  | Episode Reward: 2.5  | Average Reward 6.90  | Actor loss: -0.30 | Critic loss: 3.63 | Entropy loss: -0.0044  | Total Loss: 3.32 | Total Steps: 44\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1583/175267  | Episode Reward: 7.5  | Average Reward 6.90  | Actor loss: 0.02 | Critic loss: 7.60 | Entropy loss: -0.0001  | Total Loss: 7.62 | Total Steps: 29\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1584/175267  | Episode Reward: 5.0  | Average Reward 6.88  | Actor loss: 0.14 | Critic loss: 6.75 | Entropy loss: -0.0018  | Total Loss: 6.88 | Total Steps: 53\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1585/175267  | Episode Reward: 5.0  | Average Reward 6.85  | Actor loss: -0.32 | Critic loss: 8.03 | Entropy loss: -0.0025  | Total Loss: 7.70 | Total Steps: 73\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1586/175267  | Episode Reward: 7.5  | Average Reward 6.85  | Actor loss: 0.06 | Critic loss: 7.23 | Entropy loss: -0.0006  | Total Loss: 7.30 | Total Steps: 39\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1587/175267  | Episode Reward: 5.0  | Average Reward 6.86  | Actor loss: -0.07 | Critic loss: 2.59 | Entropy loss: -0.0013  | Total Loss: 2.52 | Total Steps: 53\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1588/175267  | Episode Reward: 10.0  | Average Reward 6.86  | Actor loss: 0.15 | Critic loss: 1.44 | Entropy loss: -0.0016  | Total Loss: 1.60 | Total Steps: 40\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1589/175267  | Episode Reward: 7.5  | Average Reward 6.83  | Actor loss: 0.61 | Critic loss: 6.77 | Entropy loss: -0.0022  | Total Loss: 7.38 | Total Steps: 31\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1590/175267  | Episode Reward: 10.0  | Average Reward 6.83  | Actor loss: 0.23 | Critic loss: 5.73 | Entropy loss: -0.0002  | Total Loss: 5.96 | Total Steps: 6\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1591/175267  | Episode Reward: -2.5  | Average Reward 6.74  | Actor loss: -0.66 | Critic loss: 8.72 | Entropy loss: -0.0070  | Total Loss: 8.06 | Total Steps: 87\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1592/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: 0.08 | Critic loss: 4.99 | Entropy loss: -0.0006  | Total Loss: 5.07 | Total Steps: 32\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1593/175267  | Episode Reward: 10.0  | Average Reward 6.74  | Actor loss: 0.01 | Critic loss: 0.26 | Entropy loss: -0.0001  | Total Loss: 0.28 | Total Steps: 6\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1594/175267  | Episode Reward: 2.5  | Average Reward 6.68  | Actor loss: -0.28 | Critic loss: 9.39 | Entropy loss: -0.0021  | Total Loss: 9.11 | Total Steps: 51\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1595/175267  | Episode Reward: 10.0  | Average Reward 6.71  | Actor loss: 0.01 | Critic loss: 2.02 | Entropy loss: -0.0000  | Total Loss: 2.03 | Total Steps: 6\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1596/175267  | Episode Reward: 5.0  | Average Reward 6.75  | Actor loss: -0.06 | Critic loss: 6.42 | Entropy loss: -0.0007  | Total Loss: 6.35 | Total Steps: 43\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1597/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.13 | Critic loss: 4.09 | Entropy loss: -0.0004  | Total Loss: 4.22 | Total Steps: 29\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1598/175267  | Episode Reward: 5.0  | Average Reward 6.75  | Actor loss: -0.22 | Critic loss: 7.68 | Entropy loss: -0.0014  | Total Loss: 7.46 | Total Steps: 53\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1599/175267  | Episode Reward: 10.0  | Average Reward 6.77  | Actor loss: 0.15 | Critic loss: 15.22 | Entropy loss: -0.0001  | Total Loss: 15.37 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1600/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 0.12 | Critic loss: 7.72 | Entropy loss: -0.0005  | Total Loss: 7.84 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1601/175267  | Episode Reward: 7.5  | Average Reward 6.77  | Actor loss: 0.18 | Critic loss: 3.82 | Entropy loss: -0.0008  | Total Loss: 4.00 | Total Steps: 30\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1602/175267  | Episode Reward: 7.5  | Average Reward 6.75  | Actor loss: 0.08 | Critic loss: 6.27 | Entropy loss: -0.0013  | Total Loss: 6.35 | Total Steps: 36\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1603/175267  | Episode Reward: 10.0  | Average Reward 6.75  | Actor loss: 0.09 | Critic loss: 0.67 | Entropy loss: -0.0015  | Total Loss: 0.76 | Total Steps: 7\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1604/175267  | Episode Reward: 2.5  | Average Reward 6.82  | Actor loss: -0.40 | Critic loss: 11.14 | Entropy loss: -0.0032  | Total Loss: 10.74 | Total Steps: 44\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1605/175267  | Episode Reward: 7.5  | Average Reward 6.79  | Actor loss: 0.08 | Critic loss: 3.35 | Entropy loss: -0.0049  | Total Loss: 3.43 | Total Steps: 152\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1606/175267  | Episode Reward: 7.5  | Average Reward 6.77  | Actor loss: 0.25 | Critic loss: 7.49 | Entropy loss: -0.0010  | Total Loss: 7.74 | Total Steps: 30\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1607/175267  | Episode Reward: 7.5  | Average Reward 6.80  | Actor loss: -0.05 | Critic loss: 6.28 | Entropy loss: -0.0031  | Total Loss: 6.22 | Total Steps: 43\n",
      "---red cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1608/175267  | Episode Reward: 5.0  | Average Reward 6.75  | Actor loss: -0.11 | Critic loss: 4.40 | Entropy loss: -0.0060  | Total Loss: 4.28 | Total Steps: 201\n",
      "---yellow capsule---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1609/175267  | Episode Reward: 4.0  | Average Reward 6.70  | Actor loss: -0.48 | Critic loss: 8.40 | Entropy loss: -0.0064  | Total Loss: 7.91 | Total Steps: 45\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1610/175267  | Episode Reward: 7.5  | Average Reward 6.70  | Actor loss: -0.07 | Critic loss: 5.83 | Entropy loss: -0.0012  | Total Loss: 5.76 | Total Steps: 42\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1611/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.16 | Critic loss: 1.20 | Entropy loss: -0.0017  | Total Loss: 1.36 | Total Steps: 40\n",
      "---red prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1612/175267  | Episode Reward: 10.0  | Average Reward 6.70  | Actor loss: 0.23 | Critic loss: 5.53 | Entropy loss: -0.0008  | Total Loss: 5.76 | Total Steps: 31\n",
      "---yellow sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1613/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.01 | Critic loss: 0.21 | Entropy loss: -0.0000  | Total Loss: 0.22 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1614/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.03 | Critic loss: 4.34 | Entropy loss: -0.0002  | Total Loss: 4.37 | Total Steps: 31\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1615/175267  | Episode Reward: 5.0  | Average Reward 6.84  | Actor loss: -0.27 | Critic loss: 5.89 | Entropy loss: -0.0024  | Total Loss: 5.62 | Total Steps: 49\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1616/175267  | Episode Reward: 10.0  | Average Reward 6.87  | Actor loss: 0.01 | Critic loss: 2.10 | Entropy loss: -0.0000  | Total Loss: 2.11 | Total Steps: 6\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1617/175267  | Episode Reward: 7.5  | Average Reward 6.84  | Actor loss: -0.60 | Critic loss: 5.41 | Entropy loss: -0.0019  | Total Loss: 4.81 | Total Steps: 30\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1618/175267  | Episode Reward: 5.0  | Average Reward 6.84  | Actor loss: 0.01 | Critic loss: 3.56 | Entropy loss: -0.0008  | Total Loss: 3.57 | Total Steps: 43\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1619/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.04 | Critic loss: 3.99 | Entropy loss: -0.0002  | Total Loss: 4.03 | Total Steps: 29\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1620/175267  | Episode Reward: 10.0  | Average Reward 6.84  | Actor loss: 0.01 | Critic loss: 1.76 | Entropy loss: -0.0000  | Total Loss: 1.77 | Total Steps: 6\n",
      "---red cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1621/175267  | Episode Reward: 10.0  | Average Reward 6.87  | Actor loss: 0.09 | Critic loss: 4.01 | Entropy loss: -0.0009  | Total Loss: 4.10 | Total Steps: 30\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1622/175267  | Episode Reward: 5.0  | Average Reward 6.92  | Actor loss: -0.22 | Critic loss: 6.12 | Entropy loss: -0.0030  | Total Loss: 5.90 | Total Steps: 55\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1623/175267  | Episode Reward: 7.5  | Average Reward 6.95  | Actor loss: -0.39 | Critic loss: 3.09 | Entropy loss: -0.0039  | Total Loss: 2.69 | Total Steps: 38\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1624/175267  | Episode Reward: 10.0  | Average Reward 7.02  | Actor loss: 0.02 | Critic loss: 2.96 | Entropy loss: -0.0002  | Total Loss: 2.98 | Total Steps: 34\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1625/175267  | Episode Reward: -2.5  | Average Reward 6.97  | Actor loss: -0.73 | Critic loss: 16.61 | Entropy loss: -0.0075  | Total Loss: 15.87 | Total Steps: 110\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1626/175267  | Episode Reward: 7.5  | Average Reward 6.95  | Actor loss: 0.05 | Critic loss: 4.62 | Entropy loss: -0.0008  | Total Loss: 4.67 | Total Steps: 32\n",
      "---blue sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1627/175267  | Episode Reward: 10.0  | Average Reward 6.95  | Actor loss: 0.01 | Critic loss: 0.72 | Entropy loss: -0.0000  | Total Loss: 0.73 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1628/175267  | Episode Reward: 10.0  | Average Reward 7.09  | Actor loss: 0.01 | Critic loss: 1.56 | Entropy loss: -0.0000  | Total Loss: 1.57 | Total Steps: 6\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1629/175267  | Episode Reward: 5.0  | Average Reward 7.11  | Actor loss: -0.01 | Critic loss: 3.98 | Entropy loss: -0.0019  | Total Loss: 3.96 | Total Steps: 53\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1630/175267  | Episode Reward: 7.5  | Average Reward 7.08  | Actor loss: -0.03 | Critic loss: 3.52 | Entropy loss: -0.0054  | Total Loss: 3.48 | Total Steps: 143\n",
      "---yellow cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1631/175267  | Episode Reward: 5.0  | Average Reward 7.05  | Actor loss: 0.01 | Critic loss: 7.86 | Entropy loss: -0.0012  | Total Loss: 7.87 | Total Steps: 54\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1632/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.03 | Critic loss: 3.06 | Entropy loss: -0.0004  | Total Loss: 3.09 | Total Steps: 31\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1633/175267  | Episode Reward: 5.0  | Average Reward 7.16  | Actor loss: -0.11 | Critic loss: 3.32 | Entropy loss: -0.0012  | Total Loss: 3.22 | Total Steps: 42\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1634/175267  | Episode Reward: 7.5  | Average Reward 7.18  | Actor loss: -0.77 | Critic loss: 3.34 | Entropy loss: -0.0074  | Total Loss: 2.56 | Total Steps: 55\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1635/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.17 | Critic loss: 3.85 | Entropy loss: -0.0014  | Total Loss: 4.02 | Total Steps: 30\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1636/175267  | Episode Reward: 5.0  | Average Reward 7.16  | Actor loss: 0.11 | Critic loss: 4.94 | Entropy loss: -0.0023  | Total Loss: 5.05 | Total Steps: 53\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1637/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.01 | Critic loss: 1.73 | Entropy loss: -0.0000  | Total Loss: 1.74 | Total Steps: 6\n",
      "---blue prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1638/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.01 | Critic loss: 2.44 | Entropy loss: -0.0000  | Total Loss: 2.46 | Total Steps: 6\n",
      "---blue prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1639/175267  | Episode Reward: 0.0  | Average Reward 7.21  | Actor loss: -0.71 | Critic loss: 12.00 | Entropy loss: -0.0072  | Total Loss: 11.29 | Total Steps: 115\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1640/175267  | Episode Reward: 7.5  | Average Reward 7.21  | Actor loss: 0.02 | Critic loss: 7.30 | Entropy loss: -0.0001  | Total Loss: 7.33 | Total Steps: 29\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1641/175267  | Episode Reward: 7.5  | Average Reward 7.23  | Actor loss: 0.11 | Critic loss: 4.52 | Entropy loss: -0.0006  | Total Loss: 4.63 | Total Steps: 29\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1642/175267  | Episode Reward: 7.5  | Average Reward 7.21  | Actor loss: 0.02 | Critic loss: 4.40 | Entropy loss: -0.0004  | Total Loss: 4.42 | Total Steps: 42\n",
      "---blue capsule---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1643/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.61 | Critic loss: 2.73 | Entropy loss: -0.0042  | Total Loss: 3.34 | Total Steps: 32\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1644/175267  | Episode Reward: 7.5  | Average Reward 7.18  | Actor loss: -0.00 | Critic loss: 0.98 | Entropy loss: -0.0006  | Total Loss: 0.98 | Total Steps: 38\n",
      "---yellow sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1645/175267  | Episode Reward: 10.0  | Average Reward 7.18  | Actor loss: 0.01 | Critic loss: 0.36 | Entropy loss: -0.0000  | Total Loss: 0.37 | Total Steps: 6\n",
      "---black capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1646/175267  | Episode Reward: 7.5  | Average Reward 7.18  | Actor loss: 0.31 | Critic loss: 4.69 | Entropy loss: -0.0048  | Total Loss: 5.00 | Total Steps: 41\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1647/175267  | Episode Reward: 10.0  | Average Reward 7.24  | Actor loss: 0.01 | Critic loss: 2.02 | Entropy loss: -0.0000  | Total Loss: 2.04 | Total Steps: 6\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1648/175267  | Episode Reward: 5.0  | Average Reward 7.21  | Actor loss: -0.17 | Critic loss: 5.27 | Entropy loss: -0.0016  | Total Loss: 5.10 | Total Steps: 52\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1649/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: 0.01 | Critic loss: 2.13 | Entropy loss: -0.0000  | Total Loss: 2.13 | Total Steps: 6\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1650/175267  | Episode Reward: 10.0  | Average Reward 7.26  | Actor loss: -0.05 | Critic loss: 2.74 | Entropy loss: -0.0010  | Total Loss: 2.69 | Total Steps: 42\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1651/175267  | Episode Reward: 10.0  | Average Reward 7.29  | Actor loss: 0.01 | Critic loss: 3.72 | Entropy loss: -0.0001  | Total Loss: 3.73 | Total Steps: 32\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1652/175267  | Episode Reward: 7.5  | Average Reward 7.32  | Actor loss: 0.04 | Critic loss: 2.81 | Entropy loss: -0.0010  | Total Loss: 2.84 | Total Steps: 34\n",
      "---blue cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1653/175267  | Episode Reward: 1.5  | Average Reward 7.25  | Actor loss: -0.66 | Critic loss: 11.07 | Entropy loss: -0.0027  | Total Loss: 10.41 | Total Steps: 54\n",
      "---blue cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1654/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.02 | Critic loss: 4.95 | Entropy loss: -0.0000  | Total Loss: 4.96 | Total Steps: 6\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1655/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.02 | Critic loss: 3.20 | Entropy loss: -0.0000  | Total Loss: 3.22 | Total Steps: 6\n",
      "---black cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1656/175267  | Episode Reward: 5.0  | Average Reward 7.50  | Actor loss: -0.18 | Critic loss: 5.56 | Entropy loss: -0.0021  | Total Loss: 5.38 | Total Steps: 72\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1657/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.01 | Critic loss: 1.21 | Entropy loss: -0.0000  | Total Loss: 1.22 | Total Steps: 6\n",
      "---green cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1658/175267  | Episode Reward: 7.5  | Average Reward 7.53  | Actor loss: -0.21 | Critic loss: 3.48 | Entropy loss: -0.0036  | Total Loss: 3.26 | Total Steps: 84\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1659/175267  | Episode Reward: -2.5  | Average Reward 7.41  | Actor loss: -0.49 | Critic loss: 14.12 | Entropy loss: -0.0037  | Total Loss: 13.63 | Total Steps: 113\n",
      "---yellow cube---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1660/175267  | Episode Reward: 7.5  | Average Reward 7.38  | Actor loss: -0.54 | Critic loss: 9.04 | Entropy loss: -0.0096  | Total Loss: 8.49 | Total Steps: 231\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1661/175267  | Episode Reward: 5.0  | Average Reward 7.33  | Actor loss: 0.31 | Critic loss: 8.10 | Entropy loss: -0.0024  | Total Loss: 8.41 | Total Steps: 43\n",
      "---blue sphere---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1662/175267  | Episode Reward: 7.5  | Average Reward 7.30  | Actor loss: 0.02 | Critic loss: 2.55 | Entropy loss: -0.0019  | Total Loss: 2.56 | Total Steps: 54\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1663/175267  | Episode Reward: 10.0  | Average Reward 7.33  | Actor loss: 0.02 | Critic loss: 0.53 | Entropy loss: -0.0001  | Total Loss: 0.55 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1664/175267  | Episode Reward: 5.0  | Average Reward 7.30  | Actor loss: -0.09 | Critic loss: 3.27 | Entropy loss: -0.0007  | Total Loss: 3.18 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1665/175267  | Episode Reward: 7.5  | Average Reward 7.33  | Actor loss: 0.11 | Critic loss: 2.98 | Entropy loss: -0.0010  | Total Loss: 3.10 | Total Steps: 30\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1666/175267  | Episode Reward: 7.5  | Average Reward 7.36  | Actor loss: 0.21 | Critic loss: 6.90 | Entropy loss: -0.0011  | Total Loss: 7.11 | Total Steps: 34\n",
      "---green sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1667/175267  | Episode Reward: 2.5  | Average Reward 7.28  | Actor loss: -0.30 | Critic loss: 4.58 | Entropy loss: -0.0033  | Total Loss: 4.27 | Total Steps: 47\n",
      "---red cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1668/175267  | Episode Reward: 2.5  | Average Reward 7.21  | Actor loss: 0.05 | Critic loss: 10.48 | Entropy loss: -0.0030  | Total Loss: 10.53 | Total Steps: 54\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1669/175267  | Episode Reward: 5.0  | Average Reward 7.16  | Actor loss: 0.14 | Critic loss: 3.22 | Entropy loss: -0.0013  | Total Loss: 3.36 | Total Steps: 33\n",
      "---blue capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1670/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.03 | Critic loss: 5.56 | Entropy loss: -0.0000  | Total Loss: 5.59 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1671/175267  | Episode Reward: 7.5  | Average Reward 7.13  | Actor loss: 0.03 | Critic loss: 7.24 | Entropy loss: -0.0002  | Total Loss: 7.27 | Total Steps: 30\n",
      "---blue sphere---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1672/175267  | Episode Reward: 10.0  | Average Reward 7.16  | Actor loss: 0.07 | Critic loss: 3.18 | Entropy loss: -0.0008  | Total Loss: 3.25 | Total Steps: 30\n",
      "---yellow cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1673/175267  | Episode Reward: 10.0  | Average Reward 7.21  | Actor loss: 0.04 | Critic loss: 2.86 | Entropy loss: -0.0010  | Total Loss: 2.90 | Total Steps: 43\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1674/175267  | Episode Reward: 7.5  | Average Reward 7.25  | Actor loss: 0.06 | Critic loss: 7.86 | Entropy loss: -0.0004  | Total Loss: 7.92 | Total Steps: 32\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1675/175267  | Episode Reward: 10.0  | Average Reward 7.28  | Actor loss: 0.01 | Critic loss: 1.59 | Entropy loss: -0.0000  | Total Loss: 1.60 | Total Steps: 6\n",
      "---green capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1676/175267  | Episode Reward: 10.0  | Average Reward 7.30  | Actor loss: 0.01 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.13 | Total Steps: 6\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1677/175267  | Episode Reward: 10.0  | Average Reward 7.36  | Actor loss: 0.01 | Critic loss: 1.21 | Entropy loss: -0.0000  | Total Loss: 1.21 | Total Steps: 6\n",
      "---red prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1678/175267  | Episode Reward: 5.0  | Average Reward 7.30  | Actor loss: -0.18 | Critic loss: 7.16 | Entropy loss: -0.0009  | Total Loss: 6.98 | Total Steps: 52\n",
      "---black cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1679/175267  | Episode Reward: 10.0  | Average Reward 7.36  | Actor loss: 0.03 | Critic loss: 6.75 | Entropy loss: -0.0000  | Total Loss: 6.78 | Total Steps: 6\n",
      "---green prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1680/175267  | Episode Reward: 5.0  | Average Reward 7.30  | Actor loss: 0.12 | Critic loss: 6.60 | Entropy loss: -0.0022  | Total Loss: 6.71 | Total Steps: 53\n",
      "---green capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1681/175267  | Episode Reward: 7.5  | Average Reward 7.28  | Actor loss: 0.05 | Critic loss: 5.81 | Entropy loss: -0.0014  | Total Loss: 5.86 | Total Steps: 43\n",
      "---green prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1682/175267  | Episode Reward: 10.0  | Average Reward 7.36  | Actor loss: 0.35 | Critic loss: 3.50 | Entropy loss: -0.0021  | Total Loss: 3.84 | Total Steps: 43\n",
      "---blue capsule---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1683/175267  | Episode Reward: 7.5  | Average Reward 7.36  | Actor loss: 0.02 | Critic loss: 4.52 | Entropy loss: -0.0002  | Total Loss: 4.54 | Total Steps: 29\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1684/175267  | Episode Reward: 10.0  | Average Reward 7.41  | Actor loss: -0.36 | Critic loss: 1.88 | Entropy loss: -0.0056  | Total Loss: 1.51 | Total Steps: 43\n",
      "---yellow cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1685/175267  | Episode Reward: 10.0  | Average Reward 7.46  | Actor loss: 0.18 | Critic loss: 5.30 | Entropy loss: -0.0007  | Total Loss: 5.48 | Total Steps: 29\n",
      "---black cylinder---\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1686/175267  | Episode Reward: 7.5  | Average Reward 7.46  | Actor loss: 0.05 | Critic loss: 3.80 | Entropy loss: -0.0003  | Total Loss: 3.85 | Total Steps: 29\n",
      "---green sphere---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1687/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: -0.01 | Critic loss: 4.46 | Entropy loss: -0.0005  | Total Loss: 4.44 | Total Steps: 30\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1688/175267  | Episode Reward: 10.0  | Average Reward 7.50  | Actor loss: 0.01 | Critic loss: 2.64 | Entropy loss: -0.0001  | Total Loss: 2.65 | Total Steps: 31\n",
      "---black capsule---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1689/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.01 | Critic loss: 1.29 | Entropy loss: -0.0000  | Total Loss: 1.29 | Total Steps: 6\n",
      "---yellow sphere---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1690/175267  | Episode Reward: -2.5  | Average Reward 7.41  | Actor loss: -0.44 | Critic loss: 12.66 | Entropy loss: -0.0043  | Total Loss: 12.21 | Total Steps: 88\n",
      "---black cube---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1691/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.16 | Critic loss: 3.70 | Entropy loss: -0.0007  | Total Loss: 3.86 | Total Steps: 30\n",
      "---green cylinder---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1692/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.00 | Critic loss: 1.13 | Entropy loss: -0.0000  | Total Loss: 1.14 | Total Steps: 6\n",
      "---black prism---\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1693/175267  | Episode Reward: 10.0  | Average Reward 7.53  | Actor loss: 0.00 | Critic loss: 0.99 | Entropy loss: -0.0000  | Total Loss: 1.00 | Total Steps: 6\n",
      "---black prism---\n",
      "Decision Step reward: -2.5\n",
      "Decision Step reward: -2.5\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1694/175267  | Episode Reward: 5.0  | Average Reward 7.55  | Actor loss: 0.05 | Critic loss: 6.81 | Entropy loss: -0.0042  | Total Loss: 6.85 | Total Steps: 42\n",
      "---red sphere---\n",
      "Decision Step reward: -2.5\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# import warnings\n",
    "# # Disable all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import OrderedDict\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "\n",
    "S0_ALG_NAME = 'S2'\n",
    "S0_ENV_ID = '23'\n",
    "S0_episode = 24037\n",
    "\n",
    "ALG_NAME = 'S2'\n",
    "ENV_ID = '26'\n",
    "\n",
    "TRAIN_EPISODES = 175267  # number of overall episodes for training\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 2.5e-5  #LR\n",
    "speed = 3\n",
    "num_steps = 250 # the step for updating the network\n",
    "max_step_reward = -10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #initialise pretrained model class \n",
    "#     agent = Agent1(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "#     agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "#     pretrained_dict = agent.state_dict()\n",
    "\n",
    "    # intialise new model class\n",
    "    agent = Agent2(bert_output_dim, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "#     bert_dict = agent.state_dict()\n",
    "\n",
    "#     new_dict = OrderedDict()\n",
    "#     for k in bert_dict.keys():\n",
    "#         if k in pretrained_dict.keys():\n",
    "#             new_dict[k] = pretrained_dict[k]\n",
    "#         else:\n",
    "#             new_dict[k] = bert_dict[k]\n",
    "        \n",
    "#     agent.load_state_dict(new_dict)\n",
    "    agent.to(device)\n",
    "    \n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = float('-inf')\n",
    "    hashmap = {\n",
    "    0: 'capsule',\n",
    "    1: 'cube',\n",
    "    2: 'cylinder',\n",
    "    3: 'prism',\n",
    "    4: 'sphere',\n",
    "    5: 'red',\n",
    "    6: 'green',\n",
    "    7: 'blue',\n",
    "    8: 'yellow',\n",
    "    9: 'black'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index1 = int(decision_steps.obs[1][0][0])\n",
    "            index2 = int(decision_steps.obs[1][0][1])+5\n",
    "            print(f'---{hashmap[index2]} {hashmap[index1]}---')\n",
    "            input_caption = f\"{hashmap[index2]} {hashmap[index1]}\"\n",
    "            bert_encoder=BertEncoder()\n",
    "            torch_token_ids,torch_attention_mask=bert_encoder(input_caption)\n",
    "#             text_encoder=BertEncoder(bert_output_dim,num_words)\n",
    "#             lt=text_encoder(input_caption).to(device).detach()\n",
    "            torch_token_ids=torch_token_ids.to(device)\n",
    "            torch_attention_mask=torch_attention_mask.to(device)\n",
    "        \n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "#             lt = torch.zeros(35).to(device)\n",
    "#             lt[index1],lt[index2] = 1,1\n",
    "\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    \n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,torch_token_ids,torch_attention_mask,lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    \n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                            # if reward<-1: hit = 1\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = max_step_reward\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % num_steps == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                # print(len(rewards))\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.1 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # print('updated')\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(float(episode_reward))\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(float(actor_loss))\n",
    "            all_critic_loss.append(float(critic_loss))\n",
    "            all_entropy_loss.append(float(entropy_loss))\n",
    "            all_total_loss.append(float(ac_loss))\n",
    "            if episode >= 100:\n",
    "                avg_score = np.mean(all_episode_reward[-100:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 200 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.1f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.1f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%5000 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-29T01:05:54.932978Z",
     "start_time": "2023-07-29T01:05:54.510813Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:45:55.631298Z",
     "start_time": "2023-07-30T16:45:55.618133Z"
    }
   },
   "source": [
    "# training loop to certain parts of pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T16:44:38.529279Z",
     "start_time": "2023-07-30T16:44:37.586145Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Agent1:\n\tMissing key(s) in state_dict: \"language_module.embedding.weight\", \"language_module.embedding.bias\". \n\tUnexpected key(s) in state_dict: \"bert_language_module.bert_model.embeddings.position_ids\", \"bert_language_module.bert_model.embeddings.word_embeddings.weight\", \"bert_language_module.bert_model.embeddings.position_embeddings.weight\", \"bert_language_module.bert_model.embeddings.token_type_embeddings.weight\", \"bert_language_module.bert_model.embeddings.LayerNorm.weight\", \"bert_language_module.bert_model.embeddings.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.0.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.0.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.0.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.0.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.0.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.0.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.1.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.1.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.1.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.1.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.1.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.1.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.2.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.2.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.2.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.2.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.2.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.2.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.3.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.3.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.3.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.3.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.3.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.3.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.4.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.4.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.4.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.4.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.4.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.4.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.5.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.5.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.5.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.5.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.5.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.5.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.6.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.6.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.6.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.6.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.6.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.6.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.7.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.7.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.7.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.7.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.7.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.7.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.8.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.8.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.8.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.8.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.8.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.8.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.9.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.9.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.9.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.9.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.9.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.9.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.10.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.10.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.10.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.10.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.10.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.10.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.11.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.11.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.11.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.11.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.11.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.11.output.LayerNorm.bias\", \"bert_language_module.bert_model.pooler.dense.weight\", \"bert_language_module.bert_model.pooler.dense.bias\", \"bert_language_module.embedding.weight\", \"bert_language_module.embedding.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b8bebd9afcf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m#initialise pretrained model class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvision_output_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage_output_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmixing_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_hidden_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS0_episode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS0_ALG_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS0_ENV_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mpretrained_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-886b0dbce097>\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, episode, ALG_NAME, ENV_ID)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mALG_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENV_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mALG_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENV_ID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'agent_{episode}.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\transformers_mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1483\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Agent1:\n\tMissing key(s) in state_dict: \"language_module.embedding.weight\", \"language_module.embedding.bias\". \n\tUnexpected key(s) in state_dict: \"bert_language_module.bert_model.embeddings.position_ids\", \"bert_language_module.bert_model.embeddings.word_embeddings.weight\", \"bert_language_module.bert_model.embeddings.position_embeddings.weight\", \"bert_language_module.bert_model.embeddings.token_type_embeddings.weight\", \"bert_language_module.bert_model.embeddings.LayerNorm.weight\", \"bert_language_module.bert_model.embeddings.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.0.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.0.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.0.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.0.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.0.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.0.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.1.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.1.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.1.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.1.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.1.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.1.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.2.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.2.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.2.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.2.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.2.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.2.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.3.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.3.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.3.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.3.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.3.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.3.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.4.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.4.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.4.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.4.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.4.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.4.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.5.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.5.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.5.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.5.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.5.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.5.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.6.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.6.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.6.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.6.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.6.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.6.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.7.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.7.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.7.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.7.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.7.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.7.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.8.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.8.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.8.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.8.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.8.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.8.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.9.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.9.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.9.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.9.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.9.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.9.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.10.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.10.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.10.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.10.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.10.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.10.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.query.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.query.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.key.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.key.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.value.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.self.value.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert_language_module.bert_model.encoder.layer.11.intermediate.dense.weight\", \"bert_language_module.bert_model.encoder.layer.11.intermediate.dense.bias\", \"bert_language_module.bert_model.encoder.layer.11.output.dense.weight\", \"bert_language_module.bert_model.encoder.layer.11.output.dense.bias\", \"bert_language_module.bert_model.encoder.layer.11.output.LayerNorm.weight\", \"bert_language_module.bert_model.encoder.layer.11.output.LayerNorm.bias\", \"bert_language_module.bert_model.pooler.dense.weight\", \"bert_language_module.bert_model.pooler.dense.bias\", \"bert_language_module.embedding.weight\", \"bert_language_module.embedding.bias\". "
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# import warnings\n",
    "# # Disable all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import OrderedDict\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "\n",
    "S0_ALG_NAME = 'S2'\n",
    "S0_ENV_ID = '23'\n",
    "S0_episode = 24037\n",
    "\n",
    "ALG_NAME = 'S2'\n",
    "ENV_ID = '26'\n",
    "\n",
    "TRAIN_EPISODES = 175267  # number of overall episodes for training\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 2.5e-5  #LR\n",
    "speed = 3\n",
    "num_steps = 250 # the step for updating the network\n",
    "max_step_reward = -10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #initialise pretrained model class \n",
    "    agent = Agent1(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "    pretrained_dict = agent.state_dict()\n",
    "\n",
    "    # intialise new model class\n",
    "    agent = Agent2(bert_output_dim, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    bert_dict = agent.state_dict()\n",
    "\n",
    "    new_dict = OrderedDict()\n",
    "    for k in bert_dict.keys():\n",
    "        if k in pretrained_dict.keys():\n",
    "            new_dict[k] = pretrained_dict[k]\n",
    "        else:\n",
    "            new_dict[k] = bert_dict[k]\n",
    "        \n",
    "    agent.load_state_dict(new_dict)\n",
    "    agent.to(device)\n",
    "    \n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = float('-inf')\n",
    "    hashmap = {\n",
    "    0: 'capsule',\n",
    "    1: 'cube',\n",
    "    2: 'cylinder',\n",
    "    3: 'prism',\n",
    "    4: 'sphere',\n",
    "    5: 'red',\n",
    "    6: 'green',\n",
    "    7: 'blue',\n",
    "    8: 'yellow',\n",
    "    9: 'black'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index1 = int(decision_steps.obs[1][0][0])\n",
    "            index2 = int(decision_steps.obs[1][0][1])+5\n",
    "            print(f'---{hashmap[index2]} {hashmap[index1]}---')\n",
    "            input_caption = f\"{hashmap[index2]} {hashmap[index1]}\"\n",
    "            bert_encoder=BertEncoder()\n",
    "            torch_token_ids,torch_attention_mask=bert_encoder(input_caption)\n",
    "#             text_encoder=BertEncoder(bert_output_dim,num_words)\n",
    "#             lt=text_encoder(input_caption).to(device).detach()\n",
    "            torch_token_ids=torch_token_ids.to(device)\n",
    "            torch_attention_mask=torch_attention_mask.to(device)\n",
    "        \n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "#             lt = torch.zeros(35).to(device)\n",
    "#             lt[index1],lt[index2] = 1,1\n",
    "\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    \n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,torch_token_ids,torch_attention_mask,lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    \n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                            # if reward<-1: hit = 1\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = max_step_reward\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % num_steps == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                # print(len(rewards))\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.1 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # print('updated')\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(float(episode_reward))\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(float(actor_loss))\n",
    "            all_critic_loss.append(float(critic_loss))\n",
    "            all_entropy_loss.append(float(entropy_loss))\n",
    "            all_total_loss.append(float(ac_loss))\n",
    "            if episode >= 100:\n",
    "                avg_score = np.mean(all_episode_reward[-100:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 200 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.1f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.1f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%5000 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
