{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env =  UE(file_name=\"stage0_new1\\\\build\",seed=1,side_channels=[],no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "# for i in range(len(behavior_names)):\n",
    "#     print(behavior_names[i])\n",
    "#     print(\"obs:\",behavior_value[i].observation_specs, \"   act:\", behavior_value[0].action_spec)\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agentsNum = len(DecisionSteps.agent_id)\n",
    "# print(\"exist:\",DecisionSteps.agent_id,\"   Dead:\",TerminalSteps.agent_id)\n",
    "# print(\"reward:\",DecisionSteps.reward,\"reward_dead:\",TerminalSteps.reward)\n",
    "# print(\"obs:\",DecisionSteps.obs,\"DeadObs:\",TerminalSteps.obs)\n",
    "# print(\"interrupted:\", TerminalSteps.interrupted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#(3,128,128) --> (64,7,7)\n",
    "image = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0])\n",
    "        value_estimate = self.value_estimator(lstm_output[0]) \n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode):\n",
    "        ALG_NAME = 'A2C'\n",
    "        ENV_ID = 'S0'\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode):\n",
    "        ALG_NAME = 'A2C'\n",
    "        ENV_ID = 'S0'\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of image embedding is torch.Size([3136])\n",
      "The shape of language embedding is torch.Size([128])\n",
      "The shape of mix embedding is torch.Size([256])\n",
      "The shape of lstm hidden state is torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "visual_model = VisualModule()\n",
    "vt = image\n",
    "image_emb = visual_model(vt)\n",
    "print(f'The shape of image embedding is {image_emb.size()}')\n",
    "\n",
    "index = 5\n",
    "language_model = LanguageModule(num_words,embedding_dim)\n",
    "lt = torch.eye(num_words)[:, index]\n",
    "language_emb = language_model(lt)\n",
    "print(f'The shape of language embedding is {language_emb.size()}')\n",
    "\n",
    "mixing_model = MixingModule(vision_output_dim,language_output_dim,mixing_dim)\n",
    "mix_emb = mixing_model(image_emb,language_emb)\n",
    "print(f'The shape of mix embedding is {mix_emb.size()}')\n",
    "\n",
    "lstm = LSTMModule(mixing_dim,lstm_hidden_dim)\n",
    "lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim), torch.zeros(1, lstm_hidden_dim))\n",
    "hidden_state = lstm(mix_emb.unsqueeze(0),lstm_hidden_state)\n",
    "print(f'The shape of lstm hidden state is {hidden_state[0].size()}')\n",
    "\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "action, value, lstm_hidden_state= agent(vt,lt,lstm_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from mlagents_envs.environment import ActionTuple\n",
    "\n",
    "# behavior_name=list(env.behavior_specs)[0]\n",
    "# spec=env.behavior_specs[behavior_name]\n",
    "# env.reset()\n",
    "\n",
    "# while True:\n",
    "#     agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "#     vt = torch.tensor(DecisionSteps.obs).reshape(1,3,128,128)\n",
    "#     index = 5 #e.g\n",
    "#     lt = torch.eye(num_words)[:, index]\n",
    "#     lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim), torch.zeros(1, lstm_hidden_dim))\n",
    "#     action, value,_ = agent(vt,lt,lstm_hidden_state)\n",
    "#     action = F.one_hot(torch.argmax(action),num_actions)\n",
    "#     print(action)\n",
    "\n",
    "#     continuous_actions = np.empty((1, 0))\n",
    "#     discrete_actions = np.array(action).reshape(1,4) #[forward, backward, right, left]\n",
    "#     action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "#     env.set_actions(behavior_name,action_tuple)\n",
    "#     env.step()\n",
    "#     decision_steps, _ = env.get_steps(behavior_name)\n",
    "#     tracked_agent = decision_steps.agent_id[0]\n",
    "#     reward = decision_steps[tracked_agent].reward\n",
    "#     print(reward)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from mlagents_envs.environment import ActionTuple\n",
    "\n",
    "# behavior_name=list(env.behavior_specs)[0]\n",
    "# spec=env.behavior_specs[behavior_name]\n",
    "# env.reset()\n",
    "\n",
    "\n",
    "# while True:\n",
    "#     agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "#     vt = torch.tensor(DecisionSteps.obs).reshape(1,3,128,128)\n",
    "#     index = 5 #e.g\n",
    "#     lt = torch.eye(num_words)[:, index]\n",
    "#     lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim), torch.zeros(1, lstm_hidden_dim))\n",
    "#     # action, value, lstm_output = agent(vt,lt,lstm_hidden_state)\n",
    "#     # action = F.one_hot(torch.argmax(action),num_actions)\n",
    "#     action = torch.tensor([1,0,1,0])\n",
    "#     print(action)\n",
    "\n",
    "#     continuous_actions = np.empty((1, 0))\n",
    "#     discrete_actions = np.array(action).reshape(1,4) #[forward, backward, right, left]\n",
    "#     action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "#     env.set_actions(behavior_name,action_tuple)\n",
    "#     env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Decision step and terminal step (bump into the object?)\n",
    "# 2. continuous action\n",
    "# 3. how to get the instruction word\n",
    "# 4. Reward Design (Hard to bump into the object for the first time)\n",
    "# 5. Model Structure / Training Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:166: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:168: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Model has been saved\n",
      "Training  | Episode: 1/200000  | Episode Reward: -8  | Running Time: 471.8638\n",
      "Episode: 1\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 2/200000  | Episode Reward: 10  | Running Time: 475.8311\n",
      "Episode: 2\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 3/200000  | Episode Reward: 10  | Running Time: 486.4660\n",
      "Episode: 3\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 4/200000  | Episode Reward: -13  | Running Time: 3749.6235\n",
      "Episode: 4\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 5/200000  | Episode Reward: -18  | Running Time: 4205.1928\n",
      "Episode: 5\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 6/200000  | Episode Reward: 0  | Running Time: 4490.6354\n",
      "Episode: 6\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 7/200000  | Episode Reward: 9  | Running Time: 4507.2798\n",
      "Episode: 7\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 8/200000  | Episode Reward: -9  | Running Time: 4984.0873\n",
      "Episode: 8\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 9/200000  | Episode Reward: -16  | Running Time: 5463.6599\n",
      "Episode: 9\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 10/200000  | Episode Reward: -11  | Running Time: 5942.3547\n",
      "Episode: 10\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 11/200000  | Episode Reward: -2  | Running Time: 6247.8931\n",
      "Episode: 11\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 12/200000  | Episode Reward: -6  | Running Time: 6738.3322\n",
      "Episode: 12\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 13/200000  | Episode Reward: -5  | Running Time: 7217.1987\n",
      "Episode: 13\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 14/200000  | Episode Reward: 10  | Running Time: 7219.9721\n",
      "Episode: 14\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 15/200000  | Episode Reward: 7  | Running Time: 7403.6504\n",
      "Episode: 15\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 16/200000  | Episode Reward: -11  | Running Time: 7883.2686\n",
      "Episode: 16\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 17/200000  | Episode Reward: 4  | Running Time: 8254.3606\n",
      "Episode: 17\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 18/200000  | Episode Reward: -12  | Running Time: 8734.9396\n",
      "Episode: 18\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 19/200000  | Episode Reward: -10  | Running Time: 9213.4563\n",
      "Episode: 19\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 20/200000  | Episode Reward: -17  | Running Time: 9692.9740\n",
      "Episode: 20\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 21/200000  | Episode Reward: -8  | Running Time: 10181.8475\n",
      "Episode: 21\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 22/200000  | Episode Reward: -5  | Running Time: 10662.3339\n",
      "Episode: 22\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 23/200000  | Episode Reward: -11  | Running Time: 11141.7199\n",
      "Episode: 23\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 24/200000  | Episode Reward: -10  | Running Time: 11619.6737\n",
      "Episode: 24\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 25/200000  | Episode Reward: 10  | Running Time: 11620.0974\n",
      "Episode: 25\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 26/200000  | Episode Reward: -11  | Running Time: 12101.1463\n",
      "Episode: 26\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 27/200000  | Episode Reward: 10  | Running Time: 12104.4011\n",
      "Episode: 27\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 28/200000  | Episode Reward: -5  | Running Time: 12582.5760\n",
      "Episode: 28\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 29/200000  | Episode Reward: 7  | Running Time: 12738.4631\n",
      "Episode: 29\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 30/200000  | Episode Reward: 6  | Running Time: 12838.9492\n",
      "Episode: 30\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 31/200000  | Episode Reward: -15  | Running Time: 13319.3559\n",
      "Episode: 31\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 32/200000  | Episode Reward: 7  | Running Time: 13501.7207\n",
      "Episode: 32\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 33/200000  | Episode Reward: -13  | Running Time: 13988.0543\n",
      "Episode: 33\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 34/200000  | Episode Reward: -6  | Running Time: 14464.8934\n",
      "Episode: 34\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 35/200000  | Episode Reward: 9  | Running Time: 14492.5217\n",
      "Episode: 35\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 36/200000  | Episode Reward: -7  | Running Time: 14967.6140\n",
      "Episode: 36\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 37/200000  | Episode Reward: 5  | Running Time: 15027.3152\n",
      "Episode: 37\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 38/200000  | Episode Reward: -14  | Running Time: 15517.2010\n",
      "Episode: 38\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 39/200000  | Episode Reward: -8  | Running Time: 15991.3932\n",
      "Episode: 39\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 40/200000  | Episode Reward: -10  | Running Time: 16468.2546\n",
      "Episode: 40\n",
      "Step: 100\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 41/200000  | Episode Reward: 10  | Running Time: 16504.5508\n",
      "Episode: 41\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 42/200000  | Episode Reward: -12  | Running Time: 16979.5540\n",
      "Episode: 42\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 43/200000  | Episode Reward: 9  | Running Time: 17014.8386\n",
      "Episode: 43\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 44/200000  | Episode Reward: 7  | Running Time: 17089.2744\n",
      "Episode: 44\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 45/200000  | Episode Reward: -12  | Running Time: 17578.3077\n",
      "Episode: 45\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 46/200000  | Episode Reward: -15  | Running Time: 18058.9749\n",
      "Episode: 46\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 47/200000  | Episode Reward: 10  | Running Time: 18064.6543\n",
      "Episode: 47\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 48/200000  | Episode Reward: 10  | Running Time: 18064.9835\n",
      "Episode: 48\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 49/200000  | Episode Reward: -13  | Running Time: 18540.5681\n",
      "Episode: 49\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 50/200000  | Episode Reward: 3  | Running Time: 18685.1109\n",
      "Episode: 50\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 51/200000  | Episode Reward: -13  | Running Time: 19160.8562\n",
      "Episode: 51\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 52/200000  | Episode Reward: 9  | Running Time: 19169.4603\n",
      "Episode: 52\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 53/200000  | Episode Reward: 8  | Running Time: 19223.6344\n",
      "Episode: 53\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 54/200000  | Episode Reward: -11  | Running Time: 19700.2018\n",
      "Episode: 54\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 55/200000  | Episode Reward: -8  | Running Time: 20176.7756\n",
      "Episode: 55\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 56/200000  | Episode Reward: -8  | Running Time: 20653.0980\n",
      "Episode: 56\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 57/200000  | Episode Reward: 0  | Running Time: 20931.7206\n",
      "Episode: 57\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 58/200000  | Episode Reward: -9  | Running Time: 21410.1441\n",
      "Episode: 58\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 59/200000  | Episode Reward: 10  | Running Time: 21411.8474\n",
      "Episode: 59\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 60/200000  | Episode Reward: 4  | Running Time: 21810.7811\n",
      "Episode: 60\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 61/200000  | Episode Reward: -11  | Running Time: 22286.9198\n",
      "Episode: 61\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 62/200000  | Episode Reward: -12  | Running Time: 22763.7169\n",
      "Episode: 62\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 63/200000  | Episode Reward: -10  | Running Time: 23242.4319\n",
      "Episode: 63\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 64/200000  | Episode Reward: 10  | Running Time: 23247.4355\n",
      "Episode: 64\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 65/200000  | Episode Reward: -10  | Running Time: 23722.7344\n",
      "Episode: 65\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 66/200000  | Episode Reward: -2  | Running Time: 24057.9599\n",
      "Episode: 66\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 67/200000  | Episode Reward: 7  | Running Time: 24079.5159\n",
      "Episode: 67\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 68/200000  | Episode Reward: 9  | Running Time: 24170.3638\n",
      "Episode: 68\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Step: 100\n",
      "Training  | Episode: 69/200000  | Episode Reward: 9  | Running Time: 24176.5624\n",
      "Episode: 69\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 70/200000  | Episode Reward: -13  | Running Time: 24664.3859\n",
      "Episode: 70\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 71/200000  | Episode Reward: 8  | Running Time: 25054.0999\n",
      "Episode: 71\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 72/200000  | Episode Reward: -17  | Running Time: 25530.1704\n",
      "Episode: 72\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 73/200000  | Episode Reward: -12  | Running Time: 26008.5217\n",
      "Episode: 73\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 74/200000  | Episode Reward: 5  | Running Time: 26282.0558\n",
      "Episode: 74\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 75/200000  | Episode Reward: -12  | Running Time: 26757.3164\n",
      "Episode: 75\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 76/200000  | Episode Reward: 10  | Running Time: 26778.2829\n",
      "Episode: 76\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 77/200000  | Episode Reward: 3  | Running Time: 26964.7312\n",
      "Episode: 77\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 78/200000  | Episode Reward: 5  | Running Time: 27119.1219\n",
      "Episode: 78\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 79/200000  | Episode Reward: 8  | Running Time: 27187.3784\n",
      "Episode: 79\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 80/200000  | Episode Reward: -11  | Running Time: 27662.3934\n",
      "Episode: 80\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 81/200000  | Episode Reward: -13  | Running Time: 28151.8398\n",
      "Episode: 81\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 82/200000  | Episode Reward: -19  | Running Time: 28629.3348\n",
      "Episode: 82\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 83/200000  | Episode Reward: -9  | Running Time: 29023.2695\n",
      "Episode: 83\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 84/200000  | Episode Reward: 9  | Running Time: 29178.2188\n",
      "Episode: 84\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 85/200000  | Episode Reward: -13  | Running Time: 29655.1753\n",
      "Episode: 85\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 86/200000  | Episode Reward: -13  | Running Time: 30132.4939\n",
      "Episode: 86\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 87/200000  | Episode Reward: 8  | Running Time: 30308.0050\n",
      "Episode: 87\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 88/200000  | Episode Reward: -13  | Running Time: 30784.7057\n",
      "Episode: 88\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 89/200000  | Episode Reward: -11  | Running Time: 31259.0711\n",
      "Episode: 89\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 90/200000  | Episode Reward: 10  | Running Time: 31328.2353\n",
      "Episode: 90\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 91/200000  | Episode Reward: 0  | Running Time: 31581.1356\n",
      "Episode: 91\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 92/200000  | Episode Reward: -11  | Running Time: 32067.1719\n",
      "Episode: 92\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 93/200000  | Episode Reward: 8  | Running Time: 32290.2213\n",
      "Episode: 93\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 94/200000  | Episode Reward: 2  | Running Time: 32609.8853\n",
      "Episode: 94\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 95/200000  | Episode Reward: 4  | Running Time: 32792.9552\n",
      "Episode: 95\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 96/200000  | Episode Reward: 10  | Running Time: 32800.5513\n",
      "Episode: 96\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 97/200000  | Episode Reward: -16  | Running Time: 33282.0627\n",
      "Episode: 97\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 98/200000  | Episode Reward: 4  | Running Time: 33579.5177\n",
      "Episode: 98\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 99/200000  | Episode Reward: -9  | Running Time: 34051.6323\n",
      "Episode: 99\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 100/200000  | Episode Reward: -1  | Running Time: 34516.1206\n",
      "Episode: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 101/200000  | Episode Reward: 6  | Running Time: 34791.8669\n",
      "Episode: 101\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 102/200000  | Episode Reward: -19  | Running Time: 35281.1485\n",
      "Episode: 102\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 103/200000  | Episode Reward: 2  | Running Time: 35386.8676\n",
      "Episode: 103\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 104/200000  | Episode Reward: 10  | Running Time: 35388.3848\n",
      "Episode: 104\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 105/200000  | Episode Reward: -13  | Running Time: 35866.1436\n",
      "Episode: 105\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 106/200000  | Episode Reward: -20  | Running Time: 36345.4899\n",
      "Episode: 106\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 107/200000  | Episode Reward: -11  | Running Time: 36824.2288\n",
      "Episode: 107\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 108/200000  | Episode Reward: 1  | Running Time: 37094.4077\n",
      "Episode: 108\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 109/200000  | Episode Reward: -11  | Running Time: 37568.2165\n",
      "Episode: 109\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 110/200000  | Episode Reward: 5  | Running Time: 37879.1120\n",
      "Episode: 110\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 111/200000  | Episode Reward: -9  | Running Time: 38353.9994\n",
      "Episode: 111\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 112/200000  | Episode Reward: -12  | Running Time: 38836.9434\n",
      "Episode: 112\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 113/200000  | Episode Reward: 6  | Running Time: 38944.7363\n",
      "Episode: 113\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 114/200000  | Episode Reward: 8  | Running Time: 39015.6865\n",
      "Episode: 114\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 115/200000  | Episode Reward: -14  | Running Time: 39491.5963\n",
      "Episode: 115\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 116/200000  | Episode Reward: -9  | Running Time: 39970.0996\n",
      "Episode: 116\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 117/200000  | Episode Reward: -12  | Running Time: 40448.2100\n",
      "Episode: 117\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 118/200000  | Episode Reward: 8  | Running Time: 40451.0602\n",
      "Episode: 118\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 119/200000  | Episode Reward: 10  | Running Time: 40508.8655\n",
      "Episode: 119\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 120/200000  | Episode Reward: -9  | Running Time: 40984.6021\n",
      "Episode: 120\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 121/200000  | Episode Reward: 4  | Running Time: 41055.6744\n",
      "Episode: 121\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 122/200000  | Episode Reward: -18  | Running Time: 41531.1550\n",
      "Episode: 122\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 123/200000  | Episode Reward: -15  | Running Time: 42008.3075\n",
      "Episode: 123\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 124/200000  | Episode Reward: 7  | Running Time: 42071.6963\n",
      "Episode: 124\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 125/200000  | Episode Reward: -7  | Running Time: 42561.5534\n",
      "Episode: 125\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 126/200000  | Episode Reward: -2  | Running Time: 42985.8457\n",
      "Episode: 126\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 127/200000  | Episode Reward: 6  | Running Time: 43086.5776\n",
      "Episode: 127\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 128/200000  | Episode Reward: -10  | Running Time: 43553.3704\n",
      "Episode: 128\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 129/200000  | Episode Reward: -14  | Running Time: 43982.4364\n",
      "Episode: 129\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 130/200000  | Episode Reward: -15  | Running Time: 44419.1358\n",
      "Episode: 130\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 131/200000  | Episode Reward: 6  | Running Time: 44460.2939\n",
      "Episode: 131\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 132/200000  | Episode Reward: 7  | Running Time: 44495.3981\n",
      "Episode: 132\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 133/200000  | Episode Reward: -12  | Running Time: 44931.1917\n",
      "Episode: 133\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 134/200000  | Episode Reward: 0  | Running Time: 45273.3146\n",
      "Episode: 134\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 135/200000  | Episode Reward: 3  | Running Time: 45773.6410\n",
      "Episode: 135\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 136/200000  | Episode Reward: 9  | Running Time: 46111.7059\n",
      "Episode: 136\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 137/200000  | Episode Reward: 7  | Running Time: 46356.0525\n",
      "Episode: 137\n",
      "Step: 100\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 138/200000  | Episode Reward: 10  | Running Time: 46396.3667\n",
      "Episode: 138\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 139/200000  | Episode Reward: -10  | Running Time: 46981.6495\n",
      "Episode: 139\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 140/200000  | Episode Reward: 6  | Running Time: 47268.7586\n",
      "Episode: 140\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Decision Step reward: -1.0\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 141/200000  | Episode Reward: -7  | Running Time: 47906.4366\n",
      "Episode: 141\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 142/200000  | Episode Reward: -8  | Running Time: 48336.5923\n",
      "Episode: 142\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Step: 600\n",
      "Step: 700\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Step: 1000\n",
      "Training  | Episode: 143/200000  | Episode Reward: 7  | Running Time: 48668.6311\n",
      "Episode: 143\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Step: 500\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 144/200000  | Episode Reward: 6  | Running Time: 48812.3141\n",
      "Episode: 144\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Step: 500\n",
      "Decision Step reward: -1.0\n",
      "Step: 600\n",
      "Decision Step reward: -1.0\n",
      "Step: 700\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 800\n",
      "Step: 900\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 1000\n",
      "Decision Step reward: -1.0\n",
      "Step: 1100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -1\n",
      "Step: 1200\n",
      "Training  | Episode: 145/200000  | Episode Reward: -18  | Running Time: 49285.6325\n",
      "Episode: 145\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 146/200000  | Episode Reward: 6  | Running Time: 49365.9815\n",
      "Episode: 146\n",
      "Step: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "# ENV_ID = 'CartPole-v1'  # environment id\n",
    "# RANDOM_SEED = 2  # random seed, can be either an int number or None\n",
    "# RENDER = False  # render while training\n",
    "\n",
    "# ALG_NAME = 'AC'\n",
    "ALG_NAME = 'A2C'\n",
    "ENV_ID = 'S0'\n",
    "TRAIN_EPISODES = 200000  # number of overall episodes for training\n",
    "TEST_EPISODES = 10  # number of overall episodes for testing\n",
    "MAX_STEPS = 1200  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 0.001\n",
    "speed = 2\n",
    "# LR_A = 0.001  # learning rate for actor\n",
    "# LR_C = 0.01  # learning rate for critic\n",
    "\n",
    "\n",
    "###############################  Actor-Critic  ####################################\n",
    "\n",
    "\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_num, lr=0.001):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, action_num)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, action, td_error):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         loss = td_error * torch.nn.functional.cross_entropy(logits, torch.LongTensor([action]))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def get_action(self, state, greedy=False):\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         probs = torch.nn.functional.softmax(logits, dim=-1).detach().numpy()\n",
    "#         if greedy:\n",
    "#             return np.argmax(probs)\n",
    "#         return np.random.choice(len(probs[0]), p=probs[0])\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_actor.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_actor.pt')))\n",
    "\n",
    "\n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_dim, lr=0.01):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, 1)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, reward, state_, done):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         d = 0 if done else 1\n",
    "#         v_ = self.model(torch.FloatTensor(state_))\n",
    "#         v = self.model(torch.FloatTensor(state))\n",
    "#         td_error = reward + d * LAM * v_ - v\n",
    "#         loss = td_error ** 2\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return td_error.item()\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_critic.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_critic.pt')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ''' \n",
    "    choose environment\n",
    "    1. Openai gym:\n",
    "    env = gym.make()\n",
    "    2. DeepMind Control Suite:\n",
    "    env = dm_control2gym.make()\n",
    "    '''\n",
    "    # env = gym.make(ENV_ID).unwrapped\n",
    "    # # dm_control2gym.create_render_mode('example mode', show=True, return_pixel=False, height=240, width=320, camera_id=-1, overlays=(),\n",
    "    # #              depth=False, scene_option=None)\n",
    "    # # env = dm_control2gym.make(domain_name=\"cartpole\", task_name=\"balance\")\n",
    "\n",
    "    # env.seed(RANDOM_SEED)  # reproducible\n",
    "    # np.random.seed(RANDOM_SEED)\n",
    "    # torch.manual_seed(RANDOM_SEED)  # reproducible\n",
    "\n",
    "    # N_F = env.observation_space.shape[0]\n",
    "    # N_A = env.action_space.n\n",
    "\n",
    "    # print(\"observation dimension: %d\" % N_F)  # 4\n",
    "    # print(\"observation high: %s\" % env.observation_space.high)  # [ 2.4 , inf , 0.41887902 , inf]\n",
    "    # print(\"observation low : %s\" % env.observation_space.low)  # [-2.4 , -inf , -0.41887902 , -inf]\n",
    "    # print(\"num of actions: %d\" % N_A)  # 2 : left or right\n",
    "\n",
    "    # actor = Actor(state_dim=N_F, action_num=N_A, lr=LR_A)\n",
    "    # # we need a good teacher, so the teacher should learn faster than the actor\n",
    "    # critic = Critic(state_dim=N_F, lr=LR_C)\n",
    "\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    best_score = 0\n",
    "    if train:\n",
    "        all_episode_reward = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            print(f'Episode: {episode}')\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            step = 0  # number of step in this episode\n",
    "            episode_reward = 0  # rewards of all steps\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "                # if RENDER:\n",
    "                #     env.render()\n",
    "\n",
    "                # action = actor.get_action(state) state --> vt,lt\n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state) #action is the direct linear output from the model\n",
    "                action_dist = Categorical(F.softmax(action))\n",
    "                index = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "                \n",
    "                \n",
    "                # action_onehot = F.one_hot(torch.argmax(action),num_actions)\n",
    "                # print(action_onehot)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if episode<50:\n",
    "                #     if episode%2:\n",
    "                #         action_onehot = torch.tensor([1,0,1,0])\n",
    "                #     else: action_onehot = torch.tensor([1,0,0,1])                 \n",
    "                # continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "                # action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                step += 1\n",
    "\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # print((vt == vt_new).all())\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    if reward<0:\n",
    "                        print(f'Decision Step reward: {reward}')\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "                \n",
    "\n",
    "                if step >= MAX_STEPS:\n",
    "                    reward = -1\n",
    "                    print(f'Max Step Reward: {reward}')\n",
    "                    done = True\n",
    "                if step % 100 == 0:\n",
    "                    print (f'Step: {step}')\n",
    "\n",
    "                \n",
    "                    \n",
    "                episode_reward += reward\n",
    "\n",
    "                \n",
    "\n",
    "                # try:\n",
    "                #     td_error = critic.learn(\n",
    "                #         state, reward, state_new, done\n",
    "                #     )  # learn Value-function : gradient = grad[r + lambda * V(s_new) - V(s)]\n",
    "                #     actor.learn(state, action, td_error)  # learn Policy : true_gradient = grad[logPi(s, a) * td_error]                   \n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "                    # d = 0 if done else 1\n",
    "                    # v_ = self.model(torch.FloatTensor(state_))\n",
    "                    # v = self.model(torch.FloatTensor(state))\n",
    "                    if not done:\n",
    "                        action_new, value_new, lstm_hidden_state_new = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # initialize the value first\n",
    "                        td_error = reward + LAM * value_new - value\n",
    "                    else: td_error = reward - value\n",
    "\n",
    "                    critic_loss = td_error ** 2\n",
    "\n",
    "                    log_prob = action_dist.log_prob(index) \n",
    "                    actor_loss = -(log_prob * td_error)\n",
    "\n",
    "                    logits = action # the actual model output (without softmax)\n",
    "                    labels = action_onehot.float().to(device)  # one hot vector of the sampled action\n",
    "                    entropy_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "                    total_loss = actor_loss + 0.5* critic_loss - 0.001*entropy_loss\n",
    "                    # entropy loss entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=tf.nn.softmax(logit))\n",
    "                    # comb_loss = tf.reduce_mean((0.5 * value_loss + policy_loss - 0.01 * entropy))\n",
    "\n",
    "                    total_loss = total_loss.to(device)                   \n",
    "                    total_loss.backward(retain_graph=True)                    \n",
    "                except KeyboardInterrupt:  # if Ctrl+C at running actor.learn(), then save model, or exit if not at actor.learn()\n",
    "                    agent.save(episode)\n",
    "                    print('model has been saved')\n",
    "\n",
    "                # state = state_new\n",
    "                # print(f'episode: {episode}, step:{step}')\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            if episode%500 == 0:\n",
    "                agent.save(episode)\n",
    "                print(\"Model has been saved\")\n",
    "                \n",
    "            # all_episode_reward.append(episode_reward)\n",
    "            # if episode >= 100:\n",
    "            #     avg_score = np.mean(all_episode_reward[-100:])\n",
    "            #     if avg_score > best_score:\n",
    "            #         best_score = avg_score\n",
    "            #         agent.save()\n",
    "            #         print(f'The best score for averaging previous 100 episode reward is {best_score}. Model has been saved')\n",
    "\n",
    "\n",
    "\n",
    "            # if episode == 0:\n",
    "            #     all_episode_reward.append(episode_reward)\n",
    "            # else:\n",
    "            #     reference_value = all_episode_reward[-1] * 0.9 + episode_reward * 0.1\n",
    "            #     if reference_value > all_episode_reward_best:\n",
    "            #         agent.save()\n",
    "            #         print(f'The best all_episode_reward is {reference_value}. Model has been saved')\n",
    "            #         all_episode_reward_best = reference_value\n",
    "            #     all_episode_reward.append(reference_value)\n",
    "\n",
    "            print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                  .format(episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([ALG_NAME, ENV_ID])))\n",
    "        \n",
    "\n",
    "    if not train:\n",
    "        agent.load(episode)\n",
    "        print(\"model has been loaded\")\n",
    "        tracked_agent = -1\n",
    "\n",
    "        for episode in range(TEST_EPISODES):\n",
    "            episode_time = time.time()\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index]\n",
    "            t = 0  # number of step in this episode\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # action = actor.get_action(state, greedy=True)\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if done:\n",
    "                #     reward = -20\n",
    "\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                action = F.one_hot(torch.argmax(action),num_actions)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action).reshape(1,4) #[forward, backward, right, left]\n",
    "                action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                vt_new = torch.tensor(decision_steps.obs).reshape(1,3,128,128)\n",
    "                \n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                if tracked_agent in decision_steps:\n",
    "                    # tracked_agent = decision_steps.agent_id[0]\n",
    "                    reward = decision_steps[tracked_agent].reward \n",
    "                # if done:\n",
    "                #     reward = -20  # reward shaping trick\n",
    "                if tracked_agent in terminal_steps:\n",
    "                    done = True\n",
    "                    reward = -20\n",
    "\n",
    "                episode_reward += reward\n",
    "                vt = vt_new\n",
    "                t += 1\n",
    "\n",
    "                if done or t >= MAX_STEPS:\n",
    "                    print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                          .format(episode + 1, TEST_EPISODES, episode_reward, time.time() - t0))\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 285 in total\n",
      "10.0\n",
      "1: 449 in total\n",
      "10.0\n",
      "2: 2167 in total\n",
      "10.0\n",
      "3: 1776 in total\n",
      "10.0\n",
      "4: 900 in total\n",
      "10.0\n",
      "5: 1747 in total\n",
      "10.0\n",
      "6: 158 in total\n",
      "10.0\n",
      "7: 597 in total\n",
      "10.0\n",
      "8: 482 in total\n",
      "10.0\n",
      "9: 881 in total\n",
      "10.0\n",
      "For speed 2, average random step for hitting the target is 944.2\n",
      "0: 929 in total\n",
      "10.0\n",
      "1: 714 in total\n",
      "10.0\n",
      "2: 1594 in total\n",
      "10.0\n",
      "3: 217 in total\n",
      "10.0\n",
      "4: 993 in total\n",
      "10.0\n",
      "5: 346 in total\n",
      "10.0\n",
      "6: 1036 in total\n",
      "10.0\n",
      "7: 3062 in total\n",
      "10.0\n",
      "8: 182 in total\n",
      "10.0\n",
      "9: 332 in total\n",
      "10.0\n",
      "For speed 3, average random step for hitting the target is 1034.92\n",
      "0: 113 in total\n",
      "10.0\n",
      "1: 558 in total\n",
      "10.0\n",
      "2: 940 in total\n",
      "10.0\n",
      "3: 1588 in total\n",
      "10.0\n",
      "4: 1060 in total\n",
      "10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: 1461 in total\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Try to find the optimized MAX_STEP\n",
    "# speed 1: 10000+ speed 2,3: 1000\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# speed = 1\n",
    "TRAIN_EPISODES = 20\n",
    "tracked_agent = -1\n",
    "num_actions = 4\n",
    "average = 0\n",
    "for speed in range(2,10):\n",
    "    for episode in range(TRAIN_EPISODES):\n",
    "        env.reset()\n",
    "        behavior_name=list(env.behavior_specs)[0]\n",
    "        step = 0\n",
    "        while True:\n",
    "            index = random.randint(0, 3) # sample an action from action_dist\n",
    "            action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "            discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(discrete_actions)\n",
    "            env.set_actions(behavior_name,action_tuple)\n",
    "            env.step()\n",
    "            step += 1\n",
    "\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "                \n",
    "            if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                reward = terminal_steps[tracked_agent].reward\n",
    "                if reward > 0: # hit the target\n",
    "                    print(f'{episode}: {step} in total')\n",
    "                    print(reward)\n",
    "                    average += step\n",
    "                    break\n",
    "                else:           # roll over or other conditions\n",
    "                    env.reset()\n",
    "                    step = 0\n",
    "                    continue # roll over or other unseen conditions\n",
    "            if tracked_agent in decision_steps: # the agent which requires action\n",
    "                continue\n",
    "    average /= 10\n",
    "    print(f'For speed {speed}, average random step for hitting the target is {average}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unity could not open in work station\n",
    "# The agent in terminal step will also in decison step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a = torch.tensor([[0.2460, 0.2503, 0.2647, 0.2389]])\n",
    "b = F.softmax(a,dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1,2,3]\n",
    "avg = np.mean(a[-10:])\n",
    "avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
