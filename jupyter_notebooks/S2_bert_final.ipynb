{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env_train =  UE(file_name=\"S2_1\\\\build\",seed=1,side_channels=[],worker_id=4,no_graphics = False)\n",
    "env_train.reset()\n",
    "env_test =  UE(file_name=\"S2_2\\\\build\",seed=1,side_channels=[],worker_id=5,no_graphics = False)\n",
    "env_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(768, embedding_dim),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.linear(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(vision_output_dim + language_output_dim, mixing_dim),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt'))) \n",
    "\n",
    "    # def load(self, episode, ALG_NAME, ENV_ID):\n",
    "    #     path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "    #     saved_state_dict = torch.load(os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    #     # Create a new state_dict for the model and only copy parameters except 'language_module'\n",
    "    #     new_state_dict = {}\n",
    "    #     for key, value in saved_state_dict.items():\n",
    "    #         if 'language_module' not in key:\n",
    "    #             new_state_dict[key] = value\n",
    "\n",
    "    #     # Load the modified state_dict into the agent\n",
    "    #     self.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss):\n",
    "    env = env_test\n",
    "    TEST_EPISODES = 100\n",
    "    tracked_agent = -1\n",
    "    entropy_term = 0\n",
    "    for episode in range(TEST_EPISODES):\n",
    "        test_episode += 1\n",
    "        t0 = time.time()\n",
    "        episode_reward = 0\n",
    "        # env.reset()\n",
    "        behavior_name=list(env.behavior_specs)[0]\n",
    "        spec=env.behavior_specs[behavior_name]\n",
    "        # state = env.reset().astype(np.float32)\n",
    "        STEPS = 0\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        # state -- vt, lt, lstm\n",
    "        vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "        index1 = int(decision_steps.obs[1][0][0])\n",
    "        index2 = int(decision_steps.obs[1][0][1])+5\n",
    "        input_string = f'{hashmap[index2]} {hashmap[index1]}'\n",
    "        print(f'TEST: ---{input_string}---')\n",
    "        lt = bert_encoder[input_string].squeeze().to(device)\n",
    "        lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "        done = False\n",
    "        while True:\n",
    "\n",
    "            # Need to use when calculating the loss\n",
    "            log_probs = []\n",
    "            # values = []\n",
    "            values = torch.empty(0).to(device)\n",
    "            rewards = []\n",
    "\n",
    "            \n",
    "            lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "            STEPS += 1\n",
    "            policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "            # value = value.detach()\n",
    "            dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "            \n",
    "\n",
    "            action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "            # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "            action = action_dist.sample() # sample an action from action_dist\n",
    "            action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "            \n",
    "            log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "            # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "            # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "            entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "            discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "            action_tuple = ActionTuple()\n",
    "            action_tuple.add_discrete(discrete_actions)\n",
    "            env.set_actions(behavior_name,action_tuple)\n",
    "            env.step()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "                # print(tracked_agent)\n",
    "\n",
    "            if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                print('TEST: Agent in terminal steps')\n",
    "                done = True\n",
    "                reward = terminal_steps[tracked_agent].reward\n",
    "                if reward > 0:\n",
    "                    pass\n",
    "                else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                print(f'TEST: Terminal Step reward: {reward}')\n",
    "\n",
    "            elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                reward = decision_steps[tracked_agent].reward\n",
    "                # print(f'Decision Step reward: {reward}')\n",
    "                if reward<0:\n",
    "                    print(f'TEST: Decision Step reward: {reward}')\n",
    "            if STEPS >= MAX_STEPS:\n",
    "                reward = -10\n",
    "                print(f'TEST: Max Step Reward: {reward}')\n",
    "                env.reset()\n",
    "                done = True\n",
    "            if STEPS % 100 == 0:\n",
    "                print (f'TEST: Step: {STEPS}')\n",
    "\n",
    "            episode_reward = episode_reward + reward\n",
    "\n",
    "            rewards.append(reward)\n",
    "            # values.append(value)\n",
    "            values = torch.cat((values, value), dim=0)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term = entropy_term + entropy\n",
    "            vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            vt = vt_new\n",
    "\n",
    "            if done:\n",
    "                # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                # Qval = Qval.detach()\n",
    "                break\n",
    "            \n",
    "            \n",
    "        discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "        cumulative = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "            discounted_rewards[t] = cumulative\n",
    "        # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "        # Advantage Actor Critic\n",
    "\n",
    "        # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "        # for t in range(len(rewards)-1):\n",
    "        #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "        \n",
    "        # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "        # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "        #     = Q_t - V_t\n",
    "        \n",
    "        # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "        # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "        # Critic_loss = A_t.pow(2) *0.5\n",
    "        # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "        # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "        \n",
    "        #update actor critic\n",
    "        \n",
    "        # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        advantage = discounted_rewards - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "        entropy_term /= STEPS\n",
    "        entropy_loss = -0.1 * entropy_term\n",
    "        ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "        test_episode_reward.append(float(episode_reward))\n",
    "        test_steps.append(STEPS)\n",
    "        test_actor_loss.append(float(actor_loss))\n",
    "        test_critic_loss.append(float(critic_loss))\n",
    "        test_entropy_loss.append(float(entropy_loss))\n",
    "        test_total_loss.append(float(ac_loss))\n",
    "\n",
    "        if test_episode >= 100:\n",
    "            avg_score = np.mean(test_episode_reward[-100:])\n",
    "            if avg_score > 9: agent.save(f'finish_{episode}', ALG_NAME, ENV_ID)\n",
    "            test_average_reward.append(avg_score)\n",
    "            print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                .format(episode + 1, TEST_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "        else:  print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                .format(episode + 1, TEST_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "    return test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "with open(r'C:\\Users\\linzj\\Desktop\\bert.pkl', 'rb') as pickle_file:\n",
    "    bert_encoder = pickle.load(pickle_file)\n",
    "entropy_term = 0\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "S0_ALG_NAME = 'S2bert_final'\n",
    "S0_ENV_ID = '6'\n",
    "S0_episode = 24607\n",
    "ALG_NAME = 'S2bert_final'\n",
    "ENV_ID = '7'\n",
    "TRAIN_EPISODES = 250000  # number of overall episodes for training  # number of overall episodes for testing\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 2.5e-5  #0.00005 \n",
    "speed = 3\n",
    "num_steps = 250 # the step for updating the network\n",
    "test_episode = 0\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent(embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = float('-inf')\n",
    "    hashmap = {\n",
    "        0: 'capsule',\n",
    "        1: 'cube',\n",
    "        2: 'cylinder',\n",
    "        3: 'prism',\n",
    "        4: 'sphere',\n",
    "        5: 'red',\n",
    "        6: 'green',\n",
    "        7: 'blue',\n",
    "        8: 'yellow',\n",
    "        9: 'black'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        test_episode_reward = []\n",
    "        test_average_reward = []\n",
    "        test_steps = []\n",
    "        test_actor_loss = []\n",
    "        test_critic_loss = []\n",
    "        test_entropy_loss = []\n",
    "        test_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        test_episode = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        env = env_train\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index1 = int(decision_steps.obs[1][0][0])\n",
    "            index2 = int(decision_steps.obs[1][0][1])+5\n",
    "            input_string = f'{hashmap[index2]} {hashmap[index1]}'\n",
    "            print(f'---{input_string}---')\n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "            lt = bert_encoder[input_string].squeeze().to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                    # value = value.detach()\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % num_steps == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.1 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(float(episode_reward))\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(float(actor_loss))\n",
    "            all_critic_loss.append(float(critic_loss))\n",
    "            all_entropy_loss.append(float(entropy_loss))\n",
    "            all_total_loss.append(float(ac_loss))\n",
    "            if episode >= 100:\n",
    "                avg_score = np.mean(all_episode_reward[-100:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 100 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%5000 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "            if (episode+1)%100 == 0:\n",
    "                test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss = test(agent,test_episode,test_episode_reward,test_average_reward,test_steps,test_actor_loss,test_critic_loss,test_entropy_loss,test_total_loss)\n",
    "\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}_train.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "        \n",
    "        test_data = {\n",
    "                    'all_average_reward': test_average_reward,\n",
    "                    'all_episode_reward': test_episode_reward,\n",
    "                    'all_actor_loss': test_actor_loss,\n",
    "                    'all_critic_loss': test_critic_loss,\n",
    "                    'all_entropy_loss': test_entropy_loss,\n",
    "                    'all_total_loss': test_total_loss,\n",
    "                    'all_steps': test_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}_test.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(test_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "            'all_average_reward': all_average_reward,\n",
    "            'all_episode_reward': all_episode_reward,\n",
    "            'all_actor_loss': all_actor_loss,\n",
    "            'all_critic_loss': all_critic_loss,\n",
    "            'all_entropy_loss': all_entropy_loss,\n",
    "            'all_total_loss': all_total_loss,\n",
    "            'all_steps': all_steps,\n",
    "        } \n",
    "file_path = f'result/{ALG_NAME}_{ENV_ID}_train.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "test_data = {\n",
    "            'all_average_reward': test_average_reward,\n",
    "            'all_episode_reward': test_episode_reward,\n",
    "            'all_actor_loss': test_actor_loss,\n",
    "            'all_critic_loss': test_critic_loss,\n",
    "            'all_entropy_loss': test_entropy_loss,\n",
    "            'all_total_loss': test_total_loss,\n",
    "            'all_steps': test_steps,\n",
    "        } \n",
    "file_path = f'result/{ALG_NAME}_{ENV_ID}_test.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(test_data, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
