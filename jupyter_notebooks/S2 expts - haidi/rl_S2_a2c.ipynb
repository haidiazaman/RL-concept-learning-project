{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T05:47:58.196412Z",
     "start_time": "2023-07-18T05:47:57.488485Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "# env1 =  UE(file_name=\"S1_4\\\\build\",seed=1,side_channels=[],worker_id=1,no_graphics = False)\n",
    "# env1.reset()\n",
    "\n",
    "# env2 =  UE(file_name=\"S1b_2\\\\build\",seed=1,side_channels=[],worker_id=2,no_graphics = False)\n",
    "# env2.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T05:48:58.829326Z",
     "start_time": "2023-07-18T05:48:53.145981Z"
    }
   },
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "# file_name=\"C:\\Users\\Palaash.HPZ\\Desktop\\RL-concept-learning_large_build_envs\\windows\\S2 180723\\build\"\n",
    "file_name = \"C:\\\\Users\\\\Palaash.HPZ\\\\Desktop\\\\RL-concept-learning_large_build_envs\\\\windows\\\\S2 180723\\\\build\"\n",
    "\n",
    "env1 =  UE(file_name=file_name,seed=1,side_channels=[],worker_id=1,no_graphics = False)\n",
    "env1.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-18T05:50:09.843204Z",
     "start_time": "2023-07-18T05:50:07.997416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\mlagents\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "---sphere---\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "\n",
    "# load from S0 - these are the S0 best model details\n",
    "S0_ALG_NAME = 'S0'\n",
    "S0_ENV_ID = '3'\n",
    "S0_episode = 7593\n",
    "\n",
    "ALG_NAME = 'S2'\n",
    "ENV_ID = '1'\n",
    "TRAIN_EPISODES = 200000  # number of overall episodes for training\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "env_per_iteration = 100\n",
    "lr = 3.5e-5  #LR\n",
    "speed = 1\n",
    "num_steps = 250 # the step for updating the network\n",
    "max_step_reward = -10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = float('-inf')\n",
    "    hashmap = {\n",
    "    0: 'capsule',\n",
    "    1: 'cube',\n",
    "    2: 'cylinder',\n",
    "    3: 'prism',\n",
    "    4: 'sphere',\n",
    "    5: 'red',\n",
    "    6: 'green',\n",
    "    7: 'blue',\n",
    "    8: 'yellow',\n",
    "    9: 'black'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            env_index = (episode // env_per_iteration) % 2\n",
    "            if env_index == 0: env = env1\n",
    "            else: env = env2\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = int(decision_steps.obs[1])\n",
    "            if env_index: index = index + 5\n",
    "            print(f'---{hashmap[index]}---')\n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    if STEPS % 5 == 0:\n",
    "                        lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                        policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    \n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                            # if reward<-1: hit = 1\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = max_step_reward\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % num_steps == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                # print(len(rewards))\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.1 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # print('updated')\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(float(episode_reward))\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(float(actor_loss))\n",
    "            all_critic_loss.append(float(critic_loss))\n",
    "            all_entropy_loss.append(float(entropy_loss))\n",
    "            all_total_loss.append(float(ac_loss))\n",
    "            if episode >= 200:\n",
    "                avg_score = np.mean(all_episode_reward[-200:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 200 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.1f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.1f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%5000 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'result/{ALG_NAME}_{ENV_ID}.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "            'all_average_reward': all_average_reward,\n",
    "            'all_episode_reward': all_episode_reward,\n",
    "            'all_actor_loss': all_actor_loss,\n",
    "            'all_critic_loss': all_critic_loss,\n",
    "            'all_entropy_loss': all_entropy_loss,\n",
    "            'all_total_loss': all_total_loss,\n",
    "            'all_steps': all_steps,\n",
    "        } \n",
    "file_path = f'result/{ALG_NAME}_{ENV_ID}.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_average_reward)\n",
    "list_string = str(all_average_reward)\n",
    "file_path = f\"{ALG_NAME}_{ENV_ID}.txt\"  # Specify the file path and name\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write the list string to the file\n",
    "    file.write(list_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-1(20000): -5 # -2.5 (10000): -6.73  #-5 (5000):-9.46  # -7.5 (5000): -7.02  #-10 (5000):-10(circle same place)\n",
    "# S1_4:12996      S1_13:6495           S1_14:4587        S1_15: 4073            S1_16:3085\n",
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cpu\")\n",
    "episode = 1523\n",
    "speed = 3\n",
    "MAX_STEPS = 500\n",
    "TEST_EPISODES = 100\n",
    "ALG_NAME = 'S1'\n",
    "ENV_ID = '55' \n",
    "tracked_agent = -1\n",
    "env.reset()\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode,ALG_NAME,ENV_ID)\n",
    "average = 0\n",
    "hashmap = {\n",
    "    0: 'capsule',\n",
    "    1: 'cube',\n",
    "    2: 'cylinder',\n",
    "    3: 'prism',\n",
    "    4: 'sphere'}\n",
    "for episode in range(TEST_EPISODES):\n",
    "            STEPS = 0\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = int(decision_steps.obs[1])\n",
    "            print(f'---{hashmap[index]}---')\n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while not done:\n",
    "                STEPS += 1                \n",
    "                lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                # value = value.detach()\n",
    "                dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                \n",
    "\n",
    "                action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                action = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    # if reward<0:\n",
    "                    #     print(f'Decision Step reward: {reward}')\n",
    "\n",
    "                if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "\n",
    "                episode_reward = episode_reward + reward\n",
    "                vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                vt = vt_new\n",
    "            average += episode_reward / TEST_EPISODES\n",
    "            print(f'Episode: {episode}, Episode reward: {episode_reward}')\n",
    "print(f'Average Episode Reward: {average}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0, 2.5, 5, 7.5, 10]\n",
    "y = [-7.26, -6.73, -9.46, -7.02, -10]\n",
    "#Except -10 reward, all settings are approximatly the average of (10+0+-10+-20) = -5\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Negative Reward')\n",
    "plt.ylabel('Average Reward for 100 episodes')\n",
    "plt.title('Plot')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis labels\n",
    "plt.xticks(x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate x-axis values as the indices of the list\n",
    "x_axis = range(len(all_average_reward))\n",
    "\n",
    "# Plotting the figure\n",
    "plt.plot(x_axis, all_average_reward)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Rewards (Average 100 episodes) vs Episode')+\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 738\n",
    "env =  UE(file_name=\"280623_1\\\\build\",seed=1,side_channels=[],worker_id=0,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# dist = np.array((2,1,2,2))\n",
    "# entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "# entropy\n",
    "\n",
    "action = torch.tensor([[0.1,0.2,0.1,1]])\n",
    "index = torch.tensor([0])\n",
    "print(action,index)\n",
    "entropy_loss = F.cross_entropy(action, index)\n",
    "entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need env.reset() at the beginning of each episode or unity file will reset it\n",
    "# The utility of workstation GPU is only 1 percent (Memory Leak?) in laptop GPU 8 seconds / episode on average\n",
    "# Can we make the unity environment accept action from GPU?\n",
    "#  Model is still hard to learn meaningful actions -- Make sure the algorithm is correct (Weight Initialization? Experience Replay?) value layer how to update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "plt.plot(all_rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_lengths)\n",
    "plt.plot(average_lengths)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "# ENV_ID = 'CartPole-v1'  # environment id\n",
    "# RANDOM_SEED = 2  # random seed, can be either an int number or None\n",
    "# RENDER = False  # render while training\n",
    "\n",
    "# ALG_NAME = 'AC'\n",
    "ALG_NAME = 'A2C'\n",
    "ENV_ID = 'S0'\n",
    "TRAIN_EPISODES = 200000  # number of overall episodes for training\n",
    "TEST_EPISODES = 10  # number of overall episodes for testing\n",
    "MAX_STEPS = 1200  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 0.001\n",
    "speed = 2\n",
    "# LR_A = 0.001  # learning rate for actor\n",
    "# LR_C = 0.01  # learning rate for critic\n",
    "\n",
    "\n",
    "###############################  Actor-Critic  ####################################\n",
    "\n",
    "\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_num, lr=0.001):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, action_num)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, action, td_error):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         loss = td_error * torch.nn.functional.cross_entropy(logits, torch.LongTensor([action]))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def get_action(self, state, greedy=False):\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         probs = torch.nn.functional.softmax(logits, dim=-1).detach().numpy()\n",
    "#         if greedy:\n",
    "#             return np.argmax(probs)\n",
    "#         return np.random.choice(len(probs[0]), p=probs[0])\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_actor.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_actor.pt')))\n",
    "\n",
    "\n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_dim, lr=0.01):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, 1)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, reward, state_, done):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         d = 0 if done else 1\n",
    "#         v_ = self.model(torch.FloatTensor(state_))\n",
    "#         v = self.model(torch.FloatTensor(state))\n",
    "#         td_error = reward + d * LAM * v_ - v\n",
    "#         loss = td_error ** 2\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return td_error.item()\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_critic.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_critic.pt')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ''' \n",
    "    choose environment\n",
    "    1. Openai gym:\n",
    "    env = gym.make()\n",
    "    2. DeepMind Control Suite:\n",
    "    env = dm_control2gym.make()\n",
    "    '''\n",
    "    # env = gym.make(ENV_ID).unwrapped\n",
    "    # # dm_control2gym.create_render_mode('example mode', show=True, return_pixel=False, height=240, width=320, camera_id=-1, overlays=(),\n",
    "    # #              depth=False, scene_option=None)\n",
    "    # # env = dm_control2gym.make(domain_name=\"cartpole\", task_name=\"balance\")\n",
    "\n",
    "    # env.seed(RANDOM_SEED)  # reproducible\n",
    "    # np.random.seed(RANDOM_SEED)\n",
    "    # torch.manual_seed(RANDOM_SEED)  # reproducible\n",
    "\n",
    "    # N_F = env.observation_space.shape[0]\n",
    "    # N_A = env.action_space.n\n",
    "\n",
    "    # print(\"observation dimension: %d\" % N_F)  # 4\n",
    "    # print(\"observation high: %s\" % env.observation_space.high)  # [ 2.4 , inf , 0.41887902 , inf]\n",
    "    # print(\"observation low : %s\" % env.observation_space.low)  # [-2.4 , -inf , -0.41887902 , -inf]\n",
    "    # print(\"num of actions: %d\" % N_A)  # 2 : left or right\n",
    "\n",
    "    # actor = Actor(state_dim=N_F, action_num=N_A, lr=LR_A)\n",
    "    # # we need a good teacher, so the teacher should learn faster than the actor\n",
    "    # critic = Critic(state_dim=N_F, lr=LR_C)\n",
    "\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    best_score = 0\n",
    "    if train:\n",
    "        all_episode_reward = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            print(f'Episode: {episode}')\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            step = 0  # number of step in this episode\n",
    "            episode_reward = 0  # rewards of all steps\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "                # if RENDER:\n",
    "                #     env.render()\n",
    "\n",
    "                # action = actor.get_action(state) state --> vt,lt\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state) #action is the direct linear output from the model\n",
    "                action_dist = Categorical(F.softmax(action,dim=1))\n",
    "                index = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "                \n",
    "                \n",
    "                # action_onehot = F.one_hot(torch.argmax(action),num_actions)\n",
    "                # print(action_onehot)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if episode<50:\n",
    "                #     if episode%2:\n",
    "                #         action_onehot = torch.tensor([1,0,1,0])\n",
    "                #     else: action_onehot = torch.tensor([1,0,0,1])                 \n",
    "                # continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "                \n",
    "                # action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                step += 1\n",
    "\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # print((vt == vt_new).all())\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    if reward<0:\n",
    "                        print(f'Decision Step reward: {reward}')\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "                \n",
    "\n",
    "                if step >= MAX_STEPS:\n",
    "                    reward = -1\n",
    "                    print(f'Max Step Reward: {reward}')\n",
    "                    done = True\n",
    "                if step % 100 == 0:\n",
    "                    print (f'Step: {step}')\n",
    "\n",
    "                \n",
    "                    \n",
    "                episode_reward = episode_reward + reward\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # try:\n",
    "                #     td_error = critic.learn(\n",
    "                #         state, reward, state_new, done\n",
    "                #     )  # learn Value-function : gradient = grad[r + lambda * V(s_new) - V(s)]\n",
    "                #     actor.learn(state, action, td_error)  # learn Policy : true_gradient = grad[logPi(s, a) * td_error]                   \n",
    "                try:\n",
    "                    # d = 0 if done else 1\n",
    "                    # v_ = self.model(torch.FloatTensor(state_))\n",
    "                    # v = self.model(torch.FloatTensor(state))\n",
    "                    if not done:\n",
    "                        action_new, value_new, lstm_hidden_state_new = agent(vt_new,lt,lstm_hidden_state)\n",
    "\n",
    "                        # initialize the value first\n",
    "                        td_error = reward + LAM * value_new - value\n",
    "                    else: td_error = reward - value\n",
    "\n",
    "\n",
    "                    critic_loss = td_error ** 2\n",
    "\n",
    "                    log_prob = action_dist.log_prob(index)\n",
    "                    actor_loss = -(log_prob * td_error)\n",
    "\n",
    "                    # logits = action # the actual model output (without softmax)\n",
    "                    # labels = index  # one hot vector of the sampled action\n",
    "                    print(action,index)\n",
    "                    entropy_loss = F.cross_entropy(action, index)\n",
    "                    print(entropy_loss)\n",
    "                    total_loss = actor_loss + 0.5* critic_loss - 0.001*entropy_loss\n",
    "                    # entropy loss entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=tf.nn.softmax(logit))\n",
    "                    # comb_loss = tf.reduce_mean((0.5 * value_loss + policy_loss - 0.01 * entropy))\n",
    "\n",
    "                    total_loss = total_loss.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward(retain_graph=True)\n",
    "                    print('hi')\n",
    "                    optimizer.step()   \n",
    "                    \n",
    "              \n",
    "                except KeyboardInterrupt:  # if Ctrl+C at running actor.learn(), then save model, or exit if not at actor.learn()\n",
    "                    agent.save(episode)\n",
    "                    print('model has been saved')\n",
    "\n",
    "                # state = state_new\n",
    "                # print(f'episode: {episode}, step:{step}')\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            if episode%500 == 0:\n",
    "                agent.save(episode)\n",
    "                print(\"Model has been saved\")\n",
    "                \n",
    "            # all_episode_reward.append(episode_reward)\n",
    "            # if episode >= 100:\n",
    "            #     avg_score = np.mean(all_episode_reward[-100:])\n",
    "            #     if avg_score > best_score:\n",
    "            #         best_score = avg_score\n",
    "            #         agent.save()\n",
    "            #         print(f'The best score for averaging previous 100 episode reward is {best_score}. Model has been saved')\n",
    "\n",
    "\n",
    "\n",
    "            # if episode == 0:\n",
    "            #     all_episode_reward.append(episode_reward)\n",
    "            # else:\n",
    "            #     reference_value = all_episode_reward[-1] * 0.9 + episode_reward * 0.1\n",
    "            #     if reference_value > all_episode_reward_best:\n",
    "            #         agent.save()\n",
    "            #         print(f'The best all_episode_reward is {reference_value}. Model has been saved')\n",
    "            #         all_episode_reward_best = reference_value\n",
    "            #     all_episode_reward.append(reference_value)\n",
    "\n",
    "            print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                  .format(episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([ALG_NAME, ENV_ID])))\n",
    "        \n",
    "\n",
    "    if not train:\n",
    "        agent.load(episode)\n",
    "        print(\"model has been loaded\")\n",
    "        tracked_agent = -1\n",
    "\n",
    "        for episode in range(TEST_EPISODES):\n",
    "            episode_time = time.time()\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index]\n",
    "            t = 0  # number of step in this episode\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # action = actor.get_action(state, greedy=True)\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if done:\n",
    "                #     reward = -20\n",
    "\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                action = F.one_hot(torch.argmax(action),num_actions)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action).reshape(1,4) #[forward, backward, right, left]\n",
    "                action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                vt_new = torch.tensor(decision_steps.obs).reshape(1,3,128,128)\n",
    "                \n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                if tracked_agent in decision_steps:\n",
    "                    # tracked_agent = decision_steps.agent_id[0]\n",
    "                    reward = decision_steps[tracked_agent].reward \n",
    "                # if done:\n",
    "                #     reward = -20  # reward shaping trick\n",
    "                if tracked_agent in terminal_steps:\n",
    "                    done = True\n",
    "                    reward = -20\n",
    "\n",
    "                episode_reward += reward\n",
    "                vt = vt_new\n",
    "                t += 1\n",
    "\n",
    "                if done or t >= MAX_STEPS:\n",
    "                    print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                          .format(episode + 1, TEST_EPISODES, episode_reward, time.time() - t0))\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to update the network every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the optimized MAX_STEP\n",
    "# speed 1: 10000+ speed 2:1822 speed 3: 1918 speed 4: 900 speed 5: 951 speed 6:964 speed 7: 1181 \n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# speed = 1\n",
    "TRAIN_EPISODES = 20\n",
    "tracked_agent = -1\n",
    "num_actions = 4\n",
    "average = 0\n",
    "# for speed in range(30,70,5):\n",
    "#     speed = speed/10\n",
    "speed = 3\n",
    "for episode in range(TRAIN_EPISODES):\n",
    "    env.reset()\n",
    "    behavior_name=list(env.behavior_specs)[0]\n",
    "    step = 0\n",
    "    while True:\n",
    "        index = random.randint(0, 3) # sample an action from action_dist\n",
    "        action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "        discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(discrete_actions)\n",
    "        env.set_actions(behavior_name,action_tuple)\n",
    "        env.step()\n",
    "        step += 1\n",
    "\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "            \n",
    "        if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "            reward = terminal_steps[tracked_agent].reward\n",
    "            if reward > 0: # hit the target\n",
    "                print(f'{episode}: {step} in total')\n",
    "                average += step\n",
    "                break\n",
    "            else:           # roll over or other conditions\n",
    "                env.reset()\n",
    "                step = 0\n",
    "                continue # roll over or other unseen conditions\n",
    "        if tracked_agent in decision_steps: # the agent which requires action\n",
    "            continue\n",
    "average /= TRAIN_EPISODES\n",
    "print(f'For speed {speed}, average random step for hitting the target is {average}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unity could not open in work station\n",
    "# The agent in terminal step will also in decison step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "object_sizes = [1, 2, 3, 4, 5]  # Object sizes on the x-axis\n",
    "max_steps = [[4721, 2468, 1587, 1249, 1084],   # Max steps for each object size\n",
    "             [4027, 1550, 738, 640, 505],\n",
    "             [3320, 1391, 464, 331, 307],\n",
    "             [3289, 882, 679, 424, 211],\n",
    "             [3057, 1203, 470, 378, 222]]\n",
    "speeds = [1, 2, 3, 4, 5]  # Discrete speeds for color-coding\n",
    "\n",
    "# Color mapping for each speed\n",
    "speed_color_mapping = {\n",
    "    1: 'red',\n",
    "    2: 'blue',\n",
    "    3: 'green',\n",
    "    4: 'orange',\n",
    "    5: 'purple'\n",
    "}\n",
    "\n",
    "# Generate scatter plot\n",
    "for i, size in enumerate(object_sizes):\n",
    "    for j, steps in enumerate(max_steps[i]):\n",
    "        speed = speeds[j]\n",
    "        color = speed_color_mapping[speed]\n",
    "        plt.scatter(size, steps, c=color)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Object Size')\n",
    "plt.ylabel('Max Step')\n",
    "plt.title('Scatter Plot')\n",
    "\n",
    "plt.xlim(0, 6)\n",
    "plt.xticks(np.arange(0, 7, 1))\n",
    "plt.ylim(0, 5000)\n",
    "plt.yticks(np.arange(0, 5001, 200))\n",
    "\n",
    "# Create legend\n",
    "legend_labels = [f'Speed {speed}' for speed, _ in speed_color_mapping.items()]\n",
    "plt.legend(legend_labels, loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env =  UE(file_name=\"S1_1\\\\build\",seed=1,side_channels=[],worker_id=6,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "# for i in range(len(behavior_names)):\n",
    "#     print(behavior_names[i])\n",
    "#     print(\"obs:\",behavior_value[i].observation_specs, \"   act:\", behavior_value[0].action_spec)\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agentsNum = len(DecisionSteps.agent_id)\n",
    "# print(\"exist:\",DecisionSteps.agent_id,\"   Dead:\",TerminalSteps.agent_id)\n",
    "# print(\"reward:\",DecisionSteps.reward,\"reward_dead:\",TerminalSteps.reward)\n",
    "# print(\"obs:\",DecisionSteps.obs,\"DeadObs:\",TerminalSteps.obs)\n",
    "# print(\"interrupted:\", TerminalSteps.interrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#(3,128,128) --> (64,7,7)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#(3,128,128) --> (64,7,7)\n",
    "action_onehot = torch.tensor((1,0,0,0))\n",
    "discrete_actions = np.array(action_onehot).reshape(1,4)\n",
    "action_tuple = ActionTuple()\n",
    "action_tuple.add_discrete(discrete_actions)\n",
    "env.set_actions(behavior_names[0],action_tuple)\n",
    "env.step()\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_names[0])\n",
    "image = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have the tensor stored in the 'tensor' variable\n",
    "reshaped_tensor = np.reshape(image.numpy(), (1, 128, 128, 3))\n",
    "reshaped_tensor = np.squeeze(reshaped_tensor)  # Remove the first dimension if it's unnecessary\n",
    "\n",
    "# Convert the tensor to the appropriate data type (e.g., unsigned 8-bit integers)\n",
    "reshaped_tensor = (reshaped_tensor * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Create an image from the tensor array\n",
    "camera = Image.fromarray(reshaped_tensor)\n",
    "\n",
    "# Display the image\n",
    "camera.save('image/image.jpg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = vt\n",
    "        \n",
    "        # Visualize feature maps of the last two convolutional layers\n",
    "        features = []\n",
    "        for module in self.conv:\n",
    "            encoded_vt = module(encoded_vt)\n",
    "            features.append(encoded_vt)\n",
    "    \n",
    "        return features[1], features[3],features[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the VisualModule\n",
    "visual_module = VisualModule()\n",
    "\n",
    "# Forward pass to get the encoded vector and feature maps\n",
    "encoded_vector, feature_maps_2nd_last, feature_maps_last = visual_module(image)\n",
    "\n",
    "\n",
    "\n",
    "feature_maps_2nd_last = feature_maps_2nd_last.squeeze()\n",
    "print(feature_maps_2nd_last[7].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "for i in range(feature_maps_2nd_last.size()[0]):\n",
    "    # Assuming you have the tensor stored in the 'tensor' variable\n",
    "    reshaped_tensor = np.reshape(feature_maps_2nd_last[i].detach().numpy(), (22, 22))\n",
    "    reshaped_tensor = np.squeeze(reshaped_tensor)  # Remove the first dimension if it's unnecessary\n",
    "\n",
    "    # Convert the tensor to the appropriate data type (e.g., unsigned 8-bit integers)\n",
    "    reshaped_tensor = (reshaped_tensor * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    # Create an image from the tensor array\n",
    "    image_save = Image.fromarray(reshaped_tensor)\n",
    "\n",
    "    # Display the image\n",
    "    image_save.save(f'image/feature_maps_2nd_last/{i}.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
