{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment as UE\n",
    "import numpy as np\n",
    "from mlagents_envs.environment import ActionTuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "env =  UE(file_name=\"S1_noref\\\\build\",seed=1,side_channels=[],worker_id=2,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "# for i in range(len(behavior_names)):\n",
    "#     print(behavior_names[i])\n",
    "#     print(\"obs:\",behavior_value[i].observation_specs, \"   act:\", behavior_value[0].action_spec)\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agentsNum = len(DecisionSteps.agent_id)\n",
    "# print(\"exist:\",DecisionSteps.agent_id,\"   Dead:\",TerminalSteps.agent_id)\n",
    "# print(\"reward:\",DecisionSteps.reward,\"reward_dead:\",TerminalSteps.reward)\n",
    "# print(\"obs:\",DecisionSteps.obs,\"DeadObs:\",TerminalSteps.obs)\n",
    "# print(\"interrupted:\", TerminalSteps.interrupted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#(3,128,128) --> (64,7,7)\n",
    "image = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vision_output_dim = 3136\n",
    "num_words = 35  # Number of unique words in the vocabulary\n",
    "language_output_dim = 128\n",
    "embedding_dim = 128\n",
    "mixing_dim = 256\n",
    "lstm_hidden_dim = 256\n",
    "num_actions = 4\n",
    "\n",
    "# (3,128,128) --> (64,7,7) = 3136 (3-layer CNN)\n",
    "class VisualModule(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(VisualModule, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=3, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        # )\n",
    "\n",
    "    def forward(self, vt):\n",
    "        encoded_vt = self.conv(vt)\n",
    "        return encoded_vt.view(vt.size(0), -1).squeeze()\n",
    "\n",
    "# one-hot encoding [0 0 1 0 0] --> 128 dimensional embedding (FF)\n",
    "# S1:5 S2:5 S3:11 S4:9 --> 30 + 5 (noun) = 35 in total\n",
    "class LanguageModule(nn.Module): \n",
    "    def __init__(self, num_words, embedding_dim):\n",
    "        super(LanguageModule, self).__init__()\n",
    "        self.embedding = nn.Linear(num_words, embedding_dim)\n",
    "\n",
    "    def forward(self, lt):\n",
    "        embedded_lt = self.embedding(lt)\n",
    "        return embedded_lt\n",
    "\n",
    "# 3136(vision) + 128 (language) --> 256 dimensional embedding (FF)\n",
    "class MixingModule(nn.Module):\n",
    "    def __init__(self, vision_output_dim, language_output_dim, mixing_dim):\n",
    "        super(MixingModule, self).__init__()\n",
    "        self.linear = nn.Linear(vision_output_dim + language_output_dim, mixing_dim)\n",
    "\n",
    "    def forward(self, vision_output, language_output):\n",
    "        combined_output = torch.cat((vision_output, language_output), dim=0)\n",
    "        mixed_output = self.linear(combined_output)\n",
    "        return mixed_output\n",
    "\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self,mixing_dim,lstm_hidden_dim):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        self.lstm = nn.LSTMCell(mixing_dim, lstm_hidden_dim)\n",
    "    \n",
    "    def forward(self,mixed_output,lstm_hidden_state):\n",
    "        lstm_hidden_state = self.lstm(mixed_output, lstm_hidden_state) \n",
    "        # lstm_output = lstm_hidden_state[0] # output is (hidden_state,cell_state), we need hidden state, shape (1,256)\n",
    "        return lstm_hidden_state\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions):\n",
    "        super(Agent, self).__init__()\n",
    "        self.language_module = LanguageModule(num_words, embedding_dim)\n",
    "        self.visual_module = VisualModule()\n",
    "        self.mixing_module = MixingModule(vision_output_dim, language_output_dim, mixing_dim)\n",
    "        self.lstm_module = LSTMModule(mixing_dim, lstm_hidden_dim)\n",
    "        self.action_predictor = nn.Linear(lstm_hidden_dim, num_actions)\n",
    "        self.value_estimator = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, vt, lt, lstm_hidden_state):\n",
    "        vision_output = self.visual_module(vt)\n",
    "        language_output = self.language_module(lt)\n",
    "        mixed_output = self.mixing_module(vision_output, language_output).unsqueeze(0)\n",
    "        lstm_output = self.lstm_module(mixed_output,lstm_hidden_state)\n",
    "        action_probs = self.action_predictor(lstm_output[0]) \n",
    "        value_estimate = self.value_estimator(lstm_output[0])\n",
    "        return action_probs,value_estimate,lstm_output\n",
    "        \n",
    "        \n",
    "    def save(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(self.state_dict(), os.path.join(path, f'agent_{episode}.pt'))\n",
    "\n",
    "    def load(self, episode, ALG_NAME, ENV_ID):\n",
    "        path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "        self.load_state_dict(torch.load(os.path.join(path, f'agent_{episode}.pt')))    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of image embedding is torch.Size([3136])\n",
      "The shape of language embedding is torch.Size([128])\n",
      "The shape of mix embedding is torch.Size([256])\n",
      "The shape of lstm hidden state is torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "visual_model = VisualModule()\n",
    "vt = image\n",
    "image_emb = visual_model(vt)\n",
    "print(f'The shape of image embedding is {image_emb.size()}')\n",
    "\n",
    "index = 5\n",
    "language_model = LanguageModule(num_words,embedding_dim)\n",
    "lt = torch.eye(num_words)[:, index]\n",
    "language_emb = language_model(lt)\n",
    "print(f'The shape of language embedding is {language_emb.size()}')\n",
    "\n",
    "mixing_model = MixingModule(vision_output_dim,language_output_dim,mixing_dim)\n",
    "mix_emb = mixing_model(image_emb,language_emb)\n",
    "print(f'The shape of mix embedding is {mix_emb.size()}')\n",
    "\n",
    "lstm = LSTMModule(mixing_dim,lstm_hidden_dim)\n",
    "lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim), torch.zeros(1, lstm_hidden_dim))\n",
    "hidden_state = lstm(mix_emb.unsqueeze(0),lstm_hidden_state)\n",
    "print(f'The shape of lstm hidden state is {hidden_state[0].size()}')\n",
    "\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "action, value, lstm_hidden_state= agent(vt,lt,lstm_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cylinder\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 1/20000  | Episode Reward: 10  | Actor loss: -0.01 | Critic loss: 0.62 | Entropy loss: -0.0000  | Total Loss: 0.62 | Total Steps: 7\n",
      "Model has been saved\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 2/20000  | Episode Reward: -22  | Actor loss: -0.53 | Critic loss: 7.85 | Entropy loss: -0.0002  | Total Loss: 7.32 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 3/20000  | Episode Reward: -20  | Actor loss: -1.00 | Critic loss: 6.85 | Entropy loss: -0.0004  | Total Loss: 5.85 | Total Steps: 500\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 4/20000  | Episode Reward: 8  | Actor loss: 3.40 | Critic loss: 28.40 | Entropy loss: -0.0001  | Total Loss: 31.80 | Total Steps: 314\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 5/20000  | Episode Reward: -19  | Actor loss: -1.03 | Critic loss: 5.81 | Entropy loss: -0.0006  | Total Loss: 4.78 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 6/20000  | Episode Reward: -23  | Actor loss: -1.21 | Critic loss: 6.13 | Entropy loss: -0.0004  | Total Loss: 4.92 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 7/20000  | Episode Reward: 5  | Actor loss: 0.82 | Critic loss: 4.87 | Entropy loss: -0.0005  | Total Loss: 5.69 | Total Steps: 86\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 8/20000  | Episode Reward: -25  | Actor loss: -0.67 | Critic loss: 5.44 | Entropy loss: -0.0004  | Total Loss: 4.77 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 9/20000  | Episode Reward: 0  | Actor loss: 0.87 | Critic loss: 6.16 | Entropy loss: -0.0003  | Total Loss: 7.03 | Total Steps: 481\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 10/20000  | Episode Reward: -17  | Actor loss: -1.13 | Critic loss: 5.59 | Entropy loss: -0.0006  | Total Loss: 4.46 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 11/20000  | Episode Reward: -18  | Actor loss: -1.16 | Critic loss: 5.74 | Entropy loss: -0.0005  | Total Loss: 4.58 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 12/20000  | Episode Reward: 3  | Actor loss: 1.20 | Critic loss: 8.90 | Entropy loss: -0.0003  | Total Loss: 10.10 | Total Steps: 257\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 13/20000  | Episode Reward: -22  | Actor loss: -1.45 | Critic loss: 6.48 | Entropy loss: -0.0005  | Total Loss: 5.04 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 14/20000  | Episode Reward: -1  | Actor loss: 1.43 | Critic loss: 5.37 | Entropy loss: -0.0004  | Total Loss: 6.79 | Total Steps: 493\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 15/20000  | Episode Reward: -15  | Actor loss: -0.82 | Critic loss: 5.52 | Entropy loss: -0.0003  | Total Loss: 4.70 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 16/20000  | Episode Reward: -18  | Actor loss: -0.73 | Critic loss: 5.62 | Entropy loss: -0.0005  | Total Loss: 4.89 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 17/20000  | Episode Reward: -5  | Actor loss: 0.89 | Critic loss: 7.24 | Entropy loss: -0.0004  | Total Loss: 8.13 | Total Steps: 469\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 18/20000  | Episode Reward: 8  | Actor loss: 6.10 | Critic loss: 34.46 | Entropy loss: -0.0001  | Total Loss: 40.56 | Total Steps: 109\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 19/20000  | Episode Reward: -20  | Actor loss: -1.51 | Critic loss: 6.47 | Entropy loss: -0.0005  | Total Loss: 4.96 | Total Steps: 500\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 20/20000  | Episode Reward: -21  | Actor loss: -0.81 | Critic loss: 5.64 | Entropy loss: -0.0004  | Total Loss: 4.83 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 21/20000  | Episode Reward: -27  | Actor loss: -2.27 | Critic loss: 6.90 | Entropy loss: -0.0007  | Total Loss: 4.63 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 22/20000  | Episode Reward: 3  | Actor loss: 1.46 | Critic loss: 9.43 | Entropy loss: -0.0002  | Total Loss: 10.89 | Total Steps: 253\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 23/20000  | Episode Reward: -16  | Actor loss: -0.82 | Critic loss: 5.57 | Entropy loss: -0.0005  | Total Loss: 4.75 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 24/20000  | Episode Reward: 6  | Actor loss: 4.54 | Critic loss: 31.53 | Entropy loss: -0.0001  | Total Loss: 36.08 | Total Steps: 210\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 25/20000  | Episode Reward: -22  | Actor loss: -0.37 | Critic loss: 5.58 | Entropy loss: -0.0002  | Total Loss: 5.21 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 26/20000  | Episode Reward: 5  | Actor loss: 1.88 | Critic loss: 8.16 | Entropy loss: -0.0003  | Total Loss: 10.05 | Total Steps: 259\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 27/20000  | Episode Reward: 2  | Actor loss: 1.22 | Critic loss: 5.50 | Entropy loss: -0.0006  | Total Loss: 6.71 | Total Steps: 278\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 28/20000  | Episode Reward: 7  | Actor loss: 0.69 | Critic loss: 5.69 | Entropy loss: -0.0002  | Total Loss: 6.38 | Total Steps: 180\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 29/20000  | Episode Reward: -18  | Actor loss: -0.91 | Critic loss: 6.27 | Entropy loss: -0.0004  | Total Loss: 5.36 | Total Steps: 500\n",
      "cylinder\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 30/20000  | Episode Reward: 10  | Actor loss: 2.77 | Critic loss: 15.73 | Entropy loss: -0.0002  | Total Loss: 18.50 | Total Steps: 128\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 31/20000  | Episode Reward: -17  | Actor loss: -1.02 | Critic loss: 7.10 | Entropy loss: -0.0003  | Total Loss: 6.09 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 32/20000  | Episode Reward: -21  | Actor loss: -1.62 | Critic loss: 6.26 | Entropy loss: -0.0006  | Total Loss: 4.64 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 33/20000  | Episode Reward: 9  | Actor loss: 1.05 | Critic loss: 5.94 | Entropy loss: -0.0005  | Total Loss: 6.99 | Total Steps: 77\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 34/20000  | Episode Reward: -36  | Actor loss: -1.50 | Critic loss: 7.23 | Entropy loss: -0.0005  | Total Loss: 5.73 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 35/20000  | Episode Reward: -26  | Actor loss: -1.09 | Critic loss: 5.72 | Entropy loss: -0.0005  | Total Loss: 4.63 | Total Steps: 500\n",
      "capsule\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 36/20000  | Episode Reward: 10  | Actor loss: 0.99 | Critic loss: 8.95 | Entropy loss: -0.0002  | Total Loss: 9.94 | Total Steps: 53\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 37/20000  | Episode Reward: 5  | Actor loss: 1.15 | Critic loss: 5.49 | Entropy loss: -0.0005  | Total Loss: 6.64 | Total Steps: 185\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 38/20000  | Episode Reward: -20  | Actor loss: -1.47 | Critic loss: 6.84 | Entropy loss: -0.0006  | Total Loss: 5.37 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 39/20000  | Episode Reward: 8  | Actor loss: 1.48 | Critic loss: 6.72 | Entropy loss: -0.0006  | Total Loss: 8.20 | Total Steps: 70\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 40/20000  | Episode Reward: 9  | Actor loss: 1.76 | Critic loss: 16.19 | Entropy loss: -0.0001  | Total Loss: 17.95 | Total Steps: 127\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 41/20000  | Episode Reward: -19  | Actor loss: -0.99 | Critic loss: 6.06 | Entropy loss: -0.0004  | Total Loss: 5.08 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 42/20000  | Episode Reward: -22  | Actor loss: -0.88 | Critic loss: 6.01 | Entropy loss: -0.0004  | Total Loss: 5.13 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 43/20000  | Episode Reward: -26  | Actor loss: -1.49 | Critic loss: 6.69 | Entropy loss: -0.0005  | Total Loss: 5.20 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 44/20000  | Episode Reward: -22  | Actor loss: -1.16 | Critic loss: 6.37 | Entropy loss: -0.0004  | Total Loss: 5.21 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 45/20000  | Episode Reward: 3  | Actor loss: 1.29 | Critic loss: 11.56 | Entropy loss: -0.0001  | Total Loss: 12.85 | Total Steps: 241\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 46/20000  | Episode Reward: 5  | Actor loss: 4.23 | Critic loss: 22.32 | Entropy loss: -0.0001  | Total Loss: 26.56 | Total Steps: 318\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 47/20000  | Episode Reward: 7  | Actor loss: 2.23 | Critic loss: 13.46 | Entropy loss: -0.0002  | Total Loss: 15.69 | Total Steps: 233\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 48/20000  | Episode Reward: -1  | Actor loss: 3.56 | Critic loss: 19.83 | Entropy loss: -0.0001  | Total Loss: 23.39 | Total Steps: 320\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 49/20000  | Episode Reward: 7  | Actor loss: 1.68 | Critic loss: 10.19 | Entropy loss: -0.0002  | Total Loss: 11.87 | Total Steps: 242\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 50/20000  | Episode Reward: 9  | Actor loss: 0.57 | Critic loss: 4.59 | Entropy loss: -0.0003  | Total Loss: 5.16 | Total Steps: 93\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 51/20000  | Episode Reward: -13  | Actor loss: -0.62 | Critic loss: 6.33 | Entropy loss: -0.0004  | Total Loss: 5.71 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 52/20000  | Episode Reward: -24  | Actor loss: -0.75 | Critic loss: 6.17 | Entropy loss: -0.0003  | Total Loss: 5.42 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 53/20000  | Episode Reward: -23  | Actor loss: -0.97 | Critic loss: 6.02 | Entropy loss: -0.0004  | Total Loss: 5.05 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 54/20000  | Episode Reward: -16  | Actor loss: -0.73 | Critic loss: 5.94 | Entropy loss: -0.0002  | Total Loss: 5.21 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 55/20000  | Episode Reward: -32  | Actor loss: -2.25 | Critic loss: 9.12 | Entropy loss: -0.0006  | Total Loss: 6.87 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 56/20000  | Episode Reward: -18  | Actor loss: -1.17 | Critic loss: 5.77 | Entropy loss: -0.0005  | Total Loss: 4.59 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 57/20000  | Episode Reward: -2  | Actor loss: 0.51 | Critic loss: 6.30 | Entropy loss: -0.0004  | Total Loss: 6.80 | Total Steps: 184\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 58/20000  | Episode Reward: 8  | Actor loss: 5.76 | Critic loss: 34.08 | Entropy loss: -0.0001  | Total Loss: 39.84 | Total Steps: 108\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 59/20000  | Episode Reward: 6  | Actor loss: 1.71 | Critic loss: 7.79 | Entropy loss: -0.0003  | Total Loss: 9.50 | Total Steps: 158\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 60/20000  | Episode Reward: -27  | Actor loss: -1.36 | Critic loss: 6.84 | Entropy loss: -0.0005  | Total Loss: 5.48 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 61/20000  | Episode Reward: 3  | Actor loss: 1.27 | Critic loss: 5.20 | Entropy loss: -0.0005  | Total Loss: 6.46 | Total Steps: 488\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 62/20000  | Episode Reward: 8  | Actor loss: 0.99 | Critic loss: 8.11 | Entropy loss: -0.0002  | Total Loss: 9.10 | Total Steps: 53\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 63/20000  | Episode Reward: 6  | Actor loss: 2.90 | Critic loss: 23.18 | Entropy loss: -0.0001  | Total Loss: 26.08 | Total Steps: 116\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 64/20000  | Episode Reward: -18  | Actor loss: -0.82 | Critic loss: 6.34 | Entropy loss: -0.0004  | Total Loss: 5.51 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 65/20000  | Episode Reward: -18  | Actor loss: -1.76 | Critic loss: 6.42 | Entropy loss: -0.0005  | Total Loss: 4.66 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 66/20000  | Episode Reward: 6  | Actor loss: 2.18 | Critic loss: 12.40 | Entropy loss: -0.0002  | Total Loss: 14.57 | Total Steps: 335\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 67/20000  | Episode Reward: 0  | Actor loss: 1.74 | Critic loss: 8.29 | Entropy loss: -0.0003  | Total Loss: 10.03 | Total Steps: 445\n",
      "capsule\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 68/20000  | Episode Reward: 10  | Actor loss: 1.19 | Critic loss: 4.54 | Entropy loss: -0.0003  | Total Loss: 5.74 | Total Steps: 94\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 69/20000  | Episode Reward: -20  | Actor loss: -0.87 | Critic loss: 7.22 | Entropy loss: -0.0004  | Total Loss: 6.35 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 70/20000  | Episode Reward: -20  | Actor loss: -0.94 | Critic loss: 6.30 | Entropy loss: -0.0004  | Total Loss: 5.36 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 71/20000  | Episode Reward: -24  | Actor loss: -2.53 | Critic loss: 7.31 | Entropy loss: -0.0007  | Total Loss: 4.79 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 72/20000  | Episode Reward: 7  | Actor loss: 2.33 | Critic loss: 13.80 | Entropy loss: -0.0002  | Total Loss: 16.14 | Total Steps: 231\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 73/20000  | Episode Reward: 8  | Actor loss: 1.02 | Critic loss: 5.98 | Entropy loss: -0.0002  | Total Loss: 6.99 | Total Steps: 173\n",
      "cube\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 74/20000  | Episode Reward: 5  | Actor loss: 1.65 | Critic loss: 6.50 | Entropy loss: -0.0004  | Total Loss: 8.16 | Total Steps: 258\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 75/20000  | Episode Reward: 3  | Actor loss: 1.39 | Critic loss: 11.20 | Entropy loss: -0.0001  | Total Loss: 12.58 | Total Steps: 238\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 76/20000  | Episode Reward: 9  | Actor loss: 2.59 | Critic loss: 7.75 | Entropy loss: -0.0004  | Total Loss: 10.34 | Total Steps: 254\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 77/20000  | Episode Reward: -19  | Actor loss: -1.08 | Critic loss: 6.58 | Entropy loss: -0.0004  | Total Loss: 5.49 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 78/20000  | Episode Reward: -25  | Actor loss: -0.97 | Critic loss: 6.20 | Entropy loss: -0.0003  | Total Loss: 5.23 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 79/20000  | Episode Reward: 3  | Actor loss: 0.49 | Critic loss: 6.41 | Entropy loss: -0.0004  | Total Loss: 6.89 | Total Steps: 172\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 80/20000  | Episode Reward: -29  | Actor loss: -1.50 | Critic loss: 6.98 | Entropy loss: -0.0004  | Total Loss: 5.48 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 81/20000  | Episode Reward: 4  | Actor loss: 3.99 | Critic loss: 17.85 | Entropy loss: -0.0001  | Total Loss: 21.84 | Total Steps: 223\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 82/20000  | Episode Reward: -31  | Actor loss: -1.28 | Critic loss: 7.74 | Entropy loss: -0.0004  | Total Loss: 6.46 | Total Steps: 500\n",
      "cube\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 83/20000  | Episode Reward: -19  | Actor loss: -0.94 | Critic loss: 6.01 | Entropy loss: -0.0004  | Total Loss: 5.07 | Total Steps: 500\n",
      "sphere\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 84/20000  | Episode Reward: 10  | Actor loss: 1.43 | Critic loss: 7.64 | Entropy loss: -0.0003  | Total Loss: 9.07 | Total Steps: 59\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 85/20000  | Episode Reward: -17  | Actor loss: -1.36 | Critic loss: 6.98 | Entropy loss: -0.0004  | Total Loss: 5.62 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 86/20000  | Episode Reward: -21  | Actor loss: -0.80 | Critic loss: 7.16 | Entropy loss: -0.0003  | Total Loss: 6.36 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 87/20000  | Episode Reward: -22  | Actor loss: -0.90 | Critic loss: 6.41 | Entropy loss: -0.0004  | Total Loss: 5.52 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 88/20000  | Episode Reward: -23  | Actor loss: -1.68 | Critic loss: 8.50 | Entropy loss: -0.0005  | Total Loss: 6.82 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 89/20000  | Episode Reward: 4  | Actor loss: 2.18 | Critic loss: 13.29 | Entropy loss: -0.0001  | Total Loss: 15.47 | Total Steps: 439\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 90/20000  | Episode Reward: -14  | Actor loss: -0.81 | Critic loss: 5.63 | Entropy loss: -0.0004  | Total Loss: 4.82 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 91/20000  | Episode Reward: 3  | Actor loss: 1.73 | Critic loss: 11.21 | Entropy loss: -0.0002  | Total Loss: 12.94 | Total Steps: 243\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 92/20000  | Episode Reward: -33  | Actor loss: -3.47 | Critic loss: 11.75 | Entropy loss: -0.0008  | Total Loss: 8.27 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 93/20000  | Episode Reward: 6  | Actor loss: 2.38 | Critic loss: 12.05 | Entropy loss: -0.0002  | Total Loss: 14.43 | Total Steps: 139\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 94/20000  | Episode Reward: -1  | Actor loss: 1.38 | Critic loss: 4.81 | Entropy loss: -0.0006  | Total Loss: 6.19 | Total Steps: 398\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 95/20000  | Episode Reward: -18  | Actor loss: -1.18 | Critic loss: 7.02 | Entropy loss: -0.0005  | Total Loss: 5.83 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 96/20000  | Episode Reward: 9  | Actor loss: 2.79 | Critic loss: 22.77 | Entropy loss: -0.0001  | Total Loss: 25.56 | Total Steps: 117\n",
      "capsule\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 97/20000  | Episode Reward: -23  | Actor loss: -0.82 | Critic loss: 6.96 | Entropy loss: -0.0003  | Total Loss: 6.15 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 98/20000  | Episode Reward: 7  | Actor loss: 2.40 | Critic loss: 11.10 | Entropy loss: -0.0002  | Total Loss: 13.50 | Total Steps: 141\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 99/20000  | Episode Reward: 8  | Actor loss: 0.94 | Critic loss: 5.54 | Entropy loss: -0.0003  | Total Loss: 6.47 | Total Steps: 83\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 100/20000  | Episode Reward: -20  | Actor loss: -0.96 | Critic loss: 6.08 | Entropy loss: -0.0004  | Total Loss: 5.12 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 101/20000  | Episode Reward: -23  | Average Reward -8.70  | Actor loss: -0.74 | Critic loss: 6.80 | Entropy loss: -0.0003  | Total Loss: 6.06 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 102/20000  | Episode Reward: 1  | Average Reward -8.47  | Actor loss: 1.29 | Critic loss: 8.57 | Entropy loss: -0.0003  | Total Loss: 9.86 | Total Steps: 245\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 103/20000  | Episode Reward: -24  | Average Reward -8.51  | Actor loss: -2.20 | Critic loss: 7.27 | Entropy loss: -0.0006  | Total Loss: 5.07 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 104/20000  | Episode Reward: -15  | Average Reward -8.74  | Actor loss: -1.20 | Critic loss: 5.57 | Entropy loss: -0.0005  | Total Loss: 4.37 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 105/20000  | Episode Reward: -19  | Average Reward -8.74  | Actor loss: -1.33 | Critic loss: 6.11 | Entropy loss: -0.0005  | Total Loss: 4.77 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 106/20000  | Episode Reward: 7  | Average Reward -8.44  | Actor loss: 2.07 | Critic loss: 7.29 | Entropy loss: -0.0003  | Total Loss: 9.35 | Total Steps: 57\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 107/20000  | Episode Reward: -17  | Average Reward -8.66  | Actor loss: -0.63 | Critic loss: 5.76 | Entropy loss: -0.0004  | Total Loss: 5.13 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 108/20000  | Episode Reward: 9  | Average Reward -8.32  | Actor loss: 1.96 | Critic loss: 6.28 | Entropy loss: -0.0006  | Total Loss: 8.24 | Total Steps: 70\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 109/20000  | Episode Reward: 8  | Average Reward -8.24  | Actor loss: 6.27 | Critic loss: 30.21 | Entropy loss: -0.0001  | Total Loss: 36.48 | Total Steps: 210\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 110/20000  | Episode Reward: 2  | Average Reward -8.05  | Actor loss: 2.64 | Critic loss: 11.79 | Entropy loss: -0.0002  | Total Loss: 14.44 | Total Steps: 237\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 111/20000  | Episode Reward: 0  | Average Reward -7.87  | Actor loss: 0.42 | Critic loss: 5.26 | Entropy loss: -0.0005  | Total Loss: 5.68 | Total Steps: 491\n",
      "cylinder\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 112/20000  | Episode Reward: 9  | Average Reward -7.81  | Actor loss: 0.89 | Critic loss: 4.46 | Entropy loss: -0.0005  | Total Loss: 5.34 | Total Steps: 198\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 113/20000  | Episode Reward: -2  | Average Reward -7.61  | Actor loss: 1.24 | Critic loss: 5.39 | Entropy loss: -0.0005  | Total Loss: 6.63 | Total Steps: 381\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 114/20000  | Episode Reward: 0  | Average Reward -7.60  | Actor loss: 3.35 | Critic loss: 16.08 | Entropy loss: -0.0002  | Total Loss: 19.42 | Total Steps: 126\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 115/20000  | Episode Reward: -1  | Average Reward -7.46  | Actor loss: 5.66 | Critic loss: 33.24 | Entropy loss: -0.0001  | Total Loss: 38.91 | Total Steps: 307\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 116/20000  | Episode Reward: 0  | Average Reward -7.28  | Actor loss: 6.28 | Critic loss: 17.55 | Entropy loss: -0.0002  | Total Loss: 23.84 | Total Steps: 221\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 117/20000  | Episode Reward: -34  | Average Reward -7.57  | Actor loss: -1.87 | Critic loss: 7.70 | Entropy loss: -0.0006  | Total Loss: 5.83 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 118/20000  | Episode Reward: -27  | Average Reward -7.92  | Actor loss: -1.70 | Critic loss: 7.82 | Entropy loss: -0.0007  | Total Loss: 6.12 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 119/20000  | Episode Reward: -38  | Average Reward -8.10  | Actor loss: -2.16 | Critic loss: 8.20 | Entropy loss: -0.0008  | Total Loss: 6.05 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 120/20000  | Episode Reward: 5  | Average Reward -7.84  | Actor loss: 0.67 | Critic loss: 5.54 | Entropy loss: -0.0007  | Total Loss: 6.21 | Total Steps: 89\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 121/20000  | Episode Reward: -4  | Average Reward -7.61  | Actor loss: 4.24 | Critic loss: 18.41 | Entropy loss: -0.0002  | Total Loss: 22.65 | Total Steps: 124\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 122/20000  | Episode Reward: 0  | Average Reward -7.64  | Actor loss: 2.71 | Critic loss: 6.21 | Entropy loss: -0.0005  | Total Loss: 8.92 | Total Steps: 165\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 123/20000  | Episode Reward: -13  | Average Reward -7.61  | Actor loss: 1.33 | Critic loss: 6.07 | Entropy loss: -0.0005  | Total Loss: 7.40 | Total Steps: 268\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 124/20000  | Episode Reward: 4  | Average Reward -7.63  | Actor loss: 0.29 | Critic loss: 5.28 | Entropy loss: -0.0007  | Total Loss: 5.57 | Total Steps: 90\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 125/20000  | Episode Reward: -1  | Average Reward -7.42  | Actor loss: 3.46 | Critic loss: 10.78 | Entropy loss: -0.0003  | Total Loss: 14.24 | Total Steps: 139\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 126/20000  | Episode Reward: 7  | Average Reward -7.40  | Actor loss: 7.50 | Critic loss: 31.13 | Entropy loss: -0.0001  | Total Loss: 38.62 | Total Steps: 109\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 127/20000  | Episode Reward: -41  | Average Reward -7.83  | Actor loss: -3.16 | Critic loss: 7.82 | Entropy loss: -0.0009  | Total Loss: 4.65 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 128/20000  | Episode Reward: -35  | Average Reward -8.25  | Actor loss: 3.47 | Critic loss: 8.66 | Entropy loss: -0.0004  | Total Loss: 12.13 | Total Steps: 434\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 129/20000  | Episode Reward: -36  | Average Reward -8.43  | Actor loss: -2.45 | Critic loss: 6.56 | Entropy loss: -0.0008  | Total Loss: 4.11 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 130/20000  | Episode Reward: 5  | Average Reward -8.48  | Actor loss: 4.12 | Critic loss: 13.27 | Entropy loss: -0.0004  | Total Loss: 17.40 | Total Steps: 134\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 131/20000  | Episode Reward: -56  | Average Reward -8.87  | Actor loss: -2.30 | Critic loss: 7.63 | Entropy loss: -0.0007  | Total Loss: 5.33 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 132/20000  | Episode Reward: -19  | Average Reward -8.85  | Actor loss: 4.81 | Critic loss: 14.51 | Entropy loss: -0.0003  | Total Loss: 19.32 | Total Steps: 431\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 133/20000  | Episode Reward: -28  | Average Reward -9.22  | Actor loss: -2.55 | Critic loss: 6.56 | Entropy loss: -0.0008  | Total Loss: 4.01 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 134/20000  | Episode Reward: 1  | Average Reward -8.85  | Actor loss: 0.43 | Critic loss: 6.15 | Entropy loss: -0.0009  | Total Loss: 6.58 | Total Steps: 98\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 135/20000  | Episode Reward: -19  | Average Reward -8.78  | Actor loss: 1.78 | Critic loss: 6.99 | Entropy loss: -0.0006  | Total Loss: 8.76 | Total Steps: 167\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 136/20000  | Episode Reward: -29  | Average Reward -9.17  | Actor loss: 7.79 | Critic loss: 27.27 | Entropy loss: -0.0001  | Total Loss: 35.06 | Total Steps: 413\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 137/20000  | Episode Reward: -38  | Average Reward -9.60  | Actor loss: -2.74 | Critic loss: 6.54 | Entropy loss: -0.0011  | Total Loss: 3.80 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 138/20000  | Episode Reward: -76  | Average Reward -10.16  | Actor loss: -4.75 | Critic loss: 12.92 | Entropy loss: -0.0010  | Total Loss: 8.16 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 139/20000  | Episode Reward: -3  | Average Reward -10.27  | Actor loss: 2.62 | Critic loss: 8.12 | Entropy loss: -0.0006  | Total Loss: 10.74 | Total Steps: 161\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 140/20000  | Episode Reward: -39  | Average Reward -10.75  | Actor loss: -3.44 | Critic loss: 8.12 | Entropy loss: -0.0009  | Total Loss: 4.68 | Total Steps: 500\n",
      "prism\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 141/20000  | Episode Reward: 10  | Average Reward -10.46  | Actor loss: 4.43 | Critic loss: 12.35 | Entropy loss: -0.0004  | Total Loss: 16.78 | Total Steps: 40\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 142/20000  | Episode Reward: -61  | Average Reward -10.85  | Actor loss: -3.06 | Critic loss: 6.36 | Entropy loss: -0.0010  | Total Loss: 3.30 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 143/20000  | Episode Reward: -79  | Average Reward -11.38  | Actor loss: -5.10 | Critic loss: 13.15 | Entropy loss: -0.0011  | Total Loss: 8.05 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 144/20000  | Episode Reward: -55  | Average Reward -11.71  | Actor loss: -2.44 | Critic loss: 7.08 | Entropy loss: -0.0008  | Total Loss: 4.64 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 145/20000  | Episode Reward: -79  | Average Reward -12.53  | Actor loss: -3.87 | Critic loss: 11.19 | Entropy loss: -0.0009  | Total Loss: 7.32 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 146/20000  | Episode Reward: -57  | Average Reward -13.15  | Actor loss: -2.95 | Critic loss: 7.39 | Entropy loss: -0.0010  | Total Loss: 4.44 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 147/20000  | Episode Reward: -22  | Average Reward -13.44  | Actor loss: 0.52 | Critic loss: 8.48 | Entropy loss: -0.0008  | Total Loss: 9.00 | Total Steps: 176\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 148/20000  | Episode Reward: -50  | Average Reward -13.93  | Actor loss: 8.44 | Critic loss: 46.22 | Entropy loss: -0.0000  | Total Loss: 54.66 | Total Steps: 404\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 149/20000  | Episode Reward: -86  | Average Reward -14.86  | Actor loss: -5.78 | Critic loss: 20.46 | Entropy loss: -0.0009  | Total Loss: 14.68 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 150/20000  | Episode Reward: -139  | Average Reward -16.34  | Actor loss: -6.71 | Critic loss: 20.74 | Entropy loss: -0.0011  | Total Loss: 14.04 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 151/20000  | Episode Reward: -137  | Average Reward -17.58  | Actor loss: -4.86 | Critic loss: 17.95 | Entropy loss: -0.0008  | Total Loss: 13.09 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 152/20000  | Episode Reward: -190  | Average Reward -19.24  | Actor loss: -5.11 | Critic loss: 19.67 | Entropy loss: -0.0008  | Total Loss: 14.56 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 153/20000  | Episode Reward: -209  | Average Reward -21.10  | Actor loss: -4.88 | Critic loss: 22.15 | Entropy loss: -0.0008  | Total Loss: 17.28 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 154/20000  | Episode Reward: -204  | Average Reward -22.98  | Actor loss: -4.91 | Critic loss: 20.19 | Entropy loss: -0.0008  | Total Loss: 15.28 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 155/20000  | Episode Reward: -151  | Average Reward -24.17  | Actor loss: -3.46 | Critic loss: 7.57 | Entropy loss: -0.0009  | Total Loss: 4.11 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 156/20000  | Episode Reward: -152  | Average Reward -25.51  | Actor loss: -4.32 | Critic loss: 10.54 | Entropy loss: -0.0010  | Total Loss: 6.22 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 157/20000  | Episode Reward: -165  | Average Reward -27.14  | Actor loss: -4.63 | Critic loss: 14.86 | Entropy loss: -0.0009  | Total Loss: 10.23 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 158/20000  | Episode Reward: -180  | Average Reward -29.02  | Actor loss: -4.53 | Critic loss: 12.36 | Entropy loss: -0.0009  | Total Loss: 7.83 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 159/20000  | Episode Reward: -174  | Average Reward -30.82  | Actor loss: -4.67 | Critic loss: 10.37 | Entropy loss: -0.0011  | Total Loss: 5.70 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 160/20000  | Episode Reward: -167  | Average Reward -32.22  | Actor loss: -2.94 | Critic loss: 4.36 | Entropy loss: -0.0011  | Total Loss: 1.41 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 161/20000  | Episode Reward: -121  | Average Reward -33.46  | Actor loss: -2.76 | Critic loss: 5.33 | Entropy loss: -0.0009  | Total Loss: 2.56 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 162/20000  | Episode Reward: -54  | Average Reward -34.08  | Actor loss: 3.75 | Critic loss: 23.75 | Entropy loss: -0.0003  | Total Loss: 27.50 | Total Steps: 450\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 163/20000  | Episode Reward: -101  | Average Reward -35.15  | Actor loss: -1.67 | Critic loss: 3.89 | Entropy loss: -0.0009  | Total Loss: 2.22 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 164/20000  | Episode Reward: -35  | Average Reward -35.32  | Actor loss: 2.03 | Critic loss: 10.51 | Entropy loss: -0.0008  | Total Loss: 12.55 | Total Steps: 299\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 165/20000  | Episode Reward: -20  | Average Reward -35.34  | Actor loss: 2.69 | Critic loss: 16.86 | Entropy loss: -0.0004  | Total Loss: 19.56 | Total Steps: 464\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 166/20000  | Episode Reward: -43  | Average Reward -35.83  | Actor loss: -0.23 | Critic loss: 2.71 | Entropy loss: -0.0006  | Total Loss: 2.47 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 167/20000  | Episode Reward: -6  | Average Reward -35.89  | Actor loss: 1.57 | Critic loss: 13.56 | Entropy loss: -0.0003  | Total Loss: 15.13 | Total Steps: 365\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 168/20000  | Episode Reward: -2  | Average Reward -36.01  | Actor loss: 6.38 | Critic loss: 30.75 | Entropy loss: -0.0002  | Total Loss: 37.13 | Total Steps: 224\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 169/20000  | Episode Reward: 8  | Average Reward -35.73  | Actor loss: 2.03 | Critic loss: 12.42 | Entropy loss: -0.0003  | Total Loss: 14.45 | Total Steps: 60\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 170/20000  | Episode Reward: -5  | Average Reward -35.58  | Actor loss: 2.96 | Critic loss: 12.40 | Entropy loss: -0.0004  | Total Loss: 15.35 | Total Steps: 262\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 171/20000  | Episode Reward: 1  | Average Reward -35.33  | Actor loss: 1.43 | Critic loss: 11.20 | Entropy loss: -0.0003  | Total Loss: 12.63 | Total Steps: 166\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 172/20000  | Episode Reward: -45  | Average Reward -35.85  | Actor loss: -1.10 | Critic loss: 3.60 | Entropy loss: -0.0006  | Total Loss: 2.50 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 173/20000  | Episode Reward: -12  | Average Reward -36.05  | Actor loss: 0.99 | Critic loss: 8.62 | Entropy loss: -0.0003  | Total Loss: 9.60 | Total Steps: 482\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 174/20000  | Episode Reward: -19  | Average Reward -36.29  | Actor loss: -0.16 | Critic loss: 4.81 | Entropy loss: -0.0002  | Total Loss: 4.65 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 175/20000  | Episode Reward: -15  | Average Reward -36.47  | Actor loss: -0.23 | Critic loss: 5.15 | Entropy loss: -0.0003  | Total Loss: 4.92 | Total Steps: 500\n",
      "capsule\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 176/20000  | Episode Reward: -11  | Average Reward -36.67  | Actor loss: -0.82 | Critic loss: 4.89 | Entropy loss: -0.0002  | Total Loss: 4.07 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 177/20000  | Episode Reward: -12  | Average Reward -36.60  | Actor loss: -0.23 | Critic loss: 4.99 | Entropy loss: -0.0001  | Total Loss: 4.76 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 178/20000  | Episode Reward: -11  | Average Reward -36.46  | Actor loss: -0.14 | Critic loss: 5.03 | Entropy loss: -0.0001  | Total Loss: 4.90 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 179/20000  | Episode Reward: -14  | Average Reward -36.63  | Actor loss: -0.67 | Critic loss: 5.07 | Entropy loss: -0.0003  | Total Loss: 4.41 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 180/20000  | Episode Reward: -16  | Average Reward -36.50  | Actor loss: -0.07 | Critic loss: 4.71 | Entropy loss: -0.0002  | Total Loss: 4.63 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 181/20000  | Episode Reward: -15  | Average Reward -36.69  | Actor loss: -0.41 | Critic loss: 4.74 | Entropy loss: -0.0001  | Total Loss: 4.33 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 182/20000  | Episode Reward: -14  | Average Reward -36.52  | Actor loss: -0.12 | Critic loss: 4.76 | Entropy loss: -0.0001  | Total Loss: 4.64 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 183/20000  | Episode Reward: -19  | Average Reward -36.52  | Actor loss: -0.84 | Critic loss: 4.85 | Entropy loss: -0.0005  | Total Loss: 4.02 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 184/20000  | Episode Reward: 7  | Average Reward -36.55  | Actor loss: 11.89 | Critic loss: 48.14 | Entropy loss: -0.0000  | Total Loss: 60.03 | Total Steps: 203\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 185/20000  | Episode Reward: 1  | Average Reward -36.37  | Actor loss: 1.63 | Critic loss: 18.03 | Entropy loss: -0.0001  | Total Loss: 19.66 | Total Steps: 428\n",
      "cylinder\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 186/20000  | Episode Reward: 10  | Average Reward -36.06  | Actor loss: 1.23 | Critic loss: 10.69 | Entropy loss: -0.0002  | Total Loss: 11.93 | Total Steps: 49\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 187/20000  | Episode Reward: -13  | Average Reward -35.97  | Actor loss: -0.16 | Critic loss: 5.43 | Entropy loss: -0.0002  | Total Loss: 5.27 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 188/20000  | Episode Reward: -19  | Average Reward -35.93  | Actor loss: -0.67 | Critic loss: 5.76 | Entropy loss: -0.0004  | Total Loss: 5.09 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 189/20000  | Episode Reward: -15  | Average Reward -36.12  | Actor loss: -1.53 | Critic loss: 5.53 | Entropy loss: -0.0005  | Total Loss: 4.00 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 190/20000  | Episode Reward: -17  | Average Reward -36.15  | Actor loss: -0.17 | Critic loss: 5.20 | Entropy loss: -0.0002  | Total Loss: 5.02 | Total Steps: 500\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 191/20000  | Episode Reward: -16  | Average Reward -36.34  | Actor loss: -0.38 | Critic loss: 5.18 | Entropy loss: -0.0003  | Total Loss: 4.80 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 192/20000  | Episode Reward: -20  | Average Reward -36.21  | Actor loss: -0.72 | Critic loss: 6.12 | Entropy loss: -0.0003  | Total Loss: 5.40 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 193/20000  | Episode Reward: -17  | Average Reward -36.44  | Actor loss: -0.61 | Critic loss: 5.01 | Entropy loss: -0.0002  | Total Loss: 4.40 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 194/20000  | Episode Reward: -18  | Average Reward -36.61  | Actor loss: -0.62 | Critic loss: 4.84 | Entropy loss: -0.0003  | Total Loss: 4.22 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 195/20000  | Episode Reward: -13  | Average Reward -36.56  | Actor loss: -0.55 | Critic loss: 4.93 | Entropy loss: -0.0004  | Total Loss: 4.38 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 196/20000  | Episode Reward: -26  | Average Reward -36.91  | Actor loss: -0.85 | Critic loss: 6.75 | Entropy loss: -0.0003  | Total Loss: 5.90 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 197/20000  | Episode Reward: -15  | Average Reward -36.83  | Actor loss: -0.87 | Critic loss: 4.94 | Entropy loss: -0.0004  | Total Loss: 4.07 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 198/20000  | Episode Reward: -16  | Average Reward -37.06  | Actor loss: -0.67 | Critic loss: 4.72 | Entropy loss: -0.0003  | Total Loss: 4.05 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 199/20000  | Episode Reward: 9  | Average Reward -37.05  | Actor loss: 0.54 | Critic loss: 6.95 | Entropy loss: -0.0002  | Total Loss: 7.48 | Total Steps: 379\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 200/20000  | Episode Reward: -21  | Average Reward -37.06  | Actor loss: -0.38 | Critic loss: 4.92 | Entropy loss: -0.0002  | Total Loss: 4.54 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 201/20000  | Episode Reward: -16  | Average Reward -36.99  | Actor loss: -1.04 | Critic loss: 5.64 | Entropy loss: -0.0005  | Total Loss: 4.60 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 202/20000  | Episode Reward: 6  | Average Reward -36.94  | Actor loss: 1.75 | Critic loss: 16.29 | Entropy loss: -0.0001  | Total Loss: 18.05 | Total Steps: 332\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 203/20000  | Episode Reward: -14  | Average Reward -36.84  | Actor loss: -0.80 | Critic loss: 5.09 | Entropy loss: -0.0005  | Total Loss: 4.29 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 204/20000  | Episode Reward: -13  | Average Reward -36.82  | Actor loss: -0.36 | Critic loss: 5.02 | Entropy loss: -0.0001  | Total Loss: 4.66 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 205/20000  | Episode Reward: -20  | Average Reward -36.83  | Actor loss: -0.13 | Critic loss: 5.28 | Entropy loss: -0.0001  | Total Loss: 5.16 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 206/20000  | Episode Reward: -20  | Average Reward -37.10  | Actor loss: -0.46 | Critic loss: 5.14 | Entropy loss: -0.0003  | Total Loss: 4.68 | Total Steps: 500\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 207/20000  | Episode Reward: 4  | Average Reward -36.89  | Actor loss: 4.82 | Critic loss: 24.93 | Entropy loss: -0.0001  | Total Loss: 29.75 | Total Steps: 418\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 208/20000  | Episode Reward: -12  | Average Reward -37.10  | Actor loss: -0.50 | Critic loss: 5.16 | Entropy loss: -0.0002  | Total Loss: 4.65 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 209/20000  | Episode Reward: -25  | Average Reward -37.43  | Actor loss: -0.62 | Critic loss: 6.03 | Entropy loss: -0.0003  | Total Loss: 5.41 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 210/20000  | Episode Reward: -21  | Average Reward -37.66  | Actor loss: -0.60 | Critic loss: 5.07 | Entropy loss: -0.0004  | Total Loss: 4.47 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 211/20000  | Episode Reward: 8  | Average Reward -37.58  | Actor loss: 1.02 | Critic loss: 43.81 | Entropy loss: -0.0000  | Total Loss: 44.83 | Total Steps: 404\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 212/20000  | Episode Reward: -18  | Average Reward -37.85  | Actor loss: -0.35 | Critic loss: 4.95 | Entropy loss: -0.0001  | Total Loss: 4.60 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 213/20000  | Episode Reward: -17  | Average Reward -38.00  | Actor loss: -0.91 | Critic loss: 4.96 | Entropy loss: -0.0003  | Total Loss: 4.06 | Total Steps: 500\n",
      "cube\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 214/20000  | Episode Reward: -16  | Average Reward -38.16  | Actor loss: -1.08 | Critic loss: 5.05 | Entropy loss: -0.0003  | Total Loss: 3.97 | Total Steps: 500\n",
      "cube\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 215/20000  | Episode Reward: -14  | Average Reward -38.29  | Actor loss: -0.89 | Critic loss: 5.02 | Entropy loss: -0.0002  | Total Loss: 4.13 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 216/20000  | Episode Reward: -18  | Average Reward -38.47  | Actor loss: -0.49 | Critic loss: 4.78 | Entropy loss: -0.0003  | Total Loss: 4.29 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 217/20000  | Episode Reward: -14  | Average Reward -38.27  | Actor loss: -0.07 | Critic loss: 5.12 | Entropy loss: -0.0002  | Total Loss: 5.05 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 218/20000  | Episode Reward: -17  | Average Reward -38.17  | Actor loss: -0.59 | Critic loss: 5.88 | Entropy loss: -0.0003  | Total Loss: 5.29 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 219/20000  | Episode Reward: -16  | Average Reward -37.95  | Actor loss: -0.53 | Critic loss: 5.21 | Entropy loss: -0.0002  | Total Loss: 4.67 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Training  | Episode: 220/20000  | Episode Reward: 7  | Average Reward -37.93  | Actor loss: 0.89 | Critic loss: 6.61 | Entropy loss: -0.0002  | Total Loss: 7.50 | Total Steps: 286\n",
      "capsule\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 221/20000  | Episode Reward: -15  | Average Reward -38.04  | Actor loss: -0.15 | Critic loss: 5.03 | Entropy loss: -0.0001  | Total Loss: 4.88 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 222/20000  | Episode Reward: -17  | Average Reward -38.21  | Actor loss: -0.76 | Critic loss: 4.91 | Entropy loss: -0.0003  | Total Loss: 4.15 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 223/20000  | Episode Reward: -21  | Average Reward -38.29  | Actor loss: -0.53 | Critic loss: 5.47 | Entropy loss: -0.0002  | Total Loss: 4.94 | Total Steps: 500\n",
      "cube\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 224/20000  | Episode Reward: -13  | Average Reward -38.46  | Actor loss: -0.12 | Critic loss: 5.24 | Entropy loss: -0.0001  | Total Loss: 5.12 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 225/20000  | Episode Reward: -15  | Average Reward -38.60  | Actor loss: -0.37 | Critic loss: 4.98 | Entropy loss: -0.0001  | Total Loss: 4.61 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 226/20000  | Episode Reward: -18  | Average Reward -38.85  | Actor loss: -0.70 | Critic loss: 4.94 | Entropy loss: -0.0002  | Total Loss: 4.24 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 227/20000  | Episode Reward: -14  | Average Reward -38.58  | Actor loss: -0.06 | Critic loss: 4.57 | Entropy loss: -0.0001  | Total Loss: 4.51 | Total Steps: 500\n",
      "capsule\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 228/20000  | Episode Reward: -19  | Average Reward -38.42  | Actor loss: -0.10 | Critic loss: 5.46 | Entropy loss: -0.0001  | Total Loss: 5.36 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 229/20000  | Episode Reward: -18  | Average Reward -38.24  | Actor loss: -0.41 | Critic loss: 6.67 | Entropy loss: -0.0001  | Total Loss: 6.26 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 230/20000  | Episode Reward: -12  | Average Reward -38.41  | Actor loss: -0.05 | Critic loss: 4.77 | Entropy loss: -0.0001  | Total Loss: 4.72 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 231/20000  | Episode Reward: -13  | Average Reward -37.98  | Actor loss: -0.06 | Critic loss: 5.16 | Entropy loss: -0.0000  | Total Loss: 5.10 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 232/20000  | Episode Reward: -16  | Average Reward -37.95  | Actor loss: -0.28 | Critic loss: 5.17 | Entropy loss: -0.0002  | Total Loss: 4.89 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 233/20000  | Episode Reward: -16  | Average Reward -37.83  | Actor loss: -0.35 | Critic loss: 4.77 | Entropy loss: -0.0002  | Total Loss: 4.42 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 234/20000  | Episode Reward: -13  | Average Reward -37.97  | Actor loss: -0.03 | Critic loss: 4.53 | Entropy loss: -0.0001  | Total Loss: 4.50 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 235/20000  | Episode Reward: -14  | Average Reward -37.92  | Actor loss: -0.11 | Critic loss: 4.67 | Entropy loss: -0.0001  | Total Loss: 4.55 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 236/20000  | Episode Reward: -16  | Average Reward -37.79  | Actor loss: -0.12 | Critic loss: 4.56 | Entropy loss: -0.0001  | Total Loss: 4.44 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 237/20000  | Episode Reward: -17  | Average Reward -37.58  | Actor loss: -0.04 | Critic loss: 4.49 | Entropy loss: -0.0000  | Total Loss: 4.45 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 238/20000  | Episode Reward: -16  | Average Reward -36.98  | Actor loss: -0.37 | Critic loss: 4.67 | Entropy loss: -0.0001  | Total Loss: 4.30 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 239/20000  | Episode Reward: -12  | Average Reward -37.07  | Actor loss: -0.00 | Critic loss: 4.42 | Entropy loss: -0.0001  | Total Loss: 4.42 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 240/20000  | Episode Reward: -15  | Average Reward -36.83  | Actor loss: -0.54 | Critic loss: 4.77 | Entropy loss: -0.0002  | Total Loss: 4.23 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 241/20000  | Episode Reward: -18  | Average Reward -37.11  | Actor loss: -0.31 | Critic loss: 4.64 | Entropy loss: -0.0002  | Total Loss: 4.33 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 242/20000  | Episode Reward: -15  | Average Reward -36.65  | Actor loss: -0.03 | Critic loss: 4.42 | Entropy loss: -0.0000  | Total Loss: 4.38 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 243/20000  | Episode Reward: -15  | Average Reward -36.01  | Actor loss: -0.59 | Critic loss: 4.84 | Entropy loss: -0.0002  | Total Loss: 4.25 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 244/20000  | Episode Reward: -12  | Average Reward -35.58  | Actor loss: -0.01 | Critic loss: 4.82 | Entropy loss: -0.0001  | Total Loss: 4.81 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 245/20000  | Episode Reward: -15  | Average Reward -34.94  | Actor loss: -0.16 | Critic loss: 4.57 | Entropy loss: -0.0001  | Total Loss: 4.41 | Total Steps: 500\n",
      "cylinder\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 246/20000  | Episode Reward: -14  | Average Reward -34.51  | Actor loss: -0.03 | Critic loss: 4.45 | Entropy loss: -0.0001  | Total Loss: 4.42 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 247/20000  | Episode Reward: -18  | Average Reward -34.47  | Actor loss: -0.14 | Critic loss: 4.70 | Entropy loss: -0.0001  | Total Loss: 4.57 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 248/20000  | Episode Reward: -15  | Average Reward -34.12  | Actor loss: -0.20 | Critic loss: 4.61 | Entropy loss: -0.0001  | Total Loss: 4.41 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 249/20000  | Episode Reward: -16  | Average Reward -33.42  | Actor loss: -0.34 | Critic loss: 5.42 | Entropy loss: -0.0001  | Total Loss: 5.07 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 250/20000  | Episode Reward: -19  | Average Reward -32.22  | Actor loss: -0.58 | Critic loss: 4.87 | Entropy loss: -0.0003  | Total Loss: 4.29 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 251/20000  | Episode Reward: -16  | Average Reward -31.01  | Actor loss: -0.09 | Critic loss: 4.34 | Entropy loss: -0.0001  | Total Loss: 4.25 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 252/20000  | Episode Reward: -13  | Average Reward -29.24  | Actor loss: -0.03 | Critic loss: 4.50 | Entropy loss: -0.0001  | Total Loss: 4.46 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 253/20000  | Episode Reward: -14  | Average Reward -27.29  | Actor loss: -0.52 | Critic loss: 4.62 | Entropy loss: -0.0001  | Total Loss: 4.10 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 254/20000  | Episode Reward: -16  | Average Reward -25.41  | Actor loss: -0.03 | Critic loss: 4.36 | Entropy loss: -0.0001  | Total Loss: 4.33 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 255/20000  | Episode Reward: -12  | Average Reward -24.02  | Actor loss: -0.32 | Critic loss: 4.79 | Entropy loss: -0.0001  | Total Loss: 4.47 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 256/20000  | Episode Reward: -18  | Average Reward -22.68  | Actor loss: -0.31 | Critic loss: 5.52 | Entropy loss: -0.0001  | Total Loss: 5.21 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 257/20000  | Episode Reward: -17  | Average Reward -21.20  | Actor loss: -0.26 | Critic loss: 4.43 | Entropy loss: -0.0001  | Total Loss: 4.16 | Total Steps: 500\n",
      "cube\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 258/20000  | Episode Reward: -16  | Average Reward -19.56  | Actor loss: -0.20 | Critic loss: 5.42 | Entropy loss: -0.0001  | Total Loss: 5.21 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 259/20000  | Episode Reward: -13  | Average Reward -17.95  | Actor loss: -0.02 | Critic loss: 4.56 | Entropy loss: -0.0001  | Total Loss: 4.54 | Total Steps: 500\n",
      "cylinder\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 260/20000  | Episode Reward: -12  | Average Reward -16.40  | Actor loss: -0.12 | Critic loss: 4.78 | Entropy loss: -0.0001  | Total Loss: 4.67 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 261/20000  | Episode Reward: -16  | Average Reward -15.35  | Actor loss: -0.05 | Critic loss: 4.99 | Entropy loss: -0.0002  | Total Loss: 4.93 | Total Steps: 500\n",
      "cube\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 262/20000  | Episode Reward: -12  | Average Reward -14.93  | Actor loss: -0.10 | Critic loss: 5.15 | Entropy loss: -0.0001  | Total Loss: 5.05 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 263/20000  | Episode Reward: -14  | Average Reward -14.06  | Actor loss: -0.17 | Critic loss: 4.47 | Entropy loss: -0.0002  | Total Loss: 4.30 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 264/20000  | Episode Reward: -17  | Average Reward -13.88  | Actor loss: -0.18 | Critic loss: 4.54 | Entropy loss: -0.0001  | Total Loss: 4.37 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 265/20000  | Episode Reward: -15  | Average Reward -13.83  | Actor loss: -0.04 | Critic loss: 4.35 | Entropy loss: -0.0001  | Total Loss: 4.31 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 266/20000  | Episode Reward: -17  | Average Reward -13.57  | Actor loss: -0.04 | Critic loss: 4.49 | Entropy loss: -0.0000  | Total Loss: 4.45 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 267/20000  | Episode Reward: -14  | Average Reward -13.65  | Actor loss: -0.78 | Critic loss: 4.49 | Entropy loss: -0.0003  | Total Loss: 3.71 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 268/20000  | Episode Reward: -16  | Average Reward -13.79  | Actor loss: -0.09 | Critic loss: 4.38 | Entropy loss: -0.0002  | Total Loss: 4.29 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 269/20000  | Episode Reward: -16  | Average Reward -14.03  | Actor loss: -0.04 | Critic loss: 5.15 | Entropy loss: -0.0000  | Total Loss: 5.11 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 270/20000  | Episode Reward: -16  | Average Reward -14.14  | Actor loss: -0.14 | Critic loss: 4.63 | Entropy loss: -0.0001  | Total Loss: 4.49 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 271/20000  | Episode Reward: -14  | Average Reward -14.29  | Actor loss: -0.23 | Critic loss: 5.08 | Entropy loss: -0.0002  | Total Loss: 4.85 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 272/20000  | Episode Reward: -16  | Average Reward -14.00  | Actor loss: -0.42 | Critic loss: 5.17 | Entropy loss: -0.0001  | Total Loss: 4.75 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 273/20000  | Episode Reward: -19  | Average Reward -14.07  | Actor loss: -0.03 | Critic loss: 4.38 | Entropy loss: -0.0000  | Total Loss: 4.34 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 274/20000  | Episode Reward: -16  | Average Reward -14.04  | Actor loss: -0.05 | Critic loss: 4.38 | Entropy loss: -0.0001  | Total Loss: 4.32 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 275/20000  | Episode Reward: -14  | Average Reward -14.03  | Actor loss: -0.27 | Critic loss: 4.33 | Entropy loss: -0.0002  | Total Loss: 4.06 | Total Steps: 500\n",
      "prism\n",
      "Step: 100\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 276/20000  | Episode Reward: -13  | Average Reward -14.05  | Actor loss: -0.07 | Critic loss: 4.48 | Entropy loss: -0.0001  | Total Loss: 4.41 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 277/20000  | Episode Reward: -17  | Average Reward -14.10  | Actor loss: -0.86 | Critic loss: 4.60 | Entropy loss: -0.0001  | Total Loss: 3.74 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 278/20000  | Episode Reward: -13  | Average Reward -14.12  | Actor loss: -0.62 | Critic loss: 4.36 | Entropy loss: -0.0002  | Total Loss: 3.74 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 279/20000  | Episode Reward: -17  | Average Reward -14.15  | Actor loss: -0.23 | Critic loss: 4.71 | Entropy loss: -0.0002  | Total Loss: 4.48 | Total Steps: 500\n",
      "prism\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 280/20000  | Episode Reward: -12  | Average Reward -14.11  | Actor loss: -0.01 | Critic loss: 4.38 | Entropy loss: -0.0001  | Total Loss: 4.37 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 281/20000  | Episode Reward: -15  | Average Reward -14.11  | Actor loss: -0.09 | Critic loss: 4.38 | Entropy loss: -0.0001  | Total Loss: 4.29 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 282/20000  | Episode Reward: -14  | Average Reward -14.11  | Actor loss: -0.04 | Critic loss: 4.44 | Entropy loss: -0.0001  | Total Loss: 4.41 | Total Steps: 500\n",
      "sphere\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 283/20000  | Episode Reward: -12  | Average Reward -14.04  | Actor loss: -0.04 | Critic loss: 4.42 | Entropy loss: -0.0000  | Total Loss: 4.39 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Decision Step reward: -1.0\n",
      "Step: 300\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 284/20000  | Episode Reward: -13  | Average Reward -14.24  | Actor loss: -0.01 | Critic loss: 4.33 | Entropy loss: -0.0001  | Total Loss: 4.32 | Total Steps: 500\n",
      "sphere\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 285/20000  | Episode Reward: -15  | Average Reward -14.40  | Actor loss: -0.24 | Critic loss: 4.76 | Entropy loss: -0.0001  | Total Loss: 4.52 | Total Steps: 500\n",
      "cylinder\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 286/20000  | Episode Reward: -14  | Average Reward -14.64  | Actor loss: -0.02 | Critic loss: 4.42 | Entropy loss: -0.0000  | Total Loss: 4.40 | Total Steps: 500\n",
      "cube\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 100\n",
      "Decision Step reward: -1.0\n",
      "Decision Step reward: -1.0\n",
      "Step: 200\n",
      "Step: 300\n",
      "Decision Step reward: -1.0\n",
      "Step: 400\n",
      "Decision Step reward: -1.0\n",
      "Max Step Reward: -10\n",
      "Step: 500\n",
      "Training  | Episode: 287/20000  | Episode Reward: -16  | Average Reward -14.67  | Actor loss: -0.18 | Critic loss: 4.43 | Entropy loss: -0.0001  | Total Loss: 4.26 | Total Steps: 500\n",
      "capsule\n",
      "Decision Step reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) \n",
    "S0_ALG_NAME = 'S0_without'\n",
    "S0_ENV_ID = '2'\n",
    "S0_episode = 5341\n",
    "ALG_NAME = 'S1_without'\n",
    "ENV_ID = '1'\n",
    "TRAIN_EPISODES = 20000  # number of overall episodes for training\n",
    "TEST_EPISODES = 10  # number of overall episodes for testing\n",
    "MAX_STEPS = 500  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 0.0001  #0.00005 \n",
    "speed = 1\n",
    "num_steps = 100 # the step for updating the network\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.load(S0_episode,S0_ALG_NAME,S0_ENV_ID)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    best_score = 0\n",
    "    hashmap = {\n",
    "    0: 'capsule',\n",
    "    1: 'cube',\n",
    "    2: 'cylinder',\n",
    "    3: 'prism',\n",
    "    4: 'sphere'}\n",
    "    if train:\n",
    "        entropy_term = 0\n",
    "        all_episode_reward = []\n",
    "        all_average_reward = []\n",
    "        all_steps = []\n",
    "        all_actor_loss = []\n",
    "        all_critic_loss = []\n",
    "        all_entropy_loss = []\n",
    "        all_total_loss = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            t0 = time.time()\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            STEPS = 0\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = int(decision_steps.obs[1])\n",
    "            print(f'---{hashmap[index]}---')\n",
    "            # 0-capsule,1-cube,2-cylinder,3-prism,4-sphere \n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "\n",
    "                # Need to use when calculating the loss\n",
    "                log_probs = []\n",
    "                # values = []\n",
    "                values = torch.empty(0).to(device)\n",
    "                rewards = []\n",
    "\n",
    "                for steps in range(num_steps):\n",
    "                    lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                    STEPS += 1\n",
    "                    policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                    # value = value.detach()\n",
    "                    dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                    \n",
    "\n",
    "                    action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                    # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                    action = action_dist.sample() # sample an action from action_dist\n",
    "                    action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "                    \n",
    "                    log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # log_prob = torch.log(F.softmax(policy_dist,dim=1)[0][action])\n",
    "                    # entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "                    entropy = F.cross_entropy(policy_dist.detach(), action)\n",
    "\n",
    "                    discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                    action_tuple = ActionTuple()\n",
    "                    action_tuple.add_discrete(discrete_actions)\n",
    "                    env.set_actions(behavior_name,action_tuple)\n",
    "                    env.step()\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                        tracked_agent = decision_steps.agent_id[0]\n",
    "                        # print(tracked_agent)\n",
    "\n",
    "                    if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                        print('Agent in terminal steps')\n",
    "                        done = True\n",
    "                        reward = terminal_steps[tracked_agent].reward\n",
    "                        if reward > 0:\n",
    "                            pass\n",
    "                        else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                        print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                    elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                        reward = decision_steps[tracked_agent].reward\n",
    "                        # print(f'Decision Step reward: {reward}')\n",
    "                        if reward<0:\n",
    "                            print(f'Decision Step reward: {reward}')\n",
    "                    if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "                    if STEPS % 100 == 0:\n",
    "                        print (f'Step: {STEPS}')\n",
    "\n",
    "                    episode_reward = episode_reward + reward\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    # values.append(value)\n",
    "                    values = torch.cat((values, value), dim=0)\n",
    "                    log_probs.append(log_prob)\n",
    "                    entropy_term = entropy_term + entropy\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "\n",
    "                    if done or steps == num_steps-1:\n",
    "                        # _, Qval,_ = agent(vt_new,lt,lstm_hidden_state)\n",
    "                        # Qval = Qval.detach()\n",
    "                        break\n",
    "                \n",
    "                \n",
    "                discounted_rewards = np.zeros_like(values.cpu().detach().numpy())\n",
    "                cumulative = 0\n",
    "                for t in reversed(range(len(rewards))):\n",
    "                    cumulative = rewards[t] + LAM * cumulative # Monte Carlo\n",
    "                    discounted_rewards[t] = cumulative\n",
    "                # print(f'rewards:{rewards}, discounted_rewards:{discounted_rewards}')\n",
    "                # Advantage Actor Critic\n",
    "\n",
    "                # Qvals[-1] = rewards[t] + LAM * Qval      or       Qvals[-1] = rewards[t]                   \n",
    "                # for t in range(len(rewards)-1):\n",
    "                #         Qvals[t] = rewards[t] + LAM * values[t+1]\n",
    "                \n",
    "                # r_(t+1) = R(s_t|a_t)--> reward[t]        a_t, V_t = agent(s_t)\n",
    "                # A_t = r_(t+1) + LAM * V_(t+1) - V_t \n",
    "                #     = Q_t - V_t\n",
    "                \n",
    "                # Monte Carlo Advantage = reward + LAM * cumulative_reward\n",
    "                # Actor_loss = -log(pai(s_t|a_t))*A_t\n",
    "                # Critic_loss = A_t.pow(2) *0.5\n",
    "                # Entropy_loss = -F.entropy(pai(St),index) * 0.001\n",
    "\n",
    "                # entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "                \n",
    "                #update actor critic\n",
    "                \n",
    "                # values = torch.FloatTensor(values).requires_grad_(True).to(device)\n",
    "                discounted_rewards = torch.FloatTensor(discounted_rewards.astype(np.float32)).to(device)\n",
    "                log_probs = torch.stack(log_probs)\n",
    "                advantage = discounted_rewards - values\n",
    "                actor_loss = (-log_probs * advantage).mean()\n",
    "                critic_loss = 0.5 * torch.square(advantage).mean()\n",
    "                entropy_term /= num_steps\n",
    "                entropy_loss = -0.001 * entropy_term\n",
    "                ac_loss = actor_loss + critic_loss + entropy_loss\n",
    "                # ac_loss = values.mean()\n",
    "                optimizer.zero_grad()\n",
    "                ac_loss.backward()\n",
    "                optimizer.step()\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if param.grad is not None:\n",
    "                #         print(name, param.grad)\n",
    "                #     else:\n",
    "                #         print(name, \"gradients not computed\")\n",
    "                # for name, param in agent.named_parameters():\n",
    "                #     if name == 'value_estimator.weight':\n",
    "                #         print(name, param)\n",
    "                \n",
    "                \n",
    "                if done: break\n",
    "\n",
    "\n",
    "            all_episode_reward.append(episode_reward)\n",
    "            all_steps.append(STEPS)\n",
    "            all_actor_loss.append(actor_loss)\n",
    "            all_critic_loss.append(critic_loss)\n",
    "            all_entropy_loss.append(entropy_loss)\n",
    "            all_total_loss.append(ac_loss)\n",
    "            if episode >= 100:\n",
    "                avg_score = np.mean(all_episode_reward[-100:])\n",
    "                all_average_reward.append(avg_score)\n",
    "                if avg_score > best_score:\n",
    "                    best_score = avg_score\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(f'-----The best score for averaging previous 100 episode reward is {best_score}. Model has been saved-----')\n",
    "                print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Average Reward {:.2f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, avg_score, actor_loss, critic_loss,entropy_loss,  ac_loss, STEPS))\n",
    "            else:  print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Actor loss: {:.2f} | Critic loss: {:.2f} | Entropy loss: {:.4f}  | Total Loss: {:.2f} | Total Steps: {}' \\\n",
    "                    .format(episode + 1, TRAIN_EPISODES, episode_reward, actor_loss, critic_loss, entropy_loss,  ac_loss, STEPS))\n",
    "            if episode%500 == 0:\n",
    "                    agent.save(episode, ALG_NAME, ENV_ID)\n",
    "                    print(\"Model has been saved\")\n",
    "        print(all_average_reward)\n",
    "        agent.save(episode ,ALG_NAME, ENV_ID)\n",
    "        print(\"Model has been saved\")\n",
    "\n",
    "        data = {\n",
    "                    'all_average_reward': all_average_reward,\n",
    "                    'all_episode_reward': all_episode_reward,\n",
    "                    'all_actor_loss': all_actor_loss,\n",
    "                    'all_critic_loss': all_critic_loss,\n",
    "                    'all_entropy_loss': all_entropy_loss,\n",
    "                    'all_total_loss': all_total_loss,\n",
    "                    'all_steps': all_steps,\n",
    "                } \n",
    "        file_path = f'{ALG_NAME}_{ENV_ID}.txt'\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.46, -10.22, -9.69, -9.25, -9.17, -9.17, -9.55, -9.33, -9.61, -10.03, -10.57, -10.27, -10.15, -10.19, -10.0, -10.12, -10.02, -10.06, -9.62, -9.94, -9.92, -10.11, -10.09, -10.14, -10.24, -10.29, -10.45, -10.15, -10.59, -10.82, -10.88, -11.2, -11.41, -11.25, -10.8, -10.32, -9.9, -10.14, -9.81, -9.81, -9.75, -9.59, -9.08, -8.95, -8.67, -8.56, -8.67, -8.37, -8.56, -8.38, -8.75, -8.28, -8.26, -8.21, -8.07, -8.14, -8.07, -8.17, -7.83, -7.48, -7.54, -7.74, -7.86, -8.18, -7.88, -8.38, -8.38, -8.31, -8.5, -8.61, -8.79, -9.02, -9.03, -9.02, -8.93, -9.01, -8.6, -8.6, -9.17, -9.27, -9.17, -8.88, -8.94, -8.91, -8.6, -8.59, -8.76, -8.69, -9.04, -9.11, -9.29, -9.42, -9.59, -9.99, -9.76, -9.73, -10.01, -10.16, -10.29, -10.18, -10.08, -9.8, -9.76, -9.88, -9.93, -10.35, -10.11, -10.31, -9.66, -9.18, -8.97, -9.01, -9.37, -8.97, -8.99, -8.93, -9.14, -9.01, -9.06, -8.77, -8.87, -9.04, -9.43, -9.4, -9.4, -9.36, -9.11, -9.67, -9.19, -8.62, -8.59, -8.66, -8.67, -8.61, -8.74, -8.75, -8.84, -8.49, -8.41, -7.93, -7.93, -7.95, -8.3, -8.49, -8.45, -8.79, -8.5, -8.26, -8.21, -8.18, -7.82, -7.84, -7.8, -8.18, -8.19, -8.07, -7.78, -7.69, -8.63, -8.62, -8.52, -8.27, -8.16, -7.99, -7.99, -7.84, -7.98, -8.49, -8.32, -8.06, -7.98, -7.85, -7.95, -7.72, -7.85, -7.49, -7.6, -7.88, -7.46, -7.27, -7.48, -7.39, -7.06, -7.14, -7.1, -7.38, -7.13, -7.08, -6.73, -6.52, -6.23, -6.5, -6.04, -5.61, -5.68, -5.76, -5.77, -5.59, -5.75, -5.94, -5.53, -5.87, -6.85, -6.72, -6.98, -6.61, -6.5, -6.51, -6.48, -6.58, -6.59, -6.61, -6.58, -7.02, -7.1, -7.29, -7.02, -7.03, -6.97, -7.14, -7.24, -7.03, -6.9, -6.81, -6.72, -6.77, -6.78, -6.29, -6.31, -6.68, -6.93, -6.47, -6.19, -6.48, -6.35, -6.35, -6.42, -6.43, -6.44, -6.82, -6.86, -6.87, -6.57, -6.43, -6.78, -6.54, -6.44, -6.44, -6.65, -6.86, -6.81, -6.83, -6.78, -6.4, -6.48, -6.57, -6.63, -6.62, -5.85, -6.36, -6.35, -6.63, -6.77, -7.28, -7.93, -7.8, -8.42, -7.93, -8.24, -8.64, -9.01, -8.88, -8.78, -8.88, -8.83, -9.15, -9.04, -8.4, -8.27, -8.89, -8.72, -8.77, -8.6, -8.62, -8.53, -7.95, -8.02, -8.02, -8.27, -8.49, -8.64, -8.35, -8.47, -8.41, -8.26, -8.28, -7.98, -8.55, -8.57, -8.53, -8.52, -8.17, -7.19, -7.29, -6.97, -7.12, -7.81, -7.55, -7.84, -7.9, -7.62, -7.47, -7.18, -6.82, -7.02, -7.52, -7.42, -7.77, -8.51, -8.33, -8.34, -8.2, -8.03, -8.56, -8.69, -8.61, -8.7, -8.79, -8.83, -8.5, -8.46, -8.69, -8.66, -8.28, -8.32, -8.52, -8.39, -8.63, -8.67, -8.96, -8.89, -9.44, -9.37, -9.3, -8.99, -8.89, -8.98, -9.14, -8.83, -8.63, -8.76, -8.87, -9.04, -9.37, -9.29, -9.49, -9.44, -9.57, -9.4, -8.97, -9.95, -9.79, -10.05, -9.97, -9.39, -9.15, -9.25, -9.18, -9.54, -9.11, -8.66, -8.67, -8.66, -8.54, -8.45, -8.08, -8.08, -8.09, -8.09, -8.1, -8.85, -9.05, -9.1, -9.44, -9.7, -9.72, -9.78, -9.85, -9.62, -9.43, -9.82, -9.59, -9.56, -9.56, -9.64, -9.52, -10.04, -9.55, -9.69, -9.79, -10.18, -10.06, -10.07, -10.06, -10.35, -10.2, -9.48, -9.86, -9.56, -9.45, -9.42, -9.39, -9.39, -9.67, -9.49, -8.7, -8.68, -9.06, -8.95, -8.94, -8.87, -9.18, -9.33, -8.87, -8.86, -8.89, -8.78, -8.74, -8.65, -8.68, -9.14, -9.0, -8.98, -8.94, -8.9, -8.7, -8.67, -8.47, -8.36, -7.72, -7.81, -7.23, -7.19, -7.26, -7.24, -7.72, -7.63, -7.55, -7.55, -7.7, -7.63, -7.55, -7.38, -7.33, -7.45, -7.16, -7.41, -7.28, -7.55, -7.51, -6.55, -6.53, -6.12, -5.51, -5.43, -6.24, -5.39, -5.57, -5.01, -4.81, -4.83, -4.82, -4.88, -4.88, -4.98, -5.05, -5.01, -4.97, -5.03, -4.55, -3.71, -3.55, -3.65, -3.52, -3.33, -3.42, -3.26, -3.18, -3.25, -3.73, -3.94, -3.9, -4.32, -4.75, -5.27, -5.74, -5.34, -5.24, -4.8, -4.86, -4.56, -4.56, -4.55, -4.58, -4.29, -4.24, -4.26, -3.89, -3.94, -4.41, -4.36, -4.46, -5.08, -4.72, -4.68, -4.67, -4.84, -4.23, -3.74, -3.85, -3.93, -3.61, -3.48, -3.41, -3.29, -3.63, -4.18, -4.13, -4.15, -4.76, -4.13, -4.06, -4.15, -4.79, -4.82, -4.81, -5.13, -5.07, -5.31, -5.27, -5.18, -5.24, -5.24, -5.19, -5.18, -4.68, -4.67, -4.68, -5.01, -5.23, -5.75, -5.7, -5.67, -5.49, -5.37, -5.42, -5.14, -5.14, -5.06, -5.12, -5.25, -5.26, -5.33, -5.37, -5.43, -4.6, -4.73, -4.78, -4.63, -4.65, -4.61, -4.82, -4.78, -5.31, -5.32, -5.32, -6.17, -6.22, -6.33, -6.15, -6.49, -6.39, -6.21, -5.96, -5.91, -5.82, -6.04, -6.15, -6.06, -5.48, -4.74, -4.78, -4.26, -3.91, -3.85, -3.56, -3.46, -3.56, -3.58, -3.27, -3.21, -3.22, -3.31, -3.22, -3.22, -3.21, -3.22, -3.02, -3.06, -2.5, -2.71, -2.69, -2.07, -2.07, -2.17, -2.61, -2.75, -2.63, -2.66, -2.54, -2.36, -2.36, -2.33, -2.33, -2.33, -2.13, -1.57, -1.45, -1.45, -0.74, -1.21, -1.26, -1.17, -0.54, -0.51, -0.93, -0.63, -0.74, -0.5, -0.55, -0.84, -0.94, -0.99, -1.15, -1.21, -1.21, -1.21, -1.22, -0.97, -0.62, -0.2, -0.26, -0.52, -0.55, -0.85, -0.95, -1.07, -1.28, -1.29, -1.51, -1.6, -1.47, -1.52, -1.52, -1.45, -1.46, -1.33, -1.24, -1.25, -1.22, -1.23, -1.02, -1.01, -0.49, -0.53, -0.46, 0.33, 0.38, 0.36, 0.28, 0.57, 0.53, 0.51, 0.57, 0.21, 0.24, -0.22, -0.19, -0.19, -0.19, -0.13, -0.17, -0.2, -0.12, 0.32, 0.48, 0.33, 0.42, 0.31, 0.1, -0.01, -0.07, 0.02, -0.06, -0.3, -0.31, -0.29, -0.32, -0.23, -0.23, -0.05, 0.04, -0.01, -0.01, 0.17, 0.42, 0.73, 0.74, 0.46, 0.39, 0.39, 0.31, 0.3, 0.29, 0.3, 0.47, 0.34, 0.27, 0.28, 0.24, 0.7, 0.8, 0.8, 0.85, 0.82, 1.26, 1.27, 1.25, 1.25, 1.19, 1.48, 1.65, 1.69, 1.77, 1.86, 1.64, 1.65, 1.62, 1.74, 1.76, 1.93, 1.87, 2.13, 2.06, 2.37, 2.5, 2.61, 2.82, 2.83, 3.15, 3.36, 3.29, 3.33, 3.35, 3.35, 3.25, 3.27, 2.85, 2.71, 2.71, 2.72, 2.69, 2.62, 2.53, 2.65, 2.51, 2.41, 2.37, 2.49, 2.44, 2.49, 2.49, 2.5, 2.38, 2.76, 2.46, 2.98, 3.05, 2.85, 2.95, 2.95, 2.8, 2.83, 2.81, 2.84, 2.81, 2.98, 2.89, 3.03, 3.23, 3.36, 3.31, 2.99, 3.08, 3.23, 3.23, 3.24, 3.25, 3.14, 3.14, 3.14, 3.16, 3.23, 3.22, 3.1, 3.05, 3.01, 2.95, 3.4, 3.47, 3.49, 3.57, 3.64, 3.65, 3.65, 3.54, 3.67, 3.58, 3.47, 3.5, 3.67, 3.6, 3.47, 3.35, 3.35, 3.36, 3.34, 3.42, 3.23, 3.32, 3.1, 2.99, 3.0, 3.12, 3.04, 3.27, 3.27, 3.3, 3.29, 3.3, 3.31, 3.35, 3.36, 3.42, 3.31, 3.32, 3.29, 3.15, 3.34, 3.34, 3.31, 3.37, 3.33, 3.22, 3.1, 3.21, 3.07, 3.64, 3.6, 3.52, 3.52, 3.41, 3.42, 3.32, 3.34, 3.4, 3.56, 3.53, 3.54, 3.67, 3.67, 3.75, 3.76, 3.85, 3.3, 3.49, 3.65, 3.66, 3.86, 3.85, 3.7, 3.92, 3.92, 3.82, 3.78, 3.74, 3.74, 3.69, 3.28, 3.28, 3.28, 3.41, 3.66, 3.69, 3.77, 3.78, 3.68, 3.68, 3.66, 3.63, 3.23, 3.23, 3.24, 3.25, 3.37, 3.57, 3.56, 3.61, 3.17, 3.15, 2.92, 2.92, 2.78, 2.6, 2.6, 2.54, 2.54, 2.71, 2.83, 2.75, 2.67, 2.64, 2.77, 2.89, 2.84, 2.64, 2.58, 2.62, 2.81, 2.82, 3.01, 3.03, 2.91, 2.86, 2.94, 2.88, 2.87, 2.96, 2.88, 2.88, 2.77, 2.87, 2.86, 2.93, 2.93, 2.91, 2.96, 2.99, 2.99, 2.84, 2.9, 2.86, 2.93, 3.01, 3.06, 3.03, 3.08, 3.08, 3.27, 3.34, 3.33, 3.33, 3.4, 3.6, 3.49, 3.52, 3.49, 3.54, 3.58, 3.53, 3.53, 3.53, 3.44, 3.43, 3.97, 4.0, 3.97, 3.9, 3.85, 3.85, 3.99, 3.93, 3.93, 3.97, 3.93, 3.94, 3.88, 4.03, 4.43, 4.39, 4.41, 4.41, 4.38, 4.27, 4.28, 4.28, 4.33, 4.35, 4.48, 4.52, 4.95, 4.9, 4.91, 4.8, 4.76, 4.73, 4.67, 4.64, 5.02, 5.01, 5.25, 5.25, 5.41, 5.59, 5.53, 5.7, 5.64, 5.58, 5.36, 5.38, 5.46, 5.56, 5.56, 5.49, 5.57, 5.76, 5.85, 5.86, 5.86, 5.85, 5.31, 5.29, 5.37, 5.19, 5.03, 5.09, 5.1, 5.12, 5.11, 4.96, 5.07, 5.07, 5.02, 5.09, 5.09, 5.12, 4.9, 4.94, 4.85, 4.96, 4.91, 4.95, 5.0, 5.05, 4.95, 4.93, 4.85, 4.68, 4.56, 4.48, 4.49, 4.57, 4.38, 4.31, 4.35, 4.22, 4.07, 4.01, 3.94, 4.0, 3.99, 3.98, 4.04, 4.08, 4.02, 4.1, 4.02, 4.09, 4.03, 3.97, 3.89, 3.96, 3.96, 4.04, 4.15, 4.23, 4.22, 4.18, 4.19, 4.24, 4.17, 4.17, 4.22, 4.33, 4.18, 4.09, 4.03, 4.03, 4.03, 4.03, 3.85, 3.89, 3.89, 3.88, 3.88, 3.91, 4.02, 3.96, 4.01, 4.01, 4.02, 4.02, 3.92, 3.92, 3.97, 3.97, 3.84, 3.9, 4.04, 4.11, 4.03, 4.03, 4.03, 3.96, 3.93, 3.81, 3.73, 3.73, 3.73, 3.75, 4.25, 4.35, 4.39, 4.62, 4.71, 4.53, 4.49, 4.49, 4.59, 4.74, 4.67, 4.66, 4.72, 4.72, 4.83, 4.83, 5.07, 5.08, 5.17, 5.21, 5.04, 4.89, 4.89, 4.88, 4.92, 4.96, 5.13, 5.3, 5.36, 5.43, 5.43, 5.04, 5.23, 5.3, 5.26, 5.43, 5.6, 5.67, 5.71, 5.64, 5.65, 5.59, 5.61, 5.53, 5.6, 5.6, 5.64, 5.56, 5.67, 5.72, 5.8, 5.8, 5.77, 5.77, 5.71, 5.71, 5.74, 5.78, 5.78, 5.74, 5.79, 5.75, 5.77, 5.77, 5.92, 6.0, 6.11, 6.11, 6.11, 6.11, 6.29, 6.3, 6.22, 6.29, 6.33, 6.32, 6.24, 6.34, 6.27, 6.29, 6.29, 6.29, 6.23, 6.22, 6.23, 6.23, 6.42, 6.42, 6.33, 6.33, 6.41, 6.37, 6.31, 6.45, 6.47, 6.57, 6.65, 6.65, 6.65, 6.65, 6.69, 6.66, 6.66, 6.52, 6.54, 6.71, 6.75, 6.67, 6.64, 6.55, 6.62, 6.63, 6.59, 6.59, 6.59, 6.59, 6.59, 5.36, 3.51, 2.89, 3.11, 2.0, 1.98, 0.83, -0.06, -0.06, -1.09, -1.09, -1.89, -2.68, -2.68, -2.23, -3.15, -3.87, -4.31, -4.3, -5.3, -6.2, -6.8, -6.73, -6.73, -7.61, -7.6, -8.37, -9.16, -9.17, -9.1, -9.02, -9.02, -9.0, -9.97, -9.97, -9.94, -10.79, -11.23, -11.68, -12.18, -12.18, -12.18, -12.14, -12.84, -13.54, -13.95, -14.55, -14.55, -14.54, -15.35, -15.87, -16.49, -17.01, -17.01, -17.65, -18.08, -18.63, -19.05, -19.0, -19.42, -19.42, -19.91, -19.9, -20.46, -20.46, -20.8, -21.25, -21.25, -21.25, -21.25, -21.25, -21.66, -21.66, -22.15, -22.11, -22.62, -23.09, -23.54, -23.89, -24.16, -24.23, -24.82, -25.12, -25.51, -25.47, -25.47, -25.33, -25.28, -25.27, -25.7, -25.97, -25.94, -26.25, -26.25, -26.25, -26.35, -26.98, -27.38, -27.45, -27.45, -26.16, -24.57, -24.49, -24.49, -23.56, -23.57, -22.75, -21.73, -21.72, -20.69, -21.11, -20.38, -19.64, -19.68, -19.78, -18.88, -18.3, -17.74, -17.93, -17.04, -16.26, -15.58, -15.75, -15.9, -15.29, -15.36, -14.51, -13.76, -14.39, -14.81, -14.81, -14.83, -14.84, -13.86, -13.92, -13.97, -13.12, -13.23, -12.78, -12.58, -12.59, -12.6, -12.6, -12.08, -11.4, -11.0, -10.7, -10.7, -10.71, -9.9, -9.57, -9.14, -8.72, -8.75, -8.14, -7.67, -7.09, -6.67, -6.77, -6.27, -6.27, -5.7, -5.7, -5.22, -5.61, -5.11, -4.75, -4.75, -4.75, -4.75, -4.75, -4.21, -4.21, -3.72, -3.72, -3.15, -2.68, -2.25, -1.87, -1.6, -1.53, -0.95, -0.67, -0.32, -0.32, -0.48, -0.48, -0.53, -0.59, -0.2, 0.15, 0.09, 0.44, 0.2, 0.1, 0.24, 0.61, 0.96, 1.03, 0.91, 0.75, 0.97, 1.51, 1.35, 1.66, 1.64, 1.98, 1.79, 1.74, 1.74, 2.1, 2.23, 2.28, 2.23, 2.33, 2.34, 2.42, 2.42, 2.61, 2.68, 2.81, 2.81, 2.94, 3.08, 3.24, 3.31, 3.31, 3.24, 3.75, 3.61, 3.5, 3.42, 3.42, 3.41, 3.47, 3.51, 3.51, 4.08, 3.99, 4.25, 4.24, 4.25, 4.08, 4.1, 4.13, 3.95, 4.06, 3.85, 3.86, 3.86, 4.05, 4.24, 4.34, 4.37, 4.3, 4.1, 4.03, 3.91, 3.94, 3.94, 3.94, 3.94, 3.82, 3.84, 4.09, 4.09, 4.19, 4.1, 4.1, 4.07, 3.91, 3.87, 3.7, 3.67, 3.67, 3.47, 3.47, 3.5, 3.5, 3.5, 3.45, 3.46, 3.44, 3.41, 3.34, 3.47, 3.47, 3.42, 3.45, 3.36, 3.21, 3.21, 3.16, 3.4, 3.07, 3.07, 3.28, 3.27, 3.2, 2.82, 2.81, 2.57, 2.46, 2.59, 2.44, 2.35, 2.28, 2.06, 2.04, 2.0, 1.94, 1.88, 1.77, 1.69, 1.37, 1.27, 1.2, 1.07, 1.01, 1.0, 0.92, 0.42, 0.3, 0.13, 0.0, -0.3, -0.32, -0.49, -0.41, -0.01, -0.08, -0.46, -0.74, -0.76, -0.77, -0.84, -0.84, -0.88, -0.81, -0.73, -0.86, -0.86, -0.76, -0.58, -0.55, -0.33, -0.46, -0.25, -0.25, -0.26, -0.26, -0.26, -0.27, -0.28, -0.18, 0.06, -0.03, 0.06, 0.13, -0.01, -0.44, -0.44, -0.32, -0.34, -0.29, -0.35, -0.35, -0.28, -0.78, -0.75, -0.59, -0.52, -0.35, -0.32, -0.32, -0.12, -0.12, -0.2, -0.22, -0.3, -0.25, -0.25, -0.21, -0.11, -0.04, -0.01, -0.01, -0.04, -0.01, 0.1, 0.25, 0.28, 0.33, 0.33, 0.73, 0.61, 0.66, 0.72, 0.79, 1.25, 1.02, 1.3, 1.4, 1.43, 1.61, 1.75, 1.82, 2.23, 2.3, 2.32, 2.43, 2.47, 2.58, 2.69, 3.01, 3.12, 2.9, 3.03, 3.09, 2.77, 2.84, 3.33, 3.49, 3.67, 3.97, 4.24, 4.25, 4.53, 4.58, 4.71, 4.89, 5.32, 5.61, 5.58, 5.59, 5.67, 5.67, 5.75, 5.76, 5.58, 5.51, 5.41, 5.48, 5.48, 5.42, 5.03, 5.33, 5.31, 5.28, 5.01, 5.01, 5.01, 5.02, 4.96, 4.94, 4.94, 5.12, 5.07, 5.03, 5.08, 5.44, 5.44, 5.37, 5.45, 5.53, 5.59, 5.59, 5.57, 6.03, 6.03, 5.98, 5.99, 5.99, 5.99, 5.99, 5.99, 5.98, 6.06, 6.08, 6.16, 6.16, 6.16, 6.16, 6.14, 6.12, 6.04, 6.04, 6.17, 6.16, 6.17, 6.17, 6.13, 6.17, 6.15, 6.17, 6.27, 6.26, 6.21, 6.21, 6.25, 6.65, 6.64, 6.65, 6.35, 6.34, 6.29, 6.29, 6.29, 6.29, 6.31, 6.32, 6.34, 6.36, 6.42, 6.42, 6.4, 6.75, 6.74, 6.74, 7.12, 7.13, 7.14, 7.14, 7.14, 7.15, 7.18, 7.19, 7.19, 7.15, 7.15, 7.13, 7.11, 7.1, 7.16, 7.15, 7.15, 7.15, 7.19, 7.13, 7.26, 7.48, 7.58, 7.58, 7.57, 7.62, 8.0, 8.01, 7.98, 7.99, 8.27, 8.27, 8.27, 8.26, 8.33, 8.34, 8.34, 8.34, 8.42, 8.44, 8.53, 8.6, 8.6, 8.55, 8.55, 8.56, 8.56, 8.56, 8.59, 8.63, 8.63, 8.67, 8.67, 8.65, 8.65, 8.64, 8.56, 8.57, 8.57, 8.57, 8.57, 8.57, 8.56, 8.51, 8.53, 8.48, 8.51, 8.24, 8.2, 8.05, 8.06, 8.06, 8.13, 8.14, 8.16, 8.17, 8.19, 8.2, 8.16, 8.16, 8.16, 8.11, 7.86, 7.84, 8.1, 8.07, 7.68, 7.58, 7.58, 7.44, 7.44, 7.29, 7.12, 7.12, 7.06, 7.06, 6.52, 6.52, 6.53, 6.51, 6.51, 6.5, 6.47, 6.41, 6.34, 6.34, 6.34, 6.34, 6.28, 6.26, 6.24, 5.8, 5.87, 5.87, 5.87, 5.88, 5.88, 5.84, 5.83, 5.9, 5.95, 5.92, 5.82, 5.81, 5.82, 5.83, 5.84, 5.85, 5.85, 5.87, 5.76, 5.76, 5.76, 5.75, 5.69, 5.7, 5.7, 5.7, 5.51, 5.52, 5.46, 5.46, 5.46, 5.51, 5.51, 5.51, 5.4, 5.4, 5.41, 5.41, 5.41, 5.42, 5.38, 5.4, 5.39, 5.35, 5.43, 5.43, 5.35, 5.35, 5.35, 5.31, 5.23, 5.28, 5.16, 5.23, 5.28, 5.48, 5.52, 5.64, 5.57, 5.57, 5.57, 5.57, 5.57, 5.51, 5.51, 5.51, 5.54, 5.54, 5.39, 5.37, 5.63, 5.65, 5.67, 5.71, 6.15, 6.25, 6.23, 6.32, 6.29, 6.44, 6.49, 6.49, 6.55, 6.54, 6.99, 6.94, 6.94, 6.96, 6.93, 6.94, 6.97, 6.96, 6.9, 6.9, 6.56, 6.56, 6.62, 6.68, 6.72, 7.13, 7.13, 7.1, 7.1, 7.04, 7.04, 7.08, 6.97, 6.97, 6.97, 6.96, 7.06, 7.07, 7.07, 6.99, 6.95, 6.88, 6.9, 6.9, 7.01, 6.98, 6.98, 6.96, 7.02, 6.91, 6.91, 6.83, 7.02, 7.03, 7.09, 7.09, 7.05, 7.05, 7.05, 7.0, 7.04, 7.04, 7.02, 6.96, 6.87, 6.87, 6.84, 6.74, 6.75, 6.74, 6.74, 6.74, 6.82, 6.82, 6.79, 6.83, 6.92, 6.86, 6.98, 6.98, 6.63, 6.69, 6.69, 6.72, 6.79, 6.79, 6.79, 6.17, 5.86, 5.92, 5.92, 5.91, 5.94, 5.81, 5.93, 5.92, 5.8, 5.73, 5.57, 5.22, 5.05, 4.96, 4.88, 4.93, 4.96, 4.96, 5.08, 5.08, 4.92, 4.88, 4.92, 4.97, 4.87, 4.87, 4.9, 4.82, 4.79, 4.86, 4.9, 4.9, 5.09, 5.05, 4.94, 4.73, 4.68, 4.73, 4.72, 4.76, 4.62, 4.65, 4.64, 4.53, 4.65, 4.58, 4.46, 4.34, 4.34, 4.34, 4.3, 4.25, 4.18, 4.19, 4.22, 4.13, 4.01, 4.01, 4.01, 4.05, 4.05, 4.16, 4.16, 4.23, 4.17, 4.17, 4.17, 4.16, 4.09, 4.16, 4.02, 4.05, 4.11, 4.11, 4.1, 4.07, 4.16, 4.16, 4.23, 4.33, 4.24, 4.23, 4.23, 4.23, 4.21, 4.13, 4.16, 4.16, 4.06, 3.98, 3.94, 3.84, 4.14, 4.15, 4.15, 3.84, 3.82, 3.7, 3.62, 4.24, 4.44, 4.44, 4.44, 4.36, 4.28, 4.41, 4.39, 4.35, 4.36, 4.43, 4.48, 4.83, 5.0, 5.05, 5.15, 5.09, 4.75, 4.75, 4.75, 4.72, 4.81, 4.86, 4.92, 4.9, 4.99, 4.95, 4.89, 4.97, 4.97, 4.85, 4.94, 4.94, 5.01, 5.05, 5.08, 5.14, 5.17, 5.17, 5.04, 4.79, 4.86, 4.89, 4.77, 4.83, 4.83, 4.82, 4.84, 4.92, 4.83, 4.83, 4.75, 4.71, 4.8, 4.75, 4.75, 4.73, 4.85, 4.71, 4.66, 4.66, 4.55, 4.48, 4.48, 4.49, 4.55, 4.48, 4.48, 4.49, 4.6, 4.57, 4.71, 4.7, 4.63, 4.58, 4.59, 4.65, 4.62, 4.6, 4.6, 4.6, 4.68, 4.75, 4.75, 4.68, 4.7, 4.71, 4.65, 4.65, 4.69, 4.83, 4.87, 4.88, 4.93, 4.85, 4.85, 5.17, 5.12, 5.24, 5.29, 5.28, 5.22, 5.22, 5.22, 5.31, 5.42, 5.42, 5.47, 5.5, 5.58, 5.58, 5.71, 5.64, 5.64, 5.67, 5.66, 5.68, 5.91, 5.91, 5.91, 5.94, 6.01, 5.89, 5.84, 5.78, 5.79, 5.77, 5.83, 5.83, 5.83, 5.95, 5.92, 5.89, 5.92, 5.92, 6.0, 6.08, 6.0, 6.0, 6.14, 6.39, 6.46, 6.45, 6.56, 6.58, 6.58, 6.54, 6.6, 6.6, 6.69, 6.69, 6.81, 6.97, 6.95, 7.01, 7.01, 7.12, 7.12, 7.29, 7.34, 7.34, 7.45, 7.52, 7.51, 7.51, 7.51, 7.58, 7.46, 7.46, 7.46, 7.46, 7.45, 7.46, 7.52, 7.57, 7.56, 7.59, 7.56, 7.58, 7.57, 7.57, 7.04, 7.04, 6.88, 6.95, 6.31, 6.34, 6.39, 6.39, 6.45, 6.45, 6.45, 6.54, 6.54, 6.62, 6.48, 6.47, 6.54, 6.53, 6.56, 6.57, 6.74, 6.73, 6.73, 6.73, 6.34, 6.34, 6.34, 6.43, 6.45, 6.42, 6.42, 6.35, 6.35, 6.36, 6.35, 6.39, 6.49, 6.49, 6.49, 6.49, 6.46, 6.58, 6.64, 6.64, 6.64, 6.7, 6.7, 6.69, 6.72, 6.72, 6.67, 6.7, 6.75, 6.73, 6.73, 6.78, 6.89, 6.89, 6.89, 6.88, 6.84, 6.82, 6.84, 6.81, 6.8, 6.92, 6.96, 7.04, 6.98, 6.97, 6.97, 6.88, 6.92, 6.97, 6.97, 6.97, 6.97, 6.97, 6.97, 6.97, 6.97, 6.97, 6.98, 6.96, 6.96, 6.95, 7.07, 7.07, 7.07, 6.78, 6.75, 6.77, 6.79, 6.52, 6.54, 6.54, 6.56, 6.56, 6.53, 6.44, 6.67, 6.67, 6.83, 6.83, 7.47, 7.3, 7.31, 7.31, 7.31, 7.31, 7.31, 7.31, 7.31, 7.31, 7.45, 7.46, 7.32, 7.33, 7.33, 7.23, 7.23, 7.24, 7.24, 7.19, 7.53, 7.53, 7.53, 7.49, 7.5, 7.53, 7.53, 7.67, 7.67, 7.57, 7.59, 7.59, 7.59, 7.59, 7.59, 7.59, 7.61, 7.61, 7.59, 7.66, 7.64, 7.64, 7.61, 7.62, 7.62, 7.62, 7.7, 7.63, 7.63, 7.65, 7.65, 7.67, 7.67, 7.6, 7.6, 7.58, 7.62, 7.65, 7.65, 7.71, 7.7, 7.65, 7.64, 7.64, 7.7, 7.71, 7.68, 7.77, 7.76, 7.76, 7.76, 7.75, 7.75, 7.75, 7.75, 7.75, 7.7, 7.7, 7.7, 7.68, 7.68, 7.67, 7.67, 7.66, 7.61, 7.91, 7.93, 7.85, 7.79, 8.06, 8.07, 8.07, 8.11, 8.11, 8.15, 8.24, 8.54, 8.54, 8.54, 8.52, 8.52, 8.73, 8.7, 8.7, 8.69, 8.68, 8.68, 8.68, 8.66, 8.66, 8.66, 8.66, 8.77, 8.77, 8.77, 8.87, 8.83, 8.82, 8.81, 8.55, 8.6, 8.6, 8.6, 8.59, 8.59, 8.59, 8.59, 8.59, 8.59, 8.69, 8.69, 8.69, 8.7, 8.7, 8.7, 8.7, 8.71, 8.71, 8.72, 8.73, 8.72, 8.72, 8.75, 8.73, 8.7, 8.7, 8.7, 8.76, 8.76, 8.76, 8.76, 8.76, 8.76, 8.83, 8.83, 8.85, 8.85, 8.85, 8.85, 8.85, 8.84, 8.86, 8.85, 8.85, 8.77, 8.77, 8.8, 8.81, 8.81, 8.81, 8.81, 8.82, 8.82, 8.82, 8.82, 8.81, 8.86, 8.86, 8.83, 8.87, 8.87, 8.88, 8.88, 8.89, 8.94, 8.95, 8.95, 9.03, 9.09, 9.09, 9.09, 9.08, 9.08, 9.08, 9.08, 9.08, 9.09, 9.09, 9.09, 9.11, 9.11, 9.11, 9.14, 9.14, 9.15, 9.16, 9.16, 9.16, 9.18, 9.18, 9.18, 9.18, 9.21, 9.2, 9.19, 9.18, 9.22, 9.23, 9.24, 9.55, 9.55, 9.54, 9.54, 9.59, 9.59, 9.59, 9.59, 9.59, 9.59, 9.59, 9.59, 9.59, 9.59, 9.59, 9.58, 9.58, 9.56, 9.56, 9.57, 9.57, 9.6, 9.6, 9.6, 9.62, 9.65, 9.65, 9.65, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.67, 9.66, 9.66, 9.66, 9.46, 9.49, 9.49, 9.49, 9.49, 9.57, 9.57, 9.57, 9.57, 9.58, 9.58, 9.58, 9.58, 9.58, 9.38, 9.38, 9.37, 9.37, 9.37, 9.4, 9.4, 9.4, 9.41, 9.41, 9.33, 9.33, 9.33, 9.35, 9.35, 9.35, 9.35, 9.35, 9.36, 9.36, 9.36, 9.35, 9.24, 9.24, 9.24, 9.24, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.22, 9.17, 9.17, 9.16, 9.16, 9.17, 9.18, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.2, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.17, 9.17, 9.17, 9.17, 9.18, 9.18, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.19, 9.2, 9.2, 9.2, 9.4, 9.4, 9.43, 9.45, 9.45, 9.45, 9.45, 9.45, 9.45, 9.45, 9.45, 9.45, 9.24, 9.24, 9.44, 9.44, 9.46, 9.46, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.31, 9.31, 9.32, 9.08, 9.07, 9.07, 9.06, 9.06, 9.05, 9.05, 9.05, 9.06, 9.17, 9.17, 9.17, 9.17, 9.18, 9.18, 9.18, 9.18, 9.17, 9.17, 9.17, 9.17, 9.17, 9.18, 9.23, 9.23, 9.24, 9.24, 9.24, 9.23, 9.23, 9.23, 9.23, 9.23, 9.22, 9.22, 9.22, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.22, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.25, 9.25, 9.25, 9.25, 9.25, 9.25, 9.25, 9.25, 9.25, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.24, 9.23, 9.23, 9.23, 9.23, 9.22, 9.22, 9.22, 9.43, 9.43, 9.42, 9.42, 9.42, 9.42, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.63, 9.61, 9.85, 9.86, 9.86, 9.87, 9.87, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.88, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.9, 9.9, 9.89, 9.89, 9.89, 9.9, 9.9, 9.9, 9.9, 9.9, 9.89, 9.89, 9.89, 9.89, 9.89, 9.9, 9.9, 9.9, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.88, 9.88, 9.88, 9.88, 9.88, 9.89, 9.89, 9.88, 9.88, 9.88, 9.88, 9.88, 9.87, 9.87, 9.87, 9.87, 9.87, 9.86, 9.86, 9.85, 9.85, 9.85, 9.85, 9.85, 9.84, 9.84, 9.83, 9.83, 9.84, 9.84, 9.84, 9.84, 9.84, 9.85, 9.85, 9.85, 9.85, 9.85, 9.85, 9.84, 9.84, 9.84, 9.84, 9.84, 9.86, 9.88, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.84, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.84, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.85, 9.85, 9.85, 9.85, 9.85, 9.85, 9.85, 9.86, 9.86, 9.86, 9.85, 9.85, 9.86, 9.86, 9.63, 9.41, 9.4, 9.41, 9.41, 9.42, 9.38, 9.38, 9.38, 9.38, 9.38, 9.37, 9.38, 9.14, 9.14, 9.12, 9.12, 9.12, 9.12, 9.11, 9.11, 9.11, 9.11, 9.11, 9.11, 9.12, 9.1, 9.1, 8.83, 8.83, 8.63, 8.6, 8.57, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.54, 8.54, 8.54, 8.54, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.55, 8.57, 8.58, 8.58, 8.58, 8.58, 8.58, 8.58, 8.58, 8.59, 8.59, 8.59, 8.59, 8.59, 8.59, 8.59, 8.59, 8.58, 8.58, 8.58, 8.58, 8.58, 8.58, 8.58, 8.59, 8.57, 8.57, 8.57, 8.57, 8.57, 8.57, 8.57, 8.57, 8.57, 8.53, 8.53, 8.53, 8.53, 8.53, 8.53, 8.52, 8.52, 8.53, 8.53, 8.53, 8.53, 8.76, 8.98, 8.99, 8.99, 8.99, 8.99, 9.03, 9.03, 9.03, 9.03, 9.05, 9.06, 9.06, 9.3, 9.29, 9.31, 9.31, 9.31, 9.3, 9.31, 9.31, 9.3, 9.29, 9.29, 9.29, 9.29, 9.31, 9.09, 9.35, 9.14, 9.34, 9.37, 9.19, 9.21, 9.21, 8.99, 8.97, 8.95, 8.73, 8.73, 8.53, 8.31, 8.31, 8.32, 8.11, 8.11, 8.11, 7.91, 7.7, 7.49, 7.49, 7.49, 7.29, 7.09, 7.08, 6.86, 6.86, 6.86, 6.86, 6.84, 6.84, 6.84, 6.64, 6.64, 6.64, 6.64, 6.64, 6.64, 6.64, 6.44, 6.44, 6.45, 6.45, 6.45, 6.45, 6.45, 6.45, 6.45, 6.45, 6.47, 6.47, 6.47, 6.47, 6.22, 6.22, 6.22, 6.22, 6.22, 6.26, 6.26, 6.26, 6.26, 6.26, 6.26, 6.27, 6.27, 6.27, 6.27, 6.27, 6.27, 6.27, 6.27, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.25, 6.26, 6.26, 6.26, 6.26, 6.27, 6.27, 6.27, 6.28, 6.29, 6.26, 6.26, 6.26, 6.26, 6.48, 6.49, 6.7, 6.7, 6.7, 6.92, 6.92, 6.92, 7.14, 7.16, 7.18, 7.4, 7.4, 7.6, 7.82, 7.82, 7.82, 8.03, 8.03, 8.03, 8.23, 8.44, 8.65, 8.65, 8.65, 8.85, 9.05, 9.06, 9.28, 9.28, 9.28, 9.28, 9.08, 9.08, 9.08, 9.28, 9.28, 9.28, 9.28, 9.28, 9.28, 9.28, 9.48, 9.48, 9.48, 9.48, 9.48, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.72, 9.72, 9.72, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.72, 9.72, 9.72, 9.72, 9.72, 9.72, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.74, 9.74, 9.74, 9.74, 9.74, 9.74, 9.74, 9.74, 9.74, 9.74, 9.74, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.92, 9.63, 9.63, 9.64, 9.64, 9.63, 9.56, 9.52, 9.52, 9.48, 9.45, 9.45, 9.45, 9.46, 9.45, 9.2, 9.19, 8.86, 8.84, 8.8, 8.74, 8.64, 8.64, 8.51, 8.42, 8.43, 8.43, 8.39, 8.39, 8.37, 8.37, 8.37, 8.37, 8.37, 8.37, 8.37, 8.37, 8.37, 8.31, 8.31, 8.31, 8.27, 8.27, 8.27, 8.25, 8.25, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.23, 8.23, 8.23, 8.23, 8.23, 8.23, 8.23, 8.23, 8.23, 8.23, 8.25, 8.25, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.24, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.21, 8.23, 8.52, 8.52, 8.52, 8.52, 8.53, 8.6, 8.64, 8.64, 8.68, 8.71, 8.71, 8.71, 8.71, 8.72, 8.97, 8.98, 9.31, 9.33, 9.37, 9.43, 9.53, 9.53, 9.66, 9.75, 9.75, 9.75, 9.79, 9.79, 9.81, 9.81, 9.81, 9.81, 9.81, 9.81, 9.81, 9.81, 9.81, 9.87, 9.87, 9.87, 9.91, 9.91, 9.91, 9.93, 9.93, 9.94, 9.94, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.99, 9.98, 9.95, 9.94, 9.94, 9.69, 9.69, 9.69, 9.69, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.67, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.65, 9.65, 9.65, 9.64, 9.64, 9.64, 9.64, 9.64, 9.63, 9.62, 9.62, 9.62, 9.62, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.61, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.6, 9.61, 9.64, 9.65, 9.65, 9.9, 9.9, 9.9, 9.9, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.9, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.92, 9.92, 9.92, 9.93, 9.93, 9.93, 9.93, 9.93, 9.94, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.77, 9.77, 9.77, 9.73, 9.72, 9.72, 9.72, 9.72, 9.72, 9.72, 9.72, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.69, 9.69, 9.69, 9.69, 9.69, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.7, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.71, 9.72, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.73, 9.91, 9.91, 9.91, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.52, 9.52, 9.52, 9.52, 9.52, 9.52, 9.52, 9.52, 9.51, 9.51, 9.51, 9.51, 9.46, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.51, 9.22, 9.22, 9.22, 9.22, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.21, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.64, 9.64, 9.64, 9.64, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.69, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.68, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.67, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.93, 9.93, 9.93, 9.93, 9.93, 9.91, 9.91, 9.91, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.94, 9.93, 9.93, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.55, 9.55, 9.55, 9.55, 9.55, 9.55, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.57, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 9.58, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.63, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.65, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.64, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.62, 9.62, 9.62, 9.62, 9.62, 9.62, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.61, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.56, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.23, 9.22, 9.22, 9.22, 9.22, 9.22, 9.22, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.66, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.79, 9.8, 9.8, 9.8, 9.8, 9.8, 9.81, 9.81, 9.81, 9.81, 9.81, 9.81, 9.81, 9.82, 9.82, 9.82, 9.82, 9.82, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.49, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.48, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.47, 9.46, 9.46, 9.46, 9.46, 9.46, 9.46, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.87, 9.87, 9.87, 9.87, 9.84, 9.84, 9.84, 9.84, 9.84, 9.84, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.83, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.82, 9.94, 9.94, 9.94, 9.94, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.9, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.87, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.86, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.89, 9.88, 9.88, 9.88, 9.88, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.91, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.95, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 9.96, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.95, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.94, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.93, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.92, 9.97, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.98, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.76, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.77, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 9.78, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 9.99, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "print(all_average_reward)\n",
    "list_string = str(all_average_reward)\n",
    "file_path = f\"{ALG_NAME}_{ENV_ID}.txt\"  # Specify the file path and name\n",
    "with open(file_path, \"w\") as file:\n",
    "    # Write the list string to the file\n",
    "    file.write(list_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linzj\\anaconda3\\envs\\rl\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 0, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 1, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 2, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 3, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 4, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 5, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 6, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 7, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 8, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 9, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 10, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 11, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 12, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 13, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 14, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 15, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 16, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 17, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 18, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 19, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 20, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 21, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 22, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 23, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 24, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 25, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 26, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 27, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 28, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 29, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 30, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 31, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 32, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 33, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 34, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 35, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 36, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 37, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 38, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 39, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 40, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 41, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 42, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 43, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 44, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 45, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 46, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 47, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 48, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 49, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 50, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 51, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 52, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 53, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 54, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 55, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 56, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 57, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 58, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 59, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 60, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 61, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 62, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 63, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 64, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 65, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 66, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 67, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 68, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 69, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 70, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 71, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 72, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 73, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 74, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 75, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 76, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 77, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 78, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 79, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 80, Episode reward: 9.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 81, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 82, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 83, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 84, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 85, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 86, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 87, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 88, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 89, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 90, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 91, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 92, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 93, Episode reward: 9.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 94, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 95, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 96, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 97, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 98, Episode reward: 10.0\n",
      "Agent in terminal steps\n",
      "Terminal Step reward: 10.0\n",
      "Episode: 99, Episode reward: 10.0\n",
      "Average Episode Reward: 9.97999999999998\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cpu\")\n",
    "episode = 5341\n",
    "speed = 1\n",
    "MAX_STEPS = 500\n",
    "TEST_EPISODES = 100\n",
    "ALG_NAME = 'S0_without'\n",
    "ENV_ID = '2'\n",
    "tracked_agent = -1\n",
    "env.reset()\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode,ALG_NAME,ENV_ID)\n",
    "average = 0\n",
    "for episode in range(TEST_EPISODES):\n",
    "            STEPS = 0\n",
    "            episode_reward = 0\n",
    "            # env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "            # state = env.reset().astype(np.float32)\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while not done:\n",
    "                STEPS += 1                \n",
    "                lstm_hidden_state = tuple(tensor.detach() for tensor in lstm_hidden_state)\n",
    "                policy_dist, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                # value = value.detach()\n",
    "                dist = F.softmax(policy_dist.detach(),dim=1).cpu().numpy()\n",
    "                \n",
    "\n",
    "                action_dist = Categorical(F.softmax(policy_dist.detach(),dim=1))\n",
    "                # action_dist = Categorical(F.softmax(policy_dist,dim=1))\n",
    "                action = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(action),num_actions).cpu()\n",
    "\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    # if reward<0:\n",
    "                    #     print(f'Decision Step reward: {reward}')\n",
    "\n",
    "                if STEPS >= MAX_STEPS:\n",
    "                        reward = -10\n",
    "                        print(f'Max Step Reward: {reward}')\n",
    "                        env.reset()\n",
    "                        done = True\n",
    "\n",
    "                episode_reward = episode_reward + reward\n",
    "                vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                vt = vt_new\n",
    "            average += episode_reward / TEST_EPISODES\n",
    "            print(f'Episode: {episode}, Episode reward: {episode_reward}')\n",
    "print(f'Average Episode Reward: {average}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate x-axis values as the indices of the list\n",
    "x_axis = range(len(all_average_reward))\n",
    "\n",
    "# Plotting the figure\n",
    "plt.plot(x_axis, all_average_reward)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Rewards (Average 100 episodes) vs Episode')+\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 738\n",
    "env =  UE(file_name=\"280623_1\\\\build\",seed=1,side_channels=[],worker_id=0,no_graphics = False)\n",
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_value = list(env.behavior_specs.values())\n",
    "DecisionSteps, TerminalSteps = env.get_steps(behavior_names[0])\n",
    "agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "agent.load(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# dist = np.array((2,1,2,2))\n",
    "# entropy = -np.sum(np.mean(dist)* np.log(dist))\n",
    "# entropy\n",
    "\n",
    "action = torch.tensor([[0.1,0.2,0.1,1]])\n",
    "index = torch.tensor([0])\n",
    "print(action,index)\n",
    "entropy_loss = F.cross_entropy(action, index)\n",
    "entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need env.reset() at the beginning of each episode or unity file will reset it\n",
    "# The utility of workstation GPU is only 1 percent (Memory Leak?) in laptop GPU 8 seconds / episode on average\n",
    "# Can we make the unity environment accept action from GPU?\n",
    "#  Model is still hard to learn meaningful actions -- Make sure the algorithm is correct (Weight Initialization? Experience Replay?) value layer how to update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "plt.plot(all_rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_lengths)\n",
    "plt.plot(average_lengths)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "# add arguments in command --train/test\n",
    "# parser = argparse.ArgumentParser(description='Train or test neural net motor controller.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true', default=False)\n",
    "# parser.add_argument('--test', dest='test', action='store_true', default=True)\n",
    "# args = parser.parse_args()\n",
    "train = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "# ENV_ID = 'CartPole-v1'  # environment id\n",
    "# RANDOM_SEED = 2  # random seed, can be either an int number or None\n",
    "# RENDER = False  # render while training\n",
    "\n",
    "# ALG_NAME = 'AC'\n",
    "ALG_NAME = 'A2C'\n",
    "ENV_ID = 'S0'\n",
    "TRAIN_EPISODES = 200000  # number of overall episodes for training\n",
    "TEST_EPISODES = 10  # number of overall episodes for testing\n",
    "MAX_STEPS = 1200  # maximum time step in one episode\n",
    "LAM = 0.95  # reward discount in TD error\n",
    "lr = 0.001\n",
    "speed = 2\n",
    "# LR_A = 0.001  # learning rate for actor\n",
    "# LR_C = 0.01  # learning rate for critic\n",
    "\n",
    "\n",
    "###############################  Actor-Critic  ####################################\n",
    "\n",
    "\n",
    "# class Actor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_num, lr=0.001):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, action_num)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, action, td_error):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         loss = td_error * torch.nn.functional.cross_entropy(logits, torch.LongTensor([action]))\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def get_action(self, state, greedy=False):\n",
    "#         logits = self.model(torch.FloatTensor(state))\n",
    "#         probs = torch.nn.functional.softmax(logits, dim=-1).detach().numpy()\n",
    "#         if greedy:\n",
    "#             return np.argmax(probs)\n",
    "#         return np.random.choice(len(probs[0]), p=probs[0])\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_actor.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_actor.pt')))\n",
    "\n",
    "\n",
    "# class Critic(nn.Module):\n",
    "#     def __init__(self, state_dim, lr=0.01):\n",
    "#         super(Critic, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(state_dim, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, 1)\n",
    "#         )\n",
    "#         self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "#     def learn(self, state, reward, state_, done):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         d = 0 if done else 1\n",
    "#         v_ = self.model(torch.FloatTensor(state_))\n",
    "#         v = self.model(torch.FloatTensor(state))\n",
    "#         td_error = reward + d * LAM * v_ - v\n",
    "#         loss = td_error ** 2\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return td_error.item()\n",
    "\n",
    "#     def save(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         if not os.path.exists(path):\n",
    "#             os.makedirs(path)\n",
    "#         torch.save(self.state_dict(), os.path.join(path, 'model_critic.pt'))\n",
    "\n",
    "#     def load(self):\n",
    "#         path = os.path.join('model', '_'.join([ALG_NAME, ENV_ID]))\n",
    "#         self.load_state_dict(torch.load(os.path.join(path, 'model_critic.pt')))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ''' \n",
    "    choose environment\n",
    "    1. Openai gym:\n",
    "    env = gym.make()\n",
    "    2. DeepMind Control Suite:\n",
    "    env = dm_control2gym.make()\n",
    "    '''\n",
    "    # env = gym.make(ENV_ID).unwrapped\n",
    "    # # dm_control2gym.create_render_mode('example mode', show=True, return_pixel=False, height=240, width=320, camera_id=-1, overlays=(),\n",
    "    # #              depth=False, scene_option=None)\n",
    "    # # env = dm_control2gym.make(domain_name=\"cartpole\", task_name=\"balance\")\n",
    "\n",
    "    # env.seed(RANDOM_SEED)  # reproducible\n",
    "    # np.random.seed(RANDOM_SEED)\n",
    "    # torch.manual_seed(RANDOM_SEED)  # reproducible\n",
    "\n",
    "    # N_F = env.observation_space.shape[0]\n",
    "    # N_A = env.action_space.n\n",
    "\n",
    "    # print(\"observation dimension: %d\" % N_F)  # 4\n",
    "    # print(\"observation high: %s\" % env.observation_space.high)  # [ 2.4 , inf , 0.41887902 , inf]\n",
    "    # print(\"observation low : %s\" % env.observation_space.low)  # [-2.4 , -inf , -0.41887902 , -inf]\n",
    "    # print(\"num of actions: %d\" % N_A)  # 2 : left or right\n",
    "\n",
    "    # actor = Actor(state_dim=N_F, action_num=N_A, lr=LR_A)\n",
    "    # # we need a good teacher, so the teacher should learn faster than the actor\n",
    "    # critic = Critic(state_dim=N_F, lr=LR_C)\n",
    "\n",
    "    agent = Agent(num_words, embedding_dim, vision_output_dim, language_output_dim, mixing_dim, lstm_hidden_dim,num_actions)\n",
    "    agent.to(device)\n",
    "    optimizer = optim.RMSprop(agent.parameters(), lr=lr)\n",
    "    t0 = time.time()\n",
    "    best_score = 0\n",
    "    if train:\n",
    "        all_episode_reward = []\n",
    "        tracked_agent = -1\n",
    "        for episode in range(TRAIN_EPISODES):\n",
    "            print(f'Episode: {episode}')\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            env.reset()\n",
    "            behavior_name=list(env.behavior_specs)[0]\n",
    "            spec=env.behavior_specs[behavior_name]\n",
    "\n",
    "            # state -- vt, lt, lstm\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index].to(device)\n",
    "            step = 0  # number of step in this episode\n",
    "            episode_reward = 0  # rewards of all steps\n",
    "            lstm_hidden_state = (torch.zeros(1, lstm_hidden_dim).to(device), torch.zeros(1, lstm_hidden_dim).to(device))\n",
    "            done = False\n",
    "            while True:\n",
    "                # if RENDER:\n",
    "                #     env.render()\n",
    "\n",
    "                # action = actor.get_action(state) state --> vt,lt\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state) #action is the direct linear output from the model\n",
    "                action_dist = Categorical(F.softmax(action,dim=1))\n",
    "                index = action_dist.sample() # sample an action from action_dist\n",
    "                action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "                \n",
    "                \n",
    "                # action_onehot = F.one_hot(torch.argmax(action),num_actions)\n",
    "                # print(action_onehot)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if episode<50:\n",
    "                #     if episode%2:\n",
    "                #         action_onehot = torch.tensor([1,0,1,0])\n",
    "                #     else: action_onehot = torch.tensor([1,0,0,1])                 \n",
    "                # continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "                \n",
    "                # action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                action_tuple = ActionTuple()\n",
    "                action_tuple.add_discrete(discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                step += 1\n",
    "\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "                # print((vt == vt_new).all())\n",
    "\n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                    # print(tracked_agent)\n",
    "\n",
    "                if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "                    print('Agent in terminal steps')\n",
    "                    done = True\n",
    "                    reward = terminal_steps[tracked_agent].reward\n",
    "                    if reward > 0:\n",
    "                        pass\n",
    "                    else: reward = -1 # roll over or other unseen conditions\n",
    "\n",
    "                    print(f'Terminal Step reward: {reward}')\n",
    "\n",
    "                elif tracked_agent in decision_steps: # the agent which requires action\n",
    "                    reward = decision_steps[tracked_agent].reward\n",
    "                    # print(f'Decision Step reward: {reward}')\n",
    "                    if reward<0:\n",
    "                        print(f'Decision Step reward: {reward}')\n",
    "                    vt_new = torch.tensor(decision_steps.obs[0]).reshape(1,3,128,128).to(device)\n",
    "                    vt = vt_new\n",
    "                \n",
    "\n",
    "                if step >= MAX_STEPS:\n",
    "                    reward = -1\n",
    "                    print(f'Max Step Reward: {reward}')\n",
    "                    done = True\n",
    "                if step % 100 == 0:\n",
    "                    print (f'Step: {step}')\n",
    "\n",
    "                \n",
    "                    \n",
    "                episode_reward = episode_reward + reward\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # try:\n",
    "                #     td_error = critic.learn(\n",
    "                #         state, reward, state_new, done\n",
    "                #     )  # learn Value-function : gradient = grad[r + lambda * V(s_new) - V(s)]\n",
    "                #     actor.learn(state, action, td_error)  # learn Policy : true_gradient = grad[logPi(s, a) * td_error]                   \n",
    "                try:\n",
    "                    # d = 0 if done else 1\n",
    "                    # v_ = self.model(torch.FloatTensor(state_))\n",
    "                    # v = self.model(torch.FloatTensor(state))\n",
    "                    if not done:\n",
    "                        action_new, value_new, lstm_hidden_state_new = agent(vt_new,lt,lstm_hidden_state)\n",
    "\n",
    "                        # initialize the value first\n",
    "                        td_error = reward + LAM * value_new - value\n",
    "                    else: td_error = reward - value\n",
    "\n",
    "\n",
    "                    critic_loss = td_error ** 2\n",
    "\n",
    "                    log_prob = action_dist.log_prob(index)\n",
    "                    actor_loss = -(log_prob * td_error)\n",
    "\n",
    "                    # logits = action # the actual model output (without softmax)\n",
    "                    # labels = index  # one hot vector of the sampled action\n",
    "                    print(action,index)\n",
    "                    entropy_loss = F.cross_entropy(action, index)\n",
    "                    print(entropy_loss)\n",
    "                    total_loss = actor_loss + 0.5* critic_loss - 0.001*entropy_loss\n",
    "                    # entropy loss entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=tf.nn.softmax(logit))\n",
    "                    # comb_loss = tf.reduce_mean((0.5 * value_loss + policy_loss - 0.01 * entropy))\n",
    "\n",
    "                    total_loss = total_loss.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward(retain_graph=True)\n",
    "                    print('hi')\n",
    "                    optimizer.step()   \n",
    "                    \n",
    "              \n",
    "                except KeyboardInterrupt:  # if Ctrl+C at running actor.learn(), then save model, or exit if not at actor.learn()\n",
    "                    agent.save(episode)\n",
    "                    print('model has been saved')\n",
    "\n",
    "                # state = state_new\n",
    "                # print(f'episode: {episode}, step:{step}')\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            if episode%500 == 0:\n",
    "                agent.save(episode)\n",
    "                print(\"Model has been saved\")\n",
    "                \n",
    "            # all_episode_reward.append(episode_reward)\n",
    "            # if episode >= 100:\n",
    "            #     avg_score = np.mean(all_episode_reward[-100:])\n",
    "            #     if avg_score > best_score:\n",
    "            #         best_score = avg_score\n",
    "            #         agent.save()\n",
    "            #         print(f'The best score for averaging previous 100 episode reward is {best_score}. Model has been saved')\n",
    "\n",
    "\n",
    "\n",
    "            # if episode == 0:\n",
    "            #     all_episode_reward.append(episode_reward)\n",
    "            # else:\n",
    "            #     reference_value = all_episode_reward[-1] * 0.9 + episode_reward * 0.1\n",
    "            #     if reference_value > all_episode_reward_best:\n",
    "            #         agent.save()\n",
    "            #         print(f'The best all_episode_reward is {reference_value}. Model has been saved')\n",
    "            #         all_episode_reward_best = reference_value\n",
    "            #     all_episode_reward.append(reference_value)\n",
    "\n",
    "            print('Training  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                  .format(episode + 1, TRAIN_EPISODES, episode_reward, time.time() - t0))\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        plt.plot(all_episode_reward)\n",
    "        if not os.path.exists('image'):\n",
    "            os.makedirs('image')\n",
    "        plt.savefig(os.path.join('image', '_'.join([ALG_NAME, ENV_ID])))\n",
    "        \n",
    "\n",
    "    if not train:\n",
    "        agent.load(episode)\n",
    "        print(\"model has been loaded\")\n",
    "        tracked_agent = -1\n",
    "\n",
    "        for episode in range(TEST_EPISODES):\n",
    "            episode_time = time.time()\n",
    "            # state = env.reset().astype(np.float32)\n",
    "            vt = torch.tensor(DecisionSteps.obs[0]).reshape(1,3,128,128)\n",
    "            index = 5 #e.g\n",
    "            lt = torch.eye(num_words)[:, index]\n",
    "            t = 0  # number of step in this episode\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # action = actor.get_action(state, greedy=True)\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                # if done:\n",
    "                #     reward = -20\n",
    "\n",
    "                \n",
    "                action, value, lstm_hidden_state = agent(vt,lt,lstm_hidden_state)\n",
    "                action = F.one_hot(torch.argmax(action),num_actions)\n",
    "\n",
    "                # state_new, reward, done, info = env.step(action)\n",
    "                # state_new = state_new.astype(np.float32)\n",
    "                continuous_actions = np.empty((1, 0))\n",
    "                discrete_actions = np.array(action).reshape(1,4) #[forward, backward, right, left]\n",
    "                action_tuple = ActionTuple(continuous_actions,discrete_actions)\n",
    "                env.set_actions(behavior_name,action_tuple)\n",
    "                env.step()\n",
    "                decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "                vt_new = torch.tensor(decision_steps.obs).reshape(1,3,128,128)\n",
    "                \n",
    "                if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "                    tracked_agent = decision_steps.agent_id[0]\n",
    "                if tracked_agent in decision_steps:\n",
    "                    # tracked_agent = decision_steps.agent_id[0]\n",
    "                    reward = decision_steps[tracked_agent].reward \n",
    "                # if done:\n",
    "                #     reward = -20  # reward shaping trick\n",
    "                if tracked_agent in terminal_steps:\n",
    "                    done = True\n",
    "                    reward = -20\n",
    "\n",
    "                episode_reward += reward\n",
    "                vt = vt_new\n",
    "                t += 1\n",
    "\n",
    "                if done or t >= MAX_STEPS:\n",
    "                    print('Testing  | Episode: {}/{}  | Episode Reward: {:.0f}  | Running Time: {:.4f}' \\\n",
    "                          .format(episode + 1, TEST_EPISODES, episode_reward, time.time() - t0))\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to update the network every step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find the optimized MAX_STEP\n",
    "# speed 1: 10000+ speed 2:1822 speed 3: 1918 speed 4: 900 speed 5: 951 speed 6:964 speed 7: 1181 \n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# speed = 1\n",
    "TRAIN_EPISODES = 20\n",
    "tracked_agent = -1\n",
    "num_actions = 4\n",
    "average = 0\n",
    "# for speed in range(30,70,5):\n",
    "#     speed = speed/10\n",
    "speed = 3\n",
    "for episode in range(TRAIN_EPISODES):\n",
    "    env.reset()\n",
    "    behavior_name=list(env.behavior_specs)[0]\n",
    "    step = 0\n",
    "    while True:\n",
    "        index = random.randint(0, 3) # sample an action from action_dist\n",
    "        action_onehot = F.one_hot(torch.tensor(index),num_actions).cpu()\n",
    "        discrete_actions = np.array(action_onehot).reshape(1,4)*speed #[forward, backward, right, left]\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(discrete_actions)\n",
    "        env.set_actions(behavior_name,action_tuple)\n",
    "        env.step()\n",
    "        step += 1\n",
    "\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "            \n",
    "        if tracked_agent in terminal_steps: # roll over or hit the target\n",
    "            reward = terminal_steps[tracked_agent].reward\n",
    "            if reward > 0: # hit the target\n",
    "                print(f'{episode}: {step} in total')\n",
    "                average += step\n",
    "                break\n",
    "            else:           # roll over or other conditions\n",
    "                env.reset()\n",
    "                step = 0\n",
    "                continue # roll over or other unseen conditions\n",
    "        if tracked_agent in decision_steps: # the agent which requires action\n",
    "            continue\n",
    "average /= TRAIN_EPISODES\n",
    "print(f'For speed {speed}, average random step for hitting the target is {average}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unity could not open in work station\n",
    "# The agent in terminal step will also in decison step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "object_sizes = [1, 2, 3, 4, 5]  # Object sizes on the x-axis\n",
    "max_steps = [[4721, 2468, 1587, 1249, 1084],   # Max steps for each object size\n",
    "             [4027, 1550, 738, 640, 505],\n",
    "             [3320, 1391, 464, 331, 307],\n",
    "             [3289, 882, 679, 424, 211],\n",
    "             [3057, 1203, 470, 378, 222]]\n",
    "speeds = [1, 2, 3, 4, 5]  # Discrete speeds for color-coding\n",
    "\n",
    "# Color mapping for each speed\n",
    "speed_color_mapping = {\n",
    "    1: 'red',\n",
    "    2: 'blue',\n",
    "    3: 'green',\n",
    "    4: 'orange',\n",
    "    5: 'purple'\n",
    "}\n",
    "\n",
    "# Generate scatter plot\n",
    "for i, size in enumerate(object_sizes):\n",
    "    for j, steps in enumerate(max_steps[i]):\n",
    "        speed = speeds[j]\n",
    "        color = speed_color_mapping[speed]\n",
    "        plt.scatter(size, steps, c=color)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Object Size')\n",
    "plt.ylabel('Max Step')\n",
    "plt.title('Scatter Plot')\n",
    "\n",
    "plt.xlim(0, 6)\n",
    "plt.xticks(np.arange(0, 7, 1))\n",
    "plt.ylim(0, 5000)\n",
    "plt.yticks(np.arange(0, 5001, 200))\n",
    "\n",
    "# Create legend\n",
    "legend_labels = [f'Speed {speed}' for speed, _ in speed_color_mapping.items()]\n",
    "plt.legend(legend_labels, loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "a = torch.tensor([[0.2460, 0.2503, 0.2647, 0.2389]])\n",
    "b = F.softmax(a,dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1,2,3]\n",
    "avg = np.mean(a[-10:])\n",
    "avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
